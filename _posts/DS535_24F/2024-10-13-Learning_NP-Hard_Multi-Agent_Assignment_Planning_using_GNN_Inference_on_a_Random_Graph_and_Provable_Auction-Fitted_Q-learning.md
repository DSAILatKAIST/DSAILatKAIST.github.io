---
title:  "[NeurIPS 2022] Learning NP-Hard Multi-Agent Assignment Planning using GNN: Inference on a Random Graph and Provable Auction-Fitted Q-learning"
permalink: 2024-10-13-Learning_NP-Hard_Multi-Agent_Assignment_Planning_using_GNN_Inference_on_a_Random_Graph_and_Provable_Auction-Fitted_Q-learning.html
tags: [reviews]
use_math: true
usemathjax: true
---


## 1. Introduction
### 1.1 ë¬¸ì œ ì •ì˜
**ë©€í‹° ë¡œë´‡ ë³´ìƒ ìˆ˜ì§‘ ë¬¸ì œ (MRRC):**  
- ì‹œê°„ì— ë”°ë¼ ë³€í™”í•˜ëŠ” ë³´ìƒì„ ê³ ë ¤í•œ ë©€í‹° ì—ì´ì „íŠ¸, ë©€í‹° íƒœìŠ¤í¬ NP-ë‚œí•´ ê³„íš ë¬¸ì œ.
- ë™ì¼í•œ ë¡œë´‡ë“¤ì´ ê³µê°„ì ìœ¼ë¡œ ë¶„í¬ëœ ì‘ì—…ì„ ìˆ˜í–‰í•˜ë ¤ê³  í•¨.
- ë¯¸ë¦¬ ì •í•´ì§„ ë³´ìƒ ê·œì¹™ì— ë”°ë¼, ë” ë¹¨ë¦¬ ì‘ì—…ì„ ì™„ë£Œí•  ë•Œ ë” ë†’ì€ ë³´ìƒì„ ë¶€ì—¬.
- MRRC ë¬¸ì œëŠ” ë¼ì´ë“œ ì‰ì–´ë§, í”½ì—…-ë”œë¦¬ë²„ë¦¬ì™€ ê°™ì€ ë¬¸ì œë¥¼ ì˜ ëª¨ë¸ë§í•¨.
- ì‘ìš© ë¶„ì•¼: ê³ ê°ì„ ìš´ì†¡í•˜ê¸° ìœ„í•œ ìš´ì „ ê¸°ì‚¬ ë°°ì°¨, ë˜ëŠ” ê³µì¥ì—ì„œì˜ ê¸°ê³„ ìŠ¤ì¼€ì¤„ë§.

### 1.2 ë¬¸ì œì 
- **ë¹„ì‹¼ ê³„ì‚° ë¹„ìš©**, íŠ¹íˆ ë¬¸ì œì˜ ê·œëª¨ê°€ ì»¤ì§ˆ ë•Œ.
- ë©€í‹° ì—ì´ì „íŠ¸ ëª¨ë¸ë§ í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ë¶„ì‚° ì ‘ê·¼ë²•ì˜ ì–´ë ¤ì›€:
  - í†µì‹  ì—†ì´ **ì—ì´ì „íŠ¸ ê°„ì— í•©ì˜**ë¥¼ ìœ ë„í•´ ê¸€ë¡œë²Œ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ëŠ” ê²ƒì´ ë¶ˆê°€ëŠ¥.

ë”°ë¼ì„œ, ì´ ì—°êµ¬ëŠ” **ì¤‘ì•™ ì§‘ì¤‘ì‹ ë°©ë²•**ì„ ì‚¬ìš©í•˜ì—¬ MRRC ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë° ì§‘ì¤‘í•¨.

### 1.3 ì—°êµ¬ ì§ˆë¬¸
ëŒ€ê·œëª¨ NP-ë‚œí•´ ìŠ¤ì¼€ì¤„ë§ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ **í•™ìŠµ ê¸°ë°˜ ì¤‘ì•™ ì§‘ì¤‘ì‹ ì˜ì‚¬ê²°ì • ë°©ì‹**ì„ ì„¤ê³„í•  ë•Œ, í•™ìŠµê³¼ ì˜ì‚¬ê²°ì • ì¸¡ë©´ì—ì„œ **íš¨ìœ¨ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ** ë°©ë²•ì„ ì–´ë–»ê²Œ ì„¤ê³„í•  ìˆ˜ ìˆì„ê¹Œ?

### 1.4 ì—°êµ¬ ê¸°ì—¬
- state-joint action ìŒì„ **random PGM(Probabilistic Graphical Model)** ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŒì„ ê´€ì°°. ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ **random PGM ê¸°ë°˜ Mean-field inference** ì´ë¡ ì„ ê°œë°œí•˜ê³ , **structure2vec** (Dai et al., 2016)ì˜ í™•ì¥íŒì¸ **random structure2vec**ì„ ì œì•ˆ.
- Q-functionì„ ëœë¤ structure2vec ê³„ì¸µì„ ì‚¬ìš©í•´ ì¶”ì •. structure2vecì˜ ê³„ì¸µì„ **Weisfeiler-Lehman ì»¤ë„**ë¡œ í•´ì„í•˜ì—¬, **order-transferability**(ìˆœì„œ ì „ë‹¬ì„±)ë¼ëŠ” ì†ì„±ì„ ê°–ë„ë¡ ì„¤ê³„. ì´ëŠ” ë¬¸ì œ ê·œëª¨ì— ë”°ë¼ **ì „ì´ ê°€ëŠ¥í•œ** ì„±ì§ˆì„ ì œê³µí•¨.
- **OTAP (Order-Transferability-Enabled Auction Policy)** ë¼ëŠ” í• ë‹¹ ê·œì¹™ì„ ì œì•ˆí•˜ì—¬, í• ë‹¹ ê³µê°„ì˜ ì§€ìˆ˜ì  ì„±ì¥ì„ í•´ê²°í•¨.
- **AFQI (Auction-Fitted Q-Iteration)** ëŠ” ê¸°ì¡´ Fitted Q-Iterationì˜ argmax ì—°ì‚°ì„ OTAPìœ¼ë¡œ ëŒ€ì²´í•´ Q-functionë¥¼ **íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµ**í•˜ë„ë¡ ì œì•ˆë¨.
- **AFQIëŠ” ë‹¤í•­ ì‹œê°„ ë‚´ì—** ê³„ì‚° ê°€ëŠ¥í•˜ë©°, ìµœì  ì •ì±…ì˜ ìµœì†Œ \(1 - 1/e\) ì„±ëŠ¥ì„ ë‹¬ì„±í•¨ì´ ì…ì¦ë¨.

## 2. Multi-Robot Reward Collection Problem (MRRC)

ë³¸ë¬¸ì—ì„œëŠ” MRRC problemì„ disctrete-time, discrete-state (DTDS) sequential decision-making problemìœ¼ë¡œ ì •ì˜í•¨.
- ì‹œê°„ ì¦ë¶„ì´ $\triangle$ ì¦‰, $t_k = t_0 + \triangle \times k$ ($t_k$: $k$ë²ˆì§¸ ê²°ì •ì˜ ì‹¤ì œ ì‹œê°„).
- ì´ í”„ë ˆì„ì›Œí¬ì—ì„œ $s_k$ëŠ” ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ê³ , $a_k$ëŠ” $k$ë²ˆì§¸ ì—í¬í¬(epoch)ì—ì„œ ë¡œë´‡/ê¸°ê³„ë¥¼ ë¯¸ì™„ë£Œ ì‘ì—…ì— í• ë‹¹í•˜ëŠ” ì¡°ì¸íŠ¸ í• ë‹¹ì„ ì˜ë¯¸í•¨.
- ì´ ë¬¸ì œì˜ ëª©í‘œëŠ” ìµœì ì˜ ìŠ¤ì¼€ì¤„ë§ ì •ì±… $\pi_\theta : s_k \rightarrow a_k$ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì´ë©°, ì´ëŠ” ìˆ˜ì§‘ëœ ë³´ìƒì„ ê·¹ëŒ€í™”í•˜ê±°ë‚˜ ì´ ì‘ì—… ì™„ë£Œ ì‹œê°„ì„ ìµœì†Œí™”í•˜ëŠ” ê²ƒì„ ëª©ì .

### 2.1 State
State $s_k = (g_k, D_k)$ë¡œ ë‚˜íƒ€ë‚´ë©° ê·¸ë˜í”„ $g_k= ((R,T_k),(E_k^{TT},E_k^{RT}))$ì™€ ê´€ë ¨ íŠ¹ì„± $D_k = (D_k^R,D_k^T,D_k^{TT},D_k^{RT})$ë¡œ ì •ì˜ë¨.

$g_k$ ì •ì˜:
- $R=${ $1,...,M$ }: ëª¨ë“  ë¡œë´‡ set, $i$ì™€ $j$ ì¸ëŒìŠ¤ë¡œ ë‚˜íƒ€ëƒ„.
- $T_k=${ $1,...,N$ }: $k$ë²ˆì§¸ ì—í¬í¬ë•Œ ë‚¨ì•„ìˆëŠ” unserved task set, $p$ì™€ $q$ ì¸ëŒìŠ¤ë¡œ ë‚˜íƒ€ëƒ„.
- $E_k^{TT} = ${ $\epsilon_{pq}^{TT}|p \in T_k, q \in T_k$ }:
  - ëª¨ë“  ì‘ì—…ì—ì„œ ë‹¤ë¥¸ ì‘ì—…ìœ¼ë¡œ í–¥í•˜ëŠ” ëª¨ë“  ë°©í–¥ì„± ìˆëŠ” ê°„ì„ ë“¤ì˜ ì§‘í•©.
  - ê° ê°„ì„ ì€ í™•ë¥  ë³€ìˆ˜ë¡œ ê°„ì£¼.
  - ì‘ì—…-ì‘ì—… ê°„ì„  $\epsilon_{pq}^{TT} = 1$ì€ ì‘ì—… $p$ë¥¼ ì™„ë£Œí•œ ë¡œë´‡ì´ ì´í›„ì— ì‘ì—… $q$ ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì´ë²¤íŠ¸ë¥¼ ì˜ë¯¸í•¨.
  - ê°„ì„  $\epsilon_{pq}^{TT}$ì˜ ì¡´ì¬ í™•ë¥ ì„ $p(\epsilon_{pq}^{TT} = 1) \in [0, 1] $ë¡œ ë‚˜íƒ€ëƒ„.
- $E_k^{RT} = ${ $\epsilon_{iq}^{RT}|i \in R, q \in T_k$ }:
  - ë¡œë´‡ $R$ì—ì„œ ì‘ì—… $T_k$ë¡œ í–¥í•˜ëŠ” ëª¨ë“  ë°©í–¥ì„± ìˆëŠ” ê°„ì„ ë“¤ì˜ ì§‘í•©
  - ë¡œë´‡-ì‘ì—… ê°„ì„  $\epsilon_{ip}^{RT} = 1$ì€ ë¡œë´‡ $i$ê°€ ì‘ì—… $p$ì— í• ë‹¹ëœ ì´ë²¤íŠ¸ë¥¼ ì˜ë¯¸
  - ì´ ê°„ì„ ì€ ê³µë™ í• ë‹¹ ì•¡ì…˜ì— ë”°ë¼ deterministic.
  - ë§Œì•½ ë¡œë´‡ $i$ê°€ ì‘ì—… $p$ì— í• ë‹¹ë˜ë©´ $p(\epsilon_{ip}^{RT}) = 1$ì´ê³ , ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ 0ì…ë‹ˆë‹¤.
 
$D_k$ ì •ì˜:
- $D_k^R=${ $d_i^R|i\in R$ }:
  - ì—í¬í¬ $k$ë•Œ ë¡œë´‡ ë…¸ë“œ $R$ì˜ ë…¸ë“œ íŠ¹ì§•ë“¤ì˜ ì§‘í•©.
  - MRRCì—ì„œëŠ” $d^R_i$ë¥¼ ì—í¬í¬ $k$ì—ì„œ ë¡œë´‡ $i$ì˜ ìœ„ì¹˜ë¡œ ì •ì˜ (ì—í¬í¬ ì¸ë±ìŠ¤ $k$ëŠ” ìƒëµë  ìˆ˜ ìˆìŒ).
- $D_k^T=${ $d_p^T|p\in T_k$ \}
  - ì—í¬í¬ $k$ë•Œ ì‘ì—… ë…¸ë“œ $T_k$ì˜ ë…¸ë“œ íŠ¹ì§•ë“¤ì˜ ì§‘í•©
  - MRRCì—ì„œëŠ” $d_p^T$ë¥¼ ì—í¬í¬ $k$ì—ì„œ ì‘ì—… $p$ì˜ ë‚˜ì´ë¡œ ì •ì˜ (ì—í¬í¬ ì¸ë±ìŠ¤ $k$ëŠ” ìƒëµë  ìˆ˜ ìˆìŒ).
- $D_k^{TT}=${ $d_{pq}^{TT}|p\in T_k, q\in T_k$}
  - ì—í¬í¬ $k$ë•Œ ì‘ì—… ê°„ì˜ ê°„ì„  featureë“¤ì˜ ì§‘í•©
  - $d_{pq}^{TT}$ëŠ” ì‘ì—… $p$ë¥¼ ì™„ë£Œí•œ ë¡œë´‡ì´ ì‘ì—… $q$ë¥¼ ì™„ë£Œí•˜ëŠ” ë° ê±¸ë¦¬ëŠ” ì‹œê°„ì„ ë‚˜íƒ€ëƒ„. ì´ ì‹œê°„ì„ **ì‘ì—… ì™„ë£Œ ì‹œê°„**ì´ë¼í•¨.
  - MRRCì—ì„œëŠ” ì‘ì—… ì™„ë£Œ ì‹œê°„ì´ í™•ë¥  ë³€ìˆ˜ë¡œ ì£¼ì–´ì§€ë©°, ì‹¤ì œë¡œëŠ” ì´ í™•ë¥  ë³€ìˆ˜ì˜ ìƒ˜í”Œ ì§‘í•©ë§Œ í•„ìš”í•¨.
- $D_k^{RT}=${ $d_{ip}^{RT}|i\in R, p\in T_k$}
  - ì—í¬í¬ $k$ë•Œ ë¡œë´‡-ì‘ì—… ê°„ì˜ ê°„ì„  íŠ¹ì§•ë“¤ì˜ ì§‘í•©.
  - $d_{ip}^{RT}$ëŠ” ë¡œë´‡ $i$ê°€ ì‘ì—… $p$ì— ë„ë‹¬í•˜ëŠ” ë° ê±¸ë¦¬ëŠ” ì‹œê°„ì„ ë‚˜íƒ€ëƒ„.

### 2.2 Action
- ì—í¬í¬ $k$ë•Œ ì•¡ì…˜ $a_k$ëŠ” ì™„ì „ ì´ë¶„ ê·¸ë˜í”„ $(R, T_k, E_k^{RT})$ ì˜ ìµœëŒ€ ì´ë¶„ ë§¤ì¹­(maximal bipartite matching)ìœ¼ë¡œ ì •ì˜ë¨. 
- ì¦‰, í˜„ì¬ ìƒíƒœ $s_k = (g_k, D_k)$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, $a_k$ëŠ” ë‹¤ìŒ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” $E_k^{RT}$ ì˜ ë¶€ë¶„ ì§‘í•©ì…ë‹ˆë‹¤:
  1. ë‘ ë¡œë´‡ì´ ë™ì¼í•œ ì‘ì—…ì— í• ë‹¹ë  ìˆ˜ ì—†ìŒ.  
  2. ë‚¨ì•„ ìˆëŠ” ì‘ì—… ìˆ˜ë³´ë‹¤ ë¡œë´‡ ìˆ˜ê°€ ë” ë§ì€ ê²½ìš°ì—ë§Œ ì¼ë¶€ ë¡œë´‡ì´ í• ë‹¹ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ.
- ë§Œì•½ $\epsilon^{RT}_{ip} \in a_k$ ì´ë©´, ì´ëŠ” ì—í¬í¬ $k$ ì—ì„œ ë¡œë´‡ $i$ê°€ ì‘ì—… $p$ì— í• ë‹¹ëœë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•¨.
- ëª¨ë“  ë¡œë´‡ì— í• ë‹¹ëœ ê²ƒì€ ë§¤ ì• í¬í¬ ë§ˆë‹¤ ë°”ë€” ìˆ˜ ìˆìŒ.
  
### 2.3 State transition
- Graph update: ì‘ì—… $p$ê°€ ì™„ë£Œë˜ëŠ” ì‹œì ì´ ë˜ë©´, í•´ë‹¹ ì‘ì—… ë…¸ë“œëŠ” ì—…ë°ì´íŠ¸ëœ ì‘ì—… ë…¸ë“œì—ì„œ ì œê±°ë¨. ì¦‰, $T_{k+1} = T_k \setminus${ $p$ }. ë˜í•œ, ì‘ì—…-ì‘ì—… ê°„ì„  $E_{k+1}^{TT}$ê³¼ ë¡œë´‡-ì‘ì—… ê°„ì„  $E_{k+1}^{RT}$ë„ ì´ì— ë§ê²Œ ì—…ë°ì´íŠ¸ë¨.
- Feature update: $D_k+1= (D_{k+1}^R,D_{k+1}^T,D_{k+1}^{TT},D_{k+1}^{RT})$ì€ determined.

### 2.4 Reward and objective
- ì‹œê°„ 0ì—ì„œ, ê° ì‘ì—…ì—ëŠ” ì´ˆê¸° ë‚˜ì´ê°€ ì£¼ì–´ì§€ë©°, ì´ ë‚˜ì´ëŠ” ì‹œê°„ì— ë”°ë¼ ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€í•¨.
- ì—í¬í¬ $k$ì—ì„œ ë‚˜ì´ê°€ $d_p^T$ ì¸ ì‘ì—… $p \in T_k$ ê°€ ìˆ˜í–‰ë  ë•Œ ì£¼ì–´ì§€ëŠ” ë³´ìƒ $r_k$ëŠ” $r_k = r - d_p^T$ë¡œ ì •ì˜
- MRRCì—ì„œëŠ” ì„ í˜• ë° ë¹„ì„ í˜• ë³´ìƒ í•¨ìˆ˜ $r$ë¥¼ ê³ ë ¤í•¨.
- ëª©í‘œëŠ” ì •ì±… $\pi$ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒ: ì •ì±… $\pi$ ëŠ” í˜„ì¬ ìƒíƒœ $s$ë¥¼ í˜„ì¬ ì•¡ì…˜ $a$ë¡œ ë§¤í•‘í•˜ëŠ” í•¨ìˆ˜ë¡œ, ì£¼ì–´ì§„ ì •ì±…ì— ë”°ë¼ ì´ ê¸°ëŒ€ ë³´ìƒì„ ìµœëŒ€í™”í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•¨.
  
$$Q^\pi (s, a):=E_{P,\pi} \left\[ \sum_{k=0}^{\infty} R(s_{t_k}, a_{t_k}, s_{t_{k+1}}) \mid s_{t_0} = s, a_{t_0} = a \right\]$$

## 3. Random graph embedding: RandStructure2Vec
![ex_screenshot](../../images/DS535_24F/Learning_NP-Hard_Multi-Agent_Assignment_Planning_using_GNN_Inference_on_a_Random_Graph_and_Provable_Auction-Fitted_Q-learning/fig_1.png)
### 3.1 Random PGM for representing a state of MRRC
- Random probabilistic graphical model (PGM) $\chi=${ $X_p$ } (random variable)

  $$p(\chi) = \frac{1}{Z}\prod_i \phi_i(D_i)$$
  - $Z$: normalizing constant
  - $\phi_i(D_i)$: clique potnetial for $D_i$
  - $D_i$: clique (scope of $\phi_i$)
- Scenarios:
  - ì£¼ì–´ì§„ ìƒíƒœ $s_k$ì™€ í–‰ë™ $a_k$ ì—ì„œ ì‹œì‘í•˜ì—¬, â€œì •ì±… $\pi$ë¥¼ ì‚¬ìš©í•œ ìˆœì°¨ì  ì˜ì‚¬ê²°ì •â€ì´ë¼ëŠ” ëœë¤ ì‹¤í—˜ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŒ.
  - random experimentì—ì„œ 'ë¡œë´‡ë“¤ì€ ë‚¨ì•„ ìˆëŠ” ëª¨ë“  ì‘ì—…ì„ ì–´ë–¤ ìˆœì„œë¡œ ìˆ˜í–‰í•˜ëŠ”ê°€?'ë¥¼ ë‚˜íƒ€ëƒ„.
  - 1ê°œì˜ scenarioëŠ” 1ê°œì˜ Bayesian Networkë¡œ ë‚˜íƒ€ëƒ„.
  - scenario realizationì€ randomí•˜ê¸° ë•Œë¬¸ì—, random node $X_k=(s_k,a_k)$ì™€ clique potential $\phi$ë¡œ ì´ë£¨ì–´ì§„ **random** Bayesian Networkë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŒ.
### 3.2 Mean-field inference with random PGM
- random variableì¸ $\chi =${ $X_p$}ë¥¼ ì¶”ë¡ í•˜ëŠ” ë¬¸ì œì—ì„œ $G_\chi$ë¥¼ ê°€ëŠ¥í•œ ëª¨ë“  PGM set, $P: G_\chi \to [0,1]$ probability measureë¼ í•˜ë©´ $|G_\chi|$ê°€ ë„ˆë¬´ ì»¤ì„œ Monte-Carlo sampling ë°©ë²•ìœ¼ë¡œëŠ” { $G_\chi,P$ } ì¶”ë¡ ì´ ì–´ë ¤ì›€.
- semi-cliques $D_m$ë¥¼ ì‚¬ìš©í•´ì„œ approximationí•  ê²ƒ:
  - $C_\chi$ë¥¼ ê°€ëŠ¥í•œ ëª¨ë“  cliqueë“¤ì˜ ì§‘í•©ì´ë¼ í• ë•Œ $P$ì— ë”°ë¥´ë©´ ì‹¤ì œ realizationë˜ëŠ” cliqueëŠ” ì¼ë¶€ë¿ì¸ë° ê·¸ ì ì¬ì  cliqueë“¤ semi-cliqueë¼ í•¨.
  - semi-clique $D_m$ì— ëŒ€í•œ í™•ë¥  $p_m = \sum_{G\in G_\chi} P(G)1_{D_m\in G} $
#### Mean-field inference with random PGM
- Random PGM on $\chi =$ ({ $H_i$ },{ $X_j$ }) ($H_K$: ê´€ì¸¡ë³€ìˆ˜ $X_k$ì— ëŒ€ì‘ë˜ëŠ” ì ì¬ë³€ìˆ˜)
- ëª©í‘œ: p({ $H_i$ }|{ $x_j$ })ë¥¼ ì°¾ì•„ { $X_j$ }ê°€ ì£¼ì–´ì¡Œì„ë•Œ { $H_i$ }ë¥¼ ì¶”ë¡ 
- Mean-field inferenceì—ì„œëŠ” { $H_i$ }ë“¤ì´ independentí•œ surrogate distribution  $q^{ \lbrace x_j \rbrace }(H_i)$ì˜ setì„ ì°¾ëŠ” ê²ƒì´ ëª©í‘œ ($q^{ \lbrace x_j \rbrace }$ëŠ” $q$ê°€ { $x_j$ } ë¡œ ì´ë£¨ì–´ì§ì„ ëœ»í•¨.)

    ![ex_screenshot](../../images/DS535_24F/Learning_NP-Hard_Multi-Agent_Assignment_Planning_using_GNN_Inference_on_a_Random_Graph_and_Provable_Auction-Fitted_Q-learning/thm_1.png)

- Theorem 1ì€  ê° semi-cliqueì˜ í™•ë¥  $p_m$ì„ ì¶”ë¡  í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œ mean-field inferenceë¥¼ í•˜ëŠ”ë° ì¶©ë¶„í•˜ë©°, { $G_\chi,P$ } ì¶”ë¡ ì´ í•„ìš” ì—†ìŒì„ ì˜ë¯¸í•¨. 
#### RadStructure2Vec
- **structure2vec**: Dai et al.(2016)ì—ì„œ mean-field inferenceì™€ PGMë¥¼ í†µí•´ vector space embeddingì„ ë„ì¶œí•¨.
  - PGMì´ realizationë  ê²½ìš° PGMì˜ joint distributionì€ ë‹¤ìŒê³¼ ê°™ì´ factorization ëœë‹¤ê³  ê°€ì •
  
$$\prod_p \phi(H_p|I_p)\prod_{p,q} \phi(H_p|H_q) $$
  - ìœ„ ê°€ì • í•˜ì—ì„œ { $q^{ \lbrace x_j \rbrace }(H_i)$ } ë¥¼  { $q^{ x_j  }(H_i)$ } ë¡œ ì“¸ ìˆ˜ ìˆìŒ.
  - Fixed point iteration
    
    $$\tilde{\mu_p} \gets \sigma ( W_1 x_p +W_2 \sum_{q \neq p} \tilde{\mu_q} ) $$
  - $\tilde{\mu_p}$ëŠ” ë…¸ë“œ $p$ì˜ ì ì¬ ë²¡í„°ì´ê³ , $x_pëŠ” ë…¸ë“œ $p$ì˜ input
  - $\tilde{\mu_p}$ë¥¼ injective embeddingìœ¼ë¡œ í•´ì„í•  ì‹œ structure2vecì˜ fixed point iteration == Mean-field inferenceì˜ fiexed point inferenceì„ì„ ë³´ì„

$$\tilde{\mu_i} = \int _H \phi(h_i) q^{x_i}(h_i) dh_i $$
- Random structure2vec
  - Theorem 1ì— ë”°ë¼ random structure2vecì€ mean-field inferenceì™€ random PGMë¥¼ í†µí•´ vector space embeddingì„ ë„ì¶œí•¨.
 
      ![ex_screenshot](../../images/DS535_24F/Learning_NP-Hard_Multi-Agent_Assignment_Planning_using_GNN_Inference_on_a_Random_Graph_and_Provable_Auction-Fitted_Q-learning/lem_1.png)

  - Lemma 1ì€ GNNì„ ì‚¬ìš©í•´ ë¬´ì‘ìœ„ ê·¸ë˜í”„ë¥¼ ì„ë² ë”©í•  ë•Œ, ê°„ì„  ì¡´ì¬ ì—¬ë¶€ ê°„ì˜ ìƒí˜¸ ì˜ì¡´ì„±ì„ ë¬´ì‹œí•´ë„ ëœë‹¤ëŠ” ì´ë¡ ì  ê·¼ê±°ë¥¼ ì œê³µí•¨.
  - ê·¸ë˜í”„ì˜ ê°„ì„ ì´ ëª…ì‹œì ìœ¼ë¡œ ì£¼ì–´ì§€ì§€ ì•Šê±°ë‚˜ ë¬´ì‘ìœ„ë¡œ ì•Œë ¤ì§„ ê²½ìš°, ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê°€ì¥ ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±ì€ ëª¨ë“  ê°„ì„ ì˜ ì¡´ì¬ í™•ë¥ ì„ ê°œë³„ì ìœ¼ë¡œ ì¶”ë¡ í•˜ê³ , GNNì˜ message propagation ê³¼ì •ì—ì„œ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•˜ëŠ” ê²ƒ.
  - Lemma 1ì— ë”°ë¥´ë©´, ê°„ì„  ê°„ì˜ ìƒí˜¸ ì˜ì¡´ì„±ì€ ì´ëŸ¬í•œ íœ´ë¦¬ìŠ¤í‹± ì¶”ë¡ ì˜ í’ˆì§ˆì— ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠìŒ.

## 4. Solving MRRC with RandStructure2Vec
![ex_screenshot](../../images/DS535_24F/Learning_NP-Hard_Multi-Agent_Assignment_Planning_using_GNN_Inference_on_a_Random_Graph_and_Provable_Auction-Fitted_Q-learning/fig_2.png)
- random structure2vecì„ ì´ìš©í•´ MRRC ë¬¸ì œë¥¼ í‘¸ëŠ” ë°©ë²•ì— ëŒ€í•´ ì„¤ëª…
- state $s_k=(g_k,D_k)$ ê°€ ì£¼ì–´ì¡Œì„ë•Œ $a_k$ë¥¼ ì–´ë–»ê²Œ í• ì§€ ì„¤ëª…:
  1. random Bayesian Networkë¥¼ í†µí•´ state í‘œí˜„
  2. random graph embeddingì„ í†µí•´ Q-value ì¶”ì •
  3. joint assignment ì„ íƒ
### 4.1 Representing a state using a random PGM
- í•˜ë‚˜ì˜ $s_k$ì™€ $a_k$ë¥¼ bayesian networkë¡œ í‘œí˜„
- $H_p$: ì‘ì—… $p$ì— ëŒ€í•œ hidden random variable - ì‘ì—… $p$ì˜ ì´ìµì— ëŒ€í•œ ì •ë³´ë¥¼ ë‹´ìŒ.
- ì‹œë‚˜ë¦¬ì˜¤ê°€ ì£¼ì–´ì¡Œì„ë•Œ, $H_p$ëŠ” ì‘ì—… $p$ì˜ feature $X_p$ì— dependentí•˜ê³  ë§Œì•½ ê°™ì€ ë¡œë´‡ì´ ì‘ì—… $p$ì´í›„ $q$ë¥¼ í•œë‹¤ë©´ $H_q$ì—ë„ dependentí•¨.
- Bayesian NetworkëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë˜ë©° ì´ëŠ” í•˜ë‚˜ì˜ ì‹œë‚˜ë¦¬ì˜¤ì— ëŒ€ì‘ë¨.

  $$p(\lbrace H_p \rbrace | \lbrace X_p \rbrace) = \prod_p \phi(H_p|X_p) \prod_{p,q} \phi(H_p|H_q) $$
- Lemma 1ì— ë”°ë¥´ë©´ random PGMì´ ì´ëŸ¬í•œ íŠ¹ì„±ì„ ëª¨ë¸ë§í•˜ê¸° ë•Œë¬¸ì— edge(semi-clique)ì— ëŒ€í•œ í™•ë¥  { $p(\epsilon_{pq}^{TT}$) }ì„ ì‚¬ìš©í•œ random structure2vecì„ ì ìš©
- ì‘ì—… $p$ì—ëŒ€í•œ embedding $\tilde{ \mu_p } $ëŠ” ë‹¤ìŒê³¼ ê°™ìŒ.
  
$$\tilde{\mu_p} \gets \sigma \left( W_1 x_p +W_2 \sum_{p \neq q} p_{qp} \tilde{\mu_q} \right) $$
### 4.2 Estimating state-action value using Order trainability-enabled Q-function
- MRRC ë¬¸ì œì—ì„œ ì£¼ì–´ì§€ëŠ” $X_p$ëŠ” $d_{ip}^{RT}$ (ë¡œë´‡ $i$ê³¼ ì‘ì—… $p$ì™€ì˜ ê±°ë¦¬)ì™€ $d_p^T$ (ì‘ì—… $p$ì˜ ë‚˜ì´)ë¡œ ë‘ ì¢…ë¥˜ ì´ë‹¤. 
- ì´ë¥¼ í•˜ë‚˜ì˜ embeddingìœ¼ë¡œ ë‚˜íƒ€ë‚´ê¸° ìœ„í•´ action embeddingê³¼ value embeddingë¡œ êµ¬ë¶„ë˜ëŠ” two-stepì˜ sequntial random structure2vec network êµ¬ì¡°ë¥¼ ì œì•ˆí•¨.
- ë‘ step ëª¨ë‘ random structure2vecì„ ì‚¬ìš©í•˜ë©° ê·¸ë•Œ ë“¤ì–´ê°€ëŠ” feature ì¢…ë¥˜ë§Œ ë‹¤ë¦„
  - Action embedding: ë¡œë´‡ê³¼ í• ë‹¹ëœ ì‘ì—… ê°„ì˜ ìƒëŒ€ì ì¸ ìœ„ì¹˜ ì •ë³´ë¥¼ ì¶©ë¶„íˆ ì œê³µ

$$ \tilde{\mu_p}^A = \sigma \left( W_1^A x_p^A +W_2^A \sum_{p \neq q} p_{qp} \tilde{\mu_q}^A \right)  \text{, where } x_p^A =d_{ip}^{RT} $$

  - Value embedding: ì£¼ì–´ì§„ ê³µë™ í• ë‹¹ì— ë”°ë¼ ê° ì‘ì—… ì£¼ë³€ì˜ ë¡œì»¬ ê·¸ë˜í”„ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ê°€ì¹˜ë¥¼ ì¶©ë¶„íˆ í‘œí˜„


$$ \tilde{\mu_p}^V = \sigma \left( W_1^V x_p^V +W_2^A \sum_{p \neq q} p_{qp} \tilde{\mu_q}^V \right)  \text{, where } x_p^V =(\tilde{\mu_p}^A,d_p^T) $$

- ìµœì¢…ì ìœ¼ë¡œëŠ” ëª¨ë“  nodeì˜ embedding vectorë¥¼ ë”í•´ aggregationí•œ graphì˜ embeddingì„ $(s_k,a_k)$ì˜ represnetationìœ¼ë¡œ ì‚¬ìš©í•˜ë©° $Q_\theta(s_k,a_k)$ ì— input

$$ \tilde{\mu}^V  = \sum_p \tilde{\mu_p}^V $$

- **Order-Transferability**: ë¬¸ì œ í¬ê¸°(ê·¸ë˜í”„ í¬ê¸°) ì™€ ë¬´ê´€í•˜ê²Œ Q-valueë¥¼ estimateí•  ìˆ˜ ìˆìŒ
  - action embedding: ê° ë…¸ë“œ ì£¼ë³€ì—ì„œ ì§€ì—­ì ìœ¼ë¡œ ê·œëª¨ì™€ ë¬´ê´€í•œ ì‘ì—…ì´ê¸° ë•Œë¬¸ì— ì „ì´ ê°€ëŠ¥ì„±ì´ ìëª….
  - value embedding: ë¡œë´‡ê³¼ ì‘ì—…ì˜ ë¹„ìœ¨ì´ ì¤‘ìš”. ë§Œì•½ í›ˆë ¨ í™˜ê²½ì˜ ë¡œë´‡-ì‘ì—… ë¹„ìœ¨ì´ í…ŒìŠ¤íŠ¸ í™˜ê²½ë³´ë‹¤ ì‘ìœ¼ë©´ ì „ì²´ ì„ë² ë”© ê°’ì´ ê³¼ì†Œ ì¶”ì •ë  ìˆ˜ ìˆê³ , ê·¸ ë°˜ëŒ€ì˜ ê²½ìš° ê³¼ëŒ€ ì¶”ì •ë  ìˆ˜ ìˆë‹¤.
  - í•˜ì§€ë§Œ, Q-function ê¸°ë°˜ì˜ policyì—ì„œ Q-functionì˜ ê°’ ìˆœì„œë§Œ ë™ì¼í•˜ë©´ ê³¼ëŒ€/ê³¼ì†Œ ì¶”ì •ì€ ë¬¸ì œë˜ì§€ ì•ŠìŒ

### 4.3 Selecting a joint assignment using OTAP
- ìƒíƒœ â€‹$s_k$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ê³µë™ í• ë‹¹(action) $ğ‘_k$ = a maximal bipartite matching in the bipartite graph $(R,T_k,E_k^{RT})$
- Order Trasferability-enabled Aution Policy(OTAP): Bidding phaseì™€ Consensus phase ë§ˆë‹¤ í•˜ë‚˜ì˜ ë¡œë´‡-ì‘ì—… í• ë‹¹ì„ ì¶”ê°€í•´ê°€ë©° $N=\max(|R|,|k|)$ë²ˆ ë°˜ë³µí•˜ë©° ëª¨ë“  ì‘ì—…í• ë‹¹ì´ ëë‚ ë–„ê¹Œì§€ ë°˜ë³µí•¨.
  - Bidding-phase:
    - ì•„ì§ í• ë‹¹ë˜ì§€ì•Šì€ ë¡œë´‡ë³„ë¡œ ì´ì „ iterationë“¤ì—ì„œ ì´ë¯¸ í• ë‹¹ëœ ë¡œë´‡-ì‘ì—…ì€ ê³ ì •í•˜ê³  ìê¸°ìì‹ ê³¼ ë‹¤ë¥¸ ì‘ì—… pairë¥¼ ì¶”ê°€í–ˆì„ë•Œì˜ Q-valueë¥¼ ê³„ì‚°í•˜ê³  ê·¸ ì¤‘ ê°€ì¥ í° ì‘ì—…ê³¼ Q-value bidding
  - Consensus-phase:
    - ì•„ì§ í• ë‹¹ë˜ì§€ì•Šì€ ë¡œë´‡ë“¤ì˜ biddingê°’ ì¤‘ ê°€ì¥ í° biddingê°’ì„ ì œì‹œí•œ ë¡œë´‡ì—ê²Œ í•´ë‹¹ ì‘ì—…ì„ í• ë‹¹
   
### 4.4 Training Q-function using AFQI
- ì¼ë°˜ì ì¸ fitted Q-learning (FQI):

$$ minimize_\theta \quad E_{(s_k,a_k,r_k,s_{k+1}) \sim D} \[Q_\theta(s_k,a_k) - \[r(s_k,a_k) + \gamma \max_a Q_\theta (s_{k+1},a) \]\]$$

- Auction fitted Q-learning (AFQI):

$$ minimize_\theta \quad E_{(s_k,a_k,r_k,s_{k+1}) \sim D} \[Q_\theta(s_k,a_k) - \[r(s_k,a_k) + \gamma Q_\theta (s_{k+1},\pi_{Q_\theta} (s_{k+1})) \]\]$$


## 5. Theoretical analysis
### 5.1 Performance bound of OTAP

![ex_screenshot](../../images/DS535_24F/Learning_NP-Hard_Multi-Agent_Assignment_Planning_using_GNN_Inference_on_a_Random_Graph_and_Provable_Auction-Fitted_Q-learning/thm_2.png)

- Theorem 2ë¥¼ í†µí•´ OTAP ì•Œê³ ë¦¬ì¦˜ì´ $1-1/e$ optimalityë¥¼ ê°€ì§


### 5.2 Performance bound of AFQI

![ex_screenshot](../../images/DS535_24F/Learning_NP-Hard_Multi-Agent_Assignment_Planning_using_GNN_Inference_on_a_Random_Graph_and_Provable_Auction-Fitted_Q-learning/lem_3.png)

- Lemma 3ì— ë”°ë¥´ë©´ FQIì˜ max operatorë¥¼ ì •ì±… $1-1/r$ì˜ ê·¼ì‚¬ ì •ì±…ìœ¼ë¡œ ëŒ€ì²´í•˜ë©´ FQIì—­ì‹œë„ $1-1/r$ optimalityë¥¼ ê°€ì§.
- AFQIì˜ ê²½ìš° $1-1/e$ì˜ ê·¼ì‚¬ ì •ì±… OTAPë¡œ max operatorë¥¼ ëŒ€ì²´í•˜ì˜€ê¸° ë•Œë¬¸ì— AFQI ì•Œê³ ë¦¬ì¦˜ì—­ì‹œ $1-1/e$ optimalityë¥¼ ê°€ì§

## 6. Experiment
### 6.1 Experiment setting
- ì‘ì—… ì™„ë£Œ ì‹œê°„ì€ deterministic í™˜ê²½ì—ì„œëŠ” ë‹¤ìµìŠ¤íŠ¸ë¼ ì•Œê³ ë¦¬ì¦˜, stochastic í™˜ê²½ì—ì„œëŠ” ë™ì  í”„ë¡œê·¸ë˜ë°ì„ ì‚¬ìš©í•˜ì—¬ ìƒì„±ë¨
- í™•ë¥ ì  í™˜ê²½ì—ì„œëŠ” ë¡œë´‡ì´ íŠ¹ì • í™•ë¥ ë¡œ ì˜ë„í•œ ëŒ€ë¡œ ì›€ì§ì„.
  - ì ì´ ìˆëŠ” ì…€: ì„±ê³µ í™•ë¥  55%, ë‚˜ë¨¸ì§€ ë°©í–¥ ê°ê° 15%.
  - ì ì´ ì—†ëŠ” ì…€: ì„±ê³µ í™•ë¥  70%, ë‚˜ë¨¸ì§€ ë°©í–¥ ê°ê° 10%.
- ë¡œë´‡ì´ ì‘ì—… ì§€ì ì— ë„ë‹¬í•˜ë©´ í•´ë‹¹ ì‘ì—…ì€ ì™„ë£Œëœ ê²ƒìœ¼ë¡œ ê°„ì£¼. ë³´ìƒ ê·œì¹™ìœ¼ë¡œëŠ” ë‘ ê°€ì§€ë¥¼ ì‚¬ìš©:
  - Linear : $f(age) = \max\lbrace 200 - age,0 \rbrace$ 
  - Nonlinear : $f(age) = \lambda^{age} $ ($\lambda = 0.99$)
- Baselines:
  - deterministic: MILP ê³µì‹í™” í›„ 2ê°€ì§€ ì•Œê³ ë¦¬ì¦˜
    - Optimal: Gurobi Optimization(2019)ì˜ MILP ìµœì í™” ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬, 60ë¶„ ì œí•œ ì‹œê°„ ë‚´ì— ë¬¸ì œë¥¼ í•´ê²°
    - Ekici et al. (2013): Operations Research ë¶„ì•¼ì—ì„œ ìµœì‹  íœ´ë¦¬ìŠ¤í‹± ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©
  - stochastic or Nonlinear: ì´ì „ ì—°êµ¬ê°€ ì—†ê¸° ë•Œë¬¸ì— ê°„ì ‘ì ì¸ benchmarkì‚¬ìš©
    -  Sequential Greedy Algorithm (SGA) :ì¼ë°˜ì ì¸ ë‹¤ì¤‘ ë¡œë´‡ ì‘ì—… í• ë‹¹ ì•Œê³ ë¦¬ì¦˜(SGA; Han-Lim Choi et al., 2009)ì„ ì‚¬ìš©í•˜ì˜€ë‹¤.
-  Performance measure:
  
$$ \rho = \frac{\text{Rewards collected by the proposed method}}{\text{Reward collected by the baseline}} $$

### 6.2 Performance test

![ex_screenshot](../../images/DS535_24F/Learning_NP-Hard_Multi-Agent_Assignment_Planning_using_GNN_Inference_on_a_Random_Graph_and_Provable_Auction-Fitted_Q-learning/tab_1.png)

-ì œì•ˆëœ ë°©ë²•ì€ ê²°ì •ë¡ ì /ì„ í˜• ë³´ìƒ í™˜ê²½ì—ì„œ ìµœì  í•´ë³´ë‹¤ í‰ê·  3% ë‚®ì€ ë³´ìƒì„ ë‹¬ì„±í•˜ë©°, ê±°ì˜ ìµœì ì˜ ì„±ëŠ¥ì„ ë³´ì„. 
- ë‹¤ë¥¸ í™˜ê²½ì—ì„œë„ SGA ë¹„ìœ¨ì´ ì˜ ìœ ì§€ë¨.

### 6.3 Transferability test

![ex_screenshot](../../images/DS535_24F/Learning_NP-Hard_Multi-Agent_Assignment_Planning_using_GNN_Inference_on_a_Random_Graph_and_Provable_Auction-Fitted_Q-learning/tab_2.png)

- í–‰ì€ í›ˆë ¨ ì¡°ê±´ì„ ë‚˜íƒ€ë‚´ê³ , ì—´ì€ í…ŒìŠ¤íŠ¸ ì¡°ê±´ì„ ë‚˜íƒ€ëƒ„.
- ëŒ€ê°ì„  ì…€(ë¹¨ê°„ìƒ‰)ì€ ë™ì¼í•œ í›ˆë ¨ ë° í…ŒìŠ¤íŠ¸ í¬ê¸°ì—ì„œì˜ ì§ì ‘ í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚´ë©°, baseline ì„±ëŠ¥ìœ¼ë¡œ ì‚¬ìš©.
- ë¹„ëŒ€ê°ì„  ì…€ì€ ì „ì´ ê°€ëŠ¥ì„± í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ë©°, ë‹¤ë¥¸ ë¬¸ì œ í¬ê¸°ì—ì„œ í›ˆë ¨ëœ ì•Œê³ ë¦¬ì¦˜ì´ í…ŒìŠ¤íŠ¸ ë¬¸ì œì—ì„œ ì–¼ë§ˆë‚˜ ì˜ ìˆ˜í–‰ë˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ„.
  - í•˜í–¥ ì „ì´ í…ŒìŠ¤íŠ¸(í° ë¬¸ì œë¡œ í›ˆë ¨í•˜ê³  ì‘ì€ ë¬¸ì œì—ì„œ í…ŒìŠ¤íŠ¸)ëŠ” ì„±ëŠ¥ ì†ì‹¤ì´ ê±°ì˜ ì—†ìŒ.
  - ìƒí–¥ ì „ì´ í…ŒìŠ¤íŠ¸(ì‘ì€ ë¬¸ì œë¡œ í›ˆë ¨í•˜ê³  í° ë¬¸ì œì—ì„œ í…ŒìŠ¤íŠ¸)ëŠ” ìµœëŒ€ 4%ì˜ ì„±ëŠ¥ ì†ì‹¤ì´ ë°œìƒí•¨.
    
### 6.4 Scalability analysis

![ex_screenshot](../../images/DS535_24F/Learning_NP-Hard_Multi-Agent_Assignment_Planning_using_GNN_Inference_on_a_Random_Graph_and_Provable_Auction-Fitted_Q-learning/tab_3.png)

- í›ˆë ¨ ë³µì¡ì„±
  - deterministicì—ì„œ linear ë³´ìƒì„ ê³ ë ¤í•  ë•Œ 93% ìµœì  ì„±ëŠ¥ì— ë„ë‹¬í•˜ëŠ” ë° í•„ìš”í•œ í›ˆë ¨ ì‹œê°„ì„ ì¸¡ì •í•¨.
  - í‘œ 4ì— ë”°ë¥´ë©´, ë¬¸ì œ í¬ê¸°ê°€ ì»¤ì§€ë”ë¼ë„ í›ˆë ¨ ì‹œê°„ì´ ë°˜ë“œì‹œ ì¦ê°€í•˜ì§€ëŠ” ì•Šìœ¼ë©°, ì„±ëŠ¥ì´ ì•ˆì •ì ìœ¼ë¡œ ìœ ì§€ë¨
- MRRC ë¬¸ì œ ë³µì¡ì„±:
  - MRRC ë¬¸ì œëŠ” semi-MDP ê¸°ë°˜ì˜ ë‹¤ì¤‘ ë¡œë´‡ ê³„íš ë¬¸ì œë¡œ ê³µì‹í™”í•  ìˆ˜ ìˆìŒ
  - $R$ëŒ€ ë¡œë´‡, $T$ê°œ ì‘ì—…, ìµœëŒ€ì‹œê°„ $H$ì¼ë•Œ, ë¬¸ì œ ë³µì¡ë„ëŠ” $O((R!/T!(R-T)!)^H)$.
  - ì œì•ˆëœ ë°©ë²•ì€ ì´ê±¸ ê³„ì‚°ë³µì¡ë„ì™€ í›ˆë ¨ ë³µì¡ë„ë¡œ ë¶„ë¦¬í•˜ì—¬ í•´ê²°.
  - ê° ì‹œê°„ ë‹¨ê³„ì—ì„œ actionì„ ìœ„í•œ ê³„ì‚°ë³µì¡ë„ëŠ” $O(|R||T|^3)$.
 
## 7. Conclusion
ë³¸ ë…¼ë¬¸ì—ì„œëŠ” NP-ë‚œí•´í•œ ë‹¤ì¤‘ ë¡œë´‡/ê¸°ê³„ ìŠ¤ì¼€ì¤„ë§ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ê·¼ì‚¬ ìµœì ì˜ í•™ìŠµ ê¸°ë°˜ ë°©ë²•ì„ ê°œë°œí•˜ëŠ” ë„ì „ì— ëŒ€í•´ ë‹¤ë£¨ì—ˆë‹¤. ìš°ë¦¬ëŠ” ìŠ¤ì¼€ì¤„ë§ ë¬¸ì œë¥¼ ìœ„í•œ mean-field inference ì´ë¡ ì„ ê°œë°œí•˜ê³ , ì´ì— ê¸°ë°˜í•œ Q-í•¨ìˆ˜ë¥¼ ì •í™•í•˜ê²Œ ì¶”ë¡ í•  ìˆ˜ ìˆëŠ” ì´ë¡ ì ìœ¼ë¡œ ì •ë‹¹í™”ëœ GNN ë°©ë²•ì„ ì œì•ˆí•˜ì˜€ë‹¤. ë˜í•œ, ë‹¤ì¤‘ ë¡œë´‡/ê¸°ê³„ ìŠ¤ì¼€ì¤„ë§ ë¬¸ì œì—ì„œ Fitted Q-Iteration ë°©ë²•ì˜ í™•ì¥ì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‹¤í•­ ì‹œê°„ ë‚´ì— ê³„ì‚° ê°€ëŠ¥í•œ ì•Œê³ ë¦¬ì¦˜ê³¼ ì„±ëŠ¥ ë³´ì¥ì„ ì œê³µí•˜ì˜€ë‹¤. ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ë¥¼ í†µí•´ ì œì•ˆëœ ë°©ë²•ì˜ íš¨ìœ¨ì„±ì„ ì…ì¦í•˜ì˜€ë‹¤.
## References
- Dai, H., Dai, B., and Song, L. Discriminative Embeddings of Latent Variable Models for Structured Data. 48:1â€“23, 2016. doi: 1603.05629. 
- Gurobi Optimization, L. Gurobi optimizer reference manual, 2019. URL http://www.gurobi.com.
- Ekici, A. and Retharekar, A. Multiple agents maximum collection problem with time dependent rewards. Computers and Industrial Engineering, 64(4):1009â€“1018, 2013. ISSN 03608352. doi: 10.1016/j.cie.2013.01.010. URL http://dx.doi.org/10.1016/j.cie.2013.01.010.
- Han-Lim Choi, Brunet, L., and How, J. Consensus-Based Decentralized Auctions for Robust Task Allocation. IEEE Transactions on Robotics, 25(4):912â€“926, aug 2009. ISSN 1552-3098. doi: 10.1109/TRO.2009.2022423.
