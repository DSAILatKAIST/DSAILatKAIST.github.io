---
title: "[ICML 2022] Structure-Aware Transformer for Graph Representation Learning"
permalink: Structure_Aware_Transformer_for_Graph_Representation_Learning.html
tags: [reviews]
use_math: true
usemathjax: true
---

  

# **Structure-Aware Transformer for Graph Representation Learning**

  

_**Background before reading this review.**_

Graphêµ¬ì¡°ì— ë§ê²Œ Transformerë¥¼ ì ìš©í•˜ì—¬ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¸ SATë¥¼ ì œì‹œí•œ ë…¼ë¬¸ [Structure-Aware Transformer for Graph Representation Learning](https://arxiv.org/abs/2202.03036)ë¥¼ ì½ê¸°ì „ì— ì•Œê³  ë„˜ì–´ê°€ì•¼í•  Graph Notation, Transformerì— ëŒ€í•œ ì„¤ëª… ë“± ê°„ë‹¨í•˜ê²Œ ì§šê³  ë„˜ì–´ê°€ë©´ ì¢‹ì€ ë‚´ìš©ë“¤ì…ë‹ˆë‹¤. ì‚¬ì „ ì§€ì‹ì´ ìˆìœ¼ì‹  ê²½ìš°, ë°”ë¡œ ë³¸ë¬¸ìœ¼ë¡œ ë„˜ì–´ê°€ì…”ë„ ì¢‹ìŠµë‹ˆë‹¤.  
  
  

*Notation

  

$G = (V, E, \mathbf X)$

  

- node $u \in V$

- node attribute $x_u \in  \mathcal X \subset  \mathbb R^d$

- $\mathbf X \in  \mathbb R^{n \times d}$

  
  
  


**Transformer êµ¬ì„± ìš”ì†Œ**

1. Self-attention module

- input node feature $\mathbf X$ê°€ linear projectionì„ í†µí•´ Query($\mathbf Q$), Key($\mathbf K$), Value($\mathbf V$)ë¡œ íˆ¬ì˜ë˜ê³ , ì´ë¥¼ í™œìš©í•˜ì—¬ self-attentionì„ ê³„ì‚°í•©ë‹ˆë‹¤.

- multi-head attention : self-attentionì˜ initializeë¥¼ ë‹¤ì–‘í•˜ê²Œ í•˜ì—¬ í‘œí˜„ë ¥ì„ ë†’ì˜€ìŠµë‹ˆë‹¤.

2. feed-forward NN

- self-attentionì˜ outputì´ skipconnectionì´ë‚˜ FFNë“±ì„ ê±°ì¹˜ë©´ í•˜ë‚˜ì˜ transforemer layerë¥¼ í†µê³¼í•œ ê²ƒ ì…ë‹ˆë‹¤.

3. Absolute encoding

- ê·¸ë˜í”„ì˜ ìœ„ì¹˜ì /êµ¬ì¡°ì ì¸ representationì„ input node featureì— ë”í•˜ê±°ë‚˜ concatenateí•˜ì—¬ Transformerì˜ inputìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. (Vanilla transformerì˜ PEì™€ ê°™ì€ ì—­í• )

**Graph Transformerì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” Positional encoding methodë“¤**

ìì£¼ ì‚¬ìš©ë˜ëŠ” PEë¡œëŠ” ë‹¤ìŒ ë‘ê°€ì§€ë¥¼ ê¼½ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ Positional Encodingë“¤ì˜ ë¬¸ì œëŠ” ë…¸ë“œì™€ ê·¸ ì´ì›ƒë“¤ ì‚¬ì´ì˜ structural similarityë¥¼ ë°˜ì˜í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ê°ê°ì— ëŒ€í•œ ì„¤ëª…ì€ ë§í¬ë¥¼ íƒ€ê³  ë“¤ì–´ê°€ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- [Laplacian PE](https://paperswithcode.com/method/laplacian-pe)

- [Random Walk PE](https://arxiv.org/pdf/2110.07875.pdf)


3. Self-attention and kernel smoothing

$\operatorname{Attn}\left(x_v\right)=\sum_ {u \in V} \frac{\kappa_ {\exp }\left(x_v, x_u\right)}{\sum_ {w \in V} \kappa_ {\exp }\left(x_v, x_w\right)} f\left(x_u\right), \forall v \in V$

- linear value function $f(x) = \mathbf W_ {\mathbf V}x$

- $\kappa_ {\exp }$ (non-symmetric) exponential kernel parameterized by $\mathbf W_ {\mathbf Q}, \mathbf W_ {\mathbf K}$

$\kappa_ {\exp }\left(x, x^{\prime}\right):=\exp  \left(\left\langle\mathbf{W}_ {\mathbf{Q}} x, \mathbf{W}_ {\mathbf{K}} x^{\prime}\right\rangle / \sqrt{d_ {\text {out }}}\right)$

- $\langle  \cdot, \cdot\rangle$ : dotproduct

- í•™ìŠµê°€ëŠ¥í•œ exponential kernel

- (-) only position-aware, not structure-aware encoding

  

# **1. Problem Definition**

  

## _**Limitations of GNN**_

  

1. limited expressiveness : GNNì€ message passingê³¼ì •ì—ì„œì˜ aggregation operationì˜ íŠ¹ì„±ìœ¼ë¡œ ì¸í•´ ìµœëŒ€ 1-WL testì˜ í‘œí˜„ë ¥ì„ ê°€ì§‘ë‹ˆë‹¤. GNNì˜ WL-testì™€ expressionì— ëŒ€í•œ ë¶„ì„ì€ GINì„ ì œì‹œí•œ ë…¼ë¬¸ì¸ [How Powerful are Graph Neural Networks?](https://arxiv.org/abs/1810.00826) ì—ì„œ ì œì‹œë˜ì—ˆìŠµë‹ˆë‹¤.

2. Over-smoothing problem : GNN layerì˜ ìˆ˜ê°€ ì¶©ë¶„íˆ ì»¤ì§€ë©´ ëª¨ë“  node representationì´ ìƒìˆ˜ë¡œ ìˆ˜ë ´í•˜ê²Œë©ë‹ˆë‹¤.

3. Over-squashing problem : ê·¸ë˜í”„ì˜ ìˆ˜ë§ì€ ë©”ì„¸ì§€ë“¤ì´ ê³ ì •ëœ ê¸¸ì´ì˜ ë²¡í„° í•˜ë‚˜ë¡œ ì••ì¶•ë˜ì–´ ë°œìƒí•˜ëŠ” ê·¸ë˜í”„ â€œbottleneckâ€ìœ¼ë¡œ ì¸í•´ ë©€ë¦¬ ìœ„ì¹˜í•œ ë…¸ë“œì˜ ë©”ì„¸ì§€ê°€ íš¨ìœ¨ì ìœ¼ë¡œ ì „íŒŒë˜ì§€ ì•ŠëŠ” ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤.

  

**â‡’ Beyond neighborhood aggregation!**

  

## _**Transformer**_

  Transformerë¥¼ ì ìš©í–ˆì„ ë•Œì˜ ì¥ì ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

- í•˜ë‚˜ì˜ self-attention layerë¥¼ í†µí•´ ê·¸ë˜í”„ë‚´ì˜ ì–´ë–¤ ë…¸ë“œìŒì´ë“ ì§€ ê·¸ ì‚¬ì´ì˜ ìƒí˜¸ì‘ìš©ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- GNNê³¼ ë‹¬ë¦¬ ì¤‘ê°„ ê³„ì¸µì—ì„œ structural inductive biasê°€ ë°œìƒí•˜ì§€ ì•Šì•„ GNNì˜ í‘œí˜„ë ¥ í•œê³„ë¥¼ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  ë°˜ë©´, ë‹¨ì ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.


- graph structure infoë¥¼ ì–¼ë§ˆë‚˜ í•™ìŠµí•˜ëŠ”ì§€ input node featureì—ë§Œ structural, positional ì •ë³´ë¥¼ ì¸ì½”ë”©í•˜ì—¬ ë„£ê¸° ë•Œë¬¸ì— ì œí•œì ì…ë‹ˆë‹¤.

- ë…¸ë“œì— ëŒ€í•œ structural, positional ì •ë³´ë§Œ input node featureë¡œ ì¸ì½”ë”©í•˜ê¸° ë•Œë¬¸ì—, ê·¸ë˜í”„ êµ¬ì¡° ìì²´ì—ì„œ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ì •ë³´ì˜ ì–‘ì´ ì œí•œì ì…ë‹ˆë‹¤.
  
  ë”°ë¼ì„œ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ê³ ì í•˜ëŠ” Graph Transformerì˜ Goalì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

> ğŸ’¡ Goal : ê·¸ë˜í”„ ë°ì´í„°ì— Transformerë¥¼ ì ì ˆíˆ ë³€í˜•í•´ ì ìš©í•˜ì—¬ ê·¸ë˜í”„ êµ¬ì¡°ë¥¼ ì˜ ë°˜ì˜í•˜ê³  ë†’ì€ í‘œí˜„ë ¥ì„ ê°€ì§€ëŠ” Achitectureë¥¼ ë””ìì¸í•˜ëŠ” ê²ƒ

  

# **2. Motivation**

  
  

## _**Message passing graph neural networks.**_

  

ìµœëŒ€ 1-WL testë¡œ ì œí•œëœ í‘œí˜„ë ¥, over-smoothing, over-quashing

  

## _**Limitations of existing approaches**_

  ê¸°ì¡´ì— Graphêµ¬ì¡°ì— Transformerë¥¼ ì ìš©í•˜ëŠ” ì‹œë„ê°€ ì—†ì—ˆë˜ ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ì–´ë–¤ê²ƒì´ ë¬¸ì œê°€ ë˜ì—ˆì„ê¹Œìš”?

- ë…¸ë“œë“¤ ì‚¬ì´ positional relationshipë§Œ ì¸ì½”ë”©í•˜ê³ , strucutral relationshipì„ ì§ì ‘ ì¸ì½”ë”©í•˜ì§€ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ì— ë”°ë¼ ë…¸ë“œë“¤ ì‚¬ì´ structural similarityë¥¼ í™•ì¸í•˜ê¸°ê°€ ì–´ë µê³ , ë…¸ë“œë“¤ ì‚¬ì´ì˜ structural interactionì„ ëª¨ë¸ë§í•˜ëŠ”ë° ì‹¤íŒ¨í•œê²ƒìœ¼ë¡œ ë¶„ì„í•˜ì˜€ìŠµë‹ˆë‹¤.

ë‹¤ìŒì˜ ê·¸ë¦¼ ì˜ˆì‹œë¥¼ ë³´ë©´ ì´í•´ê°€ ë” ì‰½ìŠµë‹ˆë‹¤.

  

ex.
![Untitled](https://github.com/sujinyun999/LearningOnGraph/assets/69068083/4472bb78-65cc-43bf-90be-8dcd203616d8)

  

G1ê³¼ G2ì—ì„œ ìµœë‹¨ê±°ë¦¬ë¥¼ í™œìš©í•œ positional encodingì„ í• ê²½ìš° node uì™€ vê°€ ë‹¤ë¥¸ ë…¸ë“œë“¤ì— ëŒ€í•´ ëª¨ë‘ ê°™ì€ representationì„ ê°€ì§€ê²Œë˜ì§€ë§Œ, ê·¸ë˜í”„ì˜ ì‹¤ì œ êµ¬ì¡°ëŠ” ë‹¤ë¦…ë‹ˆë‹¤. 
â†’ ì´ ì§€ì ì´ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” ê¸°ì¡´ Graph Transformerì˜ ë¬¸ì œ, ì¦‰, strucure awareì— ì‹¤íŒ¨í•œ ê²ƒ ì…ë‹ˆë‹¤.

  

>ğŸ’¡ Message-passing GNNê³¼ Transformer architecture ê°ê°ì˜ ì¥ì ì„ ì‚´ë ¤ local, global infoë¥¼ ëª¨ë‘ ê³ ë ¤í•˜ëŠ” transformer architectureë¥¼ ì œì•ˆ

  

## _**Contribution of this paper**_

Q. ê·¸ë ‡ë‹¤ë©´ ë…¼ë¬¸ì—ì„œ í•´ê²°í•˜ê³ ìí•˜ëŠ” Structure-Awareë¥¼ ìœ„í•´ Transformerêµ¬ì¡°ì— structural infoë¥¼ ì–´ë–»ê²Œ ì¸ì½”ë”©í• ê¹Œìš”?

  ë…¼ë¬¸ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ëŒ€ë‹µí•©ë‹ˆë‹¤.

A. Structure-aware self attentionë¥¼ ë„ì…í•œ Structre-Aware Transformer(SAT)

  

1. reformulate the self-attention mechanism

- kernel smoother

- ì›ë˜ ë…¸ë“œ featureì— ì ìš©í•˜ëŠ” exponential ì»¤ë„ì„ í™•ì¥í•˜ì—¬ ê° ë…¸ë“œê°€ ì¤‘ì‹¬ì¸ subgraph representationì„ ì¶”ì¶œí•˜ì—¬ local structureì—ë„ ì ìš©í•©ë‹ˆë‹¤.

2. subgraph representationë“¤ì„ ìë™ì ìœ¼ë¡œ ë§Œë“¤ì–´ë‚´ëŠ” ë°©ë²•ë¡  ì œì•ˆ

- ì´ë¥¼ í†µí•´ kernel smootherê°€ êµ¬ì¡°ì /íŠ¹ì„±ì  ìœ ì‚¬ì„±ì„ í¬ì°©í•  ìˆ˜ ìˆê²Œë©ë‹ˆë‹¤.

3. GNNìœ¼ë¡œ ê·¸ë˜í”„ì˜ subgraph infoë¥¼ í¬í•¨í•˜ëŠ” node representationì„ ë§Œë“¤ì–´ ê¸°ì¡´ GNNì— ì¶”ê°€ì ì¸ êµ¬ì¡° ê°œì„  ì—†ì´ë„ ë” ë†’ì€ ì„±ëŠ¥ì„ ëƒ…ë‹ˆë‹¤.

4. Transformerì˜ ì„±ëŠ¥í–¥ìƒì´ structure-awareí•œ ì¸¡ë©´ì—ì„œ ì¼ì–´ë‚œ ê²ƒì„ ì¦ëª…í•˜ê³  absolute encodingì´ ì¶”ê°€ëœ transfoemrë³´ë‹¤ SATê°€ ì–¼ë§ˆë‚˜ interpretableí•œì§€ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.

  

# **3. Method**

  
  
  

## _**Structure-Aware Transformer**_

  

### _1. **Structure-Aware Self-attention**_

  

position-awareí•œ structural encodingì— ë…¸ë“œë“¤ ì‚¬ì´ structural similarityë¥¼ í¬í•¨í•˜ê¸° ìœ„í•´ ê° ë…¸ë“œì˜ local structureì— ê´€í•œ generalized kernelì„ ì¶”ê°€í•©ë‹ˆë‹¤.

  

ê° ë…¸ë“œê°€ ì¤‘ì‹¬ì´ë˜ëŠ” subgraph setì„ ì¶”ê°€í•¨ìœ¼ë¡œì¨ structure-aware attentionì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  

$\operatorname{SA-Attn}\left(v\right):=\sum_ {u \in V} \frac{\kappa_ {\text{graph} }\left(S_G(v), S_G(u)\right)}{\sum_ {w \in V} \kappa_ {\text{graph}}\left(S_G(v), S_G(u)\right)} f\left(x_u\right)$

  

- $S_G(v)$ : node feature $\mathbf X$ì™€ ì—°ê´€ëœ $v$ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œí•˜ëŠ” subgraph

- $\kappa_ {\text{graph} }$ : subgraphìŒì„ ë¹„êµí•˜ëŠ” kernel

  

â‡’ attribute & structural similarity ëª¨ë‘ í‘œí˜„ ê°€ëŠ¥í•œ expressive node representationì„ ìƒì„± â†’ table 1

  

â‡’ ë™ì¼í•œ subgraph êµ¬ì¡°ë¥¼ ê°€ì§€ëŠ” ê²½ìš°ì—ë§Œ permutation equivariantí•œ ì„±ì§ˆì„ ê°–ê²Œë¨

  

$\kappa_ {\text {graph }}\left(S_G(v), S_G(u)\right)=\kappa_ {\exp }(\varphi(v, G), \varphi(u, G))$

  

- $\varphi(v, G)$ : feature $\mathbf X$ë¥¼ ê°€ì§€ëŠ” node $v$ê°€ ì¤‘ì‹¬ì— ìˆëŠ” subgraphì˜ vector representationì„ ë§Œë“¤ì–´ë‚´ëŠ” structure extractor

- GNNì´ë‚˜ differentiable Graph kernelë“± subgraphì˜ representationì„ ë§Œë“¤ ìˆ˜ ìˆëŠ” ì–´ëŠ ëª¨ë¸ì´ë“  ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- Task/data íŠ¹ì„±ì— ë”°ë¼ Edge attributeì„ í™œìš©í•  í•„ìš”ê°€ ìˆëŠ” ê²½ìš° ê·¸ì— ë§ëŠ”GNNì„ ì„ íƒí•˜ëŠ” ë””ìì¸ ì´ˆì´ìŠ¤ê°€ ìƒê¹ë‹ˆë‹¤. edge attributeì„ ë”°ë¡œ í™œìš©í•˜ì§€ëŠ” ì•Šê³  subgraph extractorì—ì„œ í™œìš©í•©ë‹ˆë‹¤.

  

_**k-subtree GNN extractor.**_

  

$\varphi(u, G) = \operatorname{GNN}_G^{(k)}(u)$

  

- node uì—ì„œ ì‹œì‘í•˜ëŠ” k-subtree structureì˜ representationì„ ìƒì„±í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.

- at most 1-WL test : ìœ„ì—ì„œ ì§€ì í•œ GNNì˜ í•œê³„ì™€ ê°™ì´, ìµœëŒ€ 1WL Testì˜ í‘œí˜„ë ¥ì„ ê°€ì§‘ë‹ˆë‹¤.

- ë…¼ë¬¸ì—ì„œëŠ” ì‹¤í—˜ì„ í†µí•´ ì‘ì€ k ê°’ì´ë”ë¼ë„ over-smoothing, over-squashing issueì—†ì´ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ëŠ”ê²ƒì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.

  

_**k-subgraph GNN extractor.**_

  

$\varphi(u, G) = \sum_ {v \in  \mathcal N_k(u)} \operatorname{GNN}_G^{(k)}(v)$

  

- node uì˜ representationë§Œì„ ì‚¬ìš©í•˜ëŠ”ë°ì„œ ë‚˜ì•„ê°€ node uê°€ ì¤‘ì‹¬ì´ ë˜ëŠ” k-hop subgraphì „ì²´ì˜ representationì„ ìƒì„±í•˜ê³  í™œìš©í•©ë‹ˆë‹¤.

- node u ì˜ k-hopì´ì›ƒ $\mathcal N_k(u)$ì— ëŒ€í•´ ê° ë…¸ë“œì— GNNì„ ì ìš©í•œ node representationì„ pooling(ë…¼ë¬¸ì—ì„œëŠ” summation)í•©ë‹ˆë‹¤.

- **More powerful than 1-WL test!** ìœ„ì—ì„œ k-subtree GNN extractorì™€ì˜ ê°€ì¥ í° ì°¨ì´ì…ë‹ˆë‹¤. 

- original node representationê³¼ì˜ concatenationì„ í†µí•´ structural similarityë¿ë§Œ ì•„ë‹ˆë¼ attributed similarityë„ ë°˜ì˜í•©ë‹ˆë‹¤.

  
ì´ì™¸ì— ë‹¤ë¥¸ structure extractorë¡œ ë‹¤ìŒê³¼ ê°™ì€ ê²ƒë“¤ì„ ê³ ë ¤í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

_**Other structure extractors.**_

  

- directly learn a number of â€œhidden graphsâ€ as the â€œanchor subgraphsâ€ to represent subgraphs

- domain-specific GNNs

- non-parametric graph-kernel

  

### _2. Structure-Aware Transformer_

  

![Untitled](https://user-images.githubusercontent.com/69068083/231114106-a71006e8-a9e5-44cb-b353-578ec4e09a80.png)

  

self-attentionâ†’ skipconnection â†’ normalization layer â†’ FFN â†’ normalization layer

  

_**Augmentation on skip connection.**_

  

$x'_v = x_c +1/ \sqrt {d_v} \operatorname{SA-Attn}\left(v\right)$

  

- $d_v$ : node $v$ì˜ degree

- degree factorë¥¼ í¬í•¨í•˜ì—¬ ì—°ê²°ì´ ë§ì€ graph componentë“¤ì´ ì••ë„ì ì¸ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šë„ë¡í•©ë‹ˆë‹¤.

  

*graph-level taskë¥¼ ì§„í–‰í•´ì•¼ í•  ê²½ìš° input graphì— ë‹¤ë¥¸ ë…¸ë“œì™€ì˜ connectivityì—†ì´ virtual `[cls] `nodeë¥¼ ì¶”ê°€í•˜ê±°ë‚˜, node-level representationì„ sum/average ë“±ìœ¼ë¡œ aggregation

  

### _3. Combination with Absolute Encoding_

  

ìœ„ì˜ structure aware self-attentionì— ì¶”ê°€ë¡œ absolute encodingì„ ì¶”ê°€í•˜ê²Œ ë˜ë©´ postion-awareí•œ íŠ¹ì„±ì´ ì¶”ê°€ë˜ì–´ ê¸°ì¡´ì˜ ì •ë³´ë¥¼ ë³´ì™„í•˜ëŠ” ì—­í• ì„ í•˜ê²Œë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë””ìì¸ ì´ˆì´ìŠ¤ì˜ ì¡°í•©ì„ í†µí•´ ì„±ëŠ¥í–¥ìƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.

  

**RandomWalk PE**

  

Absolute PEë§Œ ì‚¬ìš©í•  ê²½ìš° structural biasê°€ ê³¼ë„í•˜ê²Œ ë°œìƒí•˜ì§€ ì•Šì•„ì„œ ë‘ê°œì˜ ë…¸ë“œê°€ ìœ ì‚¬í•œ local structureë¥¼ ê°–ê³  ìˆë”ë¼ë„ ë¹„ìŠ·í•œ node representationì´ ìƒì„±ë˜ëŠ”ê²ƒì„ ë³´ì¥í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤!

  

â†’ ì´ëŠ” Structural, positional signìœ¼ë¡œ ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” distanceë‚˜ Laplacian-based positional representationì´ ë…¸ë“œë“¤ ì‚¬ì´ì˜ structural simialrityë¥¼ í¬í•¨í•˜ì§€ ì•Šê¸°ë•Œë¬¸ìœ¼ë¡œ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  

> ğŸ“Œ Structural aware attenrionì€ inductive biasê°€ ë” ê°•í•˜ë”ë¼ë„ ë…¸ë“œì˜ strucutral similarityë¥¼ ì¸¡ì •í•˜ëŠ”ë° ì í•©í•˜ì—¬ ìœ ì‚¬í•œ subgraphêµ¬ì¡°ë¥¼ ê°€ì§„ ë…¸ë“œë“¤ì´ ë¹„ìŠ·í•œ embeddingì„ ê°–ê²Œí•˜ê³ , expressivityê°€ í–¥ìƒë˜ì–´ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.

  
  

### _4. Expressivity Analysis_

  

SATì—ì„œëŠ” ê°ë…¸ë“œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œí•˜ëŠ” k-subgraph GNN extractorê°€ ë„ì…ë˜ì–´ ì ì–´ë„ subgraph representationë§Œí¼ì€ expressive(More than 1WL Test)í•˜ë‹¤ëŠ” ê²ƒì„ ë³´ì¥í•©ë‹ˆë‹¤.

  
  

# **4. Experiment**

  
  
  

### _**Experiment setup**_

  

_**Dataset**_

  

- ZINC : 
	- from [Automatic chemical design using a data-driven continuous representation of molecules](https://arxiv.org/abs/1610.02415)
	- 250,000ê°œì˜ ë¶„ì ê·¸ë˜í”„êµ¬ì¡°,  with up to 38 heavy atoms
	- task is to regress the penalized `logP` (also called constrained solubility)

- CLUSTER : 
	- from [Benchmarking Graph Neural Networks](https://arxiv.org/abs/2003.00982)
	- task is semi-supervised graph clustering (node classification)

- PATTERN
	- from [Benchmarking Graph Neural Networks](https://arxiv.org/abs/2003.00982)
	- task is semi-supervised graph pattern recognition

- OGBG-PPA
	- from [Open Graph Benchmark: Datasets for Machine Learning on Graphs](https://arxiv.org/abs/2005.00687)
	- Protein-Protein Association Network
	- task is to predict new association edges given the training edges

- OGBG-CODE2
	- from [Open Graph Benchmark: Datasets for Machine Learning on Graphs](https://arxiv.org/abs/2005.00687)
	- Abstract Syntax Tree of Source Code
	- ASTë¡œ í‘œì‹œë˜ëŠ” Python ë©”ì„œë“œ ë³¸ë¬¸ê³¼ í•´ë‹¹ ë…¸ë“œ ê¸°ëŠ¥ì´ ì£¼ì–´ì§€ë©´ ë©”ì„œë“œ ì´ë¦„ì„ í˜•ì„±í•˜ëŠ” í•˜ìœ„ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” task
  

_**Baseline**_

  

-  _**GNNs**_

- GCN

- GraphSAGE

- GAT

- GIN

- PNA

- Deeper GCN

- ExpC

-  _**Transformers**_

- Original Transformer with RWPE

- Graph Transformer

- SAN

- Graphormer

- GraphTrans

  

### _**Results**_

**Table1.** SATì™€ graph regression, classification taskì˜ sotaëª¨ë¸ê³¼ ë¹„êµ

  

- ZINC datasetì˜ ê²½ìš° ì‘ì„ìˆ˜ë¡ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ì˜ë¯¸í•˜ëŠ” MAE(Mean Absolute Error), CLUSTERì™€ PATTERNì˜ ê²½ìš° ë†’ì„ìˆ˜ë¡ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ì˜ë¯¸í•˜ëŠ” Acurracyê°€ í‰ê°€ì§€í‘œë¡œ ì‚¬ìš©ë˜ì—ˆìŒ.

  

![Untitled](https://user-images.githubusercontent.com/69068083/231114155-056893f6-8d16-4a59-b43b-62c76fd482a3.png)

  

**Table2.** SATì™€ OGBë°ì´í„°ì…‹ì—ì„œì˜ sotaëª¨ë¸ ë¹„êµ

- OGB datasetì˜ ê²½ìš° ë†’ì„ìˆ˜ë¡ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ì˜ë¯¸í•˜ëŠ” Acurracy, F1 scoreê°€ í‰ê°€ì§€í‘œë¡œ ì‚¬ìš©ë˜ì—ˆìŒ.

  

![Untitled](https://user-images.githubusercontent.com/69068083/231114185-23daa0d6-bc32-4838-93e8-0a6d09a17f7e.png)

  

**Table3.** structure extractorë¡œ ì‚¬ìš©í•œ GNNê³¼ì˜ ì„±ëŠ¥ë¹„êµ. Sparse GNNì„ ëª¨ë“  ê²½ìš°ì—ì„œ outperformí•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ

  

![Untitled](https://user-images.githubusercontent.com/69068083/231114223-e6e32dfd-039b-4caa-b123-14e72e9fc867.png)

  

**Fig3.** ZINCë°ì´í„°ì…‹ì— SATì˜ ë‹¤ì–‘í•œ variantì‹¤í—˜

  

- í‰ê°€ì§€í‘œ : MAE(ë” ì‘ì€ ì§€í‘œê°€ ì¢‹ì€ ì„±ëŠ¥ì„ ì˜ë¯¸)

  

![Untitled](https://user-images.githubusercontent.com/69068083/231114263-2ea26465-c8b3-4df8-b7d4-4d329d41d97b.png)

  

1. structure extractorì—ì„œì˜ kì˜ ì˜í–¥ ë¹„êµ

- k=0ì¼ë•Œ, Absolute encodingë§Œì„ í™œìš©í•˜ëŠ” vanilla transformerë‘ ê°™ë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- k=3ì¼ë•Œ, optimal performanceë¥¼ ë³´ì„ì„ ì‹¤í—˜ì„ í†µí•´ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.

- k=4ë¥¼ ë„˜ì–´ì„œë©´ ì„±ëŠ¥ì´ ì•…í™”ë˜ëŠ”ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆëŠ”ë°, ì´ëŠ” GNNì—ì„œì˜ ì•Œë ¤ì§„ ì‚¬ì‹¤ì¸ ë” ì ì€ ìˆ˜ì˜ layerë¥¼ ê°€ì§€ëŠ” networkê°€ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²ƒê³¼ ë§ˆì°¬ê°€ì§€ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.(Oversmoothing and Oversquashing)

2. Absolute encodingì˜ ì˜í–¥ ë¹„êµ

- RandomWalkPE vs. Laplacian PE

- Structure-aware attentionì˜ ë„ì…ìœ¼ë¡œ ì¸í•œ ì„±ëŠ¥í–¥ìƒë³´ë‹¤ëŠ” ê·¸ ì •ë„ê°€ ë‚®ì•˜ì§€ë§Œ, RWPEë¥¼ ë„ì…í•  ê²½ìš° ì„±ëŠ¥ì´ ë” ì¢‹ì€ê²ƒìœ¼ë¡œ ë³´ì•˜ì„ ë•Œ, ë‘ê°€ì§€ encodingì´ ìƒí˜¸ë³´ì™„ì ì¸ ì—­í• ì„ í•œë‹¤ê³  í•´ì„í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.

3. Readout methodì˜ ì˜í–¥ ë¹„êµ

- node-level representationì„ aggregateí•  ë•Œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ readoutìœ¼ë¡œ meanê³¼ sumì„ ë¹„êµí•˜ì˜€ìŠµë‹ˆë‹¤.

- ì¶”ê°€ë¡œ `[CLS]` í† í°ì„ í†µí•´ graph-level ì •ë³´ë¥¼ poolingí•˜ëŠ” ë°©ë²•ë„ ê°™ì´ ë¹„êµí•˜ì—¬ë³´ì•˜ìŠµë‹ˆë‹¤.

- GNNì—ì„œëŠ” readout methodì˜ ì˜í–¥ì´ ë§¤ìš° ì»¸ì§€ë§Œ SATì—ì„œëŠ” ë§¤ìš° ì•½í•œ ì˜í–¥ë§Œì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.

  

# **5. Conclusion**

  

_**Strong Points.**_

  

structural infoë¥¼ graphormerì—ì„œì²˜ëŸ¼ íœ´ë¦¬ìŠ¤í‹±í•˜ê²Œ shortest path distance(SPD)ë¥¼ í™œìš©í•˜ì§€ ì•Šê³ , ê·¸ëŸ¬í•œ local infoë¥¼ ì˜ ë°°ìš°ëŠ” GNNìœ¼ë¡œ ëŒ€ì²´í•œ ì ì´ novelí•˜ë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

  

Transformerì˜ global receptive field íŠ¹ì„±ê³¼ GNNì˜ local structureíŠ¹ì„±ì´ ìƒí˜¸ë³´ì™„ì ì¸ë°,

  

encodingì— ìˆì–´ì„œë„

  

1. RWPEë¥¼ í†µí•œ positional encoding

2. k-subtree/subgraph GNNì„ í†µí•œ structure-aware attention

  

ë‘ê°€ì§€ê°€ ìƒí˜¸ë³´ì™„ì ì¸ ì—­í• ì„ í•©ë‹ˆë‹¤.

  

â†’ ê°ìê°€ ì˜ ë°°ìš°ëŠ” íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ ìƒí˜¸ë³´ì™„ì ì¸ ë‘ê°€ì§€ ë°©ë²•ë¡ ì„ ì˜ ì„ì–´ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ì—ˆê³ , ê·¸ ì´ìœ ê°€ ë‚©ë“í•˜ê¸° ì‰¬ìš´ ë…¼ë¬¸ì´ë¼ê³  ìƒê°í•©ë‹ˆë‹¤.

  
  
  

_**Weak Points.**_

  

ê·¸ë˜í”„ë°ì´í„°ì— Transformerë¥¼ ì ìš©í•œ ë‹¤ë¥¸ ë…¼ë¬¸ì˜ architectureì¸ Graphormerì—ì„œ ì‚¬ìš©í•œ SPDë§Œì˜ ì¥ì ì€ ì§ì ‘ì ìœ¼ë¡œ ì—°ê²°ë˜ì–´ìˆì§€ ì•Šì€, ì•„ì£¼ ë©€ë¦¬ì— ìœ„ì¹˜í•œ ë…¸ë“œìŒì´ë”ë¼ë„ shortest pathìƒì˜ weighted edge aggregationì„ í•˜ëŠ” ë§Œí¼ ê·¸ëŸ¬í•œ íŠ¹ì„±ì´ ë°˜ì˜ë˜ë©´ ì¢‹ì€ ê·¸ë˜í”„ êµ¬ì¡°/ ë°ì´í„°ì…‹ì—ì„œëŠ” ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. ì´ì—ë”°ë¼ ì‘ì€ k-hopì˜ subgraphë¥¼ ê³ ë ¤í•˜ëŠ” SATê°€ captureí•˜ì§€ ëª»í•˜ëŠ” ë¶€ë¶„ì´ ìˆì„ ê²ƒìœ¼ë¡œ ìƒê°ë©ë‹ˆë‹¤.

  

***

# **Author Information**

  
  

- Sujin Yun

- GSDS, KAIST


  

# **6. Reference & Additional materials**

  

- Github Implementation : [](https://github.com/BorgwardtLab/SAT)[https://github.com/BorgwardtLab/SAT](https://github.com/BorgwardtLab/SAT)

- Reference : [Structure-Aware Transformer for Graph Representation Learning](https://arxiv.org/abs/2202.03036)
