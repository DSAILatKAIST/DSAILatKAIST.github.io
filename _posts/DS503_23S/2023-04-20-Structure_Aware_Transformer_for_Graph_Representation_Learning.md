---
title:  "[ICML 2022] Structure-Aware Transformer for Graph Representation Learning"
permalink: Structure_Aware_Transformer_for_Graph_Representation_Learning.html
tags: [reviews]
---

# ****Structure-Aware Transformer for Graph Representation Learning****

_**Background before reading this review.**_

*Notation

$G = (V, E, \mathbf X)$

-   node $u \in V$
-   node attribute $x_u \in \mathcal X \subset \mathbb R^d$
-   $\mathbf X \in \mathbb R^{n \times d}$




1.  Transformers on Graph
    
    GNNì—ì„œëŠ” ê·¸ë˜í”„ êµ¬ì¡°ë¥¼ explicití•˜ê²Œ í™œìš©í•˜ëŠ” ë°˜ë©´, transformerëŠ” ë…¸ë“œì˜ attributeë¥¼ í™œìš©í•˜ì—¬ ë…¸ë“œë“¤ ì‚¬ì´ relationì„ ë‚˜íƒ€ë‚´ëŠ”ë° í™œìš©
    
    Transformer êµ¬ì„± ìš”ì†Œ
    
    1.  Self-attention module
        -   input node feature $\mathbf X$ê°€ linear projectionì„ í†µí•´ Query($\mathbf Q$), Key($\mathbf K$), Value($\mathbf V$)ë¡œ íˆ¬ì˜ë˜ê³ , ì´ë¥¼ í™œìš©í•˜ì—¬ self-attentionì„ ê³„ì‚°í•  ìˆ˜ ìˆìŒ
        -   multi-head attention
    2.  feed-forward NN
        -   self-attentionì˜ outputì´ skipconnectionì´ë‚˜ FFNë“±ì„ ê±°ì¹˜ë©´ í•˜ë‚˜ì˜ transforemer layerë¥¼ í†µê³¼í•˜ê²Œë¨
2.  Absolute encoding
    
    ê·¸ë˜í”„ì˜ ìœ„ì¹˜ì /êµ¬ì¡°ì ì¸ representationì„ input node featureì— ë”í•˜ê±°ë‚˜ concatenateí•˜ì—¬ Transformerì˜ inputìœ¼ë¡œ ì‚¬ìš© : Vanilla transformerì˜ PE
    
    Graph Transformerì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” Positional encoding methodë“¤
    
    -   Laplacian PE
    -   Random Walk PE
    
    (-) ë…¸ë“œì™€ ê·¸ ì´ì›ƒë“¤ ì‚¬ì´ì˜ structural similarityë¥¼ ë°˜ì˜í•˜ì§€ëŠ” ì•ŠìŒ
    
3.  Self-attention and kernel smoothing
    
    $\operatorname{Attn}\left(x_v\right)=\sum_ {u \in V} \frac{\kappa_ {\exp }\left(x_v, x_u\right)}{\sum_ {w \in V} \kappa_ {\exp }\left(x_v, x_w\right)} f\left(x_u\right), \forall v \in V$
    
    -   linear value function $f(x) = \mathbf W_ {\mathbf V}x$
    -   $\kappa_ {\exp }$ (non-symmetric) exponential kernel parameterized by $\mathbf W_ {\mathbf Q}, \mathbf W_ {\mathbf K}$
    
    
    $\kappa_ {\exp }\left(x, x^{\prime}\right):=\exp \left(\left\langle\mathbf{W}_ {\mathbf{Q}} x, \mathbf{W}_ {\mathbf{K}} x^{\prime}\right\rangle / \sqrt{d_ {\text {out }}}\right)$
    
    -   $\langle \cdot, \cdot\rangle$ : dotproduct
    -   í•™ìŠµê°€ëŠ¥í•œ exponential kernel
    -   (-) only position-aware, not structure-aware encoding

# **1. Problem Definition**

## _**Limitations of GNN**_

1.  limited expressiveness : ìµœëŒ€ 1-WL testì˜ í‘œí˜„ë ¥ì„ ê°€ì§
2.  Over-smoothing problem : GNN layerì˜ ìˆ˜ê°€ ì¶©ë¶„íˆ ì»¤ì§€ë©´ ëª¨ë“  node representationì´ ìƒìˆ˜ë¡œ ìˆ˜ë ´í•˜ê²Œë¨
3.  Over-squashing problem : ê·¸ë˜í”„ì˜ ìˆ˜ë§ì€ ë©”ì„¸ì§€ë“¤ì´ ê³ ì •ëœ ê¸¸ì´ì˜ ë²¡í„° í•˜ë‚˜ë¡œ ì••ì¶•ë˜ì–´ ë°œìƒí•˜ëŠ” ê·¸ë˜í”„ â€œbottleneckâ€ìœ¼ë¡œ ì¸í•´ ë©€ë¦¬ ìœ„ì¹˜í•œ ë…¸ë“œì˜ ë©”ì„¸ì§€ê°€ íš¨ìœ¨ì ìœ¼ë¡œ ì „íŒŒë˜ì§€ ì•ŠëŠ” ë¬¸ì œ

â‡’ Beyond neighborhood aggregation!

## _**Transformer**_

-   í•˜ë‚˜ì˜ self-attention layerë¥¼ í†µí•´ ê·¸ë˜í”„ë‚´ì˜ ì–´ë–¤ ë…¸ë“œìŒì´ë“ ì§€ ê·¸ ì‚¬ì´ì˜ ìƒí˜¸ì‘ìš©ì„ í™•ì¸í•  ìˆ˜ ìˆìŒ
-   GNNê³¼ ë‹¬ë¦¬ ì¤‘ê°„ ê³„ì¸µì—ì„œ structural inductive biasê°€ ë°œìƒí•˜ì§€ ì•Šì•„ GNNì˜ í‘œí˜„ë ¥ í•œê³„ë¥¼ í•´ê²°
-   graph structure infoë¥¼ ì–¼ë§ˆë‚˜ í•™ìŠµí•˜ëŠ”ì§€ input node featureì—ë§Œ structural, positional infoë¥¼ encodingí•´ ë„£ìŒ
-   ë…¸ë“œì— ëŒ€í•œ structural, positional infoë§Œ input node featureë¡œ encodingí•˜ì§€ë§Œ, ê·¸ë˜í”„ êµ¬ì¡° ìì²´ì—ì„œ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ì •ë³´ì˜ ì–‘ì´ ì œí•œë¨

> ğŸ’¡ Goal : ê·¸ë˜í”„ ë°ì´í„°ì— Transformerë¥¼ ì ì ˆíˆ ë³€í˜•í•´ ì ìš©í•˜ì—¬ ê·¸ë˜í”„ êµ¬ì¡°ë¥¼ ì˜ ë°˜ì˜í•˜ê³  ë†’ì€ í‘œí˜„ë ¥ì„ ê°€ì§€ëŠ” Achitectureë¥¼ ë””ìì¸í•˜ëŠ” ê²ƒ

# **2. Motivation**


## _**Message passing graph neural networks.**_

ìµœëŒ€ 1-WL testë¡œ ì œí•œëœ í‘œí˜„ë ¥, over-smoothing, over-quashing

## _**Limitations of existing approaches**_

-   ë…¸ë“œë“¤ ì‚¬ì´ positional relationshipë§Œ encodingí•˜ê³ , strucutral relationshipì„ ì§ì ‘ encodingí•˜ì§€ì•ŠìŒ
    -   ë…¸ë“œë“¤ ì‚¬ì´ structural similarityë¥¼ í™•ì¸í•˜ê¸°ê°€ ì–´ë µê³ , ë…¸ë“œë“¤ ì‚¬ì´ì˜ structural interactionì„ ëª¨ë¸ë§í•˜ëŠ”ë° ì‹¤íŒ¨í•˜ê²Œë¨

ex.

![Untitled](https://user-images.githubusercontent.com/69068083/231114064-4063d3f0-f895-4d7c-a932-eddbeb77de34.png)

G1ê³¼ G2ì—ì„œ ìµœë‹¨ê±°ë¦¬ë¥¼ í™œìš©í•œ positional encodingì„ í• ê²½ìš° node uì™€ vê°€ ë‹¤ë¥¸ ë…¸ë“œë“¤ì— ëŒ€í•´ ëª¨ë‘ ê°™ì€ representationì„ ê°€ì§€ê²Œë˜ì§€ë§Œ, ê·¸ë˜í”„ì˜ ì‹¤ì œ êµ¬ì¡°ëŠ” ë‹¤ë¦„ â†’ strucure awareì— ì‹¤íŒ¨

>ğŸ’¡ Message-passing GNNê³¼ Transformer architecture ê°ê°ì˜ ì¥ì ì„ ì‚´ë ¤ local, global infoë¥¼ ëª¨ë‘ ê³ ë ¤í•˜ëŠ” transformer architectureë¥¼ ì œì•ˆ

## _**Contribution of this paper**_
Q. Transformerêµ¬ì¡°ì— structural infoë¥¼ ì–´ë–»ê²Œ ì¸ì½”ë”©í• ê²ƒì¸ê°€?

A. Structure-aware self attentionë¥¼ ë„ì…í•œ Structre-Aware Transformer(SAT)

1.  reformulate the self-attention mechanism
    -   kernel smoother
    -   ì›ë˜ ë…¸ë“œ featureì— ì ìš©í•˜ëŠ” exponential ì»¤ë„ì„ í™•ì¥í•˜ì—¬ ê° ë…¸ë“œê°€ ì¤‘ì‹¬ì¸ subgraph representationì„ ì¶”ì¶œí•˜ì—¬ local structureì—ë„ ì ìš©
2.  subgraph representationë“¤ì„ ìë™ì ìœ¼ë¡œ ë§Œë“¤ì–´ë‚´ëŠ” ë°©ë²•ë¡  ì œì•ˆ
    -   ì´ë¥¼ í†µí•´ kernel smootherê°€ êµ¬ì¡°ì /íŠ¹ì„±ì  ìœ ì‚¬ì„±ì„ í¬ì°©í•  ìˆ˜ ìˆê²Œë¨
3.  GNNìœ¼ë¡œ ê·¸ë˜í”„ì˜ subgraph infoë¥¼ í¬í•¨í•˜ëŠ” node representationì„ ë§Œë“¤ì–´ ê¸°ì¡´ GNNì— ì¶”ê°€ì ì¸ êµ¬ì¡° ê°œì„  ì—†ì´ë„ ë” ë†’ì€ ì„±ëŠ¥ì„ ëƒ„
4.  Transformerì˜ ì„±ëŠ¥í–¥ìƒì´ structure-awareí•œ ì¸¡ë©´ì—ì„œ ì¼ì–´ë‚œ ê²ƒì„ ì¦ëª…í•˜ê³  absolute encodingì´ ì¶”ê°€ëœ transfoemrë³´ë‹¤ SATê°€ ì–¼ë§ˆë‚˜ interpretableí•œì§€ë¥¼ ë³´ì—¬ì¤Œ

# **3. Method**



## _**Structure-Aware Transformer**_

### _1. **Structure-Aware Self-attention**_

position-awareí•œ structural encodingì— ë…¸ë“œë“¤ ì‚¬ì´ structural similarityë¥¼ í¬í•¨í•˜ê¸° ìœ„í•´ ê° ë…¸ë“œì˜ local structureì— ê´€í•œ generalized kernelì„ ì¶”ê°€

ê° ë…¸ë“œê°€ ì¤‘ì‹¬ì´ë˜ëŠ” subgraph setì„ ì¶”ê°€í•¨ìœ¼ë¡œì¨ structure-aware attentionì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë  ìˆ˜ ìˆìŒ

$\operatorname{SA-Attn}\left(v\right):=\sum_ {u \in V} \frac{\kappa_ {\text{graph} }\left(S_G(v), S_G(u)\right)}{\sum_ {w \in V} \kappa_ {\text{graph}}\left(S_G(v), S_G(u)\right)} f\left(x_u\right)$

-   $S_G(v)$ : node feature $\mathbf X$ì™€ ì—°ê´€ëœ $v$ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œí•˜ëŠ” subgraph
-   $\kappa_ {\text{graph} }$ : subgraphìŒì„ ë¹„êµí•˜ëŠ” kernel

â‡’ attribute & structural similarity ëª¨ë‘ í‘œí˜„ ê°€ëŠ¥í•œ expressive node representation ìƒì„± â†’ table 1

â‡’ ë™ì¼í•œ subgraph êµ¬ì¡°ë¥¼ ê°€ì§€ëŠ” ê²½ìš°ì—ë§Œ permutation equivariantí•œ ì„±ì§ˆì„ ê°–ê²Œë¨

$\kappa_ {\text {graph }}\left(S_G(v), S_G(u)\right)=\kappa_ {\exp }(\varphi(v, G), \varphi(u, G))$

-   $\varphi(v, G)$ : feature $\mathbf X$ë¥¼ ê°€ì§€ëŠ” node $v$ê°€ ì¤‘ì‹¬ì— ìˆëŠ” subgraphì˜ vector representationì„ ë§Œë“¤ì–´ë‚´ëŠ” structure extractor
    -   GNNì´ë‚˜ differentiable Graph kernelë“± subgraphì˜ representationì„ ë§Œë“¤ ìˆ˜ ìˆëŠ” ì–´ëŠ ëª¨ë¸ì´ë“  ë  ìˆ˜ ìˆìŒ
    -   Task/data íŠ¹ì„±ì— ë”°ë¼ Edge attributeì„ í™œìš©í•  í•„ìš”ê°€ ìˆëŠ” ê²½ìš° ê·¸ì— ë§ëŠ”GNNì„ ì„ íƒí•˜ë©´ ë¨. ë”°ë¡œ edge attributeì„ í™œìš©í•˜ì§€ëŠ” ì•Šê³  subgraph extractorì—ì„œ í™œìš©

_**k-subtree GNN extractor.**_

$\varphi(u, G) = \operatorname{GNN}_G^{(k)}(u)$

-   node uì—ì„œ ì‹œì‘í•˜ëŠ” k-subtree structureì˜ representationìƒì„±
-   at most 1-WL test
-   ì‘ì€ k ê°’ì´ë”ë¼ë„ over-smoothing, over-squashing issueì—†ì´ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ëŠ”ê²ƒì„ í™•ì¸

_**k-subgraph GNN extractor.**_

$\varphi(u, G) = \sum_ {v \in \mathcal N_k(u)} \operatorname{GNN}_G^{(k)}(v)$

-   node uì˜ representationë§Œì„ ì‚¬ìš©í•˜ëŠ”ë°ì„œ ë‚˜ì•„ê°€ node uê°€ ì¤‘ì‹¬ì´ ë˜ëŠ” k-hop subgraphì „ì²´ì˜ representationì„ ìƒì„±í•˜ê³  í™œìš©
-   node u ì˜ k-hopì´ì›ƒ $\mathcal N_k(u)$ì— ëŒ€í•´ ê° ë…¸ë“œì— GNNì„ ì ìš©í•œ node representationì„ pooling(ë…¼ë¬¸ì—ì„œëŠ” summation)
-   More powerful than 1-WL test
-   original node representationê³¼ì˜ concatenationì„ í†µã… structural similarityë¿ë§Œ ì•„ë‹ˆë¼ attributed similarityë„ ë°˜ì˜

_**Other structure extractors.**_

-   directly learn a number of â€œhidden graphsâ€ as the â€œanchor subgraphsâ€ to represent subgraphs
-   domain-specific GNNs
-   non-parametric graph-kernel

### _2. Structure-Aware Transformer_

![Untitled](https://user-images.githubusercontent.com/69068083/231114106-a71006e8-a9e5-44cb-b353-578ec4e09a80.png)

self-attentionâ†’ skipconnection â†’ normalization layer â†’ FFN â†’ normalization layer

_**Augmentation on skip connection.**_

$x'_v = x_c +1/ \sqrt {d_v} \operatorname{SA-Attn}\left(v\right)$

-   $d_v$ : node $v$ì˜ degree
-   degree factorë¥¼ í¬í•¨í•˜ì—¬ ì—°ê²°ì´ ë§ì€ graph componentë“¤ì´ ì••ë„ì ì¸ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šë„ë¡í•¨

*graph-level taskë¥¼ ì§„í–‰í•´ì•¼ í•  ê²½ìš° input graphì— ë‹¤ë¥¸ ë…¸ë“œì™€ì˜ connectivityì—†ì´ virtual `[cls] `nodeë¥¼ ì¶”ê°€í•˜ê±°ë‚˜, node-level representationì„ sum/average ë“±ìœ¼ë¡œ aggregation

### _3. Combination with Absolute Encoding_

ìœ„ì˜ structure aware self-attentionì— ì¶”ê°€ë¡œ absolute encodingì„ ì¶”ê°€í•˜ê²Œ ë˜ë©´ postion-awareí•œ íŠ¹ì„±ì´ ì¶”ê°€ë˜ì–´ ê¸°ì¡´ì˜ ì •ë³´ë¥¼ ë³´ì™„í•˜ëŠ” ì—­í• ì„ í•˜ê²Œëœë‹¤. ì´ëŸ¬í•œ ì¡°í•©ì„ í†µí•´ ì„±ëŠ¥í–¥ìƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤.

RandomWalk PE

Absolute PEë§Œ ì‚¬ìš©í•  ê²½ìš° structural biasê°€ ê³¼ë„í•˜ê²Œ ë°œìƒí•˜ì§€ ì•Šì•„ì„œ ëˆ„ê°œì˜ ë…¸ë“œê°€ ìœ ì‚¬í•œ local structureë¥¼ ê°–ê³  ìˆë”ë¼ë„ ë¹„ìŠ·í•œ node representationì´ ìƒì„±ë˜ëŠ”ê²ƒì„ ë³´ì¥í•˜ê¸° ì–´ë µë‹¤!

â†’ Structural, positional signìœ¼ë¡œ ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” distanceë‚˜ Laplacian-based positional representationì´ ë…¸ë“œë“¤ ì‚¬ì´ì˜ structural simialrityë¥¼ í¬í•¨í•˜ì§€ ì•Šê¸°ë•Œë¬¸

> ğŸ“Œ Structural aware attenrionì€ inductive biasê°€ ë” ê°•í•˜ë”ë¼ë„ ë…¸ë“œì˜ strucutral similarityë¥¼ ì¸¡ì •í•˜ëŠ”ë° ì í•©í•˜ì—¬ ìœ ì‚¬í•œ subgraphêµ¬ì¡°ë¥¼ ê°€ì§„ ë…¸ë“œë“¤ì´ ë¹„ìŠ·í•œ embeddingì„ ê°–ê²Œí•˜ê³ , expressivityê°€ í–¥ìƒë˜ì–´ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„


### _4. Expressivity Analysis_

SATì—ì„œëŠ” ê°ë…¸ë“œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œí•˜ëŠ” k-subgraph GNN extractorê°€ ë„ì…ë˜ì–´ ì ì–´ë„ subgraph representationë§Œí¼ì€ expressiveí•˜ë‹¤ëŠ” ê²ƒì„ ë³´ì¥


# **4. Experiment**



### _**Experiment setup**_

_**Dataset**_

-   ZINC
-   CLUSTER
-   PATTERN
-   OGBG-PPA
-   OGBG-CODE2

_**Baseline**_

-   _**GNNs**_
    -   GCN
    -   GraphSAGE
    -   GAT
    -   GIN
    -   PNA
    -   Deeper GCN
    -   ExpC
-   _**Transformers**_
    -   Original Transformer with RWPE
    -   Graph Transformer
    -   SAN
    -   Graphormer
    -   GraphTrans

### _**Results**_
**Table1.** SATì™€ graph regression, classification taskì˜ sotaëª¨ë¸ê³¼ ë¹„êµ

- ZINC datasetì˜ ê²½ìš° ì‘ì„ìˆ˜ë¡ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ì˜ë¯¸í•˜ëŠ” MAE(Mean Absolute Error), CLUSTERì™€ PATTERNì˜ ê²½ìš° ë†’ì„ìˆ˜ë¡ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ì˜ë¯¸í•˜ëŠ” Acurracyê°€ í‰ê°€ì§€í‘œë¡œ ì‚¬ìš©ë˜ì—ˆìŒ.

![Untitled](https://user-images.githubusercontent.com/69068083/231114155-056893f6-8d16-4a59-b43b-62c76fd482a3.png)

**Table2.** SATì™€ OGBë°ì´í„°ì…‹ì—ì„œì˜ sotaëª¨ë¸ ë¹„êµ
- OGB datasetì˜ ê²½ìš° ë†’ì„ìˆ˜ë¡ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ì˜ë¯¸í•˜ëŠ” Acurracy, F1 scoreê°€ í‰ê°€ì§€í‘œë¡œ ì‚¬ìš©ë˜ì—ˆìŒ.

![Untitled](https://user-images.githubusercontent.com/69068083/231114185-23daa0d6-bc32-4838-93e8-0a6d09a17f7e.png)

**Table3.** structure extractorë¡œ ì‚¬ìš©í•œ GNNê³¼ì˜ ì„±ëŠ¥ë¹„êµ. Sparse GNNì„ ëª¨ë“  ê²½ìš°ì—ì„œ outperformí•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ

![Untitled](https://user-images.githubusercontent.com/69068083/231114223-e6e32dfd-039b-4caa-b123-14e72e9fc867.png)

**Fig3.** ZINCë°ì´í„°ì…‹ì— SATì˜ ë‹¤ì–‘í•œ variantì‹¤í—˜

- í‰ê°€ì§€í‘œ : MAE(ë” ì‘ì€ ì§€í‘œê°€ ì¢‹ì€ ì„±ëŠ¥ì„ ì˜ë¯¸) 

![Untitled](https://user-images.githubusercontent.com/69068083/231114263-2ea26465-c8b3-4df8-b7d4-4d329d41d97b.png)

1.  structure extractorì—ì„œì˜ kì˜ ì˜í–¥ ë¹„êµ
    -   k=0ì¼ë•Œ, Absolute encodingë§Œì„ í™œìš©í•˜ëŠ” vanilla transformerë‘ ê°™ë‹¤ê³  ë³¼ ìˆ˜ ìˆìŒ
    -   k=3ì¼ë•Œ, optimal performanceë¥¼ ë³´ì„ì„ í™•ì¸
    -   k=4ë¥¼ ë„˜ì–´ì„œë©´ ì„±ëŠ¥ì´ ì•…í™”ë˜ëŠ”ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆëŠ”ë°, ì´ëŠ” GNNì—ì„œì˜ ì•Œë ¤ì§„ ì‚¬ì‹¤ì¸ ë” ì ì€ ìˆ˜ì˜ layerë¥¼ ê°€ì§€ëŠ” networkê°€ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²ƒê³¼ ë§ˆì°¬ê°€ì§€ë¼ê³  í•  ìˆ˜ ìˆìŒ
2.  Absolute encodingì˜ ì˜í–¥ ë¹„êµ
    -   RWPE vs. Laplacian PE
    -   Structure-aware attentionì˜ ë„ì…ìœ¼ë¡œ ì¸í•œ ì„±ëŠ¥í–¥ìƒë³´ë‹¤ëŠ” ê·¸ ì •ë„ê°€ ë‚®ì•˜ì§€ë§Œ, RWPEë¥¼ ë„ì…í•  ê²½ìš° ì„±ëŠ¥ì´ ë” ì¢‹ì€ê²ƒìœ¼ë¡œ ë³´ì•˜ì„ ë•Œ, ë‘ê°€ì§€ encodingì´ ìƒí˜¸ë³´ì™„ì ì¸ ì—­í• ì„ í•œë‹¤ê³  í•´ì„í•  ìˆ˜ ìˆìŒ
3.  Readout methodì˜ ì˜í–¥ ë¹„êµ
    -   node-level representationì„ aggregateí•  ë•Œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ readoutdìœ¼ë¡œ meanê³¼ sumì„ ë¹„êµí•˜ì˜€ìŒ
    -   ì¶”ê°€ë¡œ `[CLS]` í† í°ì„ í†µí•´ graph-level ì •ë³´ë¥¼ poolingí•˜ëŠ” ë°©ë²•ë„ ê°™ì´ ë¹„êµí•˜ì—¬ë³´ì•˜ìŒ
    -   GNNì—ì„œëŠ” readout methodì˜ ì˜í–¥ì´ ë§¤ìš° ì»¸ì§€ë§Œ SATì—ì„œëŠ” ë§¤ìš° ì•½í•œ ì˜í–¥ë§Œì„ í™•ì¸í•¨.

# **5. Conclusion**

_**Strong Points.**_

structural infoë¥¼ graphormerì—ì„œì²˜ëŸ¼ íœ´ë¦¬ìŠ¤í‹±í•˜ê²Œ shortest path distance(SPD)ë¥¼ í™œìš©í•˜ì§€ ì•Šê³ , ê·¸ëŸ¬í•œ local infoë¥¼ ì˜ ë°°ìš°ëŠ” GNNìœ¼ë¡œ ëŒ€ì²´í•œ ì ì´ novelí•˜ë‹¤ê³  í•  ìˆ˜ ìˆìŒ

Transformerì˜ global receptive field íŠ¹ì„±ê³¼ GNNì˜ local structureíŠ¹ì„±ì´ ìƒí˜¸ë³´ì™„ì 

encodingì— ìˆì–´ì„œë„

1.  RWPEë¥¼ í†µí•œ positional encoding
2.  k-subtree/subgraph GNNì„ í†µí•œ structure-aware attention

ë‘ê°€ì§€ê°€ ìƒí˜¸ë³´ì™„ì ì¸ ì—­í• ì„ í•¨

â†’ ê°ìê°€ ì˜ ë°°ìš°ëŠ” íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ ìƒí˜¸ë³´ì™„ì ì¸ ë‘ê°€ì§€ ë°©ë²•ë¡ ì„ ì˜ ì„ì–´ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ì—ˆê³ , ê·¸ ì´ìœ ê°€ ë‚©ë“í•˜ê¸° ì‰¬ì›€



_**Weak Points.**_

ê·¸ë˜í”„ë°ì´í„°ì— Transformerë¥¼ ì ìš©í•œ ë‹¤ë¥¸ ë…¼ë¬¸ì˜ architectureì¸ Graphormerì—ì„œ ì‚¬ìš©í•œ SPDë§Œì˜ ì¥ì  : ì§ì ‘ì ìœ¼ë¡œ ì—°ê²°ë˜ì–´ìˆì§€ ì•Šì€, ì•„ì£¼ ë©€ë¦¬ì— ìœ„ì¹˜í•œ ë…¸ë“œìŒì´ë”ë¼ë„ shortest pathìƒì˜ weighted edge aggregationì„ í•˜ëŠ” ë§Œí¼ ê·¸ëŸ¬í•œ íŠ¹ì„± ë°˜ì˜ë˜ë©´ ì¢‹ì€ ê·¸ë˜í”„ êµ¬ì¡°/ ë°ì´í„°ì…‹ì—ì„œëŠ” SATê°€ captureí•˜ì§€ ëª»í•˜ëŠ” ë¶€ë¶„ì´ ìˆì„ ê²ƒ

***
# **Author Information**


-   Dexiong Chen
    
    -   Department of Biosystems Science and Engineering, ETH Zurich, Switzerland.
    -   SIB Swiss Institute of Bioinformatics, Switzerland.
-   Leslie O'Bray
    
    -   Department of Biosystems Science and Engineering, ETH Zurich, Switzerland.
    -   SIB Swiss Institute of Bioinformatics, Switzerland.
-   Karsten Borgwardt
    
    -   Department of Biosystems Science and Engineering, ETH Zurich, Switzerland.
    -   SIB Swiss Institute of Bioinformatics, Switzerland.

# **6. Reference & Additional materials**

-   Github Implementation : [](https://github.com/BorgwardtLab/SAT)[https://github.com/BorgwardtLab/SAT](https://github.com/BorgwardtLab/SAT)
-   Reference : [Structure-Aware Transformer for Graph Representation Learning](https://arxiv.org/abs/2202.03036)
