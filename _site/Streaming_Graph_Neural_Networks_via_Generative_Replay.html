<!DOCTYPE html>
<html>
<head>
    <meta name="google-site-verification" content="1tAPTdgw0-t8G2Bya463OpBtYUbj9Um93gfnsowYKLw" />
    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-CV4PTXQSTW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-CV4PTXQSTW');
</script>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="">
<meta name="keywords" content="reviews,  ">
<title>[KDD 2022] Streaming Graph Neural Networks via Generative Replay | Awesome Reviews</title>
<link rel="stylesheet" href="css/syntax.css">

<link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
<!--<link rel="stylesheet" type="text/css" href="css/bootstrap.min.css">-->
<link rel="stylesheet" href="css/modern-business.css">
<!-- Latest compiled and minified CSS -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<link rel="stylesheet" href="css/customstyles.css">
<link rel="stylesheet" href="css/boxshadowproperties.css">
<!-- most color styles are extracted out to here -->
<link rel="stylesheet" href="css/theme-blue.css">

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="js/jquery.navgoco.min.js"></script>


<!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<!-- Anchor.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.2.0/anchor.min.js"></script>
<script src="js/toc.js"></script>
<script src="js/customscripts.js"></script>

<link rel="shortcut icon" href="images/favicon.ico">

<!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
<![endif]-->

<link rel="alternate" type="application/rss+xml" title="DSAILatKAIST.github.io" href="http://dsailatkaist.github.io/feed.xml">


	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	TeX: {
		equationNumbers: {
		autoNumber: "AMS"
		}
	},
	tex2jax: {
		inlineMath: [ ['$', '$'] ],
		displayMath: [ ['$$', '$$'] ],
		processEscapes: true,
		}
	});
	MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
		alert("Math Processing Error: "+message[1]);
	});
	MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
		alert("Math Processing Error: "+message[1]);
	});
</script>

<script type="text/javascript" async
	src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



<script type="text/javascript" async
 src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>




    <script>
        $(document).ready(function() {
            // Initialize navgoco with default options
            $("#mysidebar").navgoco({
                caretHtml: '',
                accordion: true,
                openClass: 'active', // open
                save: false, // leave false or nav highlighting doesn't work right
                cookie: {
                    name: 'navgoco',
                    expires: false,
                    path: '/'
                },
                slide: {
                    duration: 400,
                    easing: 'swing'
                }
            });

            $("#collapseAll").click(function(e) {
                e.preventDefault();
                $("#mysidebar").navgoco('toggle', false);
            });

            $("#expandAll").click(function(e) {
                e.preventDefault();
                $("#mysidebar").navgoco('toggle', true);
            });

        });

    </script>
    <script>
        $(function () {
            $('[data-toggle="tooltip"]').tooltip()
        })
    </script>
    <script>
        $(document).ready(function() {
            $("#tg-sb-link").click(function() {
                $("#tg-sb-sidebar").toggle();
                $("#tg-sb-content").toggleClass('col-md-9');
                $("#tg-sb-content").toggleClass('col-md-12');
                $("#tg-sb-icon").toggleClass('fa-toggle-on');
                $("#tg-sb-icon").toggleClass('fa-toggle-off');
            });
        });
    </script>
    


	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	TeX: {
		equationNumbers: {
		autoNumber: "AMS"
		}
	},
	tex2jax: {
		inlineMath: [ ['$', '$'] ],
		displayMath: [ ['$$', '$$'] ],
		processEscapes: true,
		}
	});
	MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
		alert("Math Processing Error: "+message[1]);
	});
	MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
		alert("Math Processing Error: "+message[1]);
	});
</script>

<script type="text/javascript" async
	src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



<script type="text/javascript" async
 src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>


</head>
<body>
<!-- Navigation -->
<nav class="navbar navbar-inverse navbar-static-top">
    <div class="container topnavlinks">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="fa fa-home fa-lg navbar-brand" href="index.html">&nbsp;<span class="projectTitle"> Awesome Reviews</span></a>
        </div>
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                <!-- toggle sidebar button -->
                <li><a id="tg-sb-link" href="#"><i id="tg-sb-icon" class="fa fa-toggle-on"></i> Nav</a></li>
                <!-- entries without drop-downs appear here -->




                
                
                
                <li><a href="introduction">Introduction</a></li>
                
                
                
                <li><a href="https://statistics.kaist.ac.kr/" target="_blank" rel="noopener">KAIST ISysE</a></li>
                
                
                
                <!-- entries with drop-downs appear here -->
                <!-- conditional logic to control which topnav appears for the audience defined in the configuration file.-->
                
                
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">Reviews<b class="caret"></b></a>
                    <ul class="dropdown-menu">
                        
                        
                        <li><a href="reviews_kse801_2022.html">KSE801 (2022)</a></li>
                        
                        
                        
                        <li><a href="reviews_DS503_2023.html">DS503 (2023)</a></li>
                        
                        
                    </ul>
                </li>
                
                
                
			<li>



  <a class="email" title="Submit feedback" href="#" onclick="javascript:window.location='mailto:swkim@kaist.ac.kr?subject=Question about reviews feedback&body=I have some feedback about the [KDD 2022] Streaming Graph Neural Networks via Generative Replay page: ' + window.location.href;"><i class="fa fa-envelope-o"></i> Feedback</a>

</li>



		
                <!--comment out this block if you want to hide search-->
                <li>
                    <!--start search-->
                    <div id="search-demo-container">
                        <input type="text" id="search-input" placeholder="search...">
                        <ul id="results-container"></ul>
                    </div>
                    <script src="js/jekyll-search.js" type="text/javascript"></script>
                    <script type="text/javascript">
                            SimpleJekyllSearch.init({
                                searchInput: document.getElementById('search-input'),
                                resultsContainer: document.getElementById('results-container'),
                                dataSource: 'search.json',
                                searchResultTemplate: '<li><a href="{url}" title="[KDD 2022] Streaming Graph Neural Networks via Generative Replay">{title}</a></li>',
                    noResultsText: 'No results found.',
                            limit: 10,
                            fuzzy: true,
                    })
                    </script>
                    <!--end search-->
                </li>
            </ul>
        </div>
        </div>
        <!-- /.container -->
</nav>



<!-- Page Content -->
<div class="container">
  <div id="main">
    <!-- Content Row -->
    <div class="row">
        
        
            <!-- Sidebar Column -->
            <div class="col-md-3" id="tg-sb-sidebar">
                

<ul id="mysidebar" class="nav">
  <li class="sidebarTitle"> </li>
  
  
  
  <li>
      <a title="How to contribute" href="#">How to contribute</a>
      <ul>
          
          
          
          <li><a title="How to contribute?" href="how_to_contribute.html">How to contribute?</a></li>
          
          
          
          
          
          
          <li><a title="Review template (Example)" href="template.html">Review template (Example)</a></li>
          
          
          
          
      </ul>
   </li>
     
      
  
  <li>
      <a title="Reviews" href="#">Reviews</a>
      <ul>
          
          
          
          <li><a title="KSE801 (2022F)" href="reviews_kse801_2022.html">KSE801 (2022F)</a></li>
          
          
          
          
          
          
          <li><a title="DS503 (2023S)" href="reviews_DS503_2023.html">DS503 (2023S)</a></li>
          
          
          
          
      </ul>
   </li>
     
      
      
      <!-- if you aren't using the accordion, uncomment this block:
         <p class="external">
             <a href="#" id="collapseAll">Collapse All</a> | <a href="#" id="expandAll">Expand All</a>
         </p>
         -->
</ul>

<!-- this highlights the active parent class in the navgoco sidebar. this is critical so that the parent expands when you're viewing a page. This must appear below the sidebar code above. Otherwise, if placed inside customscripts.js, the script runs before the sidebar code runs and the class never gets inserted.-->
<script>$("li.active").parents('li').toggleClass("active");</script>



            </div>
            
        

        <!-- Content Column -->
        <div class="col-md-9" id="tg-sb-content">
            <script type="text/x-mathjax-config">
MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/math...">
</script>
<article class="post" itemscope itemtype="https://schema.org/BlogPosting">

    <header class="post-header">
        <h1 class="post-title" itemprop="name headline">[KDD 2022] Streaming Graph Neural Networks via Generative Replay</h1>
        <p class="post-meta"><time datetime="2022-11-12T00:00:00+09:00" itemprop="datePublished">Nov 12, 2022</time> /
            
            
            
            
            

        </p>


    </header>

    <div class="post-content" itemprop="articleBody">

        

        <h1 id="streaming-graph-neural-networks-with-generative-replay">Streaming Graph Neural Networks with Generative Replay</h1>

<h2 id="1-problem-definition"><strong>1. Problem Definition</strong></h2>

<blockquote>
  <p><strong>Continual Learning에서 사용할 Replay Buffer를 Generative Model을 활용시켜 생성한다!</strong></p>
</blockquote>

<p>본 논문은 Continual Learning을 Graph Neural Network(<code class="language-plaintext highlighter-rouge">GNN</code>)과 접목시킴과 동시에 이에 사용할 <code class="language-plaintext highlighter-rouge">Replay buffer</code>를 Generative Model을 사용해 생성합니다.</p>

<p>이미 Continual Learning에 Generative Model을 사용한 연구는 예전에도 있었으나 <a href="https://proceedings.neurips.cc/paper/2017/hash/0efbe98067c6c73dba1250d2beaa81f9-Abstract.html">(여기)</a>, 이는 이미지 데이터를 주로 targeting한 반면에 지금 다루는 논문은 Graph domain의 데이터에 적용, 그에 맞는 특성(structure of graph)를 고려했다는 점에서 novelty가 있습니다.</p>

<blockquote>
  <p><strong>Continual Learning이란?</strong></p>
</blockquote>

<p>Continaul Learning은 Lifelong Learning, Incremental Learning이라고도 불리며, 과거의 정보를 최대한 유지하며 새로운 정보를 학습하는 방법론입니다.</p>

<p>예를 들어, 인간이 ‘강아지’ 라는 지식을 알고 있는 상태로, ‘고양이’라는 지식을 새로 습득했을 때, ‘강아지’를 잊지 않고 ‘강아지’와 ‘고양이’를 구별해 낼 수 있는 것 처럼, 지속적으로 들어오는 새로운 데이터를 학습함과 동시에 이전에 학습되었던 데이터를 잊지 않도록 인공지능을 설계하는 것이 Continual Learning의 목적입니다.</p>

<blockquote>
  <p><strong>Catastrophic Forgetting이란?</strong></p>

  <p>Continual Learning에서, 새로운 데이터가 들어옴에 따라 이전에 학습했던 데이터의 정보를 망각하는 현상을 <code class="language-plaintext highlighter-rouge">Catastrophic Forgetting</code>이라고 합니다. 아래 그림을 보시겠습니다.</p>
</blockquote>

<p><img src="https://user-images.githubusercontent.com/99710438/194873406-84f4a722-c562-4c39-adec-5ecc498a498f.png" alt="image" /></p>

<p>그림에서 볼 수 있듯이, Task 1에서는 파란색 node들을 구별하도록 학습합니다. Task 2에서는 새로운 보라색 node가 추가되면서 파란색과 보라색을 포함해 학습시키고, Task 3에서는 빨간색의 새로운 node가 추가되면서 새롭게 학습이 진행됩니다. 이 과정이 Continual Learning 입니다.</p>

<p>그리고 Task가 진행됨에 따라 이전 Task에서 학습했던 node들에 대한 예측 성능이 떨어지는 것을 볼 수 있습니다.</p>

<p>예를 들어, Task 1에서 파란 node들을 분류하는 데에는 95%의 성능을 보였으나, Task 2에서는 55%로 줄었고, Task 2에서 학습했던 보라색 node들도 Task 3에서의 성능은 현저히 줄어든 것을 볼 수 있습니다. 이러한 현상이 앞서 말씀드린 <code class="language-plaintext highlighter-rouge">Catastrophic Forgetting</code>입니다.</p>

<p>Continual Learning 중 Replay approach는 이전 Task에서 학습했던 데이터 중 <strong>일부</strong>만을 sampling하여 이후 Task에 사용합니다. 전체 데이터를 계속해서 누적하여 학습하면 효율이 좋지 않기 때문이죠. 이렇게 sampling되어 이후 Task에서 같이 학습될 data를 <code class="language-plaintext highlighter-rouge">Replay buffer</code>라고 부릅니다. 이 과정에서 <code class="language-plaintext highlighter-rouge">Catastrophic Forgetting</code>현상이 일어나게 되는데, 이 현상을 최소화 하도록 이전 Task에서 학습했던 데이터를 잘 대표하는 데이터를 sampling하는 것이 관건입니다.</p>

<h2 id="2-motivation"><strong>2. Motivation</strong></h2>

<blockquote>
  <p><strong>기존 Replay based Continual Learning 방법들은 storage limitation을 가진다!</strong></p>
</blockquote>

<p>앞서 언급한대로, Replay based Continual Learning은 이전에 학습했던 데이터 중 전체 데이터를 잘 represent하는 <strong>일부</strong>의 데이터를 sampling해서 이후 Task에서 등장하는 데이터와 같이 학습시킵니다.</p>

<p>하지만 이럴 경우 Task가 계속해서 진행됨에 따라, 각 Task에서 아무리 적은 데이터를 sampling한다 해도 기억해야하는 데이터가 지속적으로 늘어나고, 결국에는 memory를 다 사용해버리는 일이 생길 것입니다.</p>

<p>저자들은 이런 한계를 지적하며 Generative Model을 도입해 <code class="language-plaintext highlighter-rouge">Replay buffer</code>에 데이터를 누적시키는 것이 아니라, Task가 시작될 때 마다 필요한 만큼 Generative Model로 <code class="language-plaintext highlighter-rouge">Replay buffer</code>를 생성해 학습을 진행하고자 합니다.</p>

<blockquote>
  <p><strong>기존 Replay based Continual Learning 방법들은 온전한 graph distribution을 보존하지 못한다!</strong></p>
</blockquote>

<p>Image 도메인과 달리, Graph 도메인의 데이터를 다룰 때는 각 데이터의 성질(feature)뿐 만 아니라 그래프의 전체적인 structure도 고려해야 합니다. 어떤 node가 어떤 node와 연결되어 있으며, 연결된 node들은 어떤 특성을 가지고 있는지까지 종합적으로 고려되어야 한다는 것이죠.</p>

<p>저자들은 Continual Learning 중에서 <code class="language-plaintext highlighter-rouge">Replay buffer</code>를 생성할 때 각 node들의 feature만 고려될 뿐, 전체적인 그래프의 distribution(structure)이 보존되지 못한다고 주장합니다. 이는 <code class="language-plaintext highlighter-rouge">Graph Neural Network</code>가 학습될 때 성능 저하를 야기하는 가장 큰 문제 중 하나로, 이 논문에서는 Generative Model을 통해 이러한 topological information까지 저장하도록 하는 것을 목표로 합니다.</p>

<h2 id="3-method"><strong>3. Method</strong></h2>

<blockquote>
  <p><strong>Preliminaries: <code class="language-plaintext highlighter-rouge">GNN</code></strong></p>
</blockquote>

<p>논문에서 제안한 방법론을 이해하기 위해서는 <code class="language-plaintext highlighter-rouge">GNN</code>의 개념을 알고 있어야 합니다.</p>

<p>본 리뷰에서는 간단하게 소개를 하겠습니다.</p>

<p>\(N\)개의 노드를 가진 그래프 \(\mathcal{G}= \lbrace \mathcal{V},\mathcal{E} \rbrace\)가 주어지고, \(X = \lbrace x_{1}, x_{2}, ..., x_{N} \rbrace\) 을 node feature의 집합이라고 하고, \(A\)를 node들의 관계를 표현하는 adjacency matrix라고 하겠습니다.</p>

<p>\(l-th\) hidden layer에서의 \(v_{i}\)의 hidden representation을 \(h_{i}^{(l)}\) 이라고 할 때, 이 \(h_{i}^{(l)}\)는 다음과 같이 계산됩니다:</p>

<p>$h_{i}^{(l)} = \sigma(\sum_{j \subset \mathcal{N}(i)} \mathcal{A_{ij}}h_{j}^{(l-1)}W^{(l)})$</p>

<p>이 때, \(\mathcal{N}(i)\) 는 \(v_{i}\)의 neighbors를 의미하고, \(\sigma ( \bullet )\)는 activation function, \(W^{(l)}\)은 \(l-th\) layer의 transform matrix를 나타냅니다.</p>

<p>\(h_{i}^{(0)}\)은 node \(v_{i}\)의 input feature를 나타내고, \(\mathcal{A}\)는 neighbors의 aggregation strategy이며, <code class="language-plaintext highlighter-rouge">GNN</code>의 핵심 중 하나입니다.</p>

<p>본 논문에서는 다양한 <code class="language-plaintext highlighter-rouge">GNN</code>중 <code class="language-plaintext highlighter-rouge">GraphSAGE</code>라는 모델을 사용하는데, 이 <code class="language-plaintext highlighter-rouge">GraphSAGE</code>의 \(k\)번째 layer는 다음과 같이 정의됩니다:</p>

<p>$h_{v}^{k} = \sigma(W^k \cdot MEAN( \lbrace h_v^{k-1} \rbrace \cup \lbrace h_u^{k-1}, \forall u \in \mathcal{N}(v)\rbrace)$</p>

<blockquote>
  <p><strong>Problem Definition</strong></p>
</blockquote>

<p>Continual Learning setting에서, 데이터는 그래프의 형태를 띠고 연속적으로 들어옵니다. 이는 다음과 같이 표현이 가능합니다.</p>

<p>$\mathcal{G} = (\mathcal{G}^1, \mathcal{G}^2, …, \mathcal{G}^T)$$</p>

<p>where \(\mathcal{G^t} = \mathcal{G}^{t-1}+\Delta \mathcal{G}^t\)</p>

<p>여기서 \(\mathcal{G} = (A^t, X^t)\) 는 attributed graph at time \(t\)이고, \(\Delta \mathcal{G} = (\Delta A^t , \Delta X^t)\)는 time \(t\)에서의 node attribute와 network의 structure의 변화량을 나타냅니다.</p>

<p>이 때 Streaming <code class="language-plaintext highlighter-rouge">GNN</code>은 traditional <code class="language-plaintext highlighter-rouge">GNN</code>을 streaming setting으로 확장한 것이 됩니다. Streaming graph가 있을 때, Continual Learning의 목적은 \((\theta^1, \theta^2, ..., \theta^T)\) 를 배우는 것입니다. 이 때 \(\theta^t\) 는 time \(t\) 에서의 <code class="language-plaintext highlighter-rouge">GNN</code> parameter를 의미합니다.</p>

<blockquote>
  <p><strong>Model Framework</strong></p>
</blockquote>

<p>저자들은 이 논문에서 <code class="language-plaintext highlighter-rouge">SGNN-GR</code>이라는 방법론을 제시합니다. 모델 구조는 아래 그림과 같습니다.</p>

<p><img src="https://user-images.githubusercontent.com/99710438/194887946-3f736cc4-1c2c-47ca-97aa-4516da0ae42e.png" alt="image" /></p>

<p><strong>이 그림을 참고해 방법론을 개괄적으로 설명하자면, 아래와 같습니다.</strong></p>

<ul>
  <li>새로운 task가 오면 <code class="language-plaintext highlighter-rouge">GAN</code>으로 sequence를 생성(이게 <code class="language-plaintext highlighter-rouge">replay buffer</code>가 되는 것이죠)해서 이번 task의 그래프와 <strong>같이</strong> <code class="language-plaintext highlighter-rouge">GNN</code>을 학습합니다.</li>
  <li>이러면 이 <code class="language-plaintext highlighter-rouge">GNN</code>은 <strong>현재 그래프를 학습함과 동시에 이전의 정보까지 기억</strong>하게 될 것입니다.</li>
  <li>또한 이번 task에서 새롭게 생성된 node들과 그것들로부터 영향받은 node들을 다시 <code class="language-plaintext highlighter-rouge">GAN</code>의 input으로 주어 학습시킵니다.</li>
  <li>이러면 다음 task에서는 <code class="language-plaintext highlighter-rouge">GAN</code>은 더 양질의 <code class="language-plaintext highlighter-rouge">replay buffer</code>를 만들어 낼 수 있을 것입니다.</li>
</ul>

<p>지금부터 <code class="language-plaintext highlighter-rouge">SGNN-GR</code>의 자세한 내용을 살펴보겠습니다. 위 그림을 잘 참고하면서 아래 설명을 따라오시기 바랍니다.</p>

<p>가장 먼저, Streaming GNN의 time \(t\)에서의 loss는 다음과 같습니다.</p>

<p>$\mathcal{L}(\theta^t ; \mathcal{G}^t) = \mathcal{L}(\theta^t ; \mathcal{G}_A^t) + \lambda \mathcal{R} (\theta^{t-1} ; \mathcal{G}_S^t)$</p>

<p>우변의 첫 항은 incremental learning에 관한 것이고, 두 번째 항은 historical knowledge에 관한 것입니다.</p>

<p>본 논문에서 \(\mathcal{G}_A^t\) 는 graph의 affected part, \(\mathcal{G}_S^t\) 는 graph의 stable part로 정의합니다.</p>

<blockquote>
  <p>여기서 affected part는 계속해서 새로운 data가 들어옴에 따라 영향을 받는(변화된) part, 즉 새롭게 학습해야하는 part라고 생각하면 되고, stable part는 이전에 학습했던 part, 즉 변하지는 않았지만 기존의 지식을 잊지 않기 위해(<code class="language-plaintext highlighter-rouge">Catastrophic forgetting</code>을 방지하기 위해) 지속적으로 학습시켜야하는 part라고 생각하면 됩니다.</p>
</blockquote>

<p>이 때 \(\Delta \mathcal{G}^t \subset \mathcal{G}_A^t\) 이고 \(\mathcal{G}_S^t \subset \mathcal{G}^{t-1}\) 입니다. 몇몇 node들이 새롭게 바뀐 node들에 대해서 영향을 받는 것입니다.</p>

<p>각 time step에서 모델은 main model(<code class="language-plaintext highlighter-rouge">GNN</code>)과 Generative Model로 구성됩니다. 위 그림에서 확인할 수 있듯이, Generative Model은 \(\mathcal{G}_ A^t\)에서 바뀐 node들과 \(\mathcal{G}^{t-1}\)에서의 replayed node를 training data로 받습니다. 이 때 replayed node는 이전 time step의 Generative Model로부터 나옵니다.</p>

<p>이 논문에서는 Generative Model로 <code class="language-plaintext highlighter-rouge">GAN</code>을 사용하였습니다. <code class="language-plaintext highlighter-rouge">GAN</code>에 대한 자세한 설명은 생략하며, 원 논문은 <a href="https://dl.acm.org/doi/abs/10.1145/3422622">여기</a>를 참고하시기 바랍니다.</p>

<p><code class="language-plaintext highlighter-rouge">GNN</code> 모델도 changed node와 replayed node를 똑같이 input으로 받습니다.</p>

<p>Main model의 loss function은 다음과 같습니다.</p>

<p>$\mathcal{L}_ {GNN} (\theta^t) = r \mathbf{E}_ {v \sim \mathcal{G}_ A^t } [ l(F_{\theta^t}(\upsilon), y_{\upsilon} ) ] + (1-r) \mathbf{E}_ {v’ \sim G_{\phi^{t-1}}} [ l(F_{\theta^t}(\upsilon ‘), F_{\theta^{t-1}}(\upsilon ‘)] $</p>

<p>여기서 \(v\)는 changed node, \(v'\)는 replayed node입니다. 즉, 이 모델은 새로 들어온 node와 이전에 학습했던 node(replayed)를 동시에 학습합니다.</p>

<blockquote>
  <p><strong>Generative Model for Node Neighborhood</strong></p>
</blockquote>

<p>앞서 언급한대로, 일반적인 Generative model(ex. <code class="language-plaintext highlighter-rouge">GAN</code>)은 주로 computer vision 분야에서 활발하게 연구되었으나, graph data는 structure에 dependent하기 때문에, edge의 생성은 independent한 event가 아니라 jointly structured 되어야 합니다.</p>

<p><code class="language-plaintext highlighter-rouge">NetGan</code>이나 <code class="language-plaintext highlighter-rouge">GraphRNN</code>같은 Graph Generative model들이 있지만, 이는 전체 그래프를 생성하기 위함이지 node의 neighborhood를 생성하기 위함이 아니어서, 저자들은 <code class="language-plaintext highlighter-rouge">ego network</code>라는 node neighborhood 생성모델을 제시합니다. 이 <code class="language-plaintext highlighter-rouge">ego network</code>는 <code class="language-plaintext highlighter-rouge">GAN</code>의 프레임워크와 유사하지만, 그래프 상에서의 random walks with restart, 즉 <code class="language-plaintext highlighter-rouge">RWRs</code>를 학습하는 방향으로 사용합니다.</p>

<p><code class="language-plaintext highlighter-rouge">RWRs</code>는 일반적인 <code class="language-plaintext highlighter-rouge">Random Walk</code>모델에서 일정 확률로 starting node로 돌아가고, 그렇지 않으면 neighborhood node로 넘어갑니다. 이는 기존 <code class="language-plaintext highlighter-rouge">RWRs</code>가 <code class="language-plaintext highlighter-rouge">Random Walk</code>보다 훨씬 적은 step으로 explore가 가능하게 한다고 합니다.</p>

<hr />

<blockquote>
  <p><strong>어떻게 <code class="language-plaintext highlighter-rouge">Random Walk with Restart(RWR)</code>이 기존 <code class="language-plaintext highlighter-rouge">Random Walk</code>보다 적은 length로 graph를 explore할까?</strong></p>
</blockquote>

<p>Graph \(\mathcal{G}= \lbrace \mathcal{V},\mathcal{E} \rbrace\) 가 있다고 합시다. Starting node는 \(v_0\)이고 그 node의 degree는 \(m\)이고 neighborhood는 \(N(v_0)\)이라고 합니다. \(T_{RW}\)를 기존 <code class="language-plaintext highlighter-rouge">Random Walk</code>가 \(v_0\)의 ego network를 explore하는데 필요한 step이라고 하면, \(E[T_{RW}] = \frac{(m-1)}{c} \cdot \frac{\sum_{v \in \mathcal{V} \setminus \mathcal{E} (v_0) } deg(v)}{d_{max}}\), where \(\mathcal{E} (v_0) = \lbrace v_0 \rbrace \cup N(v_0)\), \(d_{max}\) : maximum degree of nodes in \(v\)’s neighborhood, \(c\): the size of cut set of cut \((\mathcal{E} (v_0), \mathcal{V} \setminus \mathcal{E}(v_0) )\) 이라고 합니다. 자세한 증명은 논문의 appendix를 참고하시기 바랍니다.</p>

<p>실제 그래프에는 node가 많으므로, $ \left \vert \sum_{v \in \mathcal{V} \setminus \mathcal{E} (v_0)} deg(v) \right \vert \gg \left \vert cd_{max} \right \vert $ 이고, \(E[T_{RW}]\)는 굉장히 큰 수가 되게 됩니다.</p>

<p>하지만 \(\alpha\)의 확률로 restart하는 <code class="language-plaintext highlighter-rouge">RWR</code>의 expected length to explore는 \(E[T_{RWR}] &lt; \frac{m(\ln m+1)}{\alpha (1-\alpha)}\)가 된다고 합니다. 역시 자세한 증명은 논문의 appendix를 참고하시기 바랍니다.</p>

<p>\alpha를 예를 들어 0.2로 설정하면, \(E[T_{RW}] = \frac{(m-1)}{c} \cdot \frac{\sum_{v \in \mathcal{V} \setminus \mathcal{E} (v_0) } deg(v)}{d_{max}} \gg \frac{m(\ln m+1)}{\alpha (1-\alpha)} &gt; E[T_{RWR}]\)이므로, <code class="language-plaintext highlighter-rouge">RWR</code>를 사용하는 것이 기존 <code class="language-plaintext highlighter-rouge">Random Walk</code>를 사용하는 것 보다 훨씬 빠른것을 확인할 수 있습니다.</p>

<hr />

<p>지금부터 Generative Model에 관한 설명을 보겠습니다.</p>

<p>저자들은 node간의 dependency를 capture하기 위해 <strong>m</strong>이라는 graph state를 정의합니다. 각 walk step에서 \(m_l\)과 \(v_l\)을 계산하는데, 이 때의 input은 last state \(m_{l-1}\)과 last input \(s_{l-1}\)입니다. 이 \(s_{l-1}\)은 node identity \(v_{l-1}\)과 node attribute \(x_{l-1}\)을 포함하고 있습니다.</p>

<p>Current state \(m_ l\)은 neural network \(f\)로 계산됩니다.</p>

<p>Generator의 update process는 다음과 같습니다.</p>

<p>$m_l = f(m_{l-1}, s_{l-1}),$</p>

<p>$v_l = softmax(m_l \cdot W_{up,adj}),$</p>

<p>$x_l = m_l \cdot W_{up,fea},$</p>

<p>$s_l = (v_l \oplus x_l) \cdot W_{down}$</p>

<p>여기서 \(W_{up}, W_{down}\)은 차원을 맞춰주기 위한 projection matrix라고 생각하시면 됩니다.</p>

<p>저자들은 <code class="language-plaintext highlighter-rouge">WGAN</code> 프레임워크를 사용해 모델을 학습을 진행했고, 위의 그림에서 확인할 수 있듯이 이 generator는 새로운 그래프 \(\mathcal{G}_ t\) 에서 <code class="language-plaintext highlighter-rouge">RWRs</code>로 생성된 Sequence들을 input으로 받아 학습을 진행하고, 다음 task에서 <code class="language-plaintext highlighter-rouge">replay buffer</code>에 넣을 sequence를 뱉어줍니다. <code class="language-plaintext highlighter-rouge">GNN</code>은 이 sequence까지 포함해 학습하여 <code class="language-plaintext highlighter-rouge">catastrophic forgetting</code>을 방지합니다.</p>

<p>Discriminator는 역시 sequence W의 node identity와 그에 해당하는 attrribute를 받아서 sequence의 score를 output으로 반환합니다.</p>

<p>$p_{score} (W) = q(\lbrace (v_l, x_l), l=1,…,L \rbrace)$</p>

<p>여기서 \(q\)는 일반적인 neural network입니다.</p>

<p>다들 아시다시피, 이 discriminator는 sequence가 real인지 fake인지 판별하면서 generator의 성능을 높이게 되며 positive sample들은 real graph로부터 오는 <code class="language-plaintext highlighter-rouge">RWR</code>, negative sample들은 위에서 정의한 generator로부터 오게 됩니다.</p>

<blockquote>
  <p><strong>Incremental Learning on Graphs</strong></p>
</blockquote>

<p>지금부터는 Continual Learning이 어떻게 이루어지는지 보겠습니다.</p>

<p>먼저 저자들은 affected nodes를 정의합니다.</p>

<p>그래프가 time step에 따라 변하면서, 새로운 node나 edge가 생성되면 주위 K(<code class="language-plaintext highlighter-rouge">GNN</code>의 layer 수)-hop 이내의 neighborhood만 change 됩니다. (<code class="language-plaintext highlighter-rouge">GNN</code>의 layer가 2개라면, 한 node가 변할 때 그 node와 edge 2개 이내로만 연결되어 있는 node들만 변한다는 의미입니다.)</p>

<p>Changed node중에 <strong>크게 변한 것들</strong>이 있을 것이고, <strong>유의미한 변화가 없는 것들</strong>이 있을 것입니다. 이 <strong>크게 변한 것들</strong>이 전체적인 neighborhood의 패턴을 바꿀 가능성이 있는 node 들이라, 학습에 사용해야합니다.</p>

<blockquote>
  <p>다시 말해서, node 중에 성질이(feature) 크게 변한 node들은 새로운 data의 패턴을 반영할 확률이 성질이 변하지 않은 node보다 높으므로, model을 학습할 때 train data에 포함시켜서 학습시켜야 한다는 것입니다. 성질이 변한 node를 제쳐두고 변하지 않은 node만을 사용해서 학습한다면 model은 새로 들어온 data를 충분히 반영하지 못하겠죠.</p>
</blockquote>

<p>그렇다면 어떤 node가 크게 변했다는 것을 어떻게 확인할 수 있을까요?</p>

<p>저자들은 아래와 같은 influenced degree를 정의하고 그 influence degree가 threshold \(\delta\) 보다 크다면 affected node라고 취급합니다.</p>

<p>$ \mathcal{V}_ C^t = \lbrace v \lVert F_ {\theta^{t-1}} (v, \mathcal{G}^t) - F_ {\theta^{t-1}} (v, \mathcal{G}^{t-1}) \rVert &gt; \delta \rbrace$</p>

<p>위 식을 해석해보면, 어떤 node \(v\)의 이전 그래프 \(\mathcal{G}^{t-1}\)에서의 representation와 현재 그래프 \(\mathcal{G}^t\)에서의 representation이 많이 차이난다면, 이 node는 이전 그래프에서 현재 그래프로 넘어오면서 영향을 받았다고 보는 겁니다. 꽤 직관적인 해석입니다.</p>

<p>이런 affected node들은 이전 그래프가 가지고 있지 않은 새로운 패턴을 가지고 있으므로, Generative Model에 input으로 넣어 학습시킨 뒤에 다음 task부터 새로운 패턴을 반영해서 좋은 <code class="language-plaintext highlighter-rouge">replay buffer</code>를 만들도록 합니다.</p>

<p>추가로, 저자들은 간단한 filter를 추가해 generator가 생성한 node \(v_i\)가 affected node \(v_j\)와 <strong>많이 비슷한 경우</strong>, 패턴의 redundancy를 줄이기 위해 아래의 식처럼 필터링합니다.</p>

<p>$p_{reject} = max(p_{sim} (v_i, v_j) , j \subset \mathcal{V}_ C^t) \times p_r$$</p>

<p>여기서 \(p_r\)은 disappearacne rate로 사전에 정의하고, similarity는 다음과 같이 정의됩니다.</p>

<p>$p_{sim} (v_i, v_j) = \sigma (- \lVert F_ {\theta^{t-1}}(v_i, \mathcal{G}^{t-1}) - F_ {\theta^{t-1}}(v_j, \mathcal{G}^{t-1})  \rVert)$</p>

<p>이때 \(\sigma\)는 sigmoid function이고, 위 식도 직관적으로 두 node의 representation의 차이가 적으면 비슷하다고 보는 겁니다.</p>

<p>이 filter를 통해 저자들은 중복되는 지식은 점차 잊혀지고 바뀌는 distribution이 안정적으로 학습될 것이라 했습니다.</p>

<p>아래의 알고리즘을 통해 지금까지 설명했던 내용들을 확인할 수 있습니다.</p>

<p><img src="https://user-images.githubusercontent.com/99710438/194888070-5da986d2-1702-4cd5-b77e-cfa3d76a0467.png" alt="image" /></p>

<h2 id="4-experiment"><strong>4. Experiment</strong></h2>

<blockquote>
  <p>본 논문에서 저자들은 다양한 dataset을 통해 baseline들과 <code class="language-plaintext highlighter-rouge">SGNN-GR</code>을 비교했습니다.</p>
</blockquote>

<h3 id="experiment-setup"><strong>Experiment setup</strong></h3>

<ul>
  <li>Dataset
    <ul>
      <li>Cora</li>
      <li>Citeseer</li>
      <li>Elliptic (bitcoin transaction)</li>
      <li>DBLP</li>
    </ul>
  </li>
  <li>baseline
    <ul>
      <li>SkipGram models
        <ol>
          <li>LINE</li>
          <li>DNE</li>
        </ol>
      </li>
      <li>GNNs (Retrained)
        <ol>
          <li>GraphSAGE</li>
          <li>GCN</li>
        </ol>
      </li>
      <li>GNNs (Incremental)
        <ol>
          <li>PretrainedGNN (첫 time step때만 학습되고 이후로는 학습하지 않음)</li>
          <li>SingleGNN (각 time step마다 한 번씩 학습)</li>
          <li>OnlineGNN (Continual Learning setting, without knowledge consolidation)</li>
          <li>GNN-EWC</li>
          <li>GNN-ER</li>
          <li>DiCGRL</li>
          <li>TWP</li>
          <li>ContinualGNN</li>
        </ol>
      </li>
      <li><code class="language-plaintext highlighter-rouge">SGNN-GR</code></li>
    </ul>
  </li>
</ul>

<p>여기서 Retrained <code class="language-plaintext highlighter-rouge">GNN</code>은 각 time step마다 Graph <strong>전체</strong>를 학습시킨 것으로, Continual Learning model 성능의 upper bound라고 생각하면 됩니다. Incremental <code class="language-plaintext highlighter-rouge">GNN</code>이 Continual Learning model들이라고 생각하시면 됩니다.</p>

<h3 id="result"><strong>Result</strong></h3>

<ul>
  <li>Overall Results</li>
</ul>

<p>위의 data를 사용한 실험의 결과는 아래와 같습니다. 저자들은 average Macro/Micro-F1를 성능 평가 지표로 사용했습니다.</p>

<p><img src="https://user-images.githubusercontent.com/99710438/195345047-bd69d686-e6d3-4ea6-ab81-4baff5f95e1e.png" alt="image" /></p>

<p>말씀드린대로, <code class="language-plaintext highlighter-rouge">LINE</code>, <code class="language-plaintext highlighter-rouge">RetrainedGCN</code>, <code class="language-plaintext highlighter-rouge">RetrainedSAGE</code>는 각 task에서 그래프 <strong>전부</strong>를 사용해서 Continual Learning setting의 성능을 상회합니다. 하지만 저자들의 <code class="language-plaintext highlighter-rouge">SGNN-GR</code>의 성능 또한 Retrained model과 유사한 것으로 보아 generator가 꼭 필요한 sample들만 생성해줬음을 알 수 있습니다.</p>

<ul>
  <li>Analysis of Catastrophic Forgetting</li>
</ul>

<p>앞서 <code class="language-plaintext highlighter-rouge">catastrophic forgetting</code>을 방지하는 것이 Continual Learning에서 가장 중요한 포인트 중 하나라고 말씀드렸는데, 저자들의 모델은 얼마나 이전의 정보를 잘 기억했는지 보겠습니다.</p>

<p><img src="https://user-images.githubusercontent.com/99710438/195346345-51daec92-bc57-4c36-a6d5-a4b883a6aeb2.png" alt="image" /></p>

<p>왼쪽 (a) 그림은 Cora dataset에서 모델이 14 step을 가는동안 0번째 task를 얼마나 잘 기억하는지 보여주는 그래프이고, 오른쪽 (b) 그림은 6번째 task를 얼마나 잘 기억하는지 보여주 그래프입니다.</p>

<p><code class="language-plaintext highlighter-rouge">OnlineGNN</code>은 이전 task의 정보를 거의 저장하지 못하는 것을 확인할 수 있고, 저자들의 방법론이 <code class="language-plaintext highlighter-rouge">GNN-ER</code>보다 더 이전 task의 지식을 잘 보존하는 것을 볼 수 있습니다.</p>

<ul>
  <li>Anaylsis of Generative Model</li>
</ul>

<p>그렇다면 과연 저자들이 <code class="language-plaintext highlighter-rouge">replay buffer</code>를 Generative Model로 생성한 것은 옳은 선택이었을까요?</p>

<p><img src="https://user-images.githubusercontent.com/99710438/195347882-15c5016a-3f55-4799-892a-4e73935493b6.png" alt="image" /></p>

<p>그림 (a) 는 실제 그래프의 label당 node 개수(파란색)와 Generative Model로 생성된 label당 node 개수(빨간색)을 보여줍니다. Generative Model이 실제 그래프의 label 분포와 굉장히 유사하게 node를 생성하고 있음을 보여줍니다.</p>

<p>또한 오른쪽 그림 (b) 는 generated 된 데이터를 보여주는데, 다양한 topological 정보를 담고 있음을 볼 수 있습니다.</p>

<ul>
  <li>Ablation Study</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/99710438/195348924-c5e2fe7f-5238-4acb-a127-ba4bd18bdfbc.png" alt="image" /></p>

<p>마지막으로, 저자들은 <code class="language-plaintext highlighter-rouge">SGNN-GR</code>의 두 part들이 얼마나 성능 향상에 도움을 주는지 ablation study를 통해 Cora, Citeseer에서 확인했습니다.</p>

<p>여기서 Non-Affected는 새롭게 추가된 node들만 고려하고, 그로 인한 affected node들은 고려하지 않은 모델입니다. 또한 Non-Generator는 모든 affected node를 찾아 다시 학습시키지만, generator는 쓰지 않은 모델입니다.</p>

<p>당연하게 <code class="language-plaintext highlighter-rouge">SGNN-GR</code>이 가장 좋은 성능을 보이는 것을 확인할 수 있습니다.</p>

<h2 id="5-conclusion"><strong>5. Conclusion</strong></h2>

<blockquote>
  <p><strong>Summary</strong></p>
</blockquote>

<p>이 논문에서는 지속적으로 들어오는 Graph 데이터를 학습하는 데, Generative Model을 사용해 이전에 학습했던 그래프와 비슷한 그래프를 계속 생성해 새로운 데이터와 함께 학습시킵니다.</p>

<p>저자들은 여러 Continual Learning 방법 중 regularization method는 optimal solution을 얻는 것이 어렵다고 주장하고 replay based Continual Learning은 task가 진행됨에 따라 <code class="language-plaintext highlighter-rouge">replay buffer</code>에 그래프의 일부를 저장하고, task가 많이 늘어나면 그에 따라 요구되는 메모리도 커진다고 주장하며 Generative Model로 그때그때 <code class="language-plaintext highlighter-rouge">replay buffer</code>를 생성해서 메모리 효율을 높이겠다고 했습니다.</p>

<p>본 논문은 단순히 메모리 효율을 높인 것에 그치지 않고, 새롭게 등장하는 패턴은 적극적으로 학습하면서 불필요해 보이는 패턴은 줄이도록 학습해서 단순한 Continual Learning을 보완했습니다.</p>

<p>그 사이사이에 <code class="language-plaintext highlighter-rouge">Random Walk</code>가 아니라 <code class="language-plaintext highlighter-rouge">Random Walk with Restart</code>를 씀과 동시에 그 효율을 증명으로 보인 것과 같은 디테일, 본인들이 주장하는 모델의 장점을 잘 보여주는 알찬 실험들까지, 좋은 연구인 것 같습니다.</p>

<p>이 논문 뿐만 아니라 Continual learning에서 Generative Model은 중대한 역할을 할 것으로 보이며 관련 연구들이 꼭 필요할 것으로 보입니다.</p>

<p>추가적으로, 본 논문은 task incremental setting에서 generative model을 활용하고 있습니다만, 조금 더 어려운 setting(e.g. class incremental)에서의 활용 방안도 고안할 필요가 있다고 생각합니다.</p>

<p>Class incremental setting에서는 task incremental setting과 달리 한 번 등장한 class는 이후 task에서 다시 등장하지 않기 때문에 <code class="language-plaintext highlighter-rouge">GAN</code>같은 생성 모델을 활용하는 데 추가적인 전략이 필요할 것으로 보이며, 그런 경우에 task간의 similarity를 측정해서 활용하는 것도 하나의 future work가 될 것 같습니다.</p>

<blockquote>
  <p><strong>개인적인 생각</strong></p>
</blockquote>

<p><strong>올게 왔구나</strong></p>

<p>본 논문은 Graph Neural Network에서의 Continual Learning에 Generative Model을 접목시킨 방법입니다. 사실 이 논문이 나오는 것은 시간문제라고 생각하던 찰나에 역시나 등장했습니다.</p>

<p>이미 Continual Learning에 Generative Model을 접목시킨 연구는 꽤 오래전에(AI 연구의 속도가 매우 빠른 것을 감안하면) 등장했지만, GNN에 접목된 것은 없었기 때문이죠.</p>

<p>관련 연구를 하시는 분들은 아시겠지만, 이 논문이 novelty가 엄청 높다거나, 기존의 상식을 깨는 굉장한 발견을 한 논문이라기 보단.. (<strong>분명히 좋은</strong> 논문입니다, 오해금지)</p>

<p>가장 큰 contribution은 특정 분야에서 처음 시도된 연구, 적절한 시기에 등장한 연구인 것 같습니다. Novelty만을 좇는게 아니라, trend에 맞는 연구를 하는 능력도 필요해 보입니다.</p>

<p>우리도 최신 논문을 잘 follow up 하는 ‘트렌디한’ 연구자가 되도록 합시다.</p>

<hr />

<h2 id="author-information"><strong>Author Information</strong></h2>

<ul>
  <li>Wonjoong Kim
    <ul>
      <li>Affiliation: <a href="http://dsail.kaist.ac.kr">DSAIL@KAIST</a></li>
      <li>Research Topic: Graph Neural Network, Continual Learning</li>
      <li>Contact: wjkim@kaist.ac.kr</li>
    </ul>
  </li>
</ul>

<h2 id="reference--additional-materials"><strong>Reference &amp; Additional materials</strong></h2>

<ul>
  <li>Github Implementation
    <ul>
      <li>None</li>
    </ul>
  </li>
  <li>Reference
    <ul>
      <li><a href="https://ojs.aaai.org/index.php/AAAI/article/view/16602">[AAAI-21] Overcoming catastrophic forgetting in graph neural networks with experience replay</a></li>
      <li><a href="https://proceedings.neurips.cc/paper/2017/hash/0efbe98067c6c73dba1250d2beaa81f9-Abstract.html">[NIPS-17] Continual learning with deep generative replay</a></li>
    </ul>
  </li>
</ul>

    </div>



</article>
<script src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.5.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>







<hr class="shaded"/>

<footer>
            <div class="row">
                <div class="col-lg-12 footer">
               &copy;2023 Copyright 2021 Google LLC. All rights reserved. <br />
 Site last generated: May 23, 2023 <br />
<p><img src="images/company_logo.png" alt="Company logo"/></p>
                </div>
            </div>
</footer>






        </div>
    <!-- /.row -->
</div>
<!-- /.container -->
</div>
<!-- /#main -->
    </div>

</body>

<!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-66296557-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>





</html>


