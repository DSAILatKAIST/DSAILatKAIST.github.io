<!DOCTYPE html>
<html>
<head>
    <meta name="google-site-verification" content="1tAPTdgw0-t8G2Bya463OpBtYUbj9Um93gfnsowYKLw" />
    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-CV4PTXQSTW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-CV4PTXQSTW');
</script>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="">
<meta name="keywords" content="reviews,  ">
<title>[ICML 2022] Structure-Aware Transformer for Graph Representation Learning | Awesome Reviews</title>
<link rel="stylesheet" href="css/syntax.css">

<link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
<!--<link rel="stylesheet" type="text/css" href="css/bootstrap.min.css">-->
<link rel="stylesheet" href="css/modern-business.css">
<!-- Latest compiled and minified CSS -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<link rel="stylesheet" href="css/customstyles.css">
<link rel="stylesheet" href="css/boxshadowproperties.css">
<!-- most color styles are extracted out to here -->
<link rel="stylesheet" href="css/theme-blue.css">

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="js/jquery.navgoco.min.js"></script>


<!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<!-- Anchor.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.2.0/anchor.min.js"></script>
<script src="js/toc.js"></script>
<script src="js/customscripts.js"></script>

<link rel="shortcut icon" href="images/favicon.ico">

<!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
<![endif]-->

<link rel="alternate" type="application/rss+xml" title="DSAILatKAIST.github.io" href="http://localhost:4000/feed.xml">


	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	TeX: {
		equationNumbers: {
		autoNumber: "AMS"
		}
	},
	tex2jax: {
		inlineMath: [ ['$', '$'] ],
		displayMath: [ ['$$', '$$'] ],
		processEscapes: true,
		}
	});
	MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
		alert("Math Processing Error: "+message[1]);
	});
	MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
		alert("Math Processing Error: "+message[1]);
	});
</script>

<script type="text/javascript" async
	src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



<script type="text/javascript" async
 src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>




    <script>
        $(document).ready(function() {
            // Initialize navgoco with default options
            $("#mysidebar").navgoco({
                caretHtml: '',
                accordion: true,
                openClass: 'active', // open
                save: false, // leave false or nav highlighting doesn't work right
                cookie: {
                    name: 'navgoco',
                    expires: false,
                    path: '/'
                },
                slide: {
                    duration: 400,
                    easing: 'swing'
                }
            });

            $("#collapseAll").click(function(e) {
                e.preventDefault();
                $("#mysidebar").navgoco('toggle', false);
            });

            $("#expandAll").click(function(e) {
                e.preventDefault();
                $("#mysidebar").navgoco('toggle', true);
            });

        });

    </script>
    <script>
        $(function () {
            $('[data-toggle="tooltip"]').tooltip()
        })
    </script>
    <script>
        $(document).ready(function() {
            $("#tg-sb-link").click(function() {
                $("#tg-sb-sidebar").toggle();
                $("#tg-sb-content").toggleClass('col-md-9');
                $("#tg-sb-content").toggleClass('col-md-12');
                $("#tg-sb-icon").toggleClass('fa-toggle-on');
                $("#tg-sb-icon").toggleClass('fa-toggle-off');
            });
        });
    </script>
    


	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	TeX: {
		equationNumbers: {
		autoNumber: "AMS"
		}
	},
	tex2jax: {
		inlineMath: [ ['$', '$'] ],
		displayMath: [ ['$$', '$$'] ],
		processEscapes: true,
		}
	});
	MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
		alert("Math Processing Error: "+message[1]);
	});
	MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
		alert("Math Processing Error: "+message[1]);
	});
</script>

<script type="text/javascript" async
	src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



<script type="text/javascript" async
 src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>


</head>
<body>
<!-- Navigation -->
<nav class="navbar navbar-inverse navbar-static-top">
    <div class="container topnavlinks">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="fa fa-home fa-lg navbar-brand" href="index.html">&nbsp;<span class="projectTitle"> Awesome Reviews</span></a>
        </div>
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                <!-- toggle sidebar button -->
                <li><a id="tg-sb-link" href="#"><i id="tg-sb-icon" class="fa fa-toggle-on"></i> Nav</a></li>
                <!-- entries without drop-downs appear here -->




                
                
                
                <li><a href="introduction">Introduction</a></li>
                
                
                
                <li><a href="https://statistics.kaist.ac.kr/" target="_blank" rel="noopener">KAIST ISysE</a></li>
                
                
                
                <!-- entries with drop-downs appear here -->
                <!-- conditional logic to control which topnav appears for the audience defined in the configuration file.-->
                
                
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">Reviews<b class="caret"></b></a>
                    <ul class="dropdown-menu">
                        
                        
                        <li><a href="reviews_kse801_2022.html">KSE801 (2022)</a></li>
                        
                        
                        
                        <li><a href="reviews_DS503_2023.html">DS503 (2023)</a></li>
                        
                        
                        
                        <li><a href="reviews_DS535_2023.html">DS535 (2023)</a></li>
                        
                        
                    </ul>
                </li>
                
                
                
			<li>



  <a class="email" title="Submit feedback" href="#" onclick="javascript:window.location='mailto:swkim@kaist.ac.kr?subject=Question about reviews feedback&body=I have some feedback about the [ICML 2022] Structure-Aware Transformer for Graph Representation Learning page: ' + window.location.href;"><i class="fa fa-envelope-o"></i> Feedback</a>

</li>



		
                <!--comment out this block if you want to hide search-->
                <li>
                    <!--start search-->
                    <div id="search-demo-container">
                        <input type="text" id="search-input" placeholder="search...">
                        <ul id="results-container"></ul>
                    </div>
                    <script src="js/jekyll-search.js" type="text/javascript"></script>
                    <script type="text/javascript">
                            SimpleJekyllSearch.init({
                                searchInput: document.getElementById('search-input'),
                                resultsContainer: document.getElementById('results-container'),
                                dataSource: 'search.json',
                                searchResultTemplate: '<li><a href="{url}" title="[ICML 2022] Structure-Aware Transformer for Graph Representation Learning">{title}</a></li>',
                    noResultsText: 'No results found.',
                            limit: 10,
                            fuzzy: true,
                    })
                    </script>
                    <!--end search-->
                </li>
            </ul>
        </div>
        </div>
        <!-- /.container -->
</nav>



<!-- Page Content -->
<div class="container">
  <div id="main">
    <!-- Content Row -->
    <div class="row">
        
        
            <!-- Sidebar Column -->
            <div class="col-md-3" id="tg-sb-sidebar">
                

<ul id="mysidebar" class="nav">
  <li class="sidebarTitle"> </li>
  
  
  
  <li>
      <a title="How to contribute" href="#">How to contribute</a>
      <ul>
          
          
          
          <li><a title="How to contribute?" href="how_to_contribute.html">How to contribute?</a></li>
          
          
          
          
          
          
          <li><a title="Review template (Example)" href="template.html">Review template (Example)</a></li>
          
          
          
          
      </ul>
   </li>
     
      
  
  <li>
      <a title="Reviews" href="#">Reviews</a>
      <ul>
          
          
          
          <li><a title="KSE801 (2022F)" href="reviews_kse801_2022.html">KSE801 (2022F)</a></li>
          
          
          
          
          
          
          <li><a title="DS503 (2023S)" href="reviews_DS503_2023.html">DS503 (2023S)</a></li>
          
          
          
          
          
          
          <li><a title="DS535 (2023F)" href="reviews_DS535_2023.html">DS535 (2023F)</a></li>
          
          
          
          
      </ul>
   </li>
     
      
      
      <!-- if you aren't using the accordion, uncomment this block:
         <p class="external">
             <a href="#" id="collapseAll">Collapse All</a> | <a href="#" id="expandAll">Expand All</a>
         </p>
         -->
</ul>

<!-- this highlights the active parent class in the navgoco sidebar. this is critical so that the parent expands when you're viewing a page. This must appear below the sidebar code above. Otherwise, if placed inside customscripts.js, the script runs before the sidebar code runs and the class never gets inserted.-->
<script>$("li.active").parents('li').toggleClass("active");</script>



            </div>
            
        

        <!-- Content Column -->
        <div class="col-md-9" id="tg-sb-content">
            <script type="text/x-mathjax-config">
MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/math...">
</script>
<article class="post" itemscope itemtype="https://schema.org/BlogPosting">

    <header class="post-header">
        <h1 class="post-title" itemprop="name headline">[ICML 2022] Structure-Aware Transformer for Graph Representation Learning</h1>
        <p class="post-meta"><time datetime="2023-04-20T00:00:00+09:00" itemprop="datePublished">Apr 20, 2023</time> /
            
            
            
            
            

        </p>


    </header>

    <div class="post-content" itemprop="articleBody">

        

        <h1 id="structure-aware-transformer-for-graph-representation-learning"><strong>Structure-Aware Transformer for Graph Representation Learning</strong></h1>

<p><em><strong>Background before reading this review.</strong></em></p>

<p>Graph구조에 맞게 Transformer를 적용하여 좋은 성능을 낸 SAT를 제시한 논문 <a href="https://arxiv.org/abs/2202.03036">Structure-Aware Transformer for Graph Representation Learning</a>를 읽기전에 알고 넘어가야할 Graph Notation, Transformer에 대한 설명 등 간단하게 짚고 넘어가면 좋은 내용들입니다. 사전 지식이 있으신 경우, 바로 본문으로 넘어가셔도 좋습니다.</p>

<p>*Notation</p>

<p>$G = (V, E, \mathbf X)$</p>

<ul>
  <li>
    <p>node $u \in V$</p>
  </li>
  <li>
    <p>node attribute $x_u \in  \mathcal X \subset  \mathbb R^d$</p>
  </li>
  <li>
    <p>$\mathbf X \in  \mathbb R^{n \times d}$</p>
  </li>
</ul>

<p><strong>Transformer 구성 요소</strong></p>

<ol>
  <li>Self-attention module</li>
</ol>

<ul>
  <li>
    <p>input node feature $\mathbf X$가 linear projection을 통해 Query($\mathbf Q$), Key($\mathbf K$), Value($\mathbf V$)로 투영되고, 이를 활용하여 self-attention을 계산합니다.</p>
  </li>
  <li>
    <p>multi-head attention : self-attention의 initialize를 다양하게 하여 표현력을 높였습니다.</p>
  </li>
</ul>

<ol>
  <li>feed-forward NN</li>
</ol>

<ul>
  <li>self-attention의 output이 skipconnection이나 FFN등을 거치면 하나의 transforemer layer를 통과한 것 입니다.</li>
</ul>

<ol>
  <li>Absolute encoding</li>
</ol>

<ul>
  <li>그래프의 위치적/구조적인 representation을 input node feature에 더하거나 concatenate하여 Transformer의 input으로 사용합니다. (Vanilla transformer의 PE와 같은 역할)</li>
</ul>

<p><strong>Graph Transformer에서 자주 사용되는 Positional encoding method들</strong></p>

<p>자주 사용되는 PE로는 다음 두가지를 꼽을 수 있습니다. 하지만 이 Positional Encoding들의 문제는 노드와 그 이웃들 사이의 structural similarity를 반영하지는 않는다는 것입니다. 각각에 대한 설명은 링크를 타고 들어가 확인하실 수 있습니다.</p>

<ul>
  <li>
    <p><a href="https://paperswithcode.com/method/laplacian-pe">Laplacian PE</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/pdf/2110.07875.pdf">Random Walk PE</a></p>
  </li>
</ul>

<ol>
  <li>Self-attention and kernel smoothing</li>
</ol>

<p>$\operatorname{Attn}\left(x_v\right)=\sum_ {u \in V} \frac{\kappa_ {\exp }\left(x_v, x_u\right)}{\sum_ {w \in V} \kappa_ {\exp }\left(x_v, x_w\right)} f\left(x_u\right), \forall v \in V$</p>

<ul>
  <li>
    <p>linear value function $f(x) = \mathbf W_ {\mathbf V}x$</p>
  </li>
  <li>
    <p>$\kappa_ {\exp }$ (non-symmetric) exponential kernel parameterized by $\mathbf W_ {\mathbf Q}, \mathbf W_ {\mathbf K}$</p>
  </li>
</ul>

<p>$\kappa_ {\exp }\left(x, x^{\prime}\right):=\exp  \left(\left\langle\mathbf{W}_ {\mathbf{Q}} x, \mathbf{W}_ {\mathbf{K}} x^{\prime}\right\rangle / \sqrt{d_ {\text {out }}}\right)$</p>

<ul>
  <li>
    <p>$\langle  \cdot, \cdot\rangle$ : dotproduct</p>
  </li>
  <li>
    <p>학습가능한 exponential kernel</p>
  </li>
  <li>
    <p>(-) only position-aware, not structure-aware encoding</p>
  </li>
</ul>

<h1 id="1-problem-definition"><strong>1. Problem Definition</strong></h1>

<h2 id="limitations-of-gnn"><em><strong>Limitations of GNN</strong></em></h2>

<ol>
  <li>
    <p>limited expressiveness : GNN은 message passing과정에서의 aggregation operation의 특성으로 인해 최대 1-WL test의 표현력을 가집니다. GNN의 WL-test와 expression에 대한 분석은 GIN을 제시한 논문인 <a href="https://arxiv.org/abs/1810.00826">How Powerful are Graph Neural Networks?</a> 에서 제시되었습니다.</p>
  </li>
  <li>
    <p>Over-smoothing problem : GNN layer의 수가 충분히 커지면 모든 node representation이 상수로 수렴하게됩니다.</p>
  </li>
  <li>
    <p>Over-squashing problem : 그래프의 수많은 메세지들이 고정된 길이의 벡터 하나로 압축되어 발생하는 그래프 “bottleneck”으로 인해 멀리 위치한 노드의 메세지가 효율적으로 전파되지 않는 문제가 발생합니다.</p>
  </li>
</ol>

<p><strong>⇒ Beyond neighborhood aggregation!</strong></p>

<h2 id="transformer"><em><strong>Transformer</strong></em></h2>

<p>Transformer를 적용했을 때의 장점은 다음과 같습니다.</p>

<ul>
  <li>
    <p>하나의 self-attention layer를 통해 그래프내의 어떤 노드쌍이든지 그 사이의 상호작용을 확인할 수 있습니다.</p>
  </li>
  <li>
    <p>GNN과 달리 중간 계층에서 structural inductive bias가 발생하지 않아 GNN의 표현력 한계를 해결할 수 있습니다.</p>

    <p>반면, 단점은 다음과 같습니다.</p>
  </li>
  <li>
    <p>graph structure info를 얼마나 학습하는지 input node feature에만 structural, positional 정보를 인코딩하여 넣기 때문에 제한적입니다.</p>
  </li>
  <li>
    <p>노드에 대한 structural, positional 정보만 input node feature로 인코딩하기 때문에, 그래프 구조 자체에서 학습할 수 있는 정보의 양이 제한적입니다.</p>

    <p>따라서 논문에서 제시하고자 하는 Graph Transformer의 Goal은 다음과 같습니다.</p>
  </li>
</ul>

<blockquote>
  <p>💡 Goal : 그래프 데이터에 Transformer를 적절히 변형해 적용하여 그래프 구조를 잘 반영하고 높은 표현력을 가지는 Achitecture를 디자인하는 것</p>
</blockquote>

<h1 id="2-motivation"><strong>2. Motivation</strong></h1>

<h2 id="message-passing-graph-neural-networks"><em><strong>Message passing graph neural networks.</strong></em></h2>

<p>최대 1-WL test로 제한된 표현력, over-smoothing, over-quashing</p>

<h2 id="limitations-of-existing-approaches"><em><strong>Limitations of existing approaches</strong></em></h2>

<p>기존에 Graph구조에 Transformer를 적용하는 시도가 없었던 것은 아닙니다. 그렇다면 어떤것이 문제가 되었을까요?</p>

<ul>
  <li>노드들 사이 positional relationship만 인코딩하고, strucutral relationship을 직접 인코딩하지않았습니다. 이에 따라 노드들 사이 structural similarity를 확인하기가 어렵고, 노드들 사이의 structural interaction을 모델링하는데 실패한것으로 분석하였습니다.</li>
</ul>

<p>다음의 그림 예시를 보면 이해가 더 쉽습니다.</p>

<p>ex.
<img src="https://github.com/sujinyun999/LearningOnGraph/assets/69068083/4472bb78-65cc-43bf-90be-8dcd203616d8" alt="Untitled" /></p>

<p>G1과 G2에서 최단거리를 활용한 positional encoding을 할경우 node u와 v가 다른 노드들에 대해 모두 같은 representation을 가지게되지만, 그래프의 실제 구조는 다릅니다. 
→ 이 지점이 논문에서 제시하는 기존 Graph Transformer의 문제, 즉, strucure aware에 실패한 것 입니다.</p>

<blockquote>
  <p>💡 Message-passing GNN과 Transformer architecture 각각의 장점을 살려 local, global info를 모두 고려하는 transformer architecture를 제안</p>
</blockquote>

<h2 id="contribution-of-this-paper"><em><strong>Contribution of this paper</strong></em></h2>

<p>Q. 그렇다면 논문에서 해결하고자하는 Structure-Aware를 위해 Transformer구조에 structural info를 어떻게 인코딩할까요?</p>

<p>논문에서는 다음과 같이 대답합니다.</p>

<p>A. Structure-aware self attention를 도입한 Structre-Aware Transformer(SAT)</p>

<ol>
  <li>reformulate the self-attention mechanism</li>
</ol>

<ul>
  <li>
    <p>kernel smoother</p>
  </li>
  <li>
    <p>원래 노드 feature에 적용하는 exponential 커널을 확장하여 각 노드가 중심인 subgraph representation을 추출하여 local structure에도 적용합니다.</p>
  </li>
</ul>

<ol>
  <li>subgraph representation들을 자동적으로 만들어내는 방법론 제안</li>
</ol>

<ul>
  <li>이를 통해 kernel smoother가 구조적/특성적 유사성을 포착할 수 있게됩니다.</li>
</ul>

<ol>
  <li>
    <p>GNN으로 그래프의 subgraph info를 포함하는 node representation을 만들어 기존 GNN에 추가적인 구조 개선 없이도 더 높은 성능을 냅니다.</p>
  </li>
  <li>
    <p>Transformer의 성능향상이 structure-aware한 측면에서 일어난 것을 증명하고 absolute encoding이 추가된 transfoemr보다 SAT가 얼마나 interpretable한지를 보여줍니다.</p>
  </li>
</ol>

<h1 id="3-method"><strong>3. Method</strong></h1>

<h2 id="structure-aware-transformer"><em><strong>Structure-Aware Transformer</strong></em></h2>

<h3 id="1-structure-aware-self-attention"><em>1. <strong>Structure-Aware Self-attention</strong></em></h3>

<p>position-aware한 structural encoding에 노드들 사이 structural similarity를 포함하기 위해 각 노드의 local structure에 관한 generalized kernel을 추가합니다.</p>

<p>각 노드가 중심이되는 subgraph set을 추가함으로써 structure-aware attention은 다음과 같이 정의될 수 있습니다.</p>

<p>$\operatorname{SA-Attn}\left(v\right):=\sum_ {u \in V} \frac{\kappa_ {\text{graph} }\left(S_G(v), S_G(u)\right)}{\sum_ {w \in V} \kappa_ {\text{graph}}\left(S_G(v), S_G(u)\right)} f\left(x_u\right)$</p>

<ul>
  <li>
    <p>$S_G(v)$ : node feature $\mathbf X$와 연관된 $v$를 중심으로하는 subgraph</p>
  </li>
  <li>
    <p>$\kappa_ {\text{graph} }$ : subgraph쌍을 비교하는 kernel</p>
  </li>
</ul>

<p>⇒ attribute &amp; structural similarity 모두 표현 가능한 expressive node representation을 생성 → table 1</p>

<p>⇒ 동일한 subgraph 구조를 가지는 경우에만 permutation equivariant한 성질을 갖게됨</p>

<p>$\kappa_ {\text {graph }}\left(S_G(v), S_G(u)\right)=\kappa_ {\exp }(\varphi(v, G), \varphi(u, G))$</p>

<ul>
  <li>
    <p>$\varphi(v, G)$ : feature $\mathbf X$를 가지는 node $v$가 중심에 있는 subgraph의 vector representation을 만들어내는 structure extractor</p>
  </li>
  <li>
    <p>GNN이나 differentiable Graph kernel등 subgraph의 representation을 만들 수 있는 어느 모델이든 될 수 있습니다.</p>
  </li>
  <li>
    <p>Task/data 특성에 따라 Edge attribute을 활용할 필요가 있는 경우 그에 맞는GNN을 선택하는 디자인 초이스가 생깁니다. edge attribute을 따로 활용하지는 않고 subgraph extractor에서 활용합니다.</p>
  </li>
</ul>

<p><em><strong>k-subtree GNN extractor.</strong></em></p>

<p>$\varphi(u, G) = \operatorname{GNN}_G^{(k)}(u)$</p>

<ul>
  <li>
    <p>node u에서 시작하는 k-subtree structure의 representation을 생성하는 역할을 합니다.</p>
  </li>
  <li>
    <p>at most 1-WL test : 위에서 지적한 GNN의 한계와 같이, 최대 1WL Test의 표현력을 가집니다.</p>
  </li>
  <li>
    <p>논문에서는 실험을 통해 작은 k 값이더라도 over-smoothing, over-squashing issue없이 좋은 성능을 내는것을 확인하였습니다.</p>
  </li>
</ul>

<p><em><strong>k-subgraph GNN extractor.</strong></em></p>

<p>$\varphi(u, G) = \sum_ {v \in  \mathcal N_k(u)} \operatorname{GNN}_G^{(k)}(v)$</p>

<ul>
  <li>
    <p>node u의 representation만을 사용하는데서 나아가 node u가 중심이 되는 k-hop subgraph전체의 representation을 생성하고 활용합니다.</p>
  </li>
  <li>
    <p>node u 의 k-hop이웃 $\mathcal N_k(u)$에 대해 각 노드에 GNN을 적용한 node representation을 pooling(논문에서는 summation)합니다.</p>
  </li>
  <li>
    <p><strong>More powerful than 1-WL test!</strong> 위에서 k-subtree GNN extractor와의 가장 큰 차이입니다.</p>
  </li>
  <li>
    <p>original node representation과의 concatenation을 통해 structural similarity뿐만 아니라 attributed similarity도 반영합니다.</p>
  </li>
</ul>

<p>이외에 다른 structure extractor로 다음과 같은 것들을 고려해볼 수 있습니다.</p>

<p><em><strong>Other structure extractors.</strong></em></p>

<ul>
  <li>
    <p>directly learn a number of “hidden graphs” as the “anchor subgraphs” to represent subgraphs</p>
  </li>
  <li>
    <p>domain-specific GNNs</p>
  </li>
  <li>
    <p>non-parametric graph-kernel</p>
  </li>
</ul>

<h3 id="2-structure-aware-transformer"><em>2. Structure-Aware Transformer</em></h3>

<p><img src="https://user-images.githubusercontent.com/69068083/231114106-a71006e8-a9e5-44cb-b353-578ec4e09a80.png" alt="Untitled" /></p>

<p>self-attention→ skipconnection → normalization layer → FFN → normalization layer</p>

<p><em><strong>Augmentation on skip connection.</strong></em></p>

<p>$x’_v = x_c +1/ \sqrt {d_v} \operatorname{SA-Attn}\left(v\right)$</p>

<ul>
  <li>
    <p>$d_v$ : node $v$의 degree</p>
  </li>
  <li>
    <p>degree factor를 포함하여 연결이 많은 graph component들이 압도적인 영향을 미치지 않도록합니다.</p>
  </li>
</ul>

<p>*graph-level task를 진행해야 할 경우 input graph에 다른 노드와의 connectivity없이 virtual <code class="language-plaintext highlighter-rouge">[cls] </code>node를 추가하거나, node-level representation을 sum/average 등으로 aggregation</p>

<h3 id="3-combination-with-absolute-encoding"><em>3. Combination with Absolute Encoding</em></h3>

<p>위의 structure aware self-attention에 추가로 absolute encoding을 추가하게 되면 postion-aware한 특성이 추가되어 기존의 정보를 보완하는 역할을 하게됩니다. 이러한 디자인 초이스의 조합을 통해 성능향상을 확인할 수 있었습니다.</p>

<p><strong>RandomWalk PE</strong></p>

<p>Absolute PE만 사용할 경우 structural bias가 과도하게 발생하지 않아서 두개의 노드가 유사한 local structure를 갖고 있더라도 비슷한 node representation이 생성되는것을 보장하기 어렵습니다!</p>

<p>→ 이는 Structural, positional sign으로 주로 사용되는 distance나 Laplacian-based positional representation이 노드들 사이의 structural simialrity를 포함하지 않기때문으로 분석할 수 있습니다.</p>

<blockquote>
  <p>📌 Structural aware attenrion은 inductive bias가 더 강하더라도 노드의 strucutral similarity를 측정하는데 적합하여 유사한 subgraph구조를 가진 노드들이 비슷한 embedding을 갖게하고, expressivity가 향상되어 좋은 성능을 보입니다.</p>
</blockquote>

<h3 id="4-expressivity-analysis"><em>4. Expressivity Analysis</em></h3>

<p>SAT에서는 각노드를 중심으로하는 k-subgraph GNN extractor가 도입되어 적어도 subgraph representation만큼은 expressive(More than 1WL Test)하다는 것을 보장합니다.</p>

<h1 id="4-experiment"><strong>4. Experiment</strong></h1>

<h3 id="experiment-setup"><em><strong>Experiment setup</strong></em></h3>

<p><em><strong>Dataset</strong></em></p>

<ul>
  <li>ZINC :
    <ul>
      <li>from <a href="https://arxiv.org/abs/1610.02415">Automatic chemical design using a data-driven continuous representation of molecules</a></li>
      <li>250,000개의 분자 그래프구조,  with up to 38 heavy atoms</li>
      <li>task is to regress the penalized <code class="language-plaintext highlighter-rouge">logP</code> (also called constrained solubility)</li>
    </ul>
  </li>
  <li>CLUSTER :
    <ul>
      <li>from <a href="https://arxiv.org/abs/2003.00982">Benchmarking Graph Neural Networks</a></li>
      <li>task is semi-supervised graph clustering (node classification)</li>
    </ul>
  </li>
  <li>PATTERN
    <ul>
      <li>from <a href="https://arxiv.org/abs/2003.00982">Benchmarking Graph Neural Networks</a></li>
      <li>task is semi-supervised graph pattern recognition</li>
    </ul>
  </li>
  <li>OGBG-PPA
    <ul>
      <li>from <a href="https://arxiv.org/abs/2005.00687">Open Graph Benchmark: Datasets for Machine Learning on Graphs</a></li>
      <li>Protein-Protein Association Network</li>
      <li>task is to predict new association edges given the training edges</li>
    </ul>
  </li>
  <li>OGBG-CODE2
    <ul>
      <li>from <a href="https://arxiv.org/abs/2005.00687">Open Graph Benchmark: Datasets for Machine Learning on Graphs</a></li>
      <li>Abstract Syntax Tree of Source Code</li>
      <li>AST로 표시되는 Python 메서드 본문과 해당 노드 기능이 주어지면 메서드 이름을 형성하는 하위 토큰을 예측하는 task</li>
    </ul>
  </li>
</ul>

<p><em><strong>Baseline</strong></em></p>

<ul>
  <li>
    <p><em><strong>GNNs</strong></em></p>
  </li>
  <li>
    <p>GCN</p>
  </li>
  <li>
    <p>GraphSAGE</p>
  </li>
  <li>
    <p>GAT</p>
  </li>
  <li>
    <p>GIN</p>
  </li>
  <li>
    <p>PNA</p>
  </li>
  <li>
    <p>Deeper GCN</p>
  </li>
  <li>
    <p>ExpC</p>
  </li>
  <li>
    <p><em><strong>Transformers</strong></em></p>
  </li>
  <li>
    <p>Original Transformer with RWPE</p>
  </li>
  <li>
    <p>Graph Transformer</p>
  </li>
  <li>
    <p>SAN</p>
  </li>
  <li>
    <p>Graphormer</p>
  </li>
  <li>
    <p>GraphTrans</p>
  </li>
</ul>

<h3 id="results"><em><strong>Results</strong></em></h3>

<p><strong>Table1.</strong> SAT와 graph regression, classification task의 sota모델과 비교</p>

<ul>
  <li>ZINC dataset의 경우 작을수록 더 좋은 성능을 의미하는 MAE(Mean Absolute Error), CLUSTER와 PATTERN의 경우 높을수록 더 좋은 성능을 의미하는 Acurracy가 평가지표로 사용되었음.</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/69068083/231114155-056893f6-8d16-4a59-b43b-62c76fd482a3.png" alt="Untitled" /></p>

<p><strong>Table2.</strong> SAT와 OGB데이터셋에서의 sota모델 비교</p>

<ul>
  <li>OGB dataset의 경우 높을수록 더 좋은 성능을 의미하는 Acurracy, F1 score가 평가지표로 사용되었음.</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/69068083/231114185-23daa0d6-bc32-4838-93e8-0a6d09a17f7e.png" alt="Untitled" /></p>

<p><strong>Table3.</strong> structure extractor로 사용한 GNN과의 성능비교. Sparse GNN을 모든 경우에서 outperform하는 것을 확인할 수 있음</p>

<p><img src="https://user-images.githubusercontent.com/69068083/231114223-e6e32dfd-039b-4caa-b123-14e72e9fc867.png" alt="Untitled" /></p>

<p><strong>Fig3.</strong> ZINC데이터셋에 SAT의 다양한 variant실험</p>

<ul>
  <li>평가지표 : MAE(더 작은 지표가 좋은 성능을 의미)</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/69068083/231114263-2ea26465-c8b3-4df8-b7d4-4d329d41d97b.png" alt="Untitled" /></p>

<ol>
  <li>structure extractor에서의 k의 영향 비교</li>
</ol>

<ul>
  <li>
    <p>k=0일때, Absolute encoding만을 활용하는 vanilla transformer랑 같다고 볼 수 있습니다.</p>
  </li>
  <li>
    <p>k=3일때, optimal performance를 보임을 실험을 통해 확인하였습니다.</p>
  </li>
  <li>
    <p>k=4를 넘어서면 성능이 악화되는것을 확인할 수 있었는데, 이는 GNN에서의 알려진 사실인 더 적은 수의 layer를 가지는 network가 더 좋은 성능을 보이는 것과 마찬가지라고 할 수 있습니다.(Oversmoothing and Oversquashing)</p>
  </li>
</ul>

<ol>
  <li>Absolute encoding의 영향 비교</li>
</ol>

<ul>
  <li>
    <p>RandomWalkPE vs. Laplacian PE</p>
  </li>
  <li>
    <p>Structure-aware attention의 도입으로 인한 성능향상보다는 그 정도가 낮았지만, RWPE를 도입할 경우 성능이 더 좋은것으로 보았을 때, 두가지 encoding이 상호보완적인 역할을 한다고 해석할 수 있었습니다.</p>
  </li>
</ul>

<ol>
  <li>Readout method의 영향 비교</li>
</ol>

<ul>
  <li>
    <p>node-level representation을 aggregate할 때 사용하기 위한 readout으로 mean과 sum을 비교하였습니다.</p>
  </li>
  <li>
    <p>추가로 <code class="language-plaintext highlighter-rouge">[CLS]</code> 토큰을 통해 graph-level 정보를 pooling하는 방법도 같이 비교하여보았습니다.</p>
  </li>
  <li>
    <p>GNN에서는 readout method의 영향이 매우 컸지만 SAT에서는 매우 약한 영향만을 확인하였습니다.</p>
  </li>
</ul>

<h1 id="5-conclusion"><strong>5. Conclusion</strong></h1>

<p><em><strong>Strong Points.</strong></em></p>

<p>structural info를 graphormer에서처럼 휴리스틱하게 shortest path distance(SPD)를 활용하지 않고, 그러한 local info를 잘 배우는 GNN으로 대체한 점이 novel하다고 할 수 있습니다.</p>

<p>Transformer의 global receptive field 특성과 GNN의 local structure특성이 상호보완적인데,</p>

<p>encoding에 있어서도</p>

<ol>
  <li>
    <p>RWPE를 통한 positional encoding</p>
  </li>
  <li>
    <p>k-subtree/subgraph GNN을 통한 structure-aware attention</p>
  </li>
</ol>

<p>두가지가 상호보완적인 역할을 합니다.</p>

<p>→ 각자가 잘 배우는 특성을 고려하여 상호보완적인 두가지 방법론을 잘 섞어서 좋은 성능을 내었고, 그 이유가 납득하기 쉬운 논문이라고 생각합니다.</p>

<p><em><strong>Weak Points.</strong></em></p>

<p>그래프데이터에 Transformer를 적용한 다른 논문의 architecture인 Graphormer에서 사용한 SPD만의 장점은 직접적으로 연결되어있지 않은, 아주 멀리에 위치한 노드쌍이더라도 shortest path상의 weighted edge aggregation을 하는 만큼 그러한 특성이 반영되면 좋은 그래프 구조/ 데이터셋에서는 더 좋은 성능을 보입니다. 이에따라 작은 k-hop의 subgraph를 고려하는 SAT가 capture하지 못하는 부분이 있을 것으로 생각됩니다.</p>

<hr />

<h1 id="author-information"><strong>Author Information</strong></h1>

<ul>
  <li>
    <p>Sujin Yun</p>
  </li>
  <li>
    <p>GSDS, KAIST</p>
  </li>
</ul>

<h1 id="6-reference--additional-materials"><strong>6. Reference &amp; Additional materials</strong></h1>

<ul>
  <li>
    <p>Github Implementation : <a href="https://github.com/BorgwardtLab/SAT"></a><a href="https://github.com/BorgwardtLab/SAT">https://github.com/BorgwardtLab/SAT</a></p>
  </li>
  <li>
    <p>Reference : <a href="https://arxiv.org/abs/2202.03036">Structure-Aware Transformer for Graph Representation Learning</a></p>
  </li>
</ul>

    </div>



</article>
<script src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.5.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>







<hr class="shaded"/>

<footer>
            <div class="row">
                <div class="col-lg-12 footer">
               &copy;2024 Copyright 2021 Google LLC. All rights reserved. <br />
 Site last generated: Apr 12, 2024 <br />
<p><img src="images/company_logo.png" alt="Company logo"/></p>
                </div>
            </div>
</footer>






        </div>
    <!-- /.row -->
</div>
<!-- /.container -->
</div>
<!-- /#main -->
    </div>

</body>

<!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-66296557-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>





</html>


