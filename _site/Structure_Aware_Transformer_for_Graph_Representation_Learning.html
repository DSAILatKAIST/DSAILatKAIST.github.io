<!DOCTYPE html>
<html>
<head>
    <meta name="google-site-verification" content="1tAPTdgw0-t8G2Bya463OpBtYUbj9Um93gfnsowYKLw" />
    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-CV4PTXQSTW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-CV4PTXQSTW');
</script>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="">
<meta name="keywords" content="reviews,  ">
<title>[ICML 2022] Structure-Aware Transformer for Graph Representation Learning | Awesome Reviews</title>
<link rel="stylesheet" href="css/syntax.css">

<link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
<!--<link rel="stylesheet" type="text/css" href="css/bootstrap.min.css">-->
<link rel="stylesheet" href="css/modern-business.css">
<!-- Latest compiled and minified CSS -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<link rel="stylesheet" href="css/customstyles.css">
<link rel="stylesheet" href="css/boxshadowproperties.css">
<!-- most color styles are extracted out to here -->
<link rel="stylesheet" href="css/theme-blue.css">

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="js/jquery.navgoco.min.js"></script>


<!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<!-- Anchor.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.2.0/anchor.min.js"></script>
<script src="js/toc.js"></script>
<script src="js/customscripts.js"></script>

<link rel="shortcut icon" href="images/favicon.ico">

<!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
<![endif]-->

<link rel="alternate" type="application/rss+xml" title="DSAILatKAIST.github.io" href="http://localhost:4000/feed.xml">


	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	TeX: {
		equationNumbers: {
		autoNumber: "AMS"
		}
	},
	tex2jax: {
		inlineMath: [ ['$', '$'] ],
		displayMath: [ ['$$', '$$'] ],
		processEscapes: true,
		}
	});
	MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
		alert("Math Processing Error: "+message[1]);
	});
	MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
		alert("Math Processing Error: "+message[1]);
	});
</script>

<script type="text/javascript" async
	src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



<script type="text/javascript" async
 src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>




    <script>
        $(document).ready(function() {
            // Initialize navgoco with default options
            $("#mysidebar").navgoco({
                caretHtml: '',
                accordion: true,
                openClass: 'active', // open
                save: false, // leave false or nav highlighting doesn't work right
                cookie: {
                    name: 'navgoco',
                    expires: false,
                    path: '/'
                },
                slide: {
                    duration: 400,
                    easing: 'swing'
                }
            });

            $("#collapseAll").click(function(e) {
                e.preventDefault();
                $("#mysidebar").navgoco('toggle', false);
            });

            $("#expandAll").click(function(e) {
                e.preventDefault();
                $("#mysidebar").navgoco('toggle', true);
            });

        });

    </script>
    <script>
        $(function () {
            $('[data-toggle="tooltip"]').tooltip()
        })
    </script>
    <script>
        $(document).ready(function() {
            $("#tg-sb-link").click(function() {
                $("#tg-sb-sidebar").toggle();
                $("#tg-sb-content").toggleClass('col-md-9');
                $("#tg-sb-content").toggleClass('col-md-12');
                $("#tg-sb-icon").toggleClass('fa-toggle-on');
                $("#tg-sb-icon").toggleClass('fa-toggle-off');
            });
        });
    </script>
    


	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	TeX: {
		equationNumbers: {
		autoNumber: "AMS"
		}
	},
	tex2jax: {
		inlineMath: [ ['$', '$'] ],
		displayMath: [ ['$$', '$$'] ],
		processEscapes: true,
		}
	});
	MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
		alert("Math Processing Error: "+message[1]);
	});
	MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
		alert("Math Processing Error: "+message[1]);
	});
</script>

<script type="text/javascript" async
	src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



<script type="text/javascript" async
 src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>


</head>
<body>
<!-- Navigation -->
<nav class="navbar navbar-inverse navbar-static-top">
    <div class="container topnavlinks">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="fa fa-home fa-lg navbar-brand" href="index.html">&nbsp;<span class="projectTitle"> Awesome Reviews</span></a>
        </div>
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                <!-- toggle sidebar button -->
                <li><a id="tg-sb-link" href="#"><i id="tg-sb-icon" class="fa fa-toggle-on"></i> Nav</a></li>
                <!-- entries without drop-downs appear here -->




                
                
                
                <li><a href="introduction">Introduction</a></li>
                
                
                
                <li><a href="https://statistics.kaist.ac.kr/" target="_blank" rel="noopener">KAIST ISysE</a></li>
                
                
                
                <!-- entries with drop-downs appear here -->
                <!-- conditional logic to control which topnav appears for the audience defined in the configuration file.-->
                
                
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">Reviews<b class="caret"></b></a>
                    <ul class="dropdown-menu">
                        
                        
                        <li><a href="reviews_kse801_2022.html">KSE801 (2022)</a></li>
                        
                        
                        
                        <li><a href="reviews_DS503_2023.html">DS503 (2023)</a></li>
                        
                        
                        
                        <li><a href="reviews_DS535_2023.html">DS535 (2023)</a></li>
                        
                        
                    </ul>
                </li>
                
                
                
			<li>



  <a class="email" title="Submit feedback" href="#" onclick="javascript:window.location='mailto:swkim@kaist.ac.kr?subject=Question about reviews feedback&body=I have some feedback about the [ICML 2022] Structure-Aware Transformer for Graph Representation Learning page: ' + window.location.href;"><i class="fa fa-envelope-o"></i> Feedback</a>

</li>



		
                <!--comment out this block if you want to hide search-->
                <li>
                    <!--start search-->
                    <div id="search-demo-container">
                        <input type="text" id="search-input" placeholder="search...">
                        <ul id="results-container"></ul>
                    </div>
                    <script src="js/jekyll-search.js" type="text/javascript"></script>
                    <script type="text/javascript">
                            SimpleJekyllSearch.init({
                                searchInput: document.getElementById('search-input'),
                                resultsContainer: document.getElementById('results-container'),
                                dataSource: 'search.json',
                                searchResultTemplate: '<li><a href="{url}" title="[ICML 2022] Structure-Aware Transformer for Graph Representation Learning">{title}</a></li>',
                    noResultsText: 'No results found.',
                            limit: 10,
                            fuzzy: true,
                    })
                    </script>
                    <!--end search-->
                </li>
            </ul>
        </div>
        </div>
        <!-- /.container -->
</nav>



<!-- Page Content -->
<div class="container">
  <div id="main">
    <!-- Content Row -->
    <div class="row">
        
        
            <!-- Sidebar Column -->
            <div class="col-md-3" id="tg-sb-sidebar">
                

<ul id="mysidebar" class="nav">
  <li class="sidebarTitle"> </li>
  
  
  
  <li>
      <a title="How to contribute" href="#">How to contribute</a>
      <ul>
          
          
          
          <li><a title="How to contribute?" href="how_to_contribute.html">How to contribute?</a></li>
          
          
          
          
          
          
          <li><a title="Review template (Example)" href="template.html">Review template (Example)</a></li>
          
          
          
          
      </ul>
   </li>
     
      
  
  <li>
      <a title="Reviews" href="#">Reviews</a>
      <ul>
          
          
          
          <li><a title="KSE801 (2022F)" href="reviews_kse801_2022.html">KSE801 (2022F)</a></li>
          
          
          
          
          
          
          <li><a title="DS503 (2023S)" href="reviews_DS503_2023.html">DS503 (2023S)</a></li>
          
          
          
          
          
          
          <li><a title="DS535 (2023F)" href="reviews_DS535_2023.html">DS535 (2023F)</a></li>
          
          
          
          
      </ul>
   </li>
     
      
      
      <!-- if you aren't using the accordion, uncomment this block:
         <p class="external">
             <a href="#" id="collapseAll">Collapse All</a> | <a href="#" id="expandAll">Expand All</a>
         </p>
         -->
</ul>

<!-- this highlights the active parent class in the navgoco sidebar. this is critical so that the parent expands when you're viewing a page. This must appear below the sidebar code above. Otherwise, if placed inside customscripts.js, the script runs before the sidebar code runs and the class never gets inserted.-->
<script>$("li.active").parents('li').toggleClass("active");</script>



            </div>
            
        

        <!-- Content Column -->
        <div class="col-md-9" id="tg-sb-content">
            <script type="text/x-mathjax-config">
MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/math...">
</script>
<article class="post" itemscope itemtype="https://schema.org/BlogPosting">

    <header class="post-header">
        <h1 class="post-title" itemprop="name headline">[ICML 2022] Structure-Aware Transformer for Graph Representation Learning</h1>
        <p class="post-meta"><time datetime="2023-04-20T00:00:00+09:00" itemprop="datePublished">Apr 20, 2023</time> /
            
            
            
            
            

        </p>


    </header>

    <div class="post-content" itemprop="articleBody">

        

        <h1 id="structure-aware-transformer-for-graph-representation-learning"><strong>Structure-Aware Transformer for Graph Representation Learning</strong></h1>

<p><em><strong>Background before reading this review.</strong></em></p>

<p>Graphêµ¬ì¡°ì— ë§ê²Œ Transformerë¥¼ ì ìš©í•˜ì—¬ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¸ SATë¥¼ ì œì‹œí•œ ë…¼ë¬¸ <a href="https://arxiv.org/abs/2202.03036">Structure-Aware Transformer for Graph Representation Learning</a>ë¥¼ ì½ê¸°ì „ì— ì•Œê³  ë„˜ì–´ê°€ì•¼í•  Graph Notation, Transformerì— ëŒ€í•œ ì„¤ëª… ë“± ê°„ë‹¨í•˜ê²Œ ì§šê³  ë„˜ì–´ê°€ë©´ ì¢‹ì€ ë‚´ìš©ë“¤ì…ë‹ˆë‹¤. ì‚¬ì „ ì§€ì‹ì´ ìˆìœ¼ì‹  ê²½ìš°, ë°”ë¡œ ë³¸ë¬¸ìœ¼ë¡œ ë„˜ì–´ê°€ì…”ë„ ì¢‹ìŠµë‹ˆë‹¤.</p>

<p>*Notation</p>

<p>$G = (V, E, \mathbf X)$</p>

<ul>
  <li>
    <p>node $u \in V$</p>
  </li>
  <li>
    <p>node attribute $x_u \in  \mathcal X \subset  \mathbb R^d$</p>
  </li>
  <li>
    <p>$\mathbf X \in  \mathbb R^{n \times d}$</p>
  </li>
</ul>

<p><strong>Transformer êµ¬ì„± ìš”ì†Œ</strong></p>

<ol>
  <li>Self-attention module</li>
</ol>

<ul>
  <li>
    <p>input node feature $\mathbf X$ê°€ linear projectionì„ í†µí•´ Query($\mathbf Q$), Key($\mathbf K$), Value($\mathbf V$)ë¡œ íˆ¬ì˜ë˜ê³ , ì´ë¥¼ í™œìš©í•˜ì—¬ self-attentionì„ ê³„ì‚°í•©ë‹ˆë‹¤.</p>
  </li>
  <li>
    <p>multi-head attention : self-attentionì˜ initializeë¥¼ ë‹¤ì–‘í•˜ê²Œ í•˜ì—¬ í‘œí˜„ë ¥ì„ ë†’ì˜€ìŠµë‹ˆë‹¤.</p>
  </li>
</ul>

<ol>
  <li>feed-forward NN</li>
</ol>

<ul>
  <li>self-attentionì˜ outputì´ skipconnectionì´ë‚˜ FFNë“±ì„ ê±°ì¹˜ë©´ í•˜ë‚˜ì˜ transforemer layerë¥¼ í†µê³¼í•œ ê²ƒ ì…ë‹ˆë‹¤.</li>
</ul>

<ol>
  <li>Absolute encoding</li>
</ol>

<ul>
  <li>ê·¸ë˜í”„ì˜ ìœ„ì¹˜ì /êµ¬ì¡°ì ì¸ representationì„ input node featureì— ë”í•˜ê±°ë‚˜ concatenateí•˜ì—¬ Transformerì˜ inputìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. (Vanilla transformerì˜ PEì™€ ê°™ì€ ì—­í• )</li>
</ul>

<p><strong>Graph Transformerì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” Positional encoding methodë“¤</strong></p>

<p>ìì£¼ ì‚¬ìš©ë˜ëŠ” PEë¡œëŠ” ë‹¤ìŒ ë‘ê°€ì§€ë¥¼ ê¼½ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ Positional Encodingë“¤ì˜ ë¬¸ì œëŠ” ë…¸ë“œì™€ ê·¸ ì´ì›ƒë“¤ ì‚¬ì´ì˜ structural similarityë¥¼ ë°˜ì˜í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ê°ê°ì— ëŒ€í•œ ì„¤ëª…ì€ ë§í¬ë¥¼ íƒ€ê³  ë“¤ì–´ê°€ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<ul>
  <li>
    <p><a href="https://paperswithcode.com/method/laplacian-pe">Laplacian PE</a></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/pdf/2110.07875.pdf">Random Walk PE</a></p>
  </li>
</ul>

<ol>
  <li>Self-attention and kernel smoothing</li>
</ol>

<p>$\operatorname{Attn}\left(x_v\right)=\sum_ {u \in V} \frac{\kappa_ {\exp }\left(x_v, x_u\right)}{\sum_ {w \in V} \kappa_ {\exp }\left(x_v, x_w\right)} f\left(x_u\right), \forall v \in V$</p>

<ul>
  <li>
    <p>linear value function $f(x) = \mathbf W_ {\mathbf V}x$</p>
  </li>
  <li>
    <p>$\kappa_ {\exp }$ (non-symmetric) exponential kernel parameterized by $\mathbf W_ {\mathbf Q}, \mathbf W_ {\mathbf K}$</p>
  </li>
</ul>

<p>$\kappa_ {\exp }\left(x, x^{\prime}\right):=\exp  \left(\left\langle\mathbf{W}_ {\mathbf{Q}} x, \mathbf{W}_ {\mathbf{K}} x^{\prime}\right\rangle / \sqrt{d_ {\text {out }}}\right)$</p>

<ul>
  <li>
    <p>$\langle  \cdot, \cdot\rangle$ : dotproduct</p>
  </li>
  <li>
    <p>í•™ìŠµê°€ëŠ¥í•œ exponential kernel</p>
  </li>
  <li>
    <p>(-) only position-aware, not structure-aware encoding</p>
  </li>
</ul>

<h1 id="1-problem-definition"><strong>1. Problem Definition</strong></h1>

<h2 id="limitations-of-gnn"><em><strong>Limitations of GNN</strong></em></h2>

<ol>
  <li>
    <p>limited expressiveness : GNNì€ message passingê³¼ì •ì—ì„œì˜ aggregation operationì˜ íŠ¹ì„±ìœ¼ë¡œ ì¸í•´ ìµœëŒ€ 1-WL testì˜ í‘œí˜„ë ¥ì„ ê°€ì§‘ë‹ˆë‹¤. GNNì˜ WL-testì™€ expressionì— ëŒ€í•œ ë¶„ì„ì€ GINì„ ì œì‹œí•œ ë…¼ë¬¸ì¸ <a href="https://arxiv.org/abs/1810.00826">How Powerful are Graph Neural Networks?</a> ì—ì„œ ì œì‹œë˜ì—ˆìŠµë‹ˆë‹¤.</p>
  </li>
  <li>
    <p>Over-smoothing problem : GNN layerì˜ ìˆ˜ê°€ ì¶©ë¶„íˆ ì»¤ì§€ë©´ ëª¨ë“  node representationì´ ìƒìˆ˜ë¡œ ìˆ˜ë ´í•˜ê²Œë©ë‹ˆë‹¤.</p>
  </li>
  <li>
    <p>Over-squashing problem : ê·¸ë˜í”„ì˜ ìˆ˜ë§ì€ ë©”ì„¸ì§€ë“¤ì´ ê³ ì •ëœ ê¸¸ì´ì˜ ë²¡í„° í•˜ë‚˜ë¡œ ì••ì¶•ë˜ì–´ ë°œìƒí•˜ëŠ” ê·¸ë˜í”„ â€œbottleneckâ€ìœ¼ë¡œ ì¸í•´ ë©€ë¦¬ ìœ„ì¹˜í•œ ë…¸ë“œì˜ ë©”ì„¸ì§€ê°€ íš¨ìœ¨ì ìœ¼ë¡œ ì „íŒŒë˜ì§€ ì•ŠëŠ” ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤.</p>
  </li>
</ol>

<p><strong>â‡’ Beyond neighborhood aggregation!</strong></p>

<h2 id="transformer"><em><strong>Transformer</strong></em></h2>

<p>Transformerë¥¼ ì ìš©í–ˆì„ ë•Œì˜ ì¥ì ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.</p>

<ul>
  <li>
    <p>í•˜ë‚˜ì˜ self-attention layerë¥¼ í†µí•´ ê·¸ë˜í”„ë‚´ì˜ ì–´ë–¤ ë…¸ë“œìŒì´ë“ ì§€ ê·¸ ì‚¬ì´ì˜ ìƒí˜¸ì‘ìš©ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
  </li>
  <li>
    <p>GNNê³¼ ë‹¬ë¦¬ ì¤‘ê°„ ê³„ì¸µì—ì„œ structural inductive biasê°€ ë°œìƒí•˜ì§€ ì•Šì•„ GNNì˜ í‘œí˜„ë ¥ í•œê³„ë¥¼ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

    <p>ë°˜ë©´, ë‹¨ì ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.</p>
  </li>
  <li>
    <p>graph structure infoë¥¼ ì–¼ë§ˆë‚˜ í•™ìŠµí•˜ëŠ”ì§€ input node featureì—ë§Œ structural, positional ì •ë³´ë¥¼ ì¸ì½”ë”©í•˜ì—¬ ë„£ê¸° ë•Œë¬¸ì— ì œí•œì ì…ë‹ˆë‹¤.</p>
  </li>
  <li>
    <p>ë…¸ë“œì— ëŒ€í•œ structural, positional ì •ë³´ë§Œ input node featureë¡œ ì¸ì½”ë”©í•˜ê¸° ë•Œë¬¸ì—, ê·¸ë˜í”„ êµ¬ì¡° ìì²´ì—ì„œ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ì •ë³´ì˜ ì–‘ì´ ì œí•œì ì…ë‹ˆë‹¤.</p>

    <p>ë”°ë¼ì„œ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ê³ ì í•˜ëŠ” Graph Transformerì˜ Goalì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.</p>
  </li>
</ul>

<blockquote>
  <p>ğŸ’¡ Goal : ê·¸ë˜í”„ ë°ì´í„°ì— Transformerë¥¼ ì ì ˆíˆ ë³€í˜•í•´ ì ìš©í•˜ì—¬ ê·¸ë˜í”„ êµ¬ì¡°ë¥¼ ì˜ ë°˜ì˜í•˜ê³  ë†’ì€ í‘œí˜„ë ¥ì„ ê°€ì§€ëŠ” Achitectureë¥¼ ë””ìì¸í•˜ëŠ” ê²ƒ</p>
</blockquote>

<h1 id="2-motivation"><strong>2. Motivation</strong></h1>

<h2 id="message-passing-graph-neural-networks"><em><strong>Message passing graph neural networks.</strong></em></h2>

<p>ìµœëŒ€ 1-WL testë¡œ ì œí•œëœ í‘œí˜„ë ¥, over-smoothing, over-quashing</p>

<h2 id="limitations-of-existing-approaches"><em><strong>Limitations of existing approaches</strong></em></h2>

<p>ê¸°ì¡´ì— Graphêµ¬ì¡°ì— Transformerë¥¼ ì ìš©í•˜ëŠ” ì‹œë„ê°€ ì—†ì—ˆë˜ ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ì–´ë–¤ê²ƒì´ ë¬¸ì œê°€ ë˜ì—ˆì„ê¹Œìš”?</p>

<ul>
  <li>ë…¸ë“œë“¤ ì‚¬ì´ positional relationshipë§Œ ì¸ì½”ë”©í•˜ê³ , strucutral relationshipì„ ì§ì ‘ ì¸ì½”ë”©í•˜ì§€ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ì— ë”°ë¼ ë…¸ë“œë“¤ ì‚¬ì´ structural similarityë¥¼ í™•ì¸í•˜ê¸°ê°€ ì–´ë µê³ , ë…¸ë“œë“¤ ì‚¬ì´ì˜ structural interactionì„ ëª¨ë¸ë§í•˜ëŠ”ë° ì‹¤íŒ¨í•œê²ƒìœ¼ë¡œ ë¶„ì„í•˜ì˜€ìŠµë‹ˆë‹¤.</li>
</ul>

<p>ë‹¤ìŒì˜ ê·¸ë¦¼ ì˜ˆì‹œë¥¼ ë³´ë©´ ì´í•´ê°€ ë” ì‰½ìŠµë‹ˆë‹¤.</p>

<p>ex.
<img src="https://github.com/sujinyun999/LearningOnGraph/assets/69068083/4472bb78-65cc-43bf-90be-8dcd203616d8" alt="Untitled" /></p>

<p>G1ê³¼ G2ì—ì„œ ìµœë‹¨ê±°ë¦¬ë¥¼ í™œìš©í•œ positional encodingì„ í• ê²½ìš° node uì™€ vê°€ ë‹¤ë¥¸ ë…¸ë“œë“¤ì— ëŒ€í•´ ëª¨ë‘ ê°™ì€ representationì„ ê°€ì§€ê²Œë˜ì§€ë§Œ, ê·¸ë˜í”„ì˜ ì‹¤ì œ êµ¬ì¡°ëŠ” ë‹¤ë¦…ë‹ˆë‹¤. 
â†’ ì´ ì§€ì ì´ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” ê¸°ì¡´ Graph Transformerì˜ ë¬¸ì œ, ì¦‰, strucure awareì— ì‹¤íŒ¨í•œ ê²ƒ ì…ë‹ˆë‹¤.</p>

<blockquote>
  <p>ğŸ’¡ Message-passing GNNê³¼ Transformer architecture ê°ê°ì˜ ì¥ì ì„ ì‚´ë ¤ local, global infoë¥¼ ëª¨ë‘ ê³ ë ¤í•˜ëŠ” transformer architectureë¥¼ ì œì•ˆ</p>
</blockquote>

<h2 id="contribution-of-this-paper"><em><strong>Contribution of this paper</strong></em></h2>

<p>Q. ê·¸ë ‡ë‹¤ë©´ ë…¼ë¬¸ì—ì„œ í•´ê²°í•˜ê³ ìí•˜ëŠ” Structure-Awareë¥¼ ìœ„í•´ Transformerêµ¬ì¡°ì— structural infoë¥¼ ì–´ë–»ê²Œ ì¸ì½”ë”©í• ê¹Œìš”?</p>

<p>ë…¼ë¬¸ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ëŒ€ë‹µí•©ë‹ˆë‹¤.</p>

<p>A. Structure-aware self attentionë¥¼ ë„ì…í•œ Structre-Aware Transformer(SAT)</p>

<ol>
  <li>reformulate the self-attention mechanism</li>
</ol>

<ul>
  <li>
    <p>kernel smoother</p>
  </li>
  <li>
    <p>ì›ë˜ ë…¸ë“œ featureì— ì ìš©í•˜ëŠ” exponential ì»¤ë„ì„ í™•ì¥í•˜ì—¬ ê° ë…¸ë“œê°€ ì¤‘ì‹¬ì¸ subgraph representationì„ ì¶”ì¶œí•˜ì—¬ local structureì—ë„ ì ìš©í•©ë‹ˆë‹¤.</p>
  </li>
</ul>

<ol>
  <li>subgraph representationë“¤ì„ ìë™ì ìœ¼ë¡œ ë§Œë“¤ì–´ë‚´ëŠ” ë°©ë²•ë¡  ì œì•ˆ</li>
</ol>

<ul>
  <li>ì´ë¥¼ í†µí•´ kernel smootherê°€ êµ¬ì¡°ì /íŠ¹ì„±ì  ìœ ì‚¬ì„±ì„ í¬ì°©í•  ìˆ˜ ìˆê²Œë©ë‹ˆë‹¤.</li>
</ul>

<ol>
  <li>
    <p>GNNìœ¼ë¡œ ê·¸ë˜í”„ì˜ subgraph infoë¥¼ í¬í•¨í•˜ëŠ” node representationì„ ë§Œë“¤ì–´ ê¸°ì¡´ GNNì— ì¶”ê°€ì ì¸ êµ¬ì¡° ê°œì„  ì—†ì´ë„ ë” ë†’ì€ ì„±ëŠ¥ì„ ëƒ…ë‹ˆë‹¤.</p>
  </li>
  <li>
    <p>Transformerì˜ ì„±ëŠ¥í–¥ìƒì´ structure-awareí•œ ì¸¡ë©´ì—ì„œ ì¼ì–´ë‚œ ê²ƒì„ ì¦ëª…í•˜ê³  absolute encodingì´ ì¶”ê°€ëœ transfoemrë³´ë‹¤ SATê°€ ì–¼ë§ˆë‚˜ interpretableí•œì§€ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.</p>
  </li>
</ol>

<h1 id="3-method"><strong>3. Method</strong></h1>

<h2 id="structure-aware-transformer"><em><strong>Structure-Aware Transformer</strong></em></h2>

<h3 id="1-structure-aware-self-attention"><em>1. <strong>Structure-Aware Self-attention</strong></em></h3>

<p>position-awareí•œ structural encodingì— ë…¸ë“œë“¤ ì‚¬ì´ structural similarityë¥¼ í¬í•¨í•˜ê¸° ìœ„í•´ ê° ë…¸ë“œì˜ local structureì— ê´€í•œ generalized kernelì„ ì¶”ê°€í•©ë‹ˆë‹¤.</p>

<p>ê° ë…¸ë“œê°€ ì¤‘ì‹¬ì´ë˜ëŠ” subgraph setì„ ì¶”ê°€í•¨ìœ¼ë¡œì¨ structure-aware attentionì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<p>$\operatorname{SA-Attn}\left(v\right):=\sum_ {u \in V} \frac{\kappa_ {\text{graph} }\left(S_G(v), S_G(u)\right)}{\sum_ {w \in V} \kappa_ {\text{graph}}\left(S_G(v), S_G(u)\right)} f\left(x_u\right)$</p>

<ul>
  <li>
    <p>$S_G(v)$ : node feature $\mathbf X$ì™€ ì—°ê´€ëœ $v$ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œí•˜ëŠ” subgraph</p>
  </li>
  <li>
    <p>$\kappa_ {\text{graph} }$ : subgraphìŒì„ ë¹„êµí•˜ëŠ” kernel</p>
  </li>
</ul>

<p>â‡’ attribute &amp; structural similarity ëª¨ë‘ í‘œí˜„ ê°€ëŠ¥í•œ expressive node representationì„ ìƒì„± â†’ table 1</p>

<p>â‡’ ë™ì¼í•œ subgraph êµ¬ì¡°ë¥¼ ê°€ì§€ëŠ” ê²½ìš°ì—ë§Œ permutation equivariantí•œ ì„±ì§ˆì„ ê°–ê²Œë¨</p>

<p>$\kappa_ {\text {graph }}\left(S_G(v), S_G(u)\right)=\kappa_ {\exp }(\varphi(v, G), \varphi(u, G))$</p>

<ul>
  <li>
    <p>$\varphi(v, G)$ : feature $\mathbf X$ë¥¼ ê°€ì§€ëŠ” node $v$ê°€ ì¤‘ì‹¬ì— ìˆëŠ” subgraphì˜ vector representationì„ ë§Œë“¤ì–´ë‚´ëŠ” structure extractor</p>
  </li>
  <li>
    <p>GNNì´ë‚˜ differentiable Graph kernelë“± subgraphì˜ representationì„ ë§Œë“¤ ìˆ˜ ìˆëŠ” ì–´ëŠ ëª¨ë¸ì´ë“  ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
  </li>
  <li>
    <p>Task/data íŠ¹ì„±ì— ë”°ë¼ Edge attributeì„ í™œìš©í•  í•„ìš”ê°€ ìˆëŠ” ê²½ìš° ê·¸ì— ë§ëŠ”GNNì„ ì„ íƒí•˜ëŠ” ë””ìì¸ ì´ˆì´ìŠ¤ê°€ ìƒê¹ë‹ˆë‹¤. edge attributeì„ ë”°ë¡œ í™œìš©í•˜ì§€ëŠ” ì•Šê³  subgraph extractorì—ì„œ í™œìš©í•©ë‹ˆë‹¤.</p>
  </li>
</ul>

<p><em><strong>k-subtree GNN extractor.</strong></em></p>

<p>$\varphi(u, G) = \operatorname{GNN}_G^{(k)}(u)$</p>

<ul>
  <li>
    <p>node uì—ì„œ ì‹œì‘í•˜ëŠ” k-subtree structureì˜ representationì„ ìƒì„±í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.</p>
  </li>
  <li>
    <p>at most 1-WL test : ìœ„ì—ì„œ ì§€ì í•œ GNNì˜ í•œê³„ì™€ ê°™ì´, ìµœëŒ€ 1WL Testì˜ í‘œí˜„ë ¥ì„ ê°€ì§‘ë‹ˆë‹¤.</p>
  </li>
  <li>
    <p>ë…¼ë¬¸ì—ì„œëŠ” ì‹¤í—˜ì„ í†µí•´ ì‘ì€ k ê°’ì´ë”ë¼ë„ over-smoothing, over-squashing issueì—†ì´ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ëŠ”ê²ƒì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.</p>
  </li>
</ul>

<p><em><strong>k-subgraph GNN extractor.</strong></em></p>

<p>$\varphi(u, G) = \sum_ {v \in  \mathcal N_k(u)} \operatorname{GNN}_G^{(k)}(v)$</p>

<ul>
  <li>
    <p>node uì˜ representationë§Œì„ ì‚¬ìš©í•˜ëŠ”ë°ì„œ ë‚˜ì•„ê°€ node uê°€ ì¤‘ì‹¬ì´ ë˜ëŠ” k-hop subgraphì „ì²´ì˜ representationì„ ìƒì„±í•˜ê³  í™œìš©í•©ë‹ˆë‹¤.</p>
  </li>
  <li>
    <p>node u ì˜ k-hopì´ì›ƒ $\mathcal N_k(u)$ì— ëŒ€í•´ ê° ë…¸ë“œì— GNNì„ ì ìš©í•œ node representationì„ pooling(ë…¼ë¬¸ì—ì„œëŠ” summation)í•©ë‹ˆë‹¤.</p>
  </li>
  <li>
    <p><strong>More powerful than 1-WL test!</strong> ìœ„ì—ì„œ k-subtree GNN extractorì™€ì˜ ê°€ì¥ í° ì°¨ì´ì…ë‹ˆë‹¤.</p>
  </li>
  <li>
    <p>original node representationê³¼ì˜ concatenationì„ í†µí•´ structural similarityë¿ë§Œ ì•„ë‹ˆë¼ attributed similarityë„ ë°˜ì˜í•©ë‹ˆë‹¤.</p>
  </li>
</ul>

<p>ì´ì™¸ì— ë‹¤ë¥¸ structure extractorë¡œ ë‹¤ìŒê³¼ ê°™ì€ ê²ƒë“¤ì„ ê³ ë ¤í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<p><em><strong>Other structure extractors.</strong></em></p>

<ul>
  <li>
    <p>directly learn a number of â€œhidden graphsâ€ as the â€œanchor subgraphsâ€ to represent subgraphs</p>
  </li>
  <li>
    <p>domain-specific GNNs</p>
  </li>
  <li>
    <p>non-parametric graph-kernel</p>
  </li>
</ul>

<h3 id="2-structure-aware-transformer"><em>2. Structure-Aware Transformer</em></h3>

<p><img src="https://user-images.githubusercontent.com/69068083/231114106-a71006e8-a9e5-44cb-b353-578ec4e09a80.png" alt="Untitled" /></p>

<p>self-attentionâ†’ skipconnection â†’ normalization layer â†’ FFN â†’ normalization layer</p>

<p><em><strong>Augmentation on skip connection.</strong></em></p>

<p>$xâ€™_v = x_c +1/ \sqrt {d_v} \operatorname{SA-Attn}\left(v\right)$</p>

<ul>
  <li>
    <p>$d_v$ : node $v$ì˜ degree</p>
  </li>
  <li>
    <p>degree factorë¥¼ í¬í•¨í•˜ì—¬ ì—°ê²°ì´ ë§ì€ graph componentë“¤ì´ ì••ë„ì ì¸ ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šë„ë¡í•©ë‹ˆë‹¤.</p>
  </li>
</ul>

<p>*graph-level taskë¥¼ ì§„í–‰í•´ì•¼ í•  ê²½ìš° input graphì— ë‹¤ë¥¸ ë…¸ë“œì™€ì˜ connectivityì—†ì´ virtual <code class="language-plaintext highlighter-rouge">[cls] </code>nodeë¥¼ ì¶”ê°€í•˜ê±°ë‚˜, node-level representationì„ sum/average ë“±ìœ¼ë¡œ aggregation</p>

<h3 id="3-combination-with-absolute-encoding"><em>3. Combination with Absolute Encoding</em></h3>

<p>ìœ„ì˜ structure aware self-attentionì— ì¶”ê°€ë¡œ absolute encodingì„ ì¶”ê°€í•˜ê²Œ ë˜ë©´ postion-awareí•œ íŠ¹ì„±ì´ ì¶”ê°€ë˜ì–´ ê¸°ì¡´ì˜ ì •ë³´ë¥¼ ë³´ì™„í•˜ëŠ” ì—­í• ì„ í•˜ê²Œë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë””ìì¸ ì´ˆì´ìŠ¤ì˜ ì¡°í•©ì„ í†µí•´ ì„±ëŠ¥í–¥ìƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.</p>

<p><strong>RandomWalk PE</strong></p>

<p>Absolute PEë§Œ ì‚¬ìš©í•  ê²½ìš° structural biasê°€ ê³¼ë„í•˜ê²Œ ë°œìƒí•˜ì§€ ì•Šì•„ì„œ ë‘ê°œì˜ ë…¸ë“œê°€ ìœ ì‚¬í•œ local structureë¥¼ ê°–ê³  ìˆë”ë¼ë„ ë¹„ìŠ·í•œ node representationì´ ìƒì„±ë˜ëŠ”ê²ƒì„ ë³´ì¥í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤!</p>

<p>â†’ ì´ëŠ” Structural, positional signìœ¼ë¡œ ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” distanceë‚˜ Laplacian-based positional representationì´ ë…¸ë“œë“¤ ì‚¬ì´ì˜ structural simialrityë¥¼ í¬í•¨í•˜ì§€ ì•Šê¸°ë•Œë¬¸ìœ¼ë¡œ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<blockquote>
  <p>ğŸ“Œ Structural aware attenrionì€ inductive biasê°€ ë” ê°•í•˜ë”ë¼ë„ ë…¸ë“œì˜ strucutral similarityë¥¼ ì¸¡ì •í•˜ëŠ”ë° ì í•©í•˜ì—¬ ìœ ì‚¬í•œ subgraphêµ¬ì¡°ë¥¼ ê°€ì§„ ë…¸ë“œë“¤ì´ ë¹„ìŠ·í•œ embeddingì„ ê°–ê²Œí•˜ê³ , expressivityê°€ í–¥ìƒë˜ì–´ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.</p>
</blockquote>

<h3 id="4-expressivity-analysis"><em>4. Expressivity Analysis</em></h3>

<p>SATì—ì„œëŠ” ê°ë…¸ë“œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œí•˜ëŠ” k-subgraph GNN extractorê°€ ë„ì…ë˜ì–´ ì ì–´ë„ subgraph representationë§Œí¼ì€ expressive(More than 1WL Test)í•˜ë‹¤ëŠ” ê²ƒì„ ë³´ì¥í•©ë‹ˆë‹¤.</p>

<h1 id="4-experiment"><strong>4. Experiment</strong></h1>

<h3 id="experiment-setup"><em><strong>Experiment setup</strong></em></h3>

<p><em><strong>Dataset</strong></em></p>

<ul>
  <li>ZINC :
    <ul>
      <li>from <a href="https://arxiv.org/abs/1610.02415">Automatic chemical design using a data-driven continuous representation of molecules</a></li>
      <li>250,000ê°œì˜ ë¶„ì ê·¸ë˜í”„êµ¬ì¡°,  with up to 38 heavy atoms</li>
      <li>task is to regress the penalized <code class="language-plaintext highlighter-rouge">logP</code> (also called constrained solubility)</li>
    </ul>
  </li>
  <li>CLUSTER :
    <ul>
      <li>from <a href="https://arxiv.org/abs/2003.00982">Benchmarking Graph Neural Networks</a></li>
      <li>task is semi-supervised graph clustering (node classification)</li>
    </ul>
  </li>
  <li>PATTERN
    <ul>
      <li>from <a href="https://arxiv.org/abs/2003.00982">Benchmarking Graph Neural Networks</a></li>
      <li>task is semi-supervised graph pattern recognition</li>
    </ul>
  </li>
  <li>OGBG-PPA
    <ul>
      <li>from <a href="https://arxiv.org/abs/2005.00687">Open Graph Benchmark: Datasets for Machine Learning on Graphs</a></li>
      <li>Protein-Protein Association Network</li>
      <li>task is to predict new association edges given the training edges</li>
    </ul>
  </li>
  <li>OGBG-CODE2
    <ul>
      <li>from <a href="https://arxiv.org/abs/2005.00687">Open Graph Benchmark: Datasets for Machine Learning on Graphs</a></li>
      <li>Abstract Syntax Tree of Source Code</li>
      <li>ASTë¡œ í‘œì‹œë˜ëŠ” Python ë©”ì„œë“œ ë³¸ë¬¸ê³¼ í•´ë‹¹ ë…¸ë“œ ê¸°ëŠ¥ì´ ì£¼ì–´ì§€ë©´ ë©”ì„œë“œ ì´ë¦„ì„ í˜•ì„±í•˜ëŠ” í•˜ìœ„ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” task</li>
    </ul>
  </li>
</ul>

<p><em><strong>Baseline</strong></em></p>

<ul>
  <li>
    <p><em><strong>GNNs</strong></em></p>
  </li>
  <li>
    <p>GCN</p>
  </li>
  <li>
    <p>GraphSAGE</p>
  </li>
  <li>
    <p>GAT</p>
  </li>
  <li>
    <p>GIN</p>
  </li>
  <li>
    <p>PNA</p>
  </li>
  <li>
    <p>Deeper GCN</p>
  </li>
  <li>
    <p>ExpC</p>
  </li>
  <li>
    <p><em><strong>Transformers</strong></em></p>
  </li>
  <li>
    <p>Original Transformer with RWPE</p>
  </li>
  <li>
    <p>Graph Transformer</p>
  </li>
  <li>
    <p>SAN</p>
  </li>
  <li>
    <p>Graphormer</p>
  </li>
  <li>
    <p>GraphTrans</p>
  </li>
</ul>

<h3 id="results"><em><strong>Results</strong></em></h3>

<p><strong>Table1.</strong> SATì™€ graph regression, classification taskì˜ sotaëª¨ë¸ê³¼ ë¹„êµ</p>

<ul>
  <li>ZINC datasetì˜ ê²½ìš° ì‘ì„ìˆ˜ë¡ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ì˜ë¯¸í•˜ëŠ” MAE(Mean Absolute Error), CLUSTERì™€ PATTERNì˜ ê²½ìš° ë†’ì„ìˆ˜ë¡ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ì˜ë¯¸í•˜ëŠ” Acurracyê°€ í‰ê°€ì§€í‘œë¡œ ì‚¬ìš©ë˜ì—ˆìŒ.</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/69068083/231114155-056893f6-8d16-4a59-b43b-62c76fd482a3.png" alt="Untitled" /></p>

<p><strong>Table2.</strong> SATì™€ OGBë°ì´í„°ì…‹ì—ì„œì˜ sotaëª¨ë¸ ë¹„êµ</p>

<ul>
  <li>OGB datasetì˜ ê²½ìš° ë†’ì„ìˆ˜ë¡ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ì˜ë¯¸í•˜ëŠ” Acurracy, F1 scoreê°€ í‰ê°€ì§€í‘œë¡œ ì‚¬ìš©ë˜ì—ˆìŒ.</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/69068083/231114185-23daa0d6-bc32-4838-93e8-0a6d09a17f7e.png" alt="Untitled" /></p>

<p><strong>Table3.</strong> structure extractorë¡œ ì‚¬ìš©í•œ GNNê³¼ì˜ ì„±ëŠ¥ë¹„êµ. Sparse GNNì„ ëª¨ë“  ê²½ìš°ì—ì„œ outperformí•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ</p>

<p><img src="https://user-images.githubusercontent.com/69068083/231114223-e6e32dfd-039b-4caa-b123-14e72e9fc867.png" alt="Untitled" /></p>

<p><strong>Fig3.</strong> ZINCë°ì´í„°ì…‹ì— SATì˜ ë‹¤ì–‘í•œ variantì‹¤í—˜</p>

<ul>
  <li>í‰ê°€ì§€í‘œ : MAE(ë” ì‘ì€ ì§€í‘œê°€ ì¢‹ì€ ì„±ëŠ¥ì„ ì˜ë¯¸)</li>
</ul>

<p><img src="https://user-images.githubusercontent.com/69068083/231114263-2ea26465-c8b3-4df8-b7d4-4d329d41d97b.png" alt="Untitled" /></p>

<ol>
  <li>structure extractorì—ì„œì˜ kì˜ ì˜í–¥ ë¹„êµ</li>
</ol>

<ul>
  <li>
    <p>k=0ì¼ë•Œ, Absolute encodingë§Œì„ í™œìš©í•˜ëŠ” vanilla transformerë‘ ê°™ë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>
  </li>
  <li>
    <p>k=3ì¼ë•Œ, optimal performanceë¥¼ ë³´ì„ì„ ì‹¤í—˜ì„ í†µí•´ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.</p>
  </li>
  <li>
    <p>k=4ë¥¼ ë„˜ì–´ì„œë©´ ì„±ëŠ¥ì´ ì•…í™”ë˜ëŠ”ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆëŠ”ë°, ì´ëŠ” GNNì—ì„œì˜ ì•Œë ¤ì§„ ì‚¬ì‹¤ì¸ ë” ì ì€ ìˆ˜ì˜ layerë¥¼ ê°€ì§€ëŠ” networkê°€ ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²ƒê³¼ ë§ˆì°¬ê°€ì§€ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.(Oversmoothing and Oversquashing)</p>
  </li>
</ul>

<ol>
  <li>Absolute encodingì˜ ì˜í–¥ ë¹„êµ</li>
</ol>

<ul>
  <li>
    <p>RandomWalkPE vs. Laplacian PE</p>
  </li>
  <li>
    <p>Structure-aware attentionì˜ ë„ì…ìœ¼ë¡œ ì¸í•œ ì„±ëŠ¥í–¥ìƒë³´ë‹¤ëŠ” ê·¸ ì •ë„ê°€ ë‚®ì•˜ì§€ë§Œ, RWPEë¥¼ ë„ì…í•  ê²½ìš° ì„±ëŠ¥ì´ ë” ì¢‹ì€ê²ƒìœ¼ë¡œ ë³´ì•˜ì„ ë•Œ, ë‘ê°€ì§€ encodingì´ ìƒí˜¸ë³´ì™„ì ì¸ ì—­í• ì„ í•œë‹¤ê³  í•´ì„í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.</p>
  </li>
</ul>

<ol>
  <li>Readout methodì˜ ì˜í–¥ ë¹„êµ</li>
</ol>

<ul>
  <li>
    <p>node-level representationì„ aggregateí•  ë•Œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ readoutìœ¼ë¡œ meanê³¼ sumì„ ë¹„êµí•˜ì˜€ìŠµë‹ˆë‹¤.</p>
  </li>
  <li>
    <p>ì¶”ê°€ë¡œ <code class="language-plaintext highlighter-rouge">[CLS]</code> í† í°ì„ í†µí•´ graph-level ì •ë³´ë¥¼ poolingí•˜ëŠ” ë°©ë²•ë„ ê°™ì´ ë¹„êµí•˜ì—¬ë³´ì•˜ìŠµë‹ˆë‹¤.</p>
  </li>
  <li>
    <p>GNNì—ì„œëŠ” readout methodì˜ ì˜í–¥ì´ ë§¤ìš° ì»¸ì§€ë§Œ SATì—ì„œëŠ” ë§¤ìš° ì•½í•œ ì˜í–¥ë§Œì„ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.</p>
  </li>
</ul>

<h1 id="5-conclusion"><strong>5. Conclusion</strong></h1>

<p><em><strong>Strong Points.</strong></em></p>

<p>structural infoë¥¼ graphormerì—ì„œì²˜ëŸ¼ íœ´ë¦¬ìŠ¤í‹±í•˜ê²Œ shortest path distance(SPD)ë¥¼ í™œìš©í•˜ì§€ ì•Šê³ , ê·¸ëŸ¬í•œ local infoë¥¼ ì˜ ë°°ìš°ëŠ” GNNìœ¼ë¡œ ëŒ€ì²´í•œ ì ì´ novelí•˜ë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>

<p>Transformerì˜ global receptive field íŠ¹ì„±ê³¼ GNNì˜ local structureíŠ¹ì„±ì´ ìƒí˜¸ë³´ì™„ì ì¸ë°,</p>

<p>encodingì— ìˆì–´ì„œë„</p>

<ol>
  <li>
    <p>RWPEë¥¼ í†µí•œ positional encoding</p>
  </li>
  <li>
    <p>k-subtree/subgraph GNNì„ í†µí•œ structure-aware attention</p>
  </li>
</ol>

<p>ë‘ê°€ì§€ê°€ ìƒí˜¸ë³´ì™„ì ì¸ ì—­í• ì„ í•©ë‹ˆë‹¤.</p>

<p>â†’ ê°ìê°€ ì˜ ë°°ìš°ëŠ” íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ ìƒí˜¸ë³´ì™„ì ì¸ ë‘ê°€ì§€ ë°©ë²•ë¡ ì„ ì˜ ì„ì–´ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ì—ˆê³ , ê·¸ ì´ìœ ê°€ ë‚©ë“í•˜ê¸° ì‰¬ìš´ ë…¼ë¬¸ì´ë¼ê³  ìƒê°í•©ë‹ˆë‹¤.</p>

<p><em><strong>Weak Points.</strong></em></p>

<p>ê·¸ë˜í”„ë°ì´í„°ì— Transformerë¥¼ ì ìš©í•œ ë‹¤ë¥¸ ë…¼ë¬¸ì˜ architectureì¸ Graphormerì—ì„œ ì‚¬ìš©í•œ SPDë§Œì˜ ì¥ì ì€ ì§ì ‘ì ìœ¼ë¡œ ì—°ê²°ë˜ì–´ìˆì§€ ì•Šì€, ì•„ì£¼ ë©€ë¦¬ì— ìœ„ì¹˜í•œ ë…¸ë“œìŒì´ë”ë¼ë„ shortest pathìƒì˜ weighted edge aggregationì„ í•˜ëŠ” ë§Œí¼ ê·¸ëŸ¬í•œ íŠ¹ì„±ì´ ë°˜ì˜ë˜ë©´ ì¢‹ì€ ê·¸ë˜í”„ êµ¬ì¡°/ ë°ì´í„°ì…‹ì—ì„œëŠ” ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. ì´ì—ë”°ë¼ ì‘ì€ k-hopì˜ subgraphë¥¼ ê³ ë ¤í•˜ëŠ” SATê°€ captureí•˜ì§€ ëª»í•˜ëŠ” ë¶€ë¶„ì´ ìˆì„ ê²ƒìœ¼ë¡œ ìƒê°ë©ë‹ˆë‹¤.</p>

<hr />

<h1 id="author-information"><strong>Author Information</strong></h1>

<ul>
  <li>
    <p>Sujin Yun</p>
  </li>
  <li>
    <p>GSDS, KAIST</p>
  </li>
</ul>

<h1 id="6-reference--additional-materials"><strong>6. Reference &amp; Additional materials</strong></h1>

<ul>
  <li>
    <p>Github Implementation : <a href="https://github.com/BorgwardtLab/SAT"></a><a href="https://github.com/BorgwardtLab/SAT">https://github.com/BorgwardtLab/SAT</a></p>
  </li>
  <li>
    <p>Reference : <a href="https://arxiv.org/abs/2202.03036">Structure-Aware Transformer for Graph Representation Learning</a></p>
  </li>
</ul>

    </div>



</article>
<script src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.5.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>







<hr class="shaded"/>

<footer>
            <div class="row">
                <div class="col-lg-12 footer">
               &copy;2024 Copyright 2021 Google LLC. All rights reserved. <br />
 Site last generated: Apr 12, 2024 <br />
<p><img src="images/company_logo.png" alt="Company logo"/></p>
                </div>
            </div>
</footer>






        </div>
    <!-- /.row -->
</div>
<!-- /.container -->
</div>
<!-- /#main -->
    </div>

</body>

<!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-66296557-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>





</html>


