<!DOCTYPE html>
<html>
<head>
    <meta name="google-site-verification" content="1tAPTdgw0-t8G2Bya463OpBtYUbj9Um93gfnsowYKLw" />
    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-CV4PTXQSTW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-CV4PTXQSTW');
</script>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="">
<meta name="keywords" content="reviews,  ">
<title>[WSDM-24] LLMRec: Large Language Models with Graph Augmentation for Recommendation | Awesome Reviews</title>
<link rel="stylesheet" href="css/syntax.css">

<link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
<!--<link rel="stylesheet" type="text/css" href="css/bootstrap.min.css">-->
<link rel="stylesheet" href="css/modern-business.css">
<!-- Latest compiled and minified CSS -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<link rel="stylesheet" href="css/customstyles.css">
<link rel="stylesheet" href="css/boxshadowproperties.css">
<!-- most color styles are extracted out to here -->
<link rel="stylesheet" href="css/theme-blue.css">

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="js/jquery.navgoco.min.js"></script>


<!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<!-- Anchor.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.2.0/anchor.min.js"></script>
<script src="js/toc.js"></script>
<script src="js/customscripts.js"></script>

<link rel="shortcut icon" href="images/favicon.ico">

<!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
<![endif]-->

<link rel="alternate" type="application/rss+xml" title="DSAILatKAIST.github.io" href="http://localhost:4000/feed.xml">


	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	TeX: {
		equationNumbers: {
		autoNumber: "AMS"
		}
	},
	tex2jax: {
		inlineMath: [ ['$', '$'] ],
		displayMath: [ ['$$', '$$'] ],
		processEscapes: true,
		}
	});
	MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
		alert("Math Processing Error: "+message[1]);
	});
	MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
		alert("Math Processing Error: "+message[1]);
	});
</script>

<script type="text/javascript" async
	src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



<script type="text/javascript" async
 src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>




    <script>
        $(document).ready(function() {
            // Initialize navgoco with default options
            $("#mysidebar").navgoco({
                caretHtml: '',
                accordion: true,
                openClass: 'active', // open
                save: false, // leave false or nav highlighting doesn't work right
                cookie: {
                    name: 'navgoco',
                    expires: false,
                    path: '/'
                },
                slide: {
                    duration: 400,
                    easing: 'swing'
                }
            });

            $("#collapseAll").click(function(e) {
                e.preventDefault();
                $("#mysidebar").navgoco('toggle', false);
            });

            $("#expandAll").click(function(e) {
                e.preventDefault();
                $("#mysidebar").navgoco('toggle', true);
            });

        });

    </script>
    <script>
        $(function () {
            $('[data-toggle="tooltip"]').tooltip()
        })
    </script>
    <script>
        $(document).ready(function() {
            $("#tg-sb-link").click(function() {
                $("#tg-sb-sidebar").toggle();
                $("#tg-sb-content").toggleClass('col-md-9');
                $("#tg-sb-content").toggleClass('col-md-12');
                $("#tg-sb-icon").toggleClass('fa-toggle-on');
                $("#tg-sb-icon").toggleClass('fa-toggle-off');
            });
        });
    </script>
    


	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	TeX: {
		equationNumbers: {
		autoNumber: "AMS"
		}
	},
	tex2jax: {
		inlineMath: [ ['$', '$'] ],
		displayMath: [ ['$$', '$$'] ],
		processEscapes: true,
		}
	});
	MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
		alert("Math Processing Error: "+message[1]);
	});
	MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
		alert("Math Processing Error: "+message[1]);
	});
</script>

<script type="text/javascript" async
	src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



<script type="text/javascript" async
 src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>


</head>
<body>
<!-- Navigation -->
<nav class="navbar navbar-inverse navbar-static-top">
    <div class="container topnavlinks">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="fa fa-home fa-lg navbar-brand" href="index.html">&nbsp;<span class="projectTitle"> Awesome Reviews</span></a>
        </div>
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                <!-- toggle sidebar button -->
                <li><a id="tg-sb-link" href="#"><i id="tg-sb-icon" class="fa fa-toggle-on"></i> Nav</a></li>
                <!-- entries without drop-downs appear here -->




                
                
                
                <li><a href="introduction">Introduction</a></li>
                
                
                
                <li><a href="https://statistics.kaist.ac.kr/" target="_blank" rel="noopener">KAIST ISysE</a></li>
                
                
                
                <!-- entries with drop-downs appear here -->
                <!-- conditional logic to control which topnav appears for the audience defined in the configuration file.-->
                
                
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">Reviews<b class="caret"></b></a>
                    <ul class="dropdown-menu">
                        
                        
                        <li><a href="reviews_kse801_2022.html">KSE801 (2022)</a></li>
                        
                        
                        
                        <li><a href="reviews_DS503_2023.html">DS503 (2023)</a></li>
                        
                        
                        
                        <li><a href="reviews_DS535_2023.html">DS535 (2023)</a></li>
                        
                        
                        
                        <li><a href="reviews_DS503_2024.html">DS503 (2024)</a></li>
                        
                        
                    </ul>
                </li>
                
                
                
			<li>



  <a class="email" title="Submit feedback" href="#" onclick="javascript:window.location='mailto:swkim@kaist.ac.kr?subject=Question about reviews feedback&body=I have some feedback about the [WSDM-24] LLMRec: Large Language Models with Graph Augmentation for Recommendation page: ' + window.location.href;"><i class="fa fa-envelope-o"></i> Feedback</a>

</li>



		
                <!--comment out this block if you want to hide search-->
                <li>
                    <!--start search-->
                    <div id="search-demo-container">
                        <input type="text" id="search-input" placeholder="search...">
                        <ul id="results-container"></ul>
                    </div>
                    <script src="js/jekyll-search.js" type="text/javascript"></script>
                    <script type="text/javascript">
                            SimpleJekyllSearch.init({
                                searchInput: document.getElementById('search-input'),
                                resultsContainer: document.getElementById('results-container'),
                                dataSource: 'search.json',
                                searchResultTemplate: '<li><a href="{url}" title="[WSDM-24] LLMRec: Large Language Models with Graph Augmentation for Recommendation">{title}</a></li>',
                    noResultsText: 'No results found.',
                            limit: 10,
                            fuzzy: true,
                    })
                    </script>
                    <!--end search-->
                </li>
            </ul>
        </div>
        </div>
        <!-- /.container -->
</nav>



<!-- Page Content -->
<div class="container">
  <div id="main">
    <!-- Content Row -->
    <div class="row">
        
        
            <!-- Sidebar Column -->
            <div class="col-md-3" id="tg-sb-sidebar">
                

<ul id="mysidebar" class="nav">
  <li class="sidebarTitle"> </li>
  
  
  
  <li>
      <a title="How to contribute" href="#">How to contribute</a>
      <ul>
          
          
          
          <li><a title="How to contribute?" href="how_to_contribute.html">How to contribute?</a></li>
          
          
          
          
          
          
          <li><a title="Review template (Example)" href="template.html">Review template (Example)</a></li>
          
          
          
          
      </ul>
   </li>
     
      
  
  <li>
      <a title="Reviews" href="#">Reviews</a>
      <ul>
          
          
          
          <li><a title="KSE801 (2022F)" href="reviews_kse801_2022.html">KSE801 (2022F)</a></li>
          
          
          
          
          
          
          <li><a title="DS503 (2023S)" href="reviews_DS503_2023.html">DS503 (2023S)</a></li>
          
          
          
          
          
          
          <li><a title="DS535 (2023F)" href="reviews_DS535_2023.html">DS535 (2023F)</a></li>
          
          
          
          
          
          
          <li><a title="DS503 (2024S)" href="reviews_DS503_2024.html">DS503 (2024S)</a></li>
          
          
          
          
          
          
          <li><a title="DS535 (2024F)" href="reviews_DS535_2024.html">DS535 (2024F)</a></li>
          
          
          
          
      </ul>
   </li>
     
      
      
      <!-- if you aren't using the accordion, uncomment this block:
         <p class="external">
             <a href="#" id="collapseAll">Collapse All</a> | <a href="#" id="expandAll">Expand All</a>
         </p>
         -->
</ul>

<!-- this highlights the active parent class in the navgoco sidebar. this is critical so that the parent expands when you're viewing a page. This must appear below the sidebar code above. Otherwise, if placed inside customscripts.js, the script runs before the sidebar code runs and the class never gets inserted.-->
<script>$("li.active").parents('li').toggleClass("active");</script>



            </div>
            
        

        <!-- Content Column -->
        <div class="col-md-9" id="tg-sb-content">
            <script type="text/x-mathjax-config">
MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/math...">
</script>
<article class="post" itemscope itemtype="https://schema.org/BlogPosting">

    <header class="post-header">
        <h1 class="post-title" itemprop="name headline">[WSDM-24] LLMRec: Large Language Models with Graph Augmentation for Recommendation</h1>
        <p class="post-meta"><time datetime="2024-10-13T00:00:00+09:00" itemprop="datePublished">Oct 13, 2024</time> /
            
            
            
            
            

        </p>


    </header>

    <div class="post-content" itemprop="articleBody">

        

        <h2 id="1-problem-definition"><strong>1. Problem Definition</strong></h2>

<p>This paper addresses a major issue faced by recommendation systems: data sparsity. Traditional methods often struggle when there are few interactions between users and items, making it difficult to accurately predict user preferences. Additionally, relying on side information like item attributes or user profiles can be problematic. While these sources can bridge the gaps, they often come with their own set of challenges such as noise, inconsistencies, and availability issues, which can further degrade the model‚Äôs performance.</p>

<p>To address these challenges, the paper introduces a novel framework called LLMRec, which leverages large language models (LLMs) for graph augmentation. The key issues LLMRec aims to solve include:</p>

<p><strong>Sparse Implicit Feedback Signals</strong>: When there are limited user-item interactions, collaborative filtering models struggle to effectively capture user preferences. This includes the cold-start problem, which arises when new users or items have little to no historical data, further complicating the recommendation process.</p>

<p><strong>Data Quality Issues with Side Information</strong>: When side information is low-quality, inconsistent, or incomplete, it can introduce noise that negatively affects model performance</p>

<p>To overcome these challenges, LLMRec employs three distinct LLM-based graph augmentation strategies, enriching the interaction graph and mitigating the impact of sparse data and unreliable side information, ultimately enhancing recommendation quality.</p>

<hr />

<h2 id="2-motivation"><strong>2. Motivation</strong></h2>

<p>The LLMRec framework was developed to address significant challenges in recommendation systems, particularly with handling sparse user-item interactions and unreliable side information. These limitations, which have been longstanding issues in the field, motivated the authors to propose an innovative solution.</p>

<p><strong>1. Challenges with Data Sparsity and Side Information Quality:</strong></p>
<ul>
  <li>Traditional recommendation models, such as collaborative filtering, typically rely on a large amount of user-item interaction data to predict preferences accurately. However, in most real-world scenarios, this data is often sparse, making it difficult to build robust user and item representations.</li>
  <li>As a result, many approaches incorporate side information like item attributes or user profiles. While this can help address sparsity, the quality of this side information is frequently inconsistent due to factors like noise, data heterogeneity, or missing information. These issues can undermine the effectiveness of the model.</li>
</ul>

<p><strong>2. Limitations of Current Augmentation Methods:</strong></p>
<ul>
  <li>Existing augmentation techniques, such as self-supervised learning or graph-based methods, typically generate additional relationships between users and items to improve recommendations. However, these approaches often fail to fully capture the subtle and complex preferences of users, particularly when it comes to understanding the rich context that can be derived from textual or other natural language-based data.</li>
  <li>Additionally, relying on side information assumes that all attributes are of equal value, but this is often not the case. Poor-quality or irrelevant side information can weaken a model‚Äôs ability to make accurate predictions.</li>
</ul>

<p><strong>3. The Role of Large Language Models (LLMs):</strong></p>
<ul>
  <li>Recent advances in large language models (LLMs) have shown their ability to capture deep semantic relationships and reason about user preferences more effectively than traditional ID-based methods. LLMs are equipped with extensive knowledge bases and are able to generate more insightful representations of user-item interactions based on natural language data.</li>
  <li>This inspired the authors to use LLMs for augmenting the recommendation process, offering a richer and more contextually aware approach than what is typically achievable with standard techniques.</li>
</ul>

<p><strong>4. The Unique Advantage of LLMRec:</strong></p>
<ul>
  <li>What sets LLMRec apart from existing methods is its use of three novel LLM-based strategies for augmenting the interaction graph. These strategies involve reinforcing user-item interaction edges, enhancing item attributes, and creating more comprehensive user profiles, all derived from natural language data.</li>
  <li>Unlike conventional models that rely on static or incomplete side information, LLMRec dynamically generates new relationships and attributes, leading to a more accurate and reliable representation of user preferences.</li>
</ul>

<p>By using LLMs to address the issues of data sparsity and unreliable side information, LLMRec presents an unprecendented approach to improve recommendation systems. It strengthens the user-item interaction graph, and enhances the interpretability and the relevance of recommendations.</p>

<!--
Please write the motivation of paper. The paper would tackle the limitations or challenges in each fields.

After writing the motivation, please write the discriminative idea compared to existing works briefly.
-->

<hr />

<h2 id="3-method"><strong>3. Method</strong></h2>

<!--
Please write the methodology author have proposed.  
We recommend you to provide example for understanding it more easily. 
-->

<p>The LLMRec framework proposes a methodology that utilizes large language models (LLMs) to augment the recommendation process through three primary strategies:</p>

<p>(i) Reinforcing user-item interaction edges 
(ii) Enhancing item node attributes
(iii) Conducting user profiling</p>

<p>The framework is designed to leverage the extensive knowledge base and reasoning abilities of LLMs to overcome the challenges of data sparsity and low-quality side information.</p>

<p>The breakdown of the methodology, including the notation and detailed explanation of each strategy, is presented below.</p>

<h3 id="31-reinforcing-user-item-interaction-edges"><strong>3.1 Reinforcing User-Item Interaction Edges</strong></h3>

<p>This strategy aims to address the scarcity of user-item interactions by generating new interaction edges using LLMs.</p>

<ul>
  <li><strong>Notation</strong>:
    <ul>
      <li>$ u $: User</li>
      <li>$ i $: Item</li>
      <li>$ C_ u = \lbrace i_ {u,1}, i_ {u,2}, \ldots, i_ {u, \vert C_u \vert} \rbrace $: Candidate item pool for user $ u $</li>
      <li>$ i^+_ u $: Positive item sample for user $ u $</li>
      <li>$ i^-_ u $: Negative item sample for user $ u $</li>
      <li>$ E^+ $: Original set of user-item interaction edges</li>
      <li>$ E_ A $: Augmented set of user-item interaction edges</li>
    </ul>
  </li>
  <li>
    <p><strong>Explanation</strong>:
LLMRec uses an <em>LLM-based Bayesian Personalized Ranking (BPR) sampling algorithm</em> to generate positive and negative item samples ($ i^+_ u $ and $ i^-_ u $) for each user $ u $. The candidate items are selected from a pool $ C_ u $, which is generated by the base recommender for each user. LLMs predict which items the user might like or dislike based on historical interactions, textual content, and contextual knowledge. The newly generated samples are added to the augmented set $E_ A $.</p>

    <p>The augmented set of user-item interactions is defined as:</p>

    <p>$
E_A = \lbrace (u, i^+_ u, i^-_ u) \mid (u, i^+_ u) \in E^+_ A, (u, i^-_ u) \in E^-_ A \rbrace
$
The set $E_A $ includes these newly generated interactions for each user, where each user has one pair: a positive item and a negative item.</p>
  </li>
  <li>
    <p><strong>Objective</strong>:
To enhance the learning process, LLMRec aims to maximize the posterior probability of the embeddings $ E = \lbrace E_ u, E_ i \rbrace )$ given the original and augmented interactions:</p>

    <p>$
E^{*} = \arg\max_ E \, p(E \mid E^+ \cup E_ A)
$</p>

    <p>This enables LLMRec to incorporate the LLM-generated interactions into the original graph, improving the model‚Äôs understanding of user preferences.</p>
  </li>
</ul>

<h3 id="32-enhancing-item-node-attributes"><strong>3.2 Enhancing Item Node Attributes</strong></h3>

<p>The goal of this strategy is to improve the quality of item representations by leveraging LLMs to generate additional attributes for items.</p>

<ul>
  <li><strong>Notation</strong>:
    <ul>
      <li>$ A_i $: Augmented attributes for item $ i $</li>
      <li>$ f_{A,i} $: LLM-generated feature representation of item $ i $</li>
      <li>$ F_A $: Set of augmented features for all items</li>
    </ul>
  </li>
  <li>
    <p><strong>Explanation</strong>:
The LLM is used to generate enriched item attributes based on existing textual content and interaction history. For example, if the item is a movie, attributes such as director, country, and language are extracted using LLM prompts. These attributes are encoded as features $ f_{A,i} $ and incorporated into the feature set $ F_A $.</p>

    <p>The final representation of the item $ i $ is then given by:</p>

    <p>$
f_ {A,i} = \text{LLM}(A_ i)
$</p>

    <p>where $f_ {A,i} \in \mathbb{R}^{d_ {LLM}} $.</p>

    <p>This augmented representation is then used to refine the item‚Äôs embedding in the recommendation model.</p>
  </li>
</ul>

<h3 id="33-conducting-user-profiling"><strong>3.3 Conducting User Profiling</strong></h3>

<p>This strategy involves using LLMs to construct comprehensive user profiles based on historical interactions and inferred preferences.</p>

<ul>
  <li><strong>Notation</strong>:
    <ul>
      <li>$ A_ u $: Augmented attributes for user $ u $</li>
      <li>$ f_ {A,u} $: LLM-generated feature representation of user $ u $</li>
    </ul>
  </li>
  <li>
    <p><strong>Explanation</strong>:
LLMRec generates user profiles using historical interaction data and side information. The LLM is used to predict missing user attributes (ex., age, gender, preferred genres) based on interaction history and other contextual information.</p>

    <p>The final user profile representation is defined as:</p>

    <p>$
f_ {A,u} = \text{LLM}(A_ u)
$</p>

    <p>where $ f_ {A,u} \in \mathbb{R}^{d_ {LLM}} $.</p>

    <p>This enables LLMRec to fill in gaps in user profiles, especially when explicit user information is incomplete or missing.</p>
  </li>
</ul>

<h3 id="34-incorporating-augmented-data-into-the-model"><strong>3.4 Incorporating Augmented Data into the Model</strong></h3>

<p>The augmented interaction edges and node features are integrated into the collaborative filtering framework to form the final user and item embeddings.</p>

<ul>
  <li>
    <p><strong>Optimization Objective for Augmenting Side Information</strong>:
To handle data sparsity and side information, the model maximizes the posterior probability of the embeddings $ \Theta = \lbrace E_u, E_i, F_ {\theta} \rbrace $ given both the original graph edges $ E^+ $ and the side information $ F $. This is formulated as:</p>

    <p>$
\Theta^{*} = \arg\max_ \Theta \, p(\Theta \mid F, E^+)
$</p>

    <p>where $ F $ refers to the side information used in the feature graph, and the function $ f_ {\theta} $ combines the signals from both the feature set $ F $ and user-item interactions.</p>
  </li>
  <li>
    <p><strong>Recommendation with Data Augmentation</strong>:
After augmenting both the side information and the interaction graph, the full optimization objective for LLMRec is:</p>

    <p>$
\Theta^{*} = \arg\max_ \Theta \, p(\Theta \mid \lbrace F, F_ A \rbrace, \lbrace E^+, E_ A \rbrace )
$</p>

    <p>where:</p>
    <ul>
      <li>$ \Theta $: Model parameters</li>
      <li>$ F $: Original feature set</li>
      <li>$ E^+ $: Original interaction set</li>
      <li>$ F_ A $ and $ E_ A $: Augmented feature set and interaction set generated by LLMRec</li>
    </ul>

    <p>The final representation for each user $ u $ and item $ i $ is a combination of the original and augmented data, which is used to predict the likelihood of user $ u $ interacting with item $ i $.</p>
  </li>
</ul>

<hr />

<h2 id="4-experiment"><strong>4. Experiment</strong></h2>
<!--
In this section, please write the overall experiment results.  
At first, write experiment setup that should be composed of contents.  

### **Experiment setup**  
* Dataset  
* baseline  
* Evaluation Metric  

### **Result**  
Then, show the experiment results which demonstrate the proposed method.  
You can attach the tables or figures, but you don't have to cover all the results.  
-->
<p>In this section, the authors present the experimental results of the proposed LLMRec framework. The authors evaluate its performance on two benchmark datasets using various evaluation metrics and compare it against state-of-the-art baselines. The study addresses the following five research questions (RQs) to comprehensively analyze the effectiveness of LLMRec:</p>

<p><strong>RQ1:</strong> <em>How does LLMRec perform compared to existing collaborative filtering and data augmentation methods?</em>
<strong>RQ2:</strong> <em>What is the effect of LLM-based graph augmentation on the recommendation quality?</em>
<strong>RQ3:</strong> <em>How do different components of LLMRec contribute to its overall performance?</em>
<strong>RQ4:</strong> <em>How sensitive is LLMRec to different hyperparameter settings?</em>
<strong>RQ5:</strong> <em>Is LLMRec‚Äôs data augmentation strategy generalizable to other recommendation models?</em></p>

<h3 id="41-experimental-setup">4.1 Experimental Setup</h3>
<p><strong>Datasets</strong>
The authors conducted experiments using two publicly available datasets: Netflix and MovieLens-10M, each chosen to highlight the performance of LLMRec in scenarios with diverse side information. The Netflix dataset is derived from the Netflix Prize Data available on Kaggle and the MovieLens-10M dataset is derived from the ML-10M dataset. Both datasets include multi-modal side information, such as textual and visual features:</p>

<ul>
  <li><strong>Netflix</strong>: Contains 13,187 users, 17,366 items, and 68,933 user-item interactions. The side information includes textual attributes (ex., titles, genres) and visual features extracted using the <em>CLIP-ViT</em> model.</li>
  <li><strong>MovieLens</strong>: Consists of 12,495 users, 10,322 items, and 57,960 interactions. The side information includes textual data (titles, genres, and release years) and visual content of movie posters, encoded using the <em>CLIP</em> model.</li>
</ul>

<p><strong>LLM-based Data Augmentation</strong>
LLMRec utilizes the OpenAI GPT-3.5-turbo-0613 model for generating new user-item interactions, item attributes, and user profiles. The augmented item attributes include details such as director, country, and language, while user profiles are inferred from historical interactions and further enhanced with attributes like age, gender, liked genres, and disliked genres. Embedding generation is performed using the text-embedding-ada-002 model, allowing LLMRec to transform textual information into dense vectors suitable for integration within the graph.</p>

<p><strong>Baselines</strong>
The authors compared LLMRec against a diverse set of baseline models to evaluate its effectiveness:</p>

<ol>
  <li><strong>General Collaborative Filtering Methods:</strong>
    <ul>
      <li><em>MF-BPR</em>: Matrix Factorization optimized for implicit feedback.</li>
      <li><em>NGCF</em>: Neural Graph Collaborative Filtering that captures high-order user-item interactions using GNNs.</li>
      <li><em>LightGCN</em>: A simplified GNN model for collaborative filtering with lightweight propagation layers.</li>
    </ul>
  </li>
  <li><strong>Methods with Side Information:</strong>
    <ul>
      <li><em>VBPR</em>: A visual-based Bayesian personalized ranking model using visual features of items.</li>
      <li><em>MMGCN</em>: A multi-modal GCN model incorporating both textual and visual features.</li>
      <li><em>GRCN</em>: A GCN-based method that utilizes item-end content for high-order content-aware relationships.</li>
    </ul>
  </li>
  <li><strong>Data Augmentation Methods:</strong>
    <ul>
      <li><em>LATTICE</em>: Uses data augmentation by establishing item-item relationships based on content similarity.</li>
      <li><em>MICRO</em>: A multi-modal content recommendation framework that leverages self-supervised learning for data augmentation.</li>
    </ul>
  </li>
  <li><strong>Self-supervised Methods:</strong>
    <ul>
      <li><em>CLCRec</em>: Contrastive learning-based collaborative filtering model using self-supervision to improve representations.</li>
      <li><em>MMSS</em>L: A multi-modal self-supervised learning model maximizing mutual information between content-augmented views.</li>
    </ul>
  </li>
</ol>

<p><strong>Implementation Details</strong>
The experiments are implemented in PyTorch and run on a 24 GB Nvidia RTX 3090 GPU. The AdamW optimizer is employed for training, with learning rates set within a range of [5ùëí‚àí5, 1ùëí‚àí3] for Netflix and [2.5ùëí‚àí4, 9.5ùëí‚àí4] for MovieLens. Temperature and top-p parameters are set within ranges {0.0, 0.6, 0.8, 1} and {0.0, 0.1, 0.4, 1}, respectively, to control LLM-generated content. For comparison, the researchers use a unified embedding size of 64 for all methods.</p>

<p><strong>Evaluation Metrics</strong>
The authors evaluate LLMRec using three common metrics: Recall@K (R@K), Normalized Discounted Cumulative Gain@K (N@K), and Precision@K (P@K), with K set to 10, 20, and 50. The authors employ an all-ranking strategy to avoid potential biases, and results are averaged over five independent runs to ensure statistical significance.</p>

<h3 id="42-experimental-results">4.2 Experimental Results</h3>
<p>The authors address the stated research questions through a detailed analysis of the experimental results:</p>

<p><img src="https://i.postimg.cc/jSCcqM6X/table-2.png" alt="image_name" /></p>

<p><img src="https://i.postimg.cc/SsL713qy/table-3.png" alt="image_name" /></p>

<p><img src="https://i.postimg.cc/x17RXLtS/table-4.png" alt="image_name" /></p>

<p><img src="https://i.postimg.cc/9f7BZznw/table-5.png" alt="image_name" /></p>

<p><img src="https://i.postimg.cc/RFxLR9ks/table-6.png" alt="image_name" /></p>

<p><strong>RQ1: How does LLMRec perform compared to existing collaborative filtering and data augmentation methods?</strong>
LLMRec demonstrates superior performance over all baseline methods on both Netflix and MovieLens datasets, achieving the highest Recall@K, NDCG@K, and Precision@K scores. This indicates that LLMRec‚Äôs ability to generate high-quality and contextually relevant user-item interactions significantly improves recommendation accuracy. The results, summarized in Table 2, show a notable improvement over general collaborative filtering methods like NGCF and LightGCN, and over state-of-the-art data augmentation methods like LATTICE and MICRO.</p>

<p><strong>RQ2: What is the effect of LLM-based graph augmentation on the recommendation quality?</strong>
The results in Table 3 show that LLM-based augmentation provides a substantial performance boost compared to models without graph augmentation. By generating new interactions and enriching side information, LLMRec effectively mitigates data sparsity and low-quality side information issues. This validates that augmenting graphs using LLMs enhances the model‚Äôs ability to capture richer patterns in user preferences and item attributes.</p>

<p><strong>RQ3: How do different components of LLMRec contribute to its overall performance?</strong>
Ablation study was conducted to understand the contributions of each component. As shown in Table 4, removing LLM-augmented user-item interactions causes a drastic performance drop, indicating that these interactions are crucial for capturing diverse user preferences. Similarly, removing user or item profiling results in lower accuracy, underscoring the importance of using LLMs to enhance side information.</p>

<p><strong>RQ4: How sensitive is LLMRec to different hyperparameter settings?</strong>
The sensitivity analysis (Table 5) reveals that LLMRec‚Äôs performance is particularly sensitive to the temperature (ùúè) and top-p parameters of the GPT-3.5-turbo model. The best performance is achieved with ùúè = 0.6 and top-p = 0.1, indicating that a moderate level of randomness and diversity in the generated content is ideal. Additionally, the authors found that a candidate pool size of |C| = 10 balances selection diversity and computational efficiency, ensuring high-quality rankings.</p>

<p><strong>RQ5: Is LLMRec‚Äôs data augmentation strategy generalizable to other recommendation models?</strong>
The research team integrated LLMRec‚Äôs augmented data into other recommendation models such as LATTICE, MICRO, and MMSSL, as shown in Table 6. All models benefit significantly from the augmented data, demonstrating that LLMRec‚Äôs data augmentation strategy is effective and generalizable across different model architectures. This highlights the potential of LLM-generated data as a versatile tool to improve recommendation quality in various contexts.</p>

<hr />

<h2 id="5-conclusion"><strong>5. Conclusion</strong></h2>
<!--
Please summarize the paper.  
It is free to write all you want. e.g, your opinion, take home message(Ïò§ÎäòÏùò ÍµêÌõà), key idea, and etc.
-->

<p>This paper presents LLMRec, a novel framework that enhances recommendation systems by leveraging Large Language Models (LLMs) for graph augmentation. Unlike traditional methods that struggle with data sparsity and low-quality side information, LLMRec enriches the interaction graph with high-quality augmented data using LLMs. What stands out to me is how the framework shifts from an ID-based recommendation paradigm to a modality-based one, leveraging natural language understanding to create new user-item interaction edges and profile users and items more comprehensively.</p>

<p>One key takeaway is LLMRec‚Äôs hybrid approach‚Äîutilizing LLMs as data augmentors rather than replacing conventional models. This approach retains the strengths of existing collaborative filtering methods while enriching them with the rich contextual knowledge provided by LLMs. The authors also address the challenge of noisy or incomplete data with robust augmentation strategies, including denoised data robustification and feature enhancement, ensuring the reliability of the augmented information.</p>

<p>Reflecting on the paper, I find the use of LLMs as enhancers rather than direct recommenders particularly interesting. It suggests that the real value of LLMs in recommendation lies in their ability to understand and augment data, not just replace models. This insight opens the door for future research to explore more sohpisticated and nuanced integration strategies between LLMs and traditional recommendation architectures.</p>

<p>Overall, I think that the proposed model, LLMRec, effectively bridges the gap between language models and recommendation systems, setting a precedent for future studies. By creatively using LLMs to overcome long-standing challenges, this paper provides a strong foundation for developing next-generation recommendation systems that are both effective and contextually aware. In addition, I also think that it would be interesting to explore this model with the updated versions of the LLM models from OpenAI.</p>

<hr />
<h2 id="author-information"><strong>Author Information</strong></h2>

<ul>
  <li>Author name: Jeongho Kim
    <ul>
      <li>Affiliation: KAIST Department of Industrial and Systems Engineering</li>
      <li>Research Topic: Human Behavior Modelling, Injury Prevention</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="6-reference--additional-materials"><strong>6. Reference &amp; Additional materials</strong></h2>

<!--
Please write the reference. If paper provides the public code or other materials, refer them.
-->

<ul>
  <li>Github Implementation<br />
https://github.com/HKUDS/LLMRec.git
<br /></li>
  <li>Reference<br />
[1] Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023. TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation. arXiv preprint arXiv:2305.00447 (2023).
[2] Chong Chen, Weizhi Ma, Min Zhang, et al. 2023. Revisiting negative sampling vs. non-sampling in implicit recommendation. TOIS 41, 1 (2023), 1‚Äì25.
[3] Chong Chen, Min Zhang, Yongfeng Zhang, et al. 2020. Efficient neural matrix factorization without sampling for recommendation. TOIS 38, 2 (2020), 1‚Äì28.
[4] Mengru Chen, Chao Huang, Lianghao Xia, Wei Wei, et al. 2023. Heterogeneous graph contrastive learning for recommendation. In Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining. 544‚Äì552.
[5] Zheng Chen. 2023. PALR: Personalization Aware LLMs for Recommendation. arXiv preprint arXiv:2305.07622 (2023).
[6] Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongxiang Sun, Xiao Zhang, and Jun Xu. 2023. Uncovering ChatGPT‚Äôs Capabilities in Recommender Systems. arXiv preprint arXiv:2305.02182 (2023).
[7] Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. 2019. Graph neural networks for social recommendation. In ACM International World Wide Web Conference. 417‚Äì426.
[8] Xinyu Fu, Jiani Zhang, et al. 2020. Magnn: Metapath aggregated graph neural network for heterogeneous graph embedding. In ACM International World Wide Web Conference. 2331‚Äì2341.
[9] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll√°r, and Ross Girshick. 2022. Masked autoencoders are scalable vision learners. In CVPR. 16000‚Äì16009.
[10] Ruining He and Julian McAuley. 2016. VBPR: visual bayesian personalized ranking from implicit feedback. In AAAI, Vol. 30.
[11] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for recommendation. In ACM SIGIR Conference on Research and Development in Information Retrieval. 639‚Äì648.
[12] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751 (2019).
[13] Tinglin Huang, Yuxiao Dong, Ming Ding, Zhen Yang, Wenzheng Feng, Xinyu Wang, and Jie Tang. 2021. MixGCF: An Improved Training Method for Graph Neural Network-based Recommender Systems. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining.
[14] Wang-Cheng Kang, Jianmo Ni, Nikhil Mehta, Maheswaran Sathiamoorthy, Lichan Hong, et al. 2023. Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction. arXiv preprint arXiv:2305.06474 (2023).
[15] Hyeyoung Ko, Suyeon Lee, Yoonseo Park, and Anna Choi. 2022. A survey of recommendation systems: recommendation models, techniques, and application fields. Electronics 11, 1 (2022), 141.
[16] Dongha Lee, SeongKu Kang, Hyunjun Ju, et al. 2021. Bootstrapping user and item representations for one-class collaborative filtering. In ACM SIGIR Conference on Research and Development in Information Retrieval. 317‚Äì326.
[17] Jiacheng Li, Ming Wang, Jin Li, Jinmiao Fu, Xin Shen, Jingbo Shang, and Julian McAuley. 2023. Text Is All You Need: Learning Language Representations for Sequential Recommendation. In ACM SIGKDD Conference on Knowledge Discovery and Data Mining.
[18] Jinming Li, Wentao Zhang, Tian Wang, Guanglei Xiong, et al. 2023. GPT4Rec: A Generative Framework for Personalized Recommendation and User Interests Interpretation. arXiv preprint arXiv:2304.03879 (2023).
[19] Ke Liang, Yue Liu, Sihang Zhou, Wenxuan Tu, Yi Wen, Xihong Yang, Xiangjun Dong, and Xinwang Liu. 2023. Knowledge Graph Contrastive Learning Based on Relation-Symmetrical Structure. IEEE Transactions on Knowledge and Data Engineering (2023), 1‚Äì12. https://doi.org/10.1109/TKDE.2023.3282989
[20] Ke Liang, Lingyuan Meng, Meng Liu, Yue Liu, Wenxuan Tu, Siwei Wang, Sihang Zhou, and Xinwang Liu. 2023. Learn from relational correlations and periodic events for temporal knowledge graph reasoning. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval Conference on Research and Development in Information Retrieval. 1559‚Äì1568.
[21] Ke Liang, Lingyuan Meng, Meng Liu, Yue Liu, Wenxuan Tu, Siwei Wang, Sihang Zhou, Xinwang Liu, and Fuchun Sun. 2022. Reasoning over different types of knowledge graphs: Static, temporal and multi-modal. arXiv preprint arXiv:2212.05767 (2022).
[22] Ke Liang, Sihang Zhou, Yue Liu, Lingyuan Meng, Meng Liu, and Xinwang Liu. 2023. Structure Guided Multi-modal Pre-trained Transformer for Knowledge Graph Reasoning. arXiv preprint arXiv:2307.03591 (2023).
[23] Zhiwei Liu, Yongjun Chen, Jia Li, Philip S Yu, Julian McAuley, and Caiming Xiong. 2021. Contrastive self-supervised sequential recommendation with robust augmentation. arXiv preprint arXiv:2108.06479 (2021).
[24] Zhiwei Liu, Ziwei Fan, et al. 2021. Augmenting sequential recommendation with pseudo-prior items via reversely pre-training transformer. In ACM SIGIR Conference on Research and Development in Information Retrieval. 1608‚Äì1612.
[25] Ilya Loshchilov et al. 2017. Decoupled weight decay regularization. In ICLR.
[26] Chang Meng, Chenhao Zhai, Yu Yang, Hengyu Zhang, and Xiu Li. 2023. Parallel Knowledge Enhancement based Framework for Multi-behavior Recommendation. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management. 1797‚Äì1806.
[27] Chang Meng, Hengyu Zhang, Wei Guo, Huifeng Guo, Haotian Liu, Yingxue Zhang, Hongkun Zheng, Ruiming Tang, Xiu Li, and Rui Zhang. 2023. Hierarchical Projection Enhanced Multi-Behavior Recommendation. In Proceedings of the 29th ACM SIGACM SIGKDD Conference on Knowledge Discovery and Data Mining Conference on Knowledge Discovery and Data Mining. 4649‚Äì4660.
[28] Chang Meng, Ziqi Zhao, Wei Guo, Yingxue Zhang, Haolun Wu, Chen Gao, Dong Li, Xiu Li, and Ruiming Tang. 2023. Coarse-to-fine knowledge-enhanced multi-interest learning framework for multi-behavior recommendation. ACM Transactions on Information Systems 42, 1 (2023), 1‚Äì27.
[29] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, et al. 2019. Pytorch: An imperative style, high-performance deep learning library. Conference on Neural Information Processing Systems 32 (2019).
[30] Aleksandr Petrov and Craig Macdonald. 2022. Effective and Efficient Training for Sequential Recommendation using Recency Sampling. In Recsys. 81‚Äì91.
[31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al. 2021. Learning transferable visual models from natural language supervision. In ICML. PMLR, 8748‚Äì8763.
[32] Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, and Chao Huang. 2023. Representation Learning with Large Language Models for Recommendation. arXiv preprint arXiv:2310.15950 (2023).
[33] Xubin Ren, Lianghao Xia, Yuhao Yang, Wei Wei, Tianle Wang, Xuheng Cai, and Chao Huang. 2023. SSLRec: A Self-Supervised Learning Library for Recommendation. arXiv preprint arXiv:2308.05697 (2023).
[34] Steffen Rendle, Christoph Freudenthaler, et al. 2012. BPR: Bayesian personalized ranking from implicit feedback. arXiv preprint arXiv:1205.2618 (2012).
[35] Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, and Chao Huang. 2023. GraphGPT: Graph Instruction Tuning for Large Language Models. arXiv preprint arXiv:2310.13023 (2023).
[36] Yijun Tian, Kaiwen Dong, Chunhui Zhang, Chuxu Zhang, and Nitesh V Chawla. 2023. Heterogeneous graph masked autoencoders. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 37. 9997‚Äì10005.
[37] Yijun Tian, Shichao Pei, Xiangliang Zhang, Chuxu Zhang, and Nitesh V Chawla. 2023. Knowledge Distillation on Graphs: A Survey. arXiv preprint arXiv:2302.00219 (2023).
[38] Yijun Tian, Huan Song, Zichen Wang, Haozhu Wang, Ziqing Hu, Fang Wang, Nitesh V Chawla, and Panpan Xu. 2023. Graph neural prompting with large language models. arXiv preprint arXiv:2309.15427 (2023).
[39] Yijun Tian, Chuxu Zhang, Zhichun Guo, Xiangliang Zhang, and Nitesh Chawla. 2022. Learning mlps on graphs: A unified view of effectiveness, robustness, and efficiency. In The Eleventh International Conference on Learning Representations.
[40] Wenjie Wang, Fuli Feng, Xiangnan He, Liqiang Nie, and Tat-Seng Chua. 2021. Denoising implicit feedback for recommendation. In WSDM. 373‚Äì381.
[41] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019. Neural graph collaborative filtering. In ACM SIGIR Conference on Research and Development in Information Retrieval. 165‚Äì174.
[42] Xiaolei Wang, Xinyu Tang, Wayne Xin Zhao, Jingyuan Wang, and Ji-Rong Wen. 2023. Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models. arXiv preprint arXiv:2305.13112 (2023).
[43] Zhenlei Wang, Jingsen Zhang, Hongteng Xu, Xu Chen, Yongfeng Zhang, Wayne Xin Zhao, and Ji-Rong Wen. 2021. Counterfactual data-augmented sequential recommendation. In ACM SIGIR Conference on Research and Development in Information Retrieval. 347‚Äì356.
[44] Wei Wei, Chao Huang, Lianghao Xia, Yong Xu, Jiashu Zhao, and Dawei Yin. 2022. Contrastive meta learning with behavior multiplicity for recommendation. In Proceedings of the fifteenth ACM international conference on web search and data mining. 1120‚Äì1128.
[45] Wei Wei, Chao Huang, Lianghao Xia, and Chuxu Zhang. 2023. Multi-Modal Self-Supervised Learning for Recommendation. In ACM International World Wide Web Conference. 790‚Äì800.
[46] Wei Wei, Lianghao Xia, and Chao Huang. 2023. Multi-Relational Contrastive Learning for Recommendation. In Proceedings of the 17th ACM Conference on Recommender Systems. 338‚Äì349.
[47] Yinwei Wei, Xiang Wang, et al. 2021. Hierarchical user intent graph network for multimedia recommendation. Transactions on Multimedia (TMM) (2021).
[48] Yinwei Wei, Xiang Wang, Qi Li, Liqiang Nie, Yan Li, et al. 2021. Contrastive learning for cold-start recommendation. In ACM MM. 5382‚Äì5390.
[49] Yinwei Wei, Xiang Wang, Liqiang Nie, Xiangnan He, and Tat-Seng Chua. 2020. Graph-refined convolutional network for multimedia recommendation with implicit feedback. In MM. 3541‚Äì3549.
[50] Yinwei Wei, Xiang Wang, Liqiang Nie, Xiangnan He, Richang Hong, and Tat-Seng Chua. 2019. MMGCN: Multi-modal graph convolution network for personalized recommendation of micro-video. In MM. 1437‚Äì1445.
[51] Jiancan Wu, Xiang Wang, Fuli Feng, Xiangnan He, Liang Chen, et al. 2021. Selfsupervised graph learning for recommendation. In ACM SIGIR Conference on Research and Development in Information Retrieval. 726‚Äì735.
[52] Zixuan Yi, Xi Wang, Iadh Ounis, and Craig Macdonald. 2022. Multi-modal Graph Contrastive Learning for Micro-video Recommendation. In ACM SIGIR Conference on Research and Development in Information Retrieval. 1807‚Äì1811.
[53] Yuxin Ying, Fuzhen Zhuang, Yongchun Zhu, Deqing Wang, and Hongwei Zheng. 2023. CAMUS: Attribute-Aware Counterfactual Augmentation for Minority Users in Recommendation. In ACM International World Wide Web Conference. 1396‚Äì1404.
[54] Junliang Yu, Hongzhi Yin, Xin Xia, Tong Chen, Lizhen Cui, and Quoc Viet Hung Nguyen. 2022. Are graph augmentations necessary? Simple graph contrastive learning for recommendation. In ACM SIGIR Conference on Research and Development in Information Retrieval. 1294‚Äì1303.
[55] Zheng Yuan, Fajie Yuan, Yu Song, Youhua Li, Junchen Fu, Fei Yang, Yunzhu Pan, and Yongxin Ni. 2023. Where to go next for recommender systems? id-vs. modality-based recommender models revisited. In ACM SIGIR Conference on Research and Development in Information Retrieval.
[56] Honglei Zhang, Fangyuan Luo, Jun Wu, Xiangnan He, and Yidong Li. 2023. LightFR: Lightweight federated recommendation with privacy-preserving matrix factorization. ACM Transactions on Information Systems 41, 4 (2023), 1‚Äì28.
[57] Junjie Zhang, Ruobing Xie, Yupeng Hou, Wayne Xin Zhao, Leyu Lin, and Ji-Rong Wen. 2023. Recommendation as instruction following: A large language model empowered recommendation approach. arXiv preprint arXiv:2305.07001 (2023).
[58] Jinghao Zhang, Yanqiao Zhu, Qiang Liu, et al. 2022. Latent structure mining with contrastive modality fusion for multimedia recommendation. TKDE (2022).
[59] Jinghao Zhang, Yanqiao Zhu, Qiang Liu, Shu Wu, et al. 2021. Mining Latent Structures for Multimedia Recommendation. In MM. 3872‚Äì3880.
[60] Shengyu Zhang, Dong Yao, Zhou Zhao, et al. 2021. Causerec: Counterfactual user sequence synthesis for sequential recommendation. In ACM SIGIR Conference on Research and Development in Information Retrieval. 367‚Äì377.
[61] Ding Zou, Wei Wei, Xian-Ling Mao, Ziyang Wang, Minghui Qiu, Feida Zhu, and
Xin Cao. 2022. Multi-level cross-view contrastive learning for knowledge-aware recommender system. In ACM SIGIR Conference on Research and Development in
Information Retrieval. 1358‚Äì1368.</li>
</ul>

    </div>



</article>
<script src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.5.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>







<hr class="shaded"/>

<footer>
            <div class="row">
                <div class="col-lg-12 footer">
               &copy;2024 Copyright 2021 Google LLC. All rights reserved. <br />
 Site last generated: Oct 18, 2024 <br />
<p><img src="images/company_logo.png" alt="Company logo"/></p>
                </div>
            </div>
</footer>






        </div>
    <!-- /.row -->
</div>
<!-- /.container -->
</div>
<!-- /#main -->
    </div>

</body>

<!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-66296557-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>





</html>


