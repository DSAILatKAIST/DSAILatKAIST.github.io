<!DOCTYPE html>
<html>
<head>
    <meta name="google-site-verification" content="1tAPTdgw0-t8G2Bya463OpBtYUbj9Um93gfnsowYKLw" />
    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-CV4PTXQSTW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-CV4PTXQSTW');
</script>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="">
<meta name="keywords" content="reviews,  ">
<title>[RecSys 2023] Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation | Awesome Reviews</title>
<link rel="stylesheet" href="css/syntax.css">

<link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
<!--<link rel="stylesheet" type="text/css" href="css/bootstrap.min.css">-->
<link rel="stylesheet" href="css/modern-business.css">
<!-- Latest compiled and minified CSS -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<link rel="stylesheet" href="css/customstyles.css">
<link rel="stylesheet" href="css/boxshadowproperties.css">
<!-- most color styles are extracted out to here -->
<link rel="stylesheet" href="css/theme-blue.css">

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="js/jquery.navgoco.min.js"></script>


<!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<!-- Anchor.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.2.0/anchor.min.js"></script>
<script src="js/toc.js"></script>
<script src="js/customscripts.js"></script>

<link rel="shortcut icon" href="images/favicon.ico">

<!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
<![endif]-->

<link rel="alternate" type="application/rss+xml" title="DSAILatKAIST.github.io" href="http://localhost:4000/feed.xml">


	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	TeX: {
		equationNumbers: {
		autoNumber: "AMS"
		}
	},
	tex2jax: {
		inlineMath: [ ['$', '$'] ],
		displayMath: [ ['$$', '$$'] ],
		processEscapes: true,
		}
	});
	MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
		alert("Math Processing Error: "+message[1]);
	});
	MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
		alert("Math Processing Error: "+message[1]);
	});
</script>

<script type="text/javascript" async
	src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



<script type="text/javascript" async
 src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>




    <script>
        $(document).ready(function() {
            // Initialize navgoco with default options
            $("#mysidebar").navgoco({
                caretHtml: '',
                accordion: true,
                openClass: 'active', // open
                save: false, // leave false or nav highlighting doesn't work right
                cookie: {
                    name: 'navgoco',
                    expires: false,
                    path: '/'
                },
                slide: {
                    duration: 400,
                    easing: 'swing'
                }
            });

            $("#collapseAll").click(function(e) {
                e.preventDefault();
                $("#mysidebar").navgoco('toggle', false);
            });

            $("#expandAll").click(function(e) {
                e.preventDefault();
                $("#mysidebar").navgoco('toggle', true);
            });

        });

    </script>
    <script>
        $(function () {
            $('[data-toggle="tooltip"]').tooltip()
        })
    </script>
    <script>
        $(document).ready(function() {
            $("#tg-sb-link").click(function() {
                $("#tg-sb-sidebar").toggle();
                $("#tg-sb-content").toggleClass('col-md-9');
                $("#tg-sb-content").toggleClass('col-md-12');
                $("#tg-sb-icon").toggleClass('fa-toggle-on');
                $("#tg-sb-icon").toggleClass('fa-toggle-off');
            });
        });
    </script>
    


	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	TeX: {
		equationNumbers: {
		autoNumber: "AMS"
		}
	},
	tex2jax: {
		inlineMath: [ ['$', '$'] ],
		displayMath: [ ['$$', '$$'] ],
		processEscapes: true,
		}
	});
	MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
		alert("Math Processing Error: "+message[1]);
	});
	MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
		alert("Math Processing Error: "+message[1]);
	});
</script>

<script type="text/javascript" async
	src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



<script type="text/javascript" async
 src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>


</head>
<body>
<!-- Navigation -->
<nav class="navbar navbar-inverse navbar-static-top">
    <div class="container topnavlinks">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="fa fa-home fa-lg navbar-brand" href="index.html">&nbsp;<span class="projectTitle"> Awesome Reviews</span></a>
        </div>
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                <!-- toggle sidebar button -->
                <li><a id="tg-sb-link" href="#"><i id="tg-sb-icon" class="fa fa-toggle-on"></i> Nav</a></li>
                <!-- entries without drop-downs appear here -->




                
                
                
                <li><a href="introduction">Introduction</a></li>
                
                
                
                <li><a href="https://statistics.kaist.ac.kr/" target="_blank" rel="noopener">KAIST ISysE</a></li>
                
                
                
                <!-- entries with drop-downs appear here -->
                <!-- conditional logic to control which topnav appears for the audience defined in the configuration file.-->
                
                
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">Reviews<b class="caret"></b></a>
                    <ul class="dropdown-menu">
                        
                        
                        <li><a href="reviews_kse801_2022.html">KSE801 (2022)</a></li>
                        
                        
                        
                        <li><a href="reviews_DS503_2023.html">DS503 (2023)</a></li>
                        
                        
                        
                        <li><a href="reviews_DS535_2023.html">DS535 (2023)</a></li>
                        
                        
                        
                        <li><a href="reviews_DS503_2024.html">DS503 (2024)</a></li>
                        
                        
                    </ul>
                </li>
                
                
                
			<li>



  <a class="email" title="Submit feedback" href="#" onclick="javascript:window.location='mailto:swkim@kaist.ac.kr?subject=Question about reviews feedback&body=I have some feedback about the [RecSys 2023] Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation page: ' + window.location.href;"><i class="fa fa-envelope-o"></i> Feedback</a>

</li>



		
                <!--comment out this block if you want to hide search-->
                <li>
                    <!--start search-->
                    <div id="search-demo-container">
                        <input type="text" id="search-input" placeholder="search...">
                        <ul id="results-container"></ul>
                    </div>
                    <script src="js/jekyll-search.js" type="text/javascript"></script>
                    <script type="text/javascript">
                            SimpleJekyllSearch.init({
                                searchInput: document.getElementById('search-input'),
                                resultsContainer: document.getElementById('results-container'),
                                dataSource: 'search.json',
                                searchResultTemplate: '<li><a href="{url}" title="[RecSys 2023] Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation">{title}</a></li>',
                    noResultsText: 'No results found.',
                            limit: 10,
                            fuzzy: true,
                    })
                    </script>
                    <!--end search-->
                </li>
            </ul>
        </div>
        </div>
        <!-- /.container -->
</nav>



<!-- Page Content -->
<div class="container">
  <div id="main">
    <!-- Content Row -->
    <div class="row">
        
        
            <!-- Sidebar Column -->
            <div class="col-md-3" id="tg-sb-sidebar">
                

<ul id="mysidebar" class="nav">
  <li class="sidebarTitle"> </li>
  
  
  
  <li>
      <a title="How to contribute" href="#">How to contribute</a>
      <ul>
          
          
          
          <li><a title="How to contribute?" href="how_to_contribute.html">How to contribute?</a></li>
          
          
          
          
          
          
          <li><a title="Review template (Example)" href="template.html">Review template (Example)</a></li>
          
          
          
          
      </ul>
   </li>
     
      
  
  <li>
      <a title="Reviews" href="#">Reviews</a>
      <ul>
          
          
          
          <li><a title="KSE801 (2022F)" href="reviews_kse801_2022.html">KSE801 (2022F)</a></li>
          
          
          
          
          
          
          <li><a title="DS503 (2023S)" href="reviews_DS503_2023.html">DS503 (2023S)</a></li>
          
          
          
          
          
          
          <li><a title="DS535 (2023F)" href="reviews_DS535_2023.html">DS535 (2023F)</a></li>
          
          
          
          
      </ul>
   </li>
     
      
      
      <!-- if you aren't using the accordion, uncomment this block:
         <p class="external">
             <a href="#" id="collapseAll">Collapse All</a> | <a href="#" id="expandAll">Expand All</a>
         </p>
         -->
</ul>

<!-- this highlights the active parent class in the navgoco sidebar. this is critical so that the parent expands when you're viewing a page. This must appear below the sidebar code above. Otherwise, if placed inside customscripts.js, the script runs before the sidebar code runs and the class never gets inserted.-->
<script>$("li.active").parents('li').toggleClass("active");</script>



            </div>
            
        

        <!-- Content Column -->
        <div class="col-md-9" id="tg-sb-content">
            <script type="text/x-mathjax-config">
MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/math...">
</script>
<article class="post" itemscope itemtype="https://schema.org/BlogPosting">

    <header class="post-header">
        <h1 class="post-title" itemprop="name headline">[RecSys 2023] Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation</h1>
        <p class="post-meta"><time datetime="2023-11-20T00:00:00+09:00" itemprop="datePublished">Nov 20, 2023</time> /
            
            
            
            
            

        </p>


    </header>

    <div class="post-content" itemprop="articleBody">

        

        <h1 id="a-review-on-fairllm">A Review on FaiRLLM</h1>

<h3 id="overall-summary">OVERALL SUMMARY</h3>
<p>This is a scientific review on the paper “Is Chat GPT fair for Recommendation? Evaluating Fairness in Large Language<br />
Model Recommendation” by Jizhi Zhang, Keqin Bao, Yan Zhang, Wenjoe Wang, Fuli Feng, and Xiangnan He.<br />
The paper was accepted not long ago in July 2023 by the RecSys conference and has been since then already cited 22 times<br />
(October 14th on Google Scholar). A preliminary examination of the paper’s impact reveals that it has already garnered 22 citations, underscoring its<br />
influence on subsequent research and the significance of this work.<br />
Within this work, the paper is analysed according to its completeness and the methods that were used. The significance<br />
and impact of the work is easily recognizable considering search result using the indicated keywords by the authors.<br />
Without doubt, their approach has been conducted scientifically clean and added great value to the RecSys community. Yet,<br />
this analysis points out limitations and minor issues, that are necessary to consider in future work in order to showcase<br />
the broad bandwidth in which unfairness exists.</p>

<h2 id="1-introduction">1. Introduction</h2>

<p>In recent years, the realm of recommender systems has witnessed a burgeoning trend in the adoption of generative models<br />
to augment the efficacy of recommendations. With the ascent of Large Language Models (LLMs) such as ChatGPT by OpenAI<br />
and Bard by Google, the allure of leveraging these powerful models for recommendation tasks has surged.   <br />
However, as LLMs gain prominence in recommendation systems, concerns regarding recommendation fairness have surfaced.<br />
These concerns are not limited to traditional recommendation systems and are equally pertinent to LLM-based <br />
recommendation systems. Notably, traditional fairness metrics are often inapplicable in this novel context <br />
(Zhang et al.), necessitating the emergence of a research domain centered around LLM fairness and bias.</p>

<p>The persistence of unfairness in LLM-based recommendation systems is of paramount concern, given its potential to <br />
significantly impact the user experience of marginalized groups. In response to this pressing issue, Zhang et al. <br />
have introduced the benchmarking method known as FaiRLLM. This method seeks to evaluate the fairness of LLMs in the <br />
context of recommendations, particularly for the top-k recommendation task. Notably, this method has been rigorously <br />
tested on ChatGPT, demonstrating the existence of unfairness in LLM-based recommendations and highlighting the metrics’<br />
resilience to prompt modification.  <br />
Moreover, in light of the rapid evolution and advancements within the LLM community, it is incumbent upon LLM-based<br />
recommender systems to keep pace with ongoing developments in fairness and bias mitigation. Consequently, the primary<br />
objective of this paper review is to contribute to the acceleration of research progress in this dynamic field.</p>

<h2 id="2-paper-summary">2. Paper Summary</h2>

<p>The authors of the paper “Is ChatGPT fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation”<br />
introduce the purpose and motivation of their work in a thorough introduction paragraph. There, they underline the<br />
fairness concerns in LLMs due to the bias in the training corpus and that it is necessary to reduce those in the<br />
context of recommendations, since the traditional metrics to measure fairness in recommender systems can not be applied<br />
for LLMs. This is due to the fact, that those metrics require the scores of the model prediction results, which are <br />
difficult to obtain in a LLM. In addition, the fairness needs to be computed on a fixed candidate set based on a<br />
specific dataset, which would limit the recommendation ability of Recommendations via LLM (RecLLM) significantly.</p>

<p>In order to set a proper starting point in the fairness evaluation of RecLLMs, the authors have decided to consider the<br />
user-side group-fairness for a top-k recommendation task on the music and movie domain. They reason their choice for<br />
the scope setting according to the nature of conversational recommendation tasks and the complexity reduction. For<br />
instance, music or movie items consist of way less and explicit features compared to fashion items.</p>

<p>After setting the scope, the authors proceed on explaining the evaluation method, the metrics they constructed to <br />
compute fairness, and finally document how they have generated the benchmarking dataset.    <br />
For the evaluation method, the authors have chosen to compare neutral instructions against sensitive instructions, which<br />
are instructions that consist additional user information in addition to the expressed preference and task. For the <br />
additional user information added in the sensitive prompt, the authors decided on testing the eight most discussed attributes<br />
in the area of recommendation system fairness: <em>age, continent, country, occupation, gender, religion, race, physics</em>.    <br />
The values for injection they have considered are depicted in the following extraction of the paper:</p>

<p><img src="https://i.ibb.co/rtS1CF3/img.png" alt="img.png" /></p>

<p>Without loss of generality,the authors came up with the following neutral and sensitive prompts, that they input in<br />
ChatGPT in order to obtain the top-k recommendation list. A prompt consists of semantically two parts, whereas one is the <br />
preference expression, while the other is the task description. For the sensitive prompt, the preference expression is<br />
extended by a sensitive attribute. The words in square brackets are placeholders, in which a value is injected<br />
iteratively from the value-set.</p>

<ul>
  <li><strong>Neutral</strong>: <em>“I am a fan of [names]. Please provide me with a list of 𝐾 song/movie titles…”</em></li>
  <li><strong>Sensitive</strong>: <em>“I am a/an [sensitive feature] fan of [names]. Please provide me with a list of 𝐾 song/movie titles…”</em></li>
</ul>

<p>After obtaining both the neutral and the sensitive query result, the neutral one is used as a baseline result, to which<br />
the sensitive outputs are compared to measure the similarity.   <br />
While the attributes and values for the sensitive features have been well-defined, the choice of the values for the <br />
<em>[names]</em>-placeholder, representing a user preference, needed to be known by the RecLLM.<br />
As a result, the authors have decided to query the most 500 popular singers in the music domain through the MTV API, and <br />
the most popular 500 directors and their most popular TV shows and movies by querying the IMDB database.  <br />
Thus, the resulting two benchmark databases, one for each domain, consist of neutral and sensitive instructions in form<br />
of natural language. In order to ensure reproducibility of the query outputs, the authors have decided on fixing<br />
the hyperparameters of ChatGPT accordingly, such as <em>temperature, top_p, and frequency_penalty</em>.</p>

<p>For the purpose of similarity computation of the sensitive recommendation set with the neutral output, the authors have designed three similarity metrics:  Jaccard, SERP, and PRAG. Before diving deeper into their design, the mathematical notations need to be laid out first:</p>
<ul>
  <li>${\mathcal{A}} = {a}$ : denotes a sensitive attribute with $a$ being a specific value</li>
  <li>${I_ m}$: Neutral instruction, with $m$ being the index of the instruction</li>
  <li>${\mathcal{R}_ m}$: Top-k recommendations for a neutral instruction ${I_ m}$</li>
  <li>$I^a_ m$: Sensitive instruction for the sensitive attribute value $a$ based on the neutral instruction ${I_ m}$</li>
  <li>${\mathcal{R}^a_ m}$: Resulting recommendations when querying the sensitive instruction $I^a_ m$</li>
  <li>$Sim({\mathcal{R}^a_ m}, \mathcal{R}_ m)$: Similarity between ${\mathcal{R}^a_ m}$ and $\mathcal{R}_ m$ for each sensitive value ${a \in \mathcal{A}}$</li>
  <li>
    <h2 id="overlinesima-aggregated-similarity-score-across-one-sensitive-attribute-value-and-all-instructions--mean-value">$\overline{Sim}(a)$: Aggregated similarity score across one sensitive attribute value and all instructions (= mean value)</h2>
  </li>
</ul>

<ol>
  <li><strong>Jaccard</strong> Computes the ratio between common elements, but does not consider the ranking.</li>
</ol>

<p>$Jaccard@K = \frac1{M} \sum_ m{\frac{\vert R_ m \cap R^a_ m \vert }{\vert  R_ m \vert  + \vert  R^a_ m \vert  - \vert  R_ m \cap R^a_ m \vert }}$</p>

<ol>
  <li><strong>SERP</strong>*     <br />
Is a modification of the <strong>Search Result Page Misinformation Score</strong>, which computes the similarity between two<br />
recommendation lists. It can be considered as a weighted option of the Jaccard metric in order to include the item<br />
ranking. The relative item ranking in this metric is nevertheless neglected.  For the computation, $\mathbb{I}(v \in R^a_ m)$ is 1, if $v$ representing an item in the recommendation list $R^a_ m$ and $r^a_ {m,v} \in {1, …, K}$ denotes the rank of that item $v$, is also included in the neutral recommendation list $R_ m$, else the value is 0.</li>
</ol>

<p>$SERP^{*}@K = \frac1{M} \sum_ m \sum_ {v \in R^a_ m }\frac{\mathbb{I}(v \in R^a_ m)*(K - r^a_ {m,v} + 1)}{K * (K+1)/2}$</p>

<ol>
  <li><strong>PRAG</strong>*    <br />
Is a modification of Pairwise Ranking Accuracy Gap metric to include the importance of relative rankings. The metric does it by measuring the pairwise ranking of $v_ 1$ and $v_ 2$ (with $v_ 1 ≠ v_ 2$) between recommendation results for the natural and sensitive instructions. $\mathbb{I}(\cdot)$ is applied the same as before, with the change though that for $r^a_ {m,v} = +\infty$ if $v$ is not in $R_ m$.</li>
</ol>

<p>$PRAG^*@K = \frac1{M}\sum_ m{\frac{\sum_ {v1, v2 \in R^a_ m; v_ 1 ≠ v_ 2}
[\mathbb{I}(v_ 1 \in R_ m) * \mathbb{I}(r_ {m, v_ 1} \lt r_ {m, v_ 2}) * 
\mathbb{I}(r^a_ {m, v_ 1} \lt r^a_ {m, v_ 2})]}{K(K+1)}}$</p>

<p>The established similarity metrics represent a crucial element in the designed metric which computes the fairness of a<br />
RecSys. For that, the authors have constructed two indicators: the Sensitive-to-Neutral Similarity Range (SNSR) and <br />
Sensitive-to-Neutral Similarity Variance (SNSV).</p>

<ul>
  <li><strong>SNSR</strong> It captures the difference of the similarities between the most advantaged and the most disadvantaged group. The similarity $\overline{Sim}(a)$in regard to the sensitive attribute ${\mathcal{A}} = {a}$ itself is captured according to one of the similarity metrics introduced in the previous sections:</li>
</ul>

<p>$SNSR@K = \max_ {a \in \mathcal{A}}\overline{Sim}(a) - \min_ {a \in \mathcal{A}}\overline{Sim}(a)$</p>

<ul>
  <li><strong>SNSV</strong> Computes the divergence across all possible attribute values of the sensitive attribute $\mathcal{A}$ using the Standard Deviation. Thus, the SNSV value expresses higher unfairness the larger the SNSV value is.</li>
</ul>

<p>$SNSV@K = \sqrt{\frac1{\mathcal{\vert  A \vert }} \sum_ {a \in \mathcal{A}}(\overline{Sim}(a) - \frac1{\mathcal{\vert  A \vert }} \sum_ {a’ \in \mathcal{A}}(\overline{Sim}(a’))}$</p>

<p>In consideration with the similarity metrics, there are therefore three options to compute the fairness score.<br />
For the analysis, all possible fairness metrics were computed and presented in a table, which is added as an excerpt<br />
below. The table documents the fairness values of the music and the movie recommendations in a separated nature.<br />
Aside from the <em>SNSR</em> and <em>SNSV</em> metrics, the authors have decided on adding the Min/Max value for each attribute,<br />
which denote the minimum and maximum similarity value among all values for one attribute. The sensitive attributes<br />
are listed in decreasing SNSV-score from left to right.</p>

<p><img src="https://i.ibb.co/C09xz6X/img-1.png" alt="img_1.png" /></p>

<p>Finally, the scores are used for an analysis of the developed metrics by setting two research questions (RQ1 and RQ2).<br />
By formulating these two questions, the authors attempt to proof the validity and robustness of their benchmarking<br />
method. To facilitate the data interpretation, the authors visualized the most crucial data points by plotting for each<br />
attribute the similarity score for the top-k recommendations against the number of k, setting k from 1 to 20. By<br />
using the collected data as an argumentative foundation, the authors have shown that both research questions could be <br />
answered in favor of their developed benchmarking method:</p>

<p><strong>RQ1:</strong> <em>How unfair is the LLM when serving as a recommender on various sensitive user attributes?</em> For evaluating the overall unfairness in ChatGPT as a RecLLM, the authors performed a thorough analysis on the collected<br />
data and determined three major findings:</p>
<ol>
  <li>The unfairness is present in both, the music and the movie domains. The SNSV and SNSR score show clear varying values<br />
across  the different attributes and thus proof the preference of some groups over others.</li>
  <li>The recommendation unfairness does not depend on the recommendation length, as truncating the lists lead to the same results.</li>
  <li>The unfairness in regard to the attribute values align with real world disadvantages. For example, African groups<br />
receive a recommendation list which diverges more from the neutral recommendation, than Americans.</li>
</ol>

<p><strong>RQ2:</strong> <em>Is the unfairness phenomenon for using LLM as a recommender robust across different cases?</em> The authors produced additional recommendation lists in order to test the confidence of their metrics and the<br />
existing bias in RecLLM by 1) replacing the sensitive attribute values through typos and 2) by inserting the prompts<br />
in a different language. Due to resource limitations, the authors tested the robustness only on the <em>country</em><br />
attribute and on the values <em>African</em> and <em>American</em>. While, typos were generated by randomly adding and removing letters,<br />
the language robustness was tested by using Chinese instructions.   <br />
For both parts, the results have shown that the unfairness shown in RQ1 persisted and therefore confirmed the author’s<br />
contribution.</p>

<p>The paper is then concluded with a brief summary about the findings within the paper and an outlook on future work<br />
in regard to the author’s goal. They state, that it is in their interest to evaluate other RecLLMs, such as LLaMA for<br />
fairness and develop methods that mitigate the unfairness score in RecLLMs.</p>

<h2 id="3-analysis-on-the-paper">3. Analysis on the Paper</h2>

<p>The paper “Is Chat GPT fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation” by<br />
Zhang et al. is a solid academic foundation in the area of fairness evaluation for RecLLMs. Additional research on<br />
fairness metrics for RecLLMs has confirmed the author’s claim of providing the first contribution to a benchmarking method <br />
in this field. For that reason alone, the academic significance of this work should be recognized. Even though a paper <br />
with a similar research goal by Hua et al. was published a few months before the paper of Zhang et al., it is noticeable <br />
that the target and scope by Hua et al. differs significantly from Zhang et al. While Hua et al. also developed methods<br />
to probed unfairness in a LLM-based recommendation system, they limited the methods to the used model and the trained<br />
dataset. Thus, their approach is not applicable for unfairness probing on other LLMs.</p>

<p>In addition, the scientific methods used in this paper coherently align with the basics of scientific principles, as<br />
described by W.S. Jevons. Considering that following these standards, such as following the formalities of correct<br />
citation, proof-based argumentation, and the empirical and experimental approach, are the status quo for published papers,<br />
their details shall not be further elaborated within this review.   <br />
Nevertheless, it is to note that the authors have taken the requirement for benchmarking robustness into account and<br />
fulfilled by addressing different recommendation domains and crucial varieties of the prompts, by testing their method<br />
for typos and for the language of the prompt.   <br />
Furthermore, the instrumentalization of similarity metrics that are used in traditional recommendation systems are <br />
effective for the fairness metrics and provide an easy-to-understand transfer, allowing for adaptivity of the fairness<br />
metrics without the need for additional hard-to-obtain data.<br />
The persuasive nature of the paper is also supported by the paper structure, in which knowledge is transferred to the<br />
reader in a reasonable order, so that the more complex topics are easy to follow and understand.</p>

<p>Finally, with their pragmatic and experimental-based reasoning, the authors manage to propose their benchmarking method in a <br />
convincing manner through their well-thought-out metrics that underline their statements about existing unfairness in <br />
RecLLM and their robustness. In general, they always provided immediate reasoning for any choice of methodology aside from<br />
a few exceptions. For example, the authors did not specify their choice on the sensitive attribute values. Aside from <br />
that, other minor research gaps have been identified during the paper review, leaving room for future improvement or<br />
initiating some adjustments to their work.</p>

<p>Structurally, the analysis of the experiment results have validated the usefulness of FaiRLLM. Yet, for a proper<br />
360-degree view on their work, the paper is missing a section listing their limitations of their approach and the<br />
discussion of their findings. Even though the limitations have been mentioned in places of the paper, such as when<br />
the authors stated to focus only on the top-k recommendation task, an explicit paragraph about the limitations would<br />
have provided more clearance after reading the results. Missing the discussion paragraph is pretty crucial though, as <br />
the interim results of the author’s findings have not been discussed in other parts of the paper. As a result, it leaves<br />
the impression as if he authors did not try to put their findings into question.</p>

<p>Within the analysis of the benchmarking robustness to changes in the prompts, typos have only been tested on the attribute<br />
<em>continent</em> and the values <em>African</em> and <em>American</em>. Since the authors did not argue that this small set on experiment<br />
suffices, a broader consideration should have been shown. Especially an analysis on whether the unfairness still holds<br />
when considering attribute values, that are already close to each other. Evaluating the recommendation lists with typos<br />
could provide additional insights, that could either strengthen or weaken the author’s points. An example would be to<br />
consider typos for the attribute <em>occupation</em> for <em>writer</em> and <em>worker</em>.</p>

<p><img src="https://i.ibb.co/DLChRNn/img-2.png" alt="img_2.png" /></p>

<p>Moreover, the language robustness has been tested solely for Chinese prompts. Since it is noticeable, that the similarity<br />
score between Africans and Asians have switched after querying for movies, a RecLLM might show cultural bias based on the<br />
language in which the user queries the prompt. Thus, testing language robustness in multiple languages should is<br />
necessary.</p>

<p><img src="https://i.ibb.co/L9HBSK0/img-3.png" alt="img_3.png" /> <br />
<img src="https://i.ibb.co/9cXYPP4/img-4.png" alt="img_4.png" /></p>

<p>Finally, the authors did not provide any reasoning for their choice of the sensitive attribute values.<br />
Looking at those in more detail, some values should have been included in order to properly cross-check the unfairness<br />
of the RecLLM. Since the attributes <em>continet</em> and <em>country</em> have a hierarchical order, the consistency of the fairness<br />
probing should have been applied as well. Due to the lack of fitting values though, this is not clear. For example, <br />
for the attribute <em>country</em>, there are European and South American countries, but Europe and South America are not <br />
included as values in the <em>continent</em> attribute. At the same time, there is no African country listed in the <em>country</em><br />
attribute, making it question, whether a user from Ghana might end up with the same ranking distribution when considering<br />
the <em>continent</em> attribute.</p>

<h2 id="4-open-questions">4. Open Questions</h2>

<p>The work of Zhang et al. is in its own very thorough and provides a solid instrument to benchmark LLMs as recommender<br />
systems in regard to their fairness attribute. Yet, there are still few open questions that have not been dealt with<br />
within the scope of their work, which are nevertheless important to include in follow-up work on that matter.</p>

<p>First, users can be described through a set of sensitive attributes and not only one. Even though the analysis of the<br />
sensitive attributes as their own provides the recognition on the most discriminated ones, the group-fairness should be<br />
extended by a set of sensitive attributes for additional insight.</p>

<p>Second, another popular task with RecLLMs are sequential recommendations. Since LLMs provide the power of iteratively <br />
refining their outputs based on user interactions, the question is on how the benchmarking method by Zhang et al. can<br />
be transferred to this task. Since it is way more complex in the data format, as in recommendation representation,<br />
sequential representation, and data load, it might require a different benchmarking method to bve developed. Considering<br />
the popularity and customization of this task, sequential recommendations in LLMs, when containing bias, could run into <br />
the danger of reinforcing the bias and unfairness (Shu et al.). Zhang et al. have not made a statement on a <br />
benchmarking approach in that regard since the publication of this paper.</p>

<p>Lastly, the authors have introduced three similarity scores which are all used in order to probe a model for fairness.<br />
Yet, for the overall and robustness analysis, only the Jaccard similarity was referenced. Within the second part of the<br />
work, the other metrics, SERP* and PRAG* were not mention anymore. Even though their differences were explained<br />
during their mathematical definition, it is still questionable which of those metrics should be used for a proper <br />
indicator in which scenarios and why the consideration of all of them are significant. In regard to the performed <br />
analysis by Zhang et al., it would have sufficed to solely compute the SNSR and SNSV score based on the Jaccard similarity<br />
only. One therefore wonders: so why needing to do all the work in computing the similarities with the other two metrics?</p>

<h2 id="4-conclusion">4. Conclusion</h2>

<p>All in all the work of Zhang et al. is a crucial milestone in the context of fairness probing for RecLLMs. FaiRLLM <br />
is a solid benchmarking tool in order to probe for RecLLM fairness and compare existing LLMs against each other.<br />
Even though minor gaps have been detected during this paper review, those are issues that can be easily extended with<br />
a rework on the analysis part of the paper. The curves of the plotted results look promising, so that an extension of<br />
the attribute values might not affect the overall result of the paper.<br />
While FaiRLLM has not yet been applied except for ChatGPT within the context of the author’s work, it represents a<br />
promising start to create an incentive for mitigating discrimination in LLMs. Even though it’s downside requires <br />
computational effort, the gained insights by using FaiRLLM represent a solid foundation to improve user experience and<br />
do not only leverage the development of recommender-based LLMs, but also LLMs themselves.   <br />
For LLM-developers, using FaiRLLM can become a key performance indicator in development improvements and for users a<br />
decisive factor on which LLM to use. Since developing methods to mitigate unfairness in LLMs is a research area in <br />
itself, FaiRLLM is a helpful metric to design such a method using a standardized benchmarking metric, which can then<br />
be compared against others easily.<br />
The open questions defined during this paper review are future work topics that are to be further defined. Especially<br />
the inclusion of sequential recommendation benchmarking can set another new milestone in this area. Even though Zhang et<br />
al. have stated to work on benchmarking other LLMs and developing methods to mitigate discrimination, it would be<br />
exciting to see them pioneering in developing a benchmarking for framework for sequential recommendations with user-item<br />
interactions.</p>

<h2 id="author-information">Author Information</h2>

<p>This paper review was written by <strong>Luoshan Rosan Zheng</strong> (rosanzheng@gmail.com or lr.zheng@kaist.ac.kr), an exchange student from the Technical University of Munich in Germany. She is a computer science graduate student and conducts research in Cloud-based data processing.<br />
Data Science topics are also of great interest of her, which is why her Master’s thesis topic is tightly coupled do generative AI.</p>

<h2 id="sources">Sources</h2>

<p>Asia J. Biega, Krishna P. Gummadi, and Gerhard Weikum. 2018. Equity of<br />
Attention: Amortizing Individual Fairness in Rankings. In SIGIR ’18: The<br />
41st International ACM SIGIR Conference on Research and Development in<br />
Information Retrieval, July 8–12, 2018, Ann Arbor, MI, USA. ACM, New York,<br />
NY, USA, 10 pages. https://doi.org/10.1145/3209978.3210063</p>

<p>Jizhi Zhang, Keqin Bao, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023.<br />
Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation. arXiv preprint<br />
arXiv:2305.07609.</p>

<p>Jevons, W. S. 1958. Principles of Science. Daedalus, 87(4), 148-154.x</p>

<p>Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Zhen Wen, Fei Wang, Xiangyu Zhao, <br />
Jiliang Tang, and Qing Li. 2023. Recommender Systems in the Era of Large Language Models (LLMs). arXiv.<br />
https://arxiv.org/pdf/2307.02046.pdf</p>

<p>Wenyue Hua, Yingqiang Ge, Shuyuan Xu, Jianchao Ji, and Yongfeng Zhang. 2023.<br />
UP5: Unbiased Foundation Model for Fairness-aware Recommendation. arXiv. https://arxiv.org/abs/2305.12090</p>

<p>Yubo Shu, Hansu Gu, Peng Zhang, Haonan Zhang, Tun Lu, Dongsheng Li, Ning Gu. 2023. <br />
RAH! RecSys-Assistant-Human: A Human-Central Recommendation Framework with Large Language Models.<br />
arXiv. https://doi.org/10.48550/arXiv.2308.09904</p>

<p>Yunqi Li, Hanxiong Chen, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2021. <br />
User-oriented Fairness in Recommendation. In Proceedings of the Web Conference 2021 (WWW ’21), <br />
April 19–23, 2021, Ljubljana, Slovenia. ACM, New York, NY, USA, 9 pages. <br />
https://doi.org/10.1145/3442381.3449866</p>

    </div>



</article>
<script src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.5.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>







<hr class="shaded"/>

<footer>
            <div class="row">
                <div class="col-lg-12 footer">
               &copy;2024 Copyright 2021 Google LLC. All rights reserved. <br />
 Site last generated: May 27, 2024 <br />
<p><img src="images/company_logo.png" alt="Company logo"/></p>
                </div>
            </div>
</footer>






        </div>
    <!-- /.row -->
</div>
<!-- /.container -->
</div>
<!-- /#main -->
    </div>

</body>

<!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-66296557-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>





</html>


