<!DOCTYPE html>
<html>
<head>
    <meta name="google-site-verification" content="1tAPTdgw0-t8G2Bya463OpBtYUbj9Um93gfnsowYKLw" />
    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-CV4PTXQSTW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-CV4PTXQSTW');
</script>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="">
<meta name="keywords" content="reviews,  ">
<title>[NIPS 2022] Pure Transformers are Powerful Graph Learner | Awesome Reviews</title>
<link rel="stylesheet" href="css/syntax.css">

<link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
<!--<link rel="stylesheet" type="text/css" href="css/bootstrap.min.css">-->
<link rel="stylesheet" href="css/modern-business.css">
<!-- Latest compiled and minified CSS -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<link rel="stylesheet" href="css/customstyles.css">
<link rel="stylesheet" href="css/boxshadowproperties.css">
<!-- most color styles are extracted out to here -->
<link rel="stylesheet" href="css/theme-blue.css">

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="js/jquery.navgoco.min.js"></script>


<!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<!-- Anchor.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.2.0/anchor.min.js"></script>
<script src="js/toc.js"></script>
<script src="js/customscripts.js"></script>

<link rel="shortcut icon" href="images/favicon.ico">

<!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
<![endif]-->

<link rel="alternate" type="application/rss+xml" title="DSAILatKAIST.github.io" href="http://dsailatkaist.github.io/feed.xml">







    <script>
        $(document).ready(function() {
            // Initialize navgoco with default options
            $("#mysidebar").navgoco({
                caretHtml: '',
                accordion: true,
                openClass: 'active', // open
                save: false, // leave false or nav highlighting doesn't work right
                cookie: {
                    name: 'navgoco',
                    expires: false,
                    path: '/'
                },
                slide: {
                    duration: 400,
                    easing: 'swing'
                }
            });

            $("#collapseAll").click(function(e) {
                e.preventDefault();
                $("#mysidebar").navgoco('toggle', false);
            });

            $("#expandAll").click(function(e) {
                e.preventDefault();
                $("#mysidebar").navgoco('toggle', true);
            });

        });

    </script>
    <script>
        $(function () {
            $('[data-toggle="tooltip"]').tooltip()
        })
    </script>
    <script>
        $(document).ready(function() {
            $("#tg-sb-link").click(function() {
                $("#tg-sb-sidebar").toggle();
                $("#tg-sb-content").toggleClass('col-md-9');
                $("#tg-sb-content").toggleClass('col-md-12');
                $("#tg-sb-icon").toggleClass('fa-toggle-on');
                $("#tg-sb-icon").toggleClass('fa-toggle-off');
            });
        });
    </script>
    





</head>
<body>
<!-- Navigation -->
<nav class="navbar navbar-inverse navbar-static-top">
    <div class="container topnavlinks">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="fa fa-home fa-lg navbar-brand" href="index.html">&nbsp;<span class="projectTitle"> Awesome Reviews</span></a>
        </div>
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                <!-- toggle sidebar button -->
                <li><a id="tg-sb-link" href="#"><i id="tg-sb-icon" class="fa fa-toggle-on"></i> Nav</a></li>
                <!-- entries without drop-downs appear here -->




                
                
                
                <li><a href="introduction">Introduction</a></li>
                
                
                
                <li><a href="https://statistics.kaist.ac.kr/" target="_blank" rel="noopener">KAIST ISysE</a></li>
                
                
                
                <!-- entries with drop-downs appear here -->
                <!-- conditional logic to control which topnav appears for the audience defined in the configuration file.-->
                
                
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">Reviews<b class="caret"></b></a>
                    <ul class="dropdown-menu">
                        
                        
                        <li><a href="reviews_kse801_2022.html">KSE801 (2022)</a></li>
                        
                        
                        
                        <li><a href="reviews_DS503_2023.html">DS503 (2023)</a></li>
                        
                        
                    </ul>
                </li>
                
                
                
			<li>



  <a class="email" title="Submit feedback" href="#" onclick="javascript:window.location='mailto:swkim@kaist.ac.kr?subject=Question about reviews feedback&body=I have some feedback about the [NIPS 2022] Pure Transformers are Powerful Graph Learner page: ' + window.location.href;"><i class="fa fa-envelope-o"></i> Feedback</a>

</li>



		
                <!--comment out this block if you want to hide search-->
                <li>
                    <!--start search-->
                    <div id="search-demo-container">
                        <input type="text" id="search-input" placeholder="search...">
                        <ul id="results-container"></ul>
                    </div>
                    <script src="js/jekyll-search.js" type="text/javascript"></script>
                    <script type="text/javascript">
                            SimpleJekyllSearch.init({
                                searchInput: document.getElementById('search-input'),
                                resultsContainer: document.getElementById('results-container'),
                                dataSource: 'search.json',
                                searchResultTemplate: '<li><a href="{url}" title="[NIPS 2022] Pure Transformers are Powerful Graph Learner">{title}</a></li>',
                    noResultsText: 'No results found.',
                            limit: 10,
                            fuzzy: true,
                    })
                    </script>
                    <!--end search-->
                </li>
            </ul>
        </div>
        </div>
        <!-- /.container -->
</nav>



<!-- Page Content -->
<div class="container">
  <div id="main">
    <!-- Content Row -->
    <div class="row">
        
        
            <!-- Sidebar Column -->
            <div class="col-md-3" id="tg-sb-sidebar">
                

<ul id="mysidebar" class="nav">
  <li class="sidebarTitle"> </li>
  
  
  
  <li>
      <a title="How to contribute" href="#">How to contribute</a>
      <ul>
          
          
          
          <li><a title="How to contribute?" href="how_to_contribute.html">How to contribute?</a></li>
          
          
          
          
          
          
          <li><a title="Review template (Example)" href="template.html">Review template (Example)</a></li>
          
          
          
          
      </ul>
   </li>
     
      
  
  <li>
      <a title="Reviews" href="#">Reviews</a>
      <ul>
          
          
          
          <li><a title="KSE801 (2022F)" href="reviews_kse801_2022.html">KSE801 (2022F)</a></li>
          
          
          
          
          
          
          <li><a title="DS503 (2023S)" href="reviews_DS503_2023.html">DS503 (2023S)</a></li>
          
          
          
          
      </ul>
   </li>
     
      
      
      <!-- if you aren't using the accordion, uncomment this block:
         <p class="external">
             <a href="#" id="collapseAll">Collapse All</a> | <a href="#" id="expandAll">Expand All</a>
         </p>
         -->
</ul>

<!-- this highlights the active parent class in the navgoco sidebar. this is critical so that the parent expands when you're viewing a page. This must appear below the sidebar code above. Otherwise, if placed inside customscripts.js, the script runs before the sidebar code runs and the class never gets inserted.-->
<script>$("li.active").parents('li').toggleClass("active");</script>



            </div>
            
        

        <!-- Content Column -->
        <div class="col-md-9" id="tg-sb-content">
            <script type="text/x-mathjax-config">
MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/math...">
</script>
<article class="post" itemscope itemtype="https://schema.org/BlogPosting">

    <header class="post-header">
        <h1 class="post-title" itemprop="name headline">[NIPS 2022] Pure Transformers are Powerful Graph Learner</h1>
        <p class="post-meta"><time datetime="2023-04-20T00:00:00+09:00" itemprop="datePublished">Apr 20, 2023</time> /
            
            
            
            
            

        </p>


    </header>

    <div class="post-content" itemprop="articleBody">

        

        <h1 id="pure-transformers-are-powerful-graph-learners"><strong>Pure Transformers are Powerful Graph Learners</strong></h1>

<h2 id="1-introduction-and-motivation"><strong>1. Introduction and Motivation</strong><a href="https://dsailatkaist.github.io/template.html#1-problem-definition"></a></h2>

<p>강력한 Natural Language Module로써의 역할을 수행하던 Attention Architecture가 Image 도메인으로 이식되어 Vision Transformer 가 탄생한 것과 같이, Graph Domain에서도 Transformer를 사용하려는 시도가 지속적으로 있어 왔습니다. Graph Attention Network 처럼 Message Passing GNN 의 Aggregate 과정에서 Attention을 활용하는 시도도 있었으며 Transformer 구조를 도입하여 Attention을 사용하려는 시도도 있었습니다.</p>

<p>Transformer 구조를 채용할 때에 그저 단순히 노드들을 Transformer의 Token으로 사용한다면 그래프 구조가 제대로 학습될 수 없는 문제가 발생합니다. Edge 정보와 그래프 연결에 대한 정보가 Token으로 전환되지 않았기 때문입니다. 그래서 Attention is All You Need 논문에서 각 Token에 Positional Encoding을 더해주듯이,  Edge 의 정보와 Vertex간의 관계 (i.e. shortest path)를 Encoding하여 그래프 구조를 Transformer Encoder에 학습시키는 기법들이 연구되어 왔으며 벤치마크 데이터셋에서 매우 우수한 성과를 거두었습니다. 아래는 Graph Transformer 관련하여 추가로 읽어 볼 만한 논문들입니다.</p>

<ul>
  <li>Self-Supervised Graph Transformer on Large-Scale Molecular Data. NeurIPS 2020</li>
  <li>Rethinking Graph Transformer with Spectral Attention? NIPS 2021</li>
  <li><a href="https://dsailatkaist.github.io/Do_Transformers_Really_Perform_Bad_for_Graph_Representation.html"><strong>Do Transformers Really Perform Bad for Graph Representation?</strong></a>  NeurIPS 2021</li>
</ul>

<p>볼드체로 표시된 Do Transformer Really Perform Bad for Graph Representation[1]에서는 각 노드간의 상관관계를 인코딩한 1)Centrality Encoding과, 두 노드 사이의 Shortest Path를 Parametrize한 2)Spatial Encoding 개념을 제시함과 동시에, 그래프의 Edge Feature 또한 Transformer에게 전달하기 위해 두 노드 사이의 shortest path들의 feature들을 Encoding 한 3)Edge Encoding을 Transformer에게 전달해 줍니다. Graphormer는 결국 Transformer에게 그래프 구조를 제대로 학습시키기 위해서 3가지의 Learnable Encoder를 Pure Transformer에 추가로 도입합니다. 아래 삽화를 통해 Graphormer에 도입된 세개의 추가적인 Encoding을 직관적으로 이해할 수 있습니다. Graphormer와 같은 경우에는 추가적인 Encoding들을 더함으로써 Pure Transformer 구조에 Graph Specific Modification이 가해진다는 사실을 알 수 있습니다.</p>

<p><img src="https://github.com/Jaewoopudding/DeepLearningPractice/blob/master/homer.PNG?raw=true" alt="" /></p>

<p>Message Passing 구조의 GNN경우에는  Attention을 Local Node에만 적용시켜 Attention의 장점을 모두 활용하지 못 하는 문제와, 학습이 진행됨에 따라 모든 Node의 Feature가 비슷비슷해지는 graph oversmoothing 문제가 있으며 잘 학습이 되더라도 1-WL test의 표현력을 넘지 못한다고 알려져 있습니다[2][3].</p>

<p>그래서   <strong>Pure Transformers are Powerful Graph Learners</strong>의 저자들은, 표현력에 제한이 있는 Message Pssing GNN을 사용하지 않으면서도, Transformer를 그래프에 맞게 변형하는 연구 흐름과는 반대로 기존 Transformer Encoder의 구조를 변형하지 않은 Pure Transformer를 사용하기로 결정하였습니다.  그 결과 논문 저자들은 Edge와 Vertex를 각각 Tokenize하여 추가적인 Learnable Encoder 도입 없이도 Vertex Feature와 Edge Feature, 그리고 그래프 구조까지도 Transformer에 효과적으로 전달 수 있는 새로운 기법인 <strong>TokenGT</strong> 를 개발했습니다.</p>

<p><img src="https://github.com/Jaewoopudding/DeepLearningPractice/blob/master/tokenGT.PNG?raw=true" alt="" /></p>

<h2 id="2-contributions"><strong>2. Contributions</strong><a href="https://dsailatkaist.github.io/template.html#2-motivation"></a></h2>

<ol>
  <li>Node Identifier와 Type Identifier의 도입을 통해 그래프 구조를 Vertex/Edge Feature에 통합하여 Pure Transformer를 사용할 수 있는 방법을 제시하였습니다.</li>
  <li>Token-wise Embedding과 그 사이의 Self Attention이 그래프 상의 Permutation Equivalent한 선형 연산자(IGN)를 근사할 수 있다는 사실을 밝힘으로써 Transformer 구조가 Message Passing GNN보다 더 우월한 표현력을 가짐을 보였습니다.</li>
  <li>Transformer Encoder 가 최소한 WL test 만큼의 강력한 표현력을 가짐을 증명하였습니다.</li>
  <li>여러 그래프 벤치마크 데이터셋에 대해서 TokenGT 가 Transformer 구조에 많은 변형을 가한 모델들 만큼  경쟁력 있음을 보였습니다.</li>
</ol>

<h2 id="3-method"><strong>3. Method</strong><a href="https://dsailatkaist.github.io/template.html#3-method"></a></h2>

<ul>
  <li>Node Identifier
    <ul>
      <li>n개의 node가 있는 그래프에 대해서, 서로 orthonormal한 $\vec{p_v} \in \mathcal{R}^{(d_p)}$ 가 Node Identifier 입니다.</li>
      <li>$v$의 Node Feature $X_v$는 다음과 같이 augmented 됩니다. $X_v = [X_v, \vec{p_v}, \vec{p_v}]$</li>
      <li>$u$와 $v$사이의 Edge Feature $X_{(u,v)}$는 다음과 같이 augmented 됩니다. $X_{(u,v)} = [X_{(u,v)}, \vec{p_v}, \vec{p_u}]$</li>
      <li>orthonormal한 $\vec{p_v} \in \mathcal{R}^{(d_p)}$을 얻는 방법
        <ul>
          <li>Graph Laplacian $I-D^{1/2}AD^{1/2}$ 를 Eigen Decomposition 해서 얻는 Eigenvector들을 사용합니다.</li>
          <li>무작위로 생성된 행렬을 QR Decomposition해서 얻은 Orthogonal Random Features을 사용합니다.</li>
          <li>Graph Laplacian Eigenvector들은 그래프 연결성에 대한 정보를 갖고 있기 때문에 Orthogonal Random Features 기법보다 더 강력한 node identifier를 제공합니다.</li>
          <li>Orthonormal한 벡터를 Node Identifier로 사용함으로써, 서로 연결되어 있는 Node와 Edge의 내적 $[\vec{p_k}, \vec{p_k}] \cdot [\vec{p_v}, \vec{p_u}]$ 은,  $k\in {(v,u)}$가 아닌 이상 모두 0이 되어 Transformer는 연결 관계를 학습할 수 있게 됩니다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Type Identifier
    <ul>
      <li>학습 가능한 두개의 vector $\vec{E^\mathcal{V}}$와 $\vec{E^\mathcal{E}}$를 통해 최종 형태의 Token을 정의합니다.</li>
      <li>Node Token:  $X_v = [X_v, \vec{p_v}, \vec{p_v}, \vec{E^\mathcal{V}}]$</li>
      <li>Edge Token: $X_{(u,v)} = [X_{(u,v)}, \vec{p_v}, \vec{p_u}, \vec{E^\mathcal{E}}]$</li>
      <li>최종 Token을 통해 우리는 Transformer 구조의 변형 없이도 해당 토큰이 Node인지, Edge인지, 서로 연결되어 있는지의 여부를 학습 할 수 있습니다.</li>
    </ul>
  </li>
  <li>Transformer
    <ul>
      <li>ViT와 Bert에 사용된 Transformer와 똑같은 Multi Head Attention 을 활용합니다.</li>
    </ul>
  </li>
</ul>

<h2 id="4-theoretical-analysis"><strong>4. Theoretical Analysis</strong><a href="https://dsailatkaist.github.io/template.html#4-theoretical_analysis"></a></h2>

<p>IGN에 관련된 구체적인 Theorem들과 증명과정들은 생략하였습니다.</p>

<ul>
  <li>k-IGN은 k-WL test와 연계되어 있으며 k-IGN은 k-WL만큼 강력하다는 것이 증명되어 있습니다. [4]</li>
  <li>2-IGN은 모든 Message Passing GNN보다 표현력이 높은 것이 증명 되어 있습니다.</li>
  <li>논문에서 IGN의 Equivariant Linear Layer를 Transformer로 근사할 수 있음을 증명했습니다.</li>
  <li>2-IGN은 모든 Message Passing GNN 보다 강력하므로, Transformer로 구현된 TokenGT는 이론적으로 모든 Message Passing GNN보다 표현력이 높습니다.</li>
</ul>

<h2 id="5-experiment"><strong>5. Experiment</strong><a href="https://dsailatkaist.github.io/template.html#5-experiment"></a></h2>

<h3 id="experiment-setup"><strong>Experiment setup</strong><a href="https://dsailatkaist.github.io/template.html#experiment-setup"></a></h3>

<ul>
  <li>Dataset
-Barabasi-Albert Random Graph: IGN Approximation 실험을 위한 그래프 실험 환경
-PCQM4Mv2: 대규모 양자 화학 데이터셋</li>
</ul>

<h3 id="result"><strong>Result</strong><a href="https://dsailatkaist.github.io/template.html#result"></a></h3>

<p>Theoretical Analysis에서 언급했다시피, Transformer는 IGN을 근사할 수 있고, IGN을 근사할 수 있다는 건 IGN의 Equivariant Basis를 근사할 수 있다는 의미입니다. 아래의 실험 결과는 unseen graph에 대해서 TokenGT가 Equivariant Basis를 얼마나 잘 근사할 수 있었는지 보여주고 있습니다. 또한 Node Identifier와 Type Identifier에 대한 Ablation Study를 통해서, 각각의 요소가 성능 향상에 기여했음을 보여줍니다.<br />
<img src="https://github.com/Jaewoopudding/DeepLearningPractice/blob/master/GT2.PNG?raw=true" alt="" /></p>

<p>아래의 Table 2를 통해 TokenGT가 Strong modification이 가해진 Transformer만큼의 성능을 가지고 있음을 보여줍니다. 또한 Graph Laplacian Eigenvector로 node identifier를 설정한 경우가 랜덤한 node identifier보다 성능이 높음을 알 수 있습니다.</p>

<p><img src="https://github.com/Jaewoopudding/DeepLearningPractice/blob/master/GT1.PNG?raw=true" alt="" /></p>

<h2 id="6-conclusion"><strong>6. Conclusion</strong><a href="https://dsailatkaist.github.io/template.html#6-conclusion"></a></h2>

<h4 id="good-points">Good Points</h4>
<ol>
  <li>Node Identifier와 Type Identifier의 도입으로 Pure Transformer가 Thoery와 Practice 모두에서 효과적임을 밝혔습니다.
    <ul>
      <li>Transformer가 최소한 k-IGN, k-WL 만큼 표현력이 강하다는 사실을 증명했습니다.</li>
      <li>TokenGT가 GNN보다는 우수한 성능을, Strongly Modified Transformer와는 비슷한 성능을 내는 것을 확인하였습니다.</li>
    </ul>
  </li>
  <li>그래프를 (n+m)개의 토큰으로 해석함으로써 Graph Learning 의 새로운 Paradigm을 열었습니다.</li>
</ol>

<h4 id="challenges">Challenges</h4>
<ol>
  <li>토큰이 (n+m)개가 되어서 큰 그래프에 적용하기에는 Computational Cost가 지수적으로 증가합니다.</li>
  <li>SOTA보다 낮은 성능을 기록했습니다.</li>
</ol>

<blockquote>
  <h4 id="transformer-구조의-가장-큰-특징중-하나는-weak-inductive-bias입니다">Transformer 구조의 가장 큰 특징중 하나는 weak inductive bias입니다.</h4>
  <ul>
    <li>CNN : Transformer와 GNN(MPNN) : Graph Transformer 간의 관계에 상사성이 존재합니다.</li>
    <li>CNN은 강력한 inductive bias를 도입하여 weight sharing을 통해 적은 parameter로 이미지 특징을 효과적으로 학습할 수 있었습니다.</li>
    <li>ViT는 induvtive bias를 도입하지 않기에, 많은 데이터가 주어져야 제대로 된 성능을 낼 수 있지만, image에 대한 global view를 가질 수 있어 CNN보다 강력한 성능을 발휘합니다.</li>
    <li>Message Passing GNN 또한 k-hop node들의 정보를 aggregation함으로써 간단한 구조임에도 불구하고 강력한 성능을 내는 strong inductive bias를 가지고 있습니다. 그러나, 멀리 있지만 유의미한 정보를 가진 Node의 신호가 제대로 전달이 안 될 수도 있습니다.</li>
    <li>Graph Transformer 계열은 weak inductive bias 를 가지고 있지만, Message Passing 과정이 없기에 Graph 전체의 신호를 한번에 파악할 수 있어 강력한 성능을 낼 수 있습니다.</li>
  </ul>
</blockquote>

<hr />

<h2 id="author-information"><strong>Author Information</strong><a href="https://dsailatkaist.github.io/template.html#author-information"></a></h2>

<ul>
  <li>Author name : Jaewoo Lee
    <ul>
      <li>Affiliation : KAIST SILAB</li>
      <li>Research Topic : Offline Reinforcement Learning, Graph Representational Learning, Meta Learning.</li>
    </ul>
  </li>
</ul>

<h2 id="7-reference--additional-materials"><strong>7. Reference &amp; Additional materials</strong><a href="https://dsailatkaist.github.io/template.html#7-reference--additional-materials"></a></h2>

<p>Please write the reference. If paper provides the public code or other materials, refer them.</p>

<ul>
  <li><a href="https://github.com/jw9730/tokengt">Github Implementation</a></li>
  <li>Reference
    <ul>
      <li><a href="https://arxiv.org/abs/2207.02505">Paper Link</a></li>
      <li>[1] Ying, C., Cai, T., Luo, S., Zheng, S., Ke, G., He, D., Shen, Y., &amp; Liu, T.-Y. (2021). <em>Do Transformers Really Perform Bad for Graph Representation?</em> http://arxiv.org/abs/2106.05234</li>
      <li>[2] Xu, K., Hu, W., Leskovec, J., &amp; Jegelka, S. (2018). <em>How Powerful are Graph Neural Networks?</em> http://arxiv.org/abs/1810.00826</li>
      <li>[3] Morris, C., Ritzert, M., Fey, M., Hamilton, W. L., Lenssen, J. E., Rattan, G., &amp; Grohe, M. (2019). <em>Weisfeiler and Leman Go Neural: Higher-order Graph Neural Networks</em>. www.aaai.org</li>
      <li>[4] Maron, H., Ben-Hamu, H., Shamir, N., &amp; Lipman, Y. (2018). <em>Invariant and Equivariant Graph Networks</em>. http://arxiv.org/abs/1812.09902</li>
      <li>[5] Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., &amp; Dahl, G. E. (2017). <em>Neural Message Passing for Quantum Chemistry</em>. http://arxiv.org/abs/1704.01212</li>
      <li>k-IGN : https://harryjo97.github.io/paper%20review/Invariant-and-Equivariant-Graph-Networks/</li>
    </ul>
  </li>
</ul>


    </div>



</article>
<script src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.5.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>







<hr class="shaded"/>

<footer>
            <div class="row">
                <div class="col-lg-12 footer">
               &copy;2023 Copyright 2021 Google LLC. All rights reserved. <br />
 Site last generated: Apr 26, 2023 <br />
<p><img src="images/company_logo.png" alt="Company logo"/></p>
                </div>
            </div>
</footer>






        </div>
    <!-- /.row -->
</div>
<!-- /.container -->
</div>
<!-- /#main -->
    </div>

</body>

<!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-66296557-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>





</html>


