<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>DSAILatKAIST.github.io</title>
        <description>Intended as a documentation theme based on Jekyll for technical writers documenting software and other technical products, this theme has all the elements you would need to handle multiple products with both multi-level sidebar navigation, tags, and other documentation features.</description>
        <link>http://localhost:4000/</link>
        <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
        <pubDate>Sun, 23 Apr 2023 14:35:18 +0900</pubDate>
        <lastBuildDate>Sun, 23 Apr 2023 14:35:18 +0900</lastBuildDate>
        <generator>Jekyll v3.9.2</generator>
        
        <item>
            <title>[CIKM 2020] Temporal and Heterogeneous Graph Neural Network for Financial Time Series Prediction</title>
            <description>&lt;h1 id=&quot;1-problem-definition&quot;&gt;&lt;strong&gt;1.	Problem Definition&lt;/strong&gt;&lt;/h1&gt;
&lt;p&gt;주식의 가격 움직임은 다양한 측면에 의해 영향을 받는다고 알려져 있다. 선행 연구에서는 가격 움직임을 예측하기 위해 기술적 지표, factor, 재무상태, 뉴스, sns 등을 input으로 구성하였다. 주식시장의 시그널을 stock fundamental과 기술적 지표를 사용해 의사결정을 내리거나 뉴스 기사와 관련된 기업 간의 연결고리를 구축하여 주가 움직임을 예측하는 등 선행 연구에서는 LSTM과 GRU 등을 활용하여 과거 정보를 학습하고 이를 다운스트림 예측 작업에 활용하였다. 하지만 기존 학습 방법은 각 주식의 시계열 정보를 독립적이고 동일하게 분포된 것으로 간주하고 각 분석을 독립적으로 수행한다. 이러한 과정은 각 주식 간의 내부관계를 무시할 밖에 없는 결과를 내고 이는 필연적으로 최적의 성능을 내지 못하는 결과를 초래한다.
&lt;br /&gt;
&amp;lt;/br&amp;gt;&lt;/p&gt;

&lt;h1 id=&quot;2-motivation&quot;&gt;&lt;strong&gt;2.	Motivation&lt;/strong&gt;&lt;/h1&gt;
&lt;p&gt;주식의 가격 변동은 그 자체의 과거 가격과 관련이 있을 뿐만 아니라 공급업체, 고객, 주주, 투자자, 연결된 기업 등과도 관련이 있다. 이러한 관계를 저장하고 표현하기 위해 knowledge graph를 활용하였고 최근에는 그래프 구조의 데이터를 효과적으로 학습할 수 있는 GNN이 방법론으로 제시되었다. 하지만 기존의 방식대로 구축된 그래프는 수작업으로 만든 라벨링과 NLP를 활용하였고 이는 기업간의 관계를 나타내기에 제한되며 많은 리소스 라벨링과 낮은 정확도가 낮은 어려움이 있다.&lt;/p&gt;

&lt;p&gt;실제 기업 관계는 시간에 따라 유동적으로 변화한다. 또한 기업간의 관계는 heterogenous하여 기업 간의 관계 유형은 다양하게 나타난다. 따라서 기존 방법으로는 실제로 기업 관계 그래프의 모든 정보를 표현할 수 없다. 이 논문에서는 이러한 문제를 해결하기 위해 실제 주식가격 시퀀스를 기반으로 relation graph를 구성하고 sequential features와 relational features를 함께 학습하여 주가를 예측을 하는 temporal and heterogenous graph neural network(THGNN) 방법론을 제시한다. 
&lt;br /&gt;
&amp;lt;/br&amp;gt;&lt;/p&gt;

&lt;h1 id=&quot;3-method&quot;&gt;&lt;strong&gt;3. Method&lt;/strong&gt;&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/hynacin121/ML_Paper_Review/blob/main/img/3MethodAlgorithm.png?raw=true&quot; /&gt;&lt;/p&gt;

&lt;p&gt;THGNN모델은 historical price sequence를 input으로 받고 probability of stock movement를 output으로 한다.&lt;/p&gt;

&lt;h3 id=&quot;a-stock-correlatiobn-graph-generation&quot;&gt;&lt;strong&gt;(a) Stock Correlatiobn Graph Generation&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;모델의 첫 파트는 주식간의 상관관계 그래프를 생성 하는 과정으로 각 거래일의 주식간의 동적관계를 구축한다. 주식간의 상관관계는 각 주식의 과거 주식데이터를 활용해 상관행렬을 계산하고, 행렬의 각 요소 값에 따라 기업간의 관계를 결정한다.&lt;/p&gt;

&lt;p&gt;기업간의 관계는 postive와 negative 두 가지로 분류한다. 이때 그래프의 노이즈를 제거하기 위해 correlation의 절댓값이 threshold보다 큰 값만 활용하고, correlation &amp;gt; threshold 일 때 positive, correlation &amp;lt; threshold 일 때 negative로 정의한다. 그래프의 edge는 앞서 설명한 relation으로 생성되고, 각 회사간의 heterogenous graph를 앞서 설명한 두개의 관계로 다음과 같이 표현한다. $G = (V, (E_{r_1}, E_{r_2}, \dots) \  \; r \in (pos, neg)$.
회사간의 관계는 동적관계이므로, 기업간의 관계 그래프를 temporal format으로 생성하였다. T 거래일의 그래프는 $ \tilde{G} = (\tilde{V}, (\tilde{E}&lt;em&gt;{pos},\tilde{E}&lt;/em&gt;{neg} ))$로 표기한다.
&lt;br /&gt;&amp;lt;/BR&amp;gt;&lt;/p&gt;

&lt;h3 id=&quot;b-historical-price-encoding&quot;&gt;&lt;strong&gt;(b) Historical Price Encoding&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Input인 t번째 거래일의 price sequence는 $X^t \in \R^{n \times T \times d_{feat}}$ 으로 정의한다. T는 t번째 거래일 이전의 거래일 숫자를 의미하고, $d_{feat}$ 은 과거 주식가격의 dimension을 의미한다. $X^t$는 linear transformation과 positional encoding 과정을 거친다.&lt;/p&gt;

\[\hat{H}^t = W_{in}X^t + b_{in} \\
H^t = \hat{H}^t + PE \\\]

\[\; PE(p,2i) = \sin(p/10000^{2i/d_{in}}) \\
PE(p,2i + 1) = \cos(p/10000^{2i/d_{in}})\]

&lt;p&gt;위 식에서 p는 거래일의 의미하고, i는 dimension, 그리고 $d_{in}$은 input feature의 dimension을 의미하고 $W_{in} \in \R^{d_{feat} \times d_{in}}$과 $b_{in} \in \R^{d_{in}}$은 학습 가능한 파라미터다.&lt;/p&gt;

\[Q_i^t =H^tW_i^Q, \; K_i^t = H^tW_i^K, V_i^t = H^tW_i^V\\
Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_{in}}})V\\
EncHead_i^t = Attention(Q_i^t, K_i^t, V_i^t)\\
H^t_{enc} = Concat(EncHead_1^t, \dots, EncHead^t_{h_{enc}})W_o\]

&lt;p&gt;앞서 Linear transformation과 PE 과정을 거친 후, Multi-head attentional transformer를 활용하여 주식의 각 input feature를 인코딩한다. $H^t$를 각각 $W_i^Q \in \R^{d_{in} \times d_{hidden}}, W_i^K \in \R^{d_{in} \times d_{hidden}}, W_i^V \in \R^{d_{in} \times d_{hidden}}$와 곱하여 Q(Query), K(Key), V(Value)를 계산한 후 softmax함수를 적용하여  $EncHead_i^T$를 구한다. concat은 Encoder head들을 concatenation하는 것을 의미한다. $W_o \in \R^{h_{enc}d_v \times d_{enc}} $는 output projection matrix로 concatenated matrix에 곱하여 input으로 들어올 때의 사이즈와 동일하게 유지되게 한다.&lt;/p&gt;

&lt;h3 id=&quot;c-temporal-graph-attention-mechanism&quot;&gt;&lt;strong&gt;(c) Temporal Graph Attention Mechanism&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Historical Price Encoding의 output인 $H^t_{enc}$와 Temporal relation Graph인 $\tilde{G}$를 활용하여 다음 단계인 temporal attention mechanism을 진행한다. $H^t_{enc}$의 모든 노드를 1차원으로 flatten하고 2단계의 temporal attention mechanism을 활용한다. 각각의 relationship인 $r \in (pos, neg)$에 대해 다음과 같은 연산을 진행한다.&lt;/p&gt;

\[\alpha^i_{u^t, v^t} = \frac{exp(LeakyReLU(a^T_{r,i}[h_{u_t} \vert h_{v_t}]))}{\sum_{k^t \in \Nu_r(v^t)}exp(LeakyReLU(a^T_{r,i}[h_{k^t} \vert h_{v_t}]))}\]

&lt;p&gt;$h_{u_t}$는 $H_{enc}^t$의 $u_t$번째 열을 의미하고, $a_{r,i} \in \R^{2Td_{enc}}$는 i번째 head의 r번째 relation에 대한 가중치 벡터를 의미한다. $\alpha^i_{u_t, v_t}$는 노드$u_t$에 대한 노드$v_t$의 중요도를 의미한다. LeakyReLU activation을 활용하여 계산된 Normalized Attention Score는 아래와 같이 i번째 노드의 neighbor importance를 결정하여 input 데이터를 재정의 한다.&lt;/p&gt;

\[TgaHead_i = \sum_{v^t \in \tilde{V}} \sigma(\sum_{u^t \in \Nu_r(v^t)}\alpha^i_{u^t, v^t}h_{u^t})\\
H^t_r = Concat(TgaHead_1, \dots, TgaHead_{h_{tga}})W_{o,r}\]

&lt;p&gt;$\sigma$ 는 activate function으로 sigmoid함수를 활용한다. $W_{o,r} \in \R^{h_{tga}Td_{enc} \times d_{att}}$ 는 output projection matrix를 의미한다.&lt;/p&gt;

&lt;h3 id=&quot;d-heterogeneous-graph-attention-mechanism&quot;&gt;&lt;strong&gt;(d) Heterogeneous Graph Attention Mechanism&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;앞선 과정을 통해 3개의 종류의 임베딩($H^t_{self}, H^t_{pos}, H^t_{neg}$)을 정의 하였다. $H^t_{self} = W_{self}H^t_{enc} + b_{self}$로 $H^t_{self}$로 부터 도출되었고, $H^t_{pos}, H^t_{neg}$는 앞선 Temporal Graph Attention Mechanism을 통해 도출되었다. $(\beta_{self}, \beta_{pos}, \beta_{neg})$ 는 3개의 임베딩을 input으로 하여 아래와 같은 과정을 통해 계산된다.&lt;/p&gt;

\[w_r  = \frac{1}{\vert \tilde{V} \vert} \sum_{v^t \in \tilde{V}}q^T \tanh(Wh_{v^t,r}+b) \\
\beta_r = \frac{exp(w_r)}{\sum_{r \in (self, pos, neg)}exp(w_r)} \\
Z^t = \sum_{r \in (self, pos, neg)} \beta_r  \cdot H^t_r\]

&lt;p&gt;3개의 임베딩을 MLP를 통해 각각 transform하고 heterogeneous attention vector인 &lt;strong&gt;q&lt;/strong&gt; 를 곱한 후 평균을 구해 $w_r$ 를 계산하였다. $w_r$ 을 활용하여 3가지 관계의 가중치인 ($\beta_r$)를 구할 수 있다. 마지막으로 가중치인 $\beta_r$ 을 활용하여 최종 임베딩인 $Z_t$ 를 구한다.&lt;/p&gt;

&lt;h3 id=&quot;e-optimzation-objectives&quot;&gt;&lt;strong&gt;(e) Optimzation Objectives&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;목적함수는 다음과 같은 과정을 통해 계산된다. 상위 100개와 하위 100개에 속하는 200개의 주식을 선택하고 해당 노드에 각각 1과 0으로 레이블을 지정한다. 그 후로 한 계층의 MLP를 분류기로 사용하여 라벨링된 노드의 분류결과를 얻는다. Binary cross-entropy를 사용하여 다음과 같이 목적함수 L을 구할 수 있다.&lt;/p&gt;

\[\hat{Y}_l = \sigma(WZ^t_l + b) \\
\mathcal L = \sum_{l \in \mathcal Y_t}[Y^t_l \log(\hat{Y}_l) + (1- Y_l^t)log(1- \hat{Y}_l)]\]

&lt;p&gt;$Y_l^t, Z_l^t$는 라벨링된 노드 $l$의 임베딩과 시그모이드 함수값을 의미한다. 계산을 위해 Adam Optimizer를 활용한다.&lt;/p&gt;

&lt;h1 id=&quot;4-experiment&quot;&gt;&lt;strong&gt;4.	Experiment&lt;/strong&gt;&lt;/h1&gt;

&lt;h2 id=&quot;experimental-settings&quot;&gt;&lt;strong&gt;Experimental Settings&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&quot;dataset&quot;&gt;&lt;strong&gt;Dataset&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;SP500과 CSI300의 2016년부터 2021년까지의 주식데이터를 활용하였다. 기업간의 관계 그래프 를 구성하기 위해 활용된 correlation matrix의 경우, 기준일로 부터 20 거래일 전까지의 주가를 활용하여 계산하였다.&lt;/p&gt;

&lt;h3 id=&quot;parameter-settings&quot;&gt;&lt;strong&gt;Parameter Settings&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;$\tilde{G}$의 경우 20 거래일동안의 기업간의 관계를 담고 있다. $d_{feat}$은 6개의 encoding layer로 구성되어 있고,  $d_{in}$과 $d_{enc}$는 모두 128개의 encoding layer로 구성되어 있다. Attention function에서 활용된 $d_{hidden}$의 경우 512개의 layer, $d_{v}$는 128개 $h_{enc}$는 8개로 구성되어 있다. Temporal graph attention layer에서는 $d_{att}$은 256개, $h_{tga}$는 4개, 그리고 heterogeneous graph attention layer의 $d_q$는 256개의 encoding layer로 구성되어 있다.&lt;/p&gt;

&lt;h3 id=&quot;compared-baselines&quot;&gt;&lt;strong&gt;Compared Baselines&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;Non-graph-based approach인 LSTM, GRU, Transformer, eLSTM과 Graph-based approach인 LSTM-GCN, LSTM-RGCN, TGC, MAN-SF, HATS, REST, AD-GAT가 비교군으로 구성되어 있다.&lt;/p&gt;

&lt;h3 id=&quot;evaluating-metrics&quot;&gt;&lt;strong&gt;Evaluating Metrics&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;ACC(prediction accuracy, 예측정확도), ARR(annual return rate, 연간 수익률), AV(annual volatility, 연간 변동성),  MDD(Maximum DrawDown), ASR(Annual Sharpe Ratio, $ARR/AV$), CR(Calmar Ratio, $ARR/\vert MDD \vert$), IR(Information Ratio) 총 7가지의 지표를 사용하여 기준선과 모델의 성능을 기록한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&amp;lt;/br&amp;gt;&lt;/p&gt;

&lt;h2 id=&quot;financial-prediction&quot;&gt;&lt;strong&gt;Financial Prediction&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/hynacin121/ML_Paper_Review/blob/main/img/4ResultTable.png?raw=true&quot; /&gt;
논문에서 활용된 모델인 THGNN의 경우 모든 평가지표에서 대부분의 Baseline보다 우수한 지표를 보임을 알 수 있다. 이를 통해 금융 시계열 예측에서 THGNN의 우수성을 입증하였다. 
&lt;br /&gt;&amp;lt;/br&amp;gt;&lt;/p&gt;

&lt;h2 id=&quot;ablation-study&quot;&gt;&lt;strong&gt;Ablation Study&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/hynacin121/ML_Paper_Review/blob/main/img/4AblationTable.png?raw=true&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ablation Study는 3에서 진행된 각 섹션들을 하나씩 제거해보며 성능을 평가하였다. 위에서부터 각각 Historical Price Encoding, Temporal Graph Attention, Heterogeneous Graph attention을 제거하였다. 모든 평가지표에서 어떠한 과정도 제거하지 않았을 때 가장 좋은 성능이 나옴을 알 수 있다. 
&lt;br /&gt;&amp;lt;/br&amp;gt;&lt;/p&gt;

&lt;h2 id=&quot;interpretability-of-graph-neural-network&quot;&gt;&lt;strong&gt;Interpretability of Graph Neural Network&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/hynacin121/ML_Paper_Review/blob/main/img/4GraphAttention.png?raw=true&quot; /&gt;&lt;/p&gt;

&lt;p&gt;모델의 해석가능성을 살펴보기 위해 모델 예측과정에서 그래프의 관심가중치를 추출하였다. Relational graph의 메시지 전달과정에서 모든 노드에 대한 attention weight를 계산하여 각각의 일일 수익률과 node degrees에 따라 attention weight의 평균을 구하고 시각화 하였다. Pos relationship의 경우, 노드의 degree가 높을 경우 attention weight가 높음을 보여준다. 즉 이웃이 많은 노드가 주변에 더 많은 메시지를 기여한다는 것을 보여준다. 또한 일일 수익률 변동이 큰 기업의 attention weight가 높음을 보였다. 이는 가격 변동성이 클수록 주변에 더 많은 정보를 제공한다는 것을 의미하며, 가격변동이 momentum spillover effect를 일으킨다는 것을 의미한다. 반면 Neg relationship는, 반대로 degree가 낮은 노드에서 높은 attention weight를 보인다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/hynacin121/ML_Paper_Review/blob/main/img/4ACC.png?raw=true&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이는 앞선 결과에 더 나아가 각 relation의 attention weights를 시각화하여 하나의 relation만 사용했을 때 성능을 보여준다. 기존과 같이 2개의 데이터셋을 사용해 학습시키고 하나의 relationship의 메시지를 input으로 활용하여 위와 같은 결과를 보였다. self와 pos가 neg보다 예측 성능이 우수함을 보였다. 또한 pos 메시지의 리소스가 예측 모델에 대한 기여도가 높음을 볼 수 있다. 그 이유는 비슷한 가격 움직임을 보이는 기업간의 영향력이 향후 가격 움직임을 예측하는데 상대적으로 유용하기 때문이다. Temporal graph attention layer가 각 노드와 가중치 간의 차이를 적절히 보여주고, Heterogeneous graph attention은 각 메시지의 기여도를 적절히 조정할 수 있음을 알 수 있다.&lt;/p&gt;

&lt;h1 id=&quot;5-conclusion&quot;&gt;&lt;strong&gt;5.	Conclusion&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;금융 시계열 예측을 위해 Temporal and Heterogeneous graph neural network model을 제시한다. 가장 효과적인 그래프 기반과 비그래프 기반 baseline과 비교하여 제시한 방법의 효과를 종합적으로 평가하였다. 또한 실제 투자 전략에서 THGNN이 우수한 성과를 보였다. 기업간의 관계를 Heteoreneous dynamic graph로 모델링 하고 GNN을 통해 금융 시계열 예측 모델을 개선하였다. 추후 연구에서는 예측 모델이 보다 정확한 training input graph 데이터를 얻을 수 있도로 corporate relation modeling을 개선할 예정이다.&lt;/p&gt;

</description>
            <pubDate>Thu, 20 Apr 2023 00:00:00 +0900</pubDate>
            <link>http://localhost:4000/Temporal_and_Heterogeneous_Graph_Neural_Network_for_Financial_Time_Series_Prediction.html</link>
            <guid isPermaLink="true">http://localhost:4000/Temporal_and_Heterogeneous_Graph_Neural_Network_for_Financial_Time_Series_Prediction.html</guid>
            
            <category>reviews</category>
            
            
        </item>
        
        <item>
            <title>[ICLR 2023] Temporal 2D-Variation Modeling for General Time Series Analysis2</title>
            <description>&lt;p&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;시계열 분석은 기상 예측 , 모니터링 데이터의 이상 감지 및 행동 인식을 위한 궤적 분 류 와 같은 다양한 실제 응용 분야에서 널리 사용되며, 이러한 넓은 실용적 가치 때문에 시계열 분석은 큰 관심을 받고 있다.&lt;/p&gt;

&lt;p&gt;일반적으로 언어나 비디오와 같은 다른 유형의 순차 데이터와 달리 시계열은 연속적으 로 기록되며 각 시간 지점은 일부 스칼라만 저장한다. 그러나 하나의 단일 시간 지점으 로는 보통 분석에 충분한 의미적 정보를 제공할 수 없기 때문에, 많은 경우 더 많은 정 보를 제공하기 위해 시계열의 내재적인 특성인 연속성, 주기성, 추세 등을 반영할 수 있 는 시간적 변화에 초점을 맞추고 있다. 그러나 실제 시계열의 변동은 항상 복잡한 시간 패턴을 포함하며, 여러 변동(상승, 하강, 변동 등)이 서로 섞이고 중첩되어 모델링을 매우 어렵게 만든다. 특히 딥러닝 커뮤니티 에서는 이런 복잡한 시계열 Data 의 시간적 변화 를 포착하기 위해, (RNN), (TCN), Attention 메커니즘 을 사용하여 모델링을 진행하고 있 으나, 각 모델들의 한계로 인해 효율적인 모델링을 진행하는데 한계가 존재한다.&lt;/p&gt;

&lt;p&gt;본 논문에서는, 복잡한 시간적 변동을 해결하기 위해 시계열 데이터를 다양한 주기성에 서 분석한다. 실제 현실세계의 시계열 데이터는 일일, 주간, 연간 같은 주기성을 관찰할 수 있으며, 이런 주기성이 서로 상호작용하여 변동 모델링을 어렵게 한다.&lt;/p&gt;

&lt;p&gt;` `이  관점에서  해당  논문의  핵심은  이런  시계열  데이터를  주기내에서의  변동성 (intraperiod-variation)  과  주기  간  변동성(interperiod-variation)  으로  각각  나눠,  기존 Temporal Variation  을  Intraperiod - , Interperiod – variation  으로  확장하여  Multi – periodicity 특성을 반영하는 것이다.&lt;/p&gt;

&lt;p&gt;이런 Multi – periodicity 특성을 반영하기 위해, 1D 시계열을 2D 공간으로 확장한다. 구 체적으로, 그림 1처럼, 1D 시계열을 각 열이 주기 내 시간 점을 포함하고 각 행이 서로 다른 주기에서 동일한 위상의 시간 점을 포함하는 2D 텐서로 재구성 한다. 따라서 1D&lt;/p&gt;

&lt;p&gt;시계열을 2D 텐서 집합으로 변환하여 원래의 1D 공간에서 표현 능력 병목 현상을 깨고 2D 공간에서 intra-period- 및 inter-period-variations를 효과적으로 통합하여 시간적 2D- variations을 얻을 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;Aspose.Words.fe9a8a29-d51a-42e5-b229-da4476aaa176.001.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;` `또한 기술적으로, 위와 같은 동기에 기반하여, 이전 백본(backbone)을 넘어서 시계열 분 석을 위한 새로운 태스크-일반 모델로 TimesNet을 제안한다.&lt;/p&gt;

&lt;p&gt;구체적으로, TimesBlock은 학습된 주기를 기반으로 1D 시계열을, 2D 텐서 SET 으로 변 환하고, 파라미터 효율적인 inception block 을 통해 2D 공간에서 intra-period 및 inter- period 변화를 포착할 수 있다. 실험적으로, TimesNet 은 단기 및 장기 예측, 대체, 분류 및 이상 탐지를 포함한 5 가지 주요 분석 작업에서 일관된 최고 성능을 달성했다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;RELATED WORK&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;` `시계열 분석의 주요 문제인 시간적 변화 모델링은, ARIMA(Anderson &amp;amp; Kendall, 1976), Holt-Winter(Hyndman &amp;amp; Athanasopoulos, 2018) 및 Prophet(Taylor &amp;amp; Letham, 2018) 같은 고전적 모델 기반의 연구가 진행되었으나, 시계열의 변화는 일반적으로 미리 정의된 패 턴으로는 충분히 설명하기 어렵기 때문에 실용성은 제한되는 한계가 존재했다.&lt;/p&gt;

&lt;p&gt;` `최근 에는 MLP, TCN, RNN 기반의 다양한 딥 모델들이 시계열 모델링에 적용되 왔다. 우선 MLP 기반의 방법들 (Oreshkin et al., 2019; Challu et al., 2022; Zeng et al., 2023; Zhang et al., 2022)은 MLP을 시간 축으로 사용하며, 시간 의존성을 MLP 레이어의 고정 된 파라미터로 인코딩 한다. TCN 기반의 방법 (2019)은 시간 축을 따라 이동하는 컨볼루 션 커널로 시간 변화를 캡쳐하며, RNN 기반의 방법 (Hochreiter &amp;amp; Schmidhuber, 1997; Lai et al., 2018; Gu et al., 2022)은 재귀 구조를 활용하며, 시간 단계 간 상태 전이를 통해 시간  변화를  캡쳐한다.  특히  최근  딥러닝  분야에서  전반적으로  높은  성능을  보이는 Transformer는 시계열 예측에서도 큰 성능을 보여주고 있으며 (Zhou et al., 2021; Liu et al., 2021a; Wu et al., 2021; Zhou et al., 2022), 어텐션 메커니즘을 사용하여 시간 포인트 간의 시간적 종속성을 파악할 수 있다. 특히, Wu et al.은 학습된 주기를 기반으로 자기&lt;/p&gt;

&lt;p&gt;상관 메커니즘을 갖춘 Autoformer를 제시 했으며. 그 후, FEDformer (Zhou et al., 2022)는 계절성-추세 분해를 강화하고 주파수 도메인 내에서 sparse 어텐션을 제시했다. 그러나 본 논문의 경우 이전 방법과 달리, 시계열의 다중 주기성을 탐색하여 복잡한 시간적 패 턴을 해결하고, 이미 인공지능 컴퓨터 비전에서 잘 알려진 백본을 사용하여 처음으로 2D 공간에서 시간적 2D-변화를 포착한다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3 TIMESNET&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;` `해당 논문을 이해하기 위해서 우선 Multi periodicity 를 이해해야 한다. Multi periodicity 는 위에서 간단히 언급했지만, 해당 논문은 Intraperiod Variation과 Intreperiod Variation 을 구분하여 분석하였다. 우선 Intraperiod Variation 은 Period 내에서 발생하는 Variation 으로  Short Term Temporal pattern  을  반영하며,  일반적으로  1D Time Series data  를 Input 으로 모델링 진행시 고려하는 Variation 이다. 또한 추후 2D Tensor 로 변환할 때 각 Column 으로 표현되어 같은 Period 내 time Point 간의 Variation 을 의미한다. 반면 Interperiod Variation  은  서로  다른  Period  에서  발생하는  Variation  으로  Long Term Temporal Pattern  을  반영한다.  이는  2D Tensor  로  변환시  각  row  로  표현되어  다른 Period 에 존재하지만, 같은 Phase 를 가지는 time points 간 Temporal Variation 을 의미 한다. 이 논문은 이 둘을 조합하여 사용 하는 것이 key idea 이다.&lt;/p&gt;

&lt;p&gt;여기서  Interperiod  의  기준을  나누기  위해서  그림  1과  같이  해당  Data  에서  다양한 Period  를  얻어야  한다.  이를  위하해서  해당  논문은  아래  그림과  식처럼  Fast Fourier Transform 을 사용한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;Aspose.Words.fe9a8a29-d51a-42e5-b229-da4476aaa176.002.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;Aspose.Words.fe9a8a29-d51a-42e5-b229-da4476aaa176.003.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;FFT 를 적용해서 Sequence 에 대한 Amplitude 및 Frequency 를 도출 하고, Frequency 중 Amplitude 값이 가장 높은 Top K 개의 Frequency 를 선정하여, Frequency 에 상응하 는 Period 를 구한다. 여기서 top K 개의 Frequency 를 선정하는 이유는 Frequency 영역 의 sparse 함을 고려했을때, 으미 없는 high Frequency 에 의한 Nosie 를 피하기 위한것 이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;Aspose.Words.fe9a8a29-d51a-42e5-b229-da4476aaa176.004.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;여기서 선정된 Top K개의 Frequency 에 해당하는 Period 는 위 수식에서 pi 로 설정한다. 이렇게 구해진 Frequency {f1, …, fk} 와Period {p1, …, pk} 를 이용하여 1D Time series data &lt;img src=&quot;Aspose.Words.fe9a8a29-d51a-42e5-b229-da4476aaa176.005.png&quot; alt=&quot;&quot; /&gt; 를 2D Tensor 로 변환 한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;Aspose.Words.fe9a8a29-d51a-42e5-b229-da4476aaa176.006.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;여기서 Padding 은 2D tensor 로 재구성 하기 위해 시계열을 시간 축을 따라 0으로 패 딩하는 작업인데, 아래 수식 에서 T/fi 값을 맞춰주기 위해 진행한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;Aspose.Words.fe9a8a29-d51a-42e5-b229-da4476aaa176.007.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(pi와 fi는 각각 i번째 변환된 2D 텐서의 행과 열의 수를 나타낸다.)&lt;/p&gt;

&lt;p&gt;이와 같은 방식으로 선택된 주파수와 추정된 주기에 기반하여 k개의 다른 2D 변동을 나 타내는 2D 텐서 집합 &lt;img src=&quot;Aspose.Words.fe9a8a29-d51a-42e5-b229-da4476aaa176.008.png&quot; alt=&quot;&quot; /&gt; 을 얻게 된다.&lt;/p&gt;

&lt;p&gt;이러한 변환은 변환된 2D 텐서에 두 가지 유형의 지역성(locality)을 가져다 준다는 점도 주목할  만하다.  즉,  연속된  시간점(열, intra-period variation)과  인접한  주기(행, inter- period variation) 사이의 지역성이다. 따라서, 시계열 2D-변동성은 2D 커널에 의해 쉽게&lt;/p&gt;

&lt;p&gt;처리될 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3.2 TIMESBLOCK&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;Aspose.Words.fe9a8a29-d51a-42e5-b229-da4476aaa176.009.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Capturing temporal 2D-variation&lt;/p&gt;

&lt;p&gt;위 그림처럼 Reshape 된 결과를 통해 pi, fi 에 대한 Representation 을 추출한다. 여기서&lt;/p&gt;

&lt;p&gt;input  은  &lt;img src=&quot;Aspose.Words.fe9a8a29-d51a-42e5-b229-da4476aaa176.010.png&quot; alt=&quot;&quot; /&gt; 이고  output  은  &lt;img src=&quot;Aspose.Words.fe9a8a29-d51a-42e5-b229-da4476aaa176.011.png&quot; alt=&quot;&quot; /&gt; 이다.  또한  위의  그림처럼 Reshape Back 이라는 과정을 통해 Convolution 과정을 통해 추출한 Representation 을 다시 기존 형태로 Reshape 을 하게 된다.&lt;/p&gt;

&lt;p&gt;마지막으로 아래 식처럼 Adaptive Aggregation 을 수행하게 되는데,&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;Aspose.Words.fe9a8a29-d51a-42e5-b229-da4476aaa176.012.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이는 이전 과정을 통해 만들어진 1D time series data 를 Aggregation 하는 과정이다. Amplitude A 는 선택된 Top K 개의 Frequency 와 Period 간 상대적 중요도를 반영할 수 있으므로, Transform 된 Tensor 의 중요도 역시 반영할 수 있다. 이로부터 처음 FFT 과정 을 통해 구해진 Amplitude 값을 바탕으로 Aggregation 하게 된다. 이로부터 나온 결과 는  서로  다른  Period  간  Interperiod,  Intraperiod  variation을  고려한  Temporal  2D Variation 을 동시에 캡쳐한 결과를 얻을 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;4 EXPERIMENTS&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;Aspose.Words.fe9a8a29-d51a-42e5-b229-da4476aaa176.013.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위와 같은 setting 을 통해 실험을 진행하였고&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;Aspose.Words.fe9a8a29-d51a-42e5-b229-da4476aaa176.014.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림과 같이 time series 의 5개 Mainstream Task 에서 다른 모델들 대비 모든 방면에 서 SOTA 를 달성하였다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5 CONCLUSION AND FUTURE WORK&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;해당 논문의 novelty 는 다음과 같이 3가지로 요약된다.&lt;/p&gt;

&lt;p&gt;-다양한 주기성과 주기별 내/외부적 상호작용에 대한 동기부여를 바탕으로, 우리는 시간 적 변화 모델링을 위한 모듈화된 방법을 발견했습니다. 1차원 시계열을 2차원 공간으로 변환하여, 주기 내/외부적인 변화를 동시에 나타낼 수 있다.&lt;/p&gt;

&lt;p&gt;-TimesNet은  TimesBlock을  사용하여  여러  주기를  찾고, parameter-efficient inception block으로 변환된 2D tensor에서 temporal 2D-variations을 포착하기 위한 새로운 task- general 모델로 제안된다.&lt;/p&gt;

&lt;p&gt;-TimesNet은 단기 및 장기 예측, 대치, 분류 및 이상 탐지를 포함한 다섯 가지의 주요&lt;/p&gt;

&lt;p&gt;시계열 분석 작업에서 일관된 최고 성능을 달성했다.&lt;/p&gt;
</description>
            <pubDate>Thu, 20 Apr 2023 00:00:00 +0900</pubDate>
            <link>http://localhost:4000/Temporal_2D_Variation_Modeling_for_General_Time_Series_Analysis2.html</link>
            <guid isPermaLink="true">http://localhost:4000/Temporal_2D_Variation_Modeling_for_General_Time_Series_Analysis2.html</guid>
            
            <category>reviews</category>
            
            
        </item>
        
        <item>
            <title>[ICLR 2023] Temporal 2D-Variation Modeling for General Time Series Analysis</title>
            <description>&lt;h1 id=&quot;temporal-2d-variation-modeling-for-general-time-series-analysis&quot;&gt;Temporal 2D-Variation Modeling for General Time Series Analysis&lt;/h1&gt;

&lt;h1 id=&quot;1-problem-definition&quot;&gt;1. Problem Definition&lt;/h1&gt;
&lt;p&gt;시계열 분석은 일기 예보, 이상치 탐지, 행동 인식 등 많은 응용 분야에서 사용되고 있습니다. 시계열 데이터는 언어나 영상과 같은 순차적 데이터와 달리 연속적으로 기록되며, 각 시점은 일부 값만 저장합니다. 하나의 단일 시점은 충분한 의미를 갖지 않기 때문에, 연속성, 주기성, 추세 등과 같은 시계열의 고유산 속성을 반영하도록 시간적 변화에 초점을 맞추고 있습니다.&lt;/p&gt;

&lt;h1 id=&quot;2-motivation&quot;&gt;2. Motivation&lt;/h1&gt;
&lt;p&gt;그러나 실제 시계열 데이터는 여러 변화(상승, 하락, 변동 등)가 복합적으로 섞여 복잡한 패턴을 포함하기 때문에 모델링이 어렵습니다.&lt;/p&gt;

&lt;p&gt;이 논문은, 다중 주기성(multi periodicity)라는 관점에서 시계열 데이터를 분석하는 방법을 다룹니다. 날씨 관측을 예를 들면, 기온은 하루에도 주기성을 갖지만 주간, 월간, 분기간, 연간 주기성도 존재합니다.&lt;/p&gt;

&lt;p&gt;두번째로, 각 주기 내에서 시계열 데이터는 인접한 영역 뿐만 아니라 인접한 주기의 변동과도 상관관계가 있다는 사실을 이용합니다. 저자들은 이를 각각 기간 내 변동(intraperiod-variation)과 기간 간 변동(interperiod-variation)이라 부릅니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;기간 내 변동(intraperiod-variation): 기간 내의 짧은 주기성을 파악함.&lt;/p&gt;

  &lt;p&gt;기간 간 변동(interperiod-variation): 기간 간의 장기 추세를 파악함.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/ahj1592/CourseMaterials/blob/main/DS503/Paper%20Review/images/TimesNet_multiperiodicy.png?raw=true&quot; alt=&quot;Multi-Periodicity&quot; /&gt;&lt;/p&gt;

&lt;p&gt;고전적인 방법론은 시간적 변동이 미리 정의된 패턴을 따른다고 가정합니다. 이런 방법론이 적용된 모델은 ARIMA, Holt-Winter, Prophet이 있습니다. 그러나 실제 시계열의 변화는 너무 복잡하여 이러한 사전 정의된 패턴으로 다루기 어렵기에 실제 적용은 매우 제한적입니다.&lt;/p&gt;

&lt;p&gt;최근에는딥러닝을 이용한 방법론이 제안되었고, 크게 MLP, RNN, TCN 기반 모델이 있습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;MLP-based: 시간차원을 고정된 layer로 인코딩&lt;/li&gt;
  &lt;li&gt;TCN-based: 시간적 변화를 convolution-kernel을 이용하여 포착&lt;/li&gt;
  &lt;li&gt;RNN-based: time step 당 상태 전환을 통해 시간적 변화를 포착&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;그러나 딥러닝 모델 역시 주기성에 의해 파생되는 시잔적 변동을 고려하지 않습니다.&lt;/p&gt;

&lt;p&gt;다양한 분야에서 좋은 성능을 보이는 Transformer 역시 시계열에서도 성능이 좋습니다. attention-mechanism을 이용하여 시점간의 시간적 의존성을 발견할 수 있습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Autoformer: auto-correlation을 이용하여 시간적 의존성을 포착하고&lt;/li&gt;
  &lt;li&gt;FEDformer는 계절성-추세 분해를 이용하여 frequency 영역에서 attention을 포착합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이 논문은 이전 연구와 비교하여 3가지 Contribution이 있습니다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Multi-periodicity를 포착하기 위해 1D time series를 2D tensor로 변환하여, intraperiod와 interperiod-variation 모두 포착&lt;/li&gt;
  &lt;li&gt;TimesNet 모델 아키텍처 제안. 이때 parameter-efficient한 inception block이 적용된 TimesBlock 모듈 이용&lt;/li&gt;
  &lt;li&gt;Foundation model로써, 5가지 주요 task에서 SOTA 달성 및 시각화 제공&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;3-method&quot;&gt;3. Method&lt;/h1&gt;
&lt;p&gt;저자들이 제안한 TimesNet은 크게 2단계로 나누어 학습합니다. 첫번째 단계는 푸리에 변환을 이용하여 multi-periodicity를 포착하고, 두번째 단계는 앞서 얻은 period마다 2D-tensor로 변환하여 2D-variation을 포착합니다.&lt;/p&gt;

&lt;h2 id=&quot;31-fft-analysis&quot;&gt;3.1 FFT Analysis&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;$T$: 시계열 데이터의 길이&lt;/li&gt;
  &lt;li&gt;$C$: 시계열 채널 수. univariate이면 $C = 1$이다&lt;/li&gt;
  &lt;li&gt;$\mathbf{X}_ {\text{1D}} \in \mathbb{R}^{T \times C}$: 전체 시계열 데이터&lt;/li&gt;
  &lt;li&gt;$\text{FFT}(\cdot)$는 고속 푸리에 변환으로 주파수 $f_i$를 찾는다&lt;/li&gt;
  &lt;li&gt;$\text{Amp}(\cdot)$: 주파수 $f_i$의 진폭을 찾는 함수&lt;/li&gt;
  &lt;li&gt;$\text{Avg}(\cdot)$: $C$차원 시계열 데이터에 대하여 진폭의 평균 계산&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;아래 일련의 과정을 거쳐 강도(indensity) $\mathbf{A}$를 얻습니다.&lt;/p&gt;

\[\mathbf{A} = \text{Avg}\Bigl( \text{Amp}(\text{FFT}(\mathbf{X}_ {\text{1D}})) \Bigr), \quad \mathbf{A} \in \mathbb{R}^T\]

&lt;p&gt;&lt;img src=&quot;https://github.com/ahj1592/CourseMaterials/blob/main/DS503/Paper%20Review/images/TimesNet_FFT.png?raw=true&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이때 $\mathbf{A}_ j$는 주파수가 $j$(주기가 $\lceil T/j \rceil$이다.)의 intensity가 된다. 주파수 영역에서 의미없는 고주파는 noise이므로 이를 제거하기 위해 top-$k$의 진폭만 사용하기로 합니다. 
\(\{ f_1, \cdots, f_k\} = \underset{f_* \in \{1, \cdots , [\frac{T}{2}]\}}{\text{argTopK}(\mathbf{A})}, \quad p_i = \Biggl\lceil\cfrac{T}{f_i} \Biggr\rceil, \quad i \in \{ 1, \cdots, k \}\)&lt;/p&gt;

&lt;p&gt;위 과정을 요약하면, $\mathbf{X}_ {\text{1D}}$로부터 FFT를 이용하여 $k$개의 유의미한 진폭($\mathbf{A}$), 주파수($f_i$), 주기($p_i$)를 얻습니다.
\(\mathbf{A}, \{f_1, \cdots, f_k\}, \{p_1, \cdots, p_k\} = \text{Period}(\mathbf{X}_ {\text{1D}})\)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/ahj1592/CourseMaterials/blob/main/DS503/Paper%20Review/images/TimesNet_convert2D.png?raw=true&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;32-reshape-1d-time-series-to-2d-tensors&quot;&gt;3.2 Reshape 1D time series to 2D tensors&lt;/h2&gt;
&lt;p&gt;FFT로 얻은 $f$와 $p$를 이용하여 $\mathbf{X}_ {\text{1D}}$로부터 $k$개의 2D-tensor $\mathbf{X}_ {\text{2D}}$ 를 얻을 수 있습니다. 이때 $\text{Reshape}$ 결과가 $p_i \times f_i$ 모양이 되도록 zero-padding $\text{Padding}(\cdot)$이 필요합니다.&lt;/p&gt;

&lt;p&gt;$\mathbf{X}_ {\text{2D}}^i = \underset{p_i, f_i}{\text{Reshape}}(\text{Padding}(\mathbf{X}_ {\text{1D}})), \quad i \in { 1, \cdots, k } $&lt;/p&gt;

&lt;h2 id=&quot;33-timesblock&quot;&gt;3.3 TimesBlock&lt;/h2&gt;
&lt;p&gt;TimesBlock 구조는 computer vision에서 자주 사용되는 ResNet의 residual way를 적용하였다. 먼저 raw data $\mathbf{X}_ {\text{1D}} \in \mathbf{R}^{T \times C}$를 모델 차원에 맞게 임베딩하여 $\mathbf{X}_ {\text{1D}}^0 \in \mathbb{R}^{T \times d_{\text{model}}}$를 얻게됩니다. 
\(\mathbf{X}_ {\text{1D}}^0 = \text{Embed}(\mathbf{X}_ {\text{1D}})\)
그 이후 $l$ 번째 layer마다 deep feature $\mathbf{X}_ {\text{1D}}^{l}$를 구한다.
\(\mathbf{X}_ {\text{1D}}^l = \text{TimesBlock}(\mathbf{X}_ {\text{1D}}^{l-1}) + \mathbf{X}_ {\text{1D}}^{l-1}\)
TimesBlock은 크게 2가지 역할을 수행합니다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;2D-variation 포착&lt;/li&gt;
  &lt;li&gt;Adaptively aggregating representations&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Capturing temporal 2D-variations&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;TimesNet은 $\text{Reshape}(\cdot)$로 변환한 2D-tensor를 multi-scale 2D kernel로 학습합니다. 이때 다양한 vision backbone을 이용할 수 있는데, 저자들은 parameter-efficient한 inception block을 사용했습니다. $\text{Inception}(\cdot)$을 통해 표현된 $\widehat{\mathbf{X}}_ {\text{2D}}^{l, i}$은 다시 1D로 reshape하고 길이 $T$를 보존하도록 $\text{Trunc}(\cdot)$로 패딩을 제거합니다.&lt;/p&gt;

\[\begin{align*}
\mathbf{A}^{l-1}, \{ f_1, \cdots, f_k \}, \{ p_1, \cdots, p_k \} &amp;amp;= \text{Period}(\mathbf{X}_ {\text{1D}}^{l-1}) \\
\mathbf{X}_ {\text{2D}}^i &amp;amp;= \underset{p_i, f_i}{\text{Reshape}}(\text{Padding}(\mathbf{X}_ {\text{1D}})), \quad i \in \{1, \cdots, k\} \\
\widehat{\mathbf{X}}_ {\text{2D}}^{l, i} &amp;amp;= \text{Inception}(\mathbf{X}_ {\text{2D}}^{l, i}), \quad i \in \{1, \cdots, k\} \\
\widehat{\mathbf{X}}_ {\text{1D}}^{l, i}&amp;amp; = \text{Trunc}(\underset{1, \  (p_i \times f_i)}{\text{Reshape}}(\widehat{\mathbf{X}}_ {\text{2D}}^{l, i})), \quad i \in \{1, \cdots, k \} \\
\end{align*}\]

&lt;p&gt;각 $l$번째 layer를 통과한 후 $k$개의 1D-representation ${\widehat{\mathbf{X}}_ {\text{1D}}^{l, 1}, \cdots, \widehat{\mathbf{X}}_ {\text{1D}}^{l, k}}$을 얻습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/ahj1592/CourseMaterials/blob/main/DS503/Paper%20Review/images/TimesNet_TimesBlock_1.png?raw=true&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Adaptive aggregateion&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Autoformer 모델이 제안된 논문에서, Auto-Correlation은 진폭 $\mathbf{A}$는 선택된 주파수와 주기 $f, p$의 상대적 중요성을 반영한다는 사실을 알아냈습니다. 따라서 진폭을 기반으로 1D-representation을 집계합니다.
\(\widehat{\mathbf{A}}_ {f_1}^{l-1}, \cdots, \widehat{\mathbf{A}}_ {f_k}^{l-1} = \text{Softmax}\left(\mathbf{A}_ {f_1}^{l-1}, \cdots, \mathbf{A}_ {f_k}^{l-1} \right)\)
\(\mathbf{X}_ {\text{1D}}^l = \sum_{i=1}^{k} \widehat{\mathbf{A}}_ {f_i}^{l-1} \times \widehat{\mathbf{X}}_ {\text{1D}}^{l, i}\)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/ahj1592/CourseMaterials/blob/main/DS503/Paper%20Review/images/TimesNet_TimesBlock_2.png?raw=true&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Generality in 2D vision backbones&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;저자들은 다양한 computer vision backbone인 ResNet, ResNeXt, ConvNeXt 등을 적용했습니다. 일반적으로 더 좋은 2D backbone일 수록 더 좋은 결과를 얻었습니다. 저자들은 성능과 효율성을 모두 고려하여 inception block을 선택했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/ahj1592/CourseMaterials/blob/main/DS503/Paper%20Review/images/TimesNet_cmp_backbones.png?raw=true&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;4-experiment&quot;&gt;4. Experiment&lt;/h1&gt;
&lt;p&gt;저자들은 시계열 분석에서 자주 사용되는 5가지 주제에 대하여 실험을 진행했습니다. 아래 표는 5개 task에 대하여 사용된 데이터셋, 평가지표 그리고 시계열 데이터 길이를 나타낸 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/ahj1592/CourseMaterials/blob/main/DS503/Paper%20Review/images/TimesNet_exp_benchmarks.png?raw=true&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;41-main-results&quot;&gt;4.1 Main Results&lt;/h2&gt;
&lt;p&gt;TimesNet은 장기 예측, 단기 예측, 결측치 보강, 분류, 이상치 탐지 5개의 영역에서 모두 SOTA를 달성했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/ahj1592/CourseMaterials/blob/main/DS503/Paper%20Review/images/TimesNet_5tasks_SOTA.png?raw=true&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;42-shortlong-term-forecasting&quot;&gt;4.2 Short/Long-term Forecasting&lt;/h2&gt;
&lt;p&gt;TimesNet 장기 예측과 단기 예측 모두 좋은 성능을 보였습니다. 특히 장기 예측의 경우 80%의 데이터셋에서 SOTA를 달성했습니다. 특히 단기 예측에 사용된 M4 데이터셋의 경우 다양한 출처에서 데이터가 수집되어 시간적 변동이 큼에도 불구하고 다른 모델들 보다 성능이 좋습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/ahj1592/CourseMaterials/blob/main/DS503/Paper%20Review/images/TimesNet_longterm.png?raw=true&quot; alt=&quot;image&quot; /&gt;
&lt;img src=&quot;https://github.com/ahj1592/CourseMaterials/blob/main/DS503/Paper%20Review/images/TimesNet_shortterm.png?raw=true&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;43-imputation&quot;&gt;4.3 Imputation&lt;/h2&gt;
&lt;p&gt;결측치 때문에, 모델은 불규칙하고 불완전한 데이터 속에서 시간적 패턴을 찾아야 하기 때문에 어려운 문제입니다. 그럼에도 불구하고 TimesNet은 SOTA를 달성하여 극단적으로 복잡한 시계열 데이터에서 시간적 변동을 잘 포착한다는 것을 의미합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/ahj1592/CourseMaterials/blob/main/DS503/Paper%20Review/images/TimesNet_imputation.png?raw=true&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;44-classification&quot;&gt;4.4 Classification&lt;/h2&gt;
&lt;p&gt;시계열 데이터의 분류는 인지 및 의료 진단에 사용됩니다. 저자들은 UEA Time Series Classification Archive에서 행동, 동작 및 음성 인식, 심장 박동 모니터링을 통한 의료 진단 등의 실제 작업이 포함된 다변량 데이터셋 10개를 선택했습니다. 그리고 이런 데이터셋의 표준 데이터 전처리를 한 후 실험하였습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/ahj1592/CourseMaterials/blob/main/DS503/Paper%20Review/images/TimesNet_clf.png?raw=true&quot; alt=&quot;image&quot; /&gt;
결과 역시 TimesNet은 SOTA를 달성하였습니다. 주목할 점은 장단기 예측에서 성능이 좋았던 MLP-based 모델들은 분류에서는 성능이 좋지 않다는 것입니다. 이는 TimesNet이 보다 더 높은 수준의 정보를 표현하기 때문에 계층 표현이 요구되는 분류 문제에서 성능이 좋다는 것을 의미합니다.&lt;/p&gt;

&lt;h2 id=&quot;45-anomaly-detection&quot;&gt;4.5 Anomaly Detection&lt;/h2&gt;
&lt;p&gt;이상치 탐지에서도 TimesNet은 SOTA를 달성했습니다. 이상치 탐지는 이상한 시간적 변동을 찾는 것이 요구되지만, Transformer는 attention-mechanism 특성상 정상 데이터가 영향을 많이 받기 때문에 성능이 그다지 높지 않았습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/ahj1592/CourseMaterials/blob/main/DS503/Paper%20Review/images/TimesNet_anomalydet.png?raw=true&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;46-model-analysis&quot;&gt;4.6 Model Analysis&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Representation analysis&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;TimesNet은 예측과 이상치 탐지에서 CKA 유사도가 높고, 결측치 보강과 분류에서 CKA 유사도가 낮습니다. CKA 유사도가 낮다는 것은 각 layer끼리 구별된다는 뜻이고 곧 계층적 표현(hierarchical representation)을 의미합니다. 이는 TimesNet이 imputation과 classification에서 성능이 높은 이유를 설명할 수 있습니다.
반면에, FEDformer는 계층적 표현 학습에 실패하여 결측치 보강과 분류 작업에서 성능이 좋지 않음이 설명됩니다.
&lt;img src=&quot;https://github.com/ahj1592/CourseMaterials/blob/main/DS503/Paper%20Review/images/TimesNet_CKA_sim.png?raw=true&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Temporal 2D-variations&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;기간 간 변동은 시계열의 장기 추세를 나타낼 수 있습니다.&lt;/li&gt;
  &lt;li&gt;명확한 주기가 없는 시계열의 경우에도 2D-tensor는 여전히 유용합니다. Exchange 데이터셋은 명확한 주기가 없지만 2D-tensor에서 장기 추세를 파악할 수 있습니다.&lt;/li&gt;
  &lt;li&gt;각 열(기간 내 변동)의 인접한 값은 가까운 시점의 지역성을 나타냅니다.&lt;/li&gt;
  &lt;li&gt;각 행(기간 간 변동)의 인접한 값은 기간 끼리의 지역성을 나타냅니다.&lt;/li&gt;
  &lt;li&gt;이러한 지역성은 표현 학습에 2D-kernel을 이용하는 동기가 됩니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2D-tensor를 시각화면 아래 그림과 같습니다.
&lt;img src=&quot;https://github.com/ahj1592/CourseMaterials/blob/main/DS503/Paper%20Review/images/TimesNet_temporal_2D_vars.png?raw=true&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;5-conclusion&quot;&gt;5. Conclusion&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;TimesNet은 시계열 분석 영역에서 task-general foundation model입니다.&lt;/li&gt;
  &lt;li&gt;다중 주기성을 이용하여 TimesNet은 주기내 변화와 주기간 변화 모두 포착합니다.&lt;/li&gt;
  &lt;li&gt;다양한 데이터셋 실험에서 TiemsNet은 5가지 task에 SOTA를 달성했습니다.&lt;/li&gt;
&lt;/ul&gt;
</description>
            <pubDate>Thu, 20 Apr 2023 00:00:00 +0900</pubDate>
            <link>http://localhost:4000/Temporal_2D_Variation_Modeling_for_General_Time_Series_Analysis.html</link>
            <guid isPermaLink="true">http://localhost:4000/Temporal_2D_Variation_Modeling_for_General_Time_Series_Analysis.html</guid>
            
            <category>reviews</category>
            
            
        </item>
        
        <item>
            <title>[AAAI 2021] TabNet: Attentive Interpretable Tabular Learning</title>
            <description>&lt;h1 id=&quot;tabnet-attentive-interpretable-tabular-learning&quot;&gt;TabNet: Attentive Interpretable Tabular Learning&lt;/h1&gt;

&lt;p&gt;본 논문의 저자는 &lt;strong&gt;트리 기반 앙상블 모델&lt;/strong&gt;들이 딥러닝에 비해 &lt;strong&gt;정형 데이터&lt;/strong&gt;에서 학습에 보다 논리적이고 합리적인 접근 방법이라고 소개합니다.   일반적으로 관측되는 정형 데이터는 대략적인 초평면(hyperplane) 경계를 지니고 있는 매니폴드(manifolds)를 가지고 있으며 이 공간에서는 트리 기반 앙상블 모델의 결정 방식이 이해(representation)하는데 더 강점을 지니고 있기 때문입니다.   &lt;br /&gt;
 본 논문에서 제안한 TabNet은 &lt;strong&gt;decision tree-based gradient boosting&lt;/strong&gt;의 장점을 살린 인공신경망 아키텍쳐이며, &lt;strong&gt;feature engineering&lt;/strong&gt;과 &lt;strong&gt;selection&lt;/strong&gt;까지 함께 활용할 수 있는 장점이 있습니다. 또한 &lt;strong&gt;해석가능한(interpretability)&lt;/strong&gt;, 설명가능한 XAI(eXplainable Artificial Intelligence)라는 점에서 큰 장점이 있습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;두괄식으로 &lt;strong&gt;TabNet의 컨셉&lt;/strong&gt;을 간략하게 설명하자면 다음과 같이 정의됩니다.&lt;br /&gt;
 “입력된 정형데이터(tabular data)에서 Feature를 masking하며 여러 step을 거쳐서 학습”
      &lt;ul&gt;
        &lt;li&gt;각 step별 &lt;strong&gt;feature들의 importance&lt;/strong&gt; 파악 (설명력 확보)&lt;/li&gt;
        &lt;li&gt;&lt;strong&gt;masking 으로 중요한 feature 들만 선출&lt;/strong&gt;해서 학습하여 성능 향상 (모델 고도화)&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;저희 분야에서는 정형데이터인 tabular data를 활용한 연구가 많이 진행되는 편이어서, 본 논문을 흥미롭게 읽었는데요. 정형데이터를 많이 활용하시는 분이라면 도움이 될 수 있을 것 같습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-problem-definition&quot;&gt;1. Problem Definition&lt;/h2&gt;
&lt;p&gt;과거 정형 데이터를 활용한 모델들의 성능비교를 했을 때, &lt;strong&gt;&lt;em&gt;LightGBM&lt;/em&gt;&lt;/strong&gt;의 성능이 가장 좋다고 알려져있고, 혹은 앙상블을 고려한 &lt;strong&gt;&lt;em&gt;Extreme Gradient Boosting&lt;/em&gt;&lt;/strong&gt; (&lt;strong&gt;&lt;em&gt;XGBoost&lt;/em&gt;&lt;/strong&gt;)나 &lt;strong&gt;&lt;em&gt;Catboost&lt;/em&gt;&lt;/strong&gt;을 떠올릴 수 있습니다. 더하여 &lt;strong&gt;&lt;em&gt;Neural Network&lt;/em&gt;&lt;/strong&gt;를 추가하는 방법이 있는데, 정형 데이터의 경우에 &lt;em&gt;Neural Network&lt;/em&gt;는 복잡하거나 깊은 레이어로 구성되지 않는 편이며, 더 깊거나 복잡하다고 성능이 눈에 띄게 좋아지지 않았습니다. &lt;strong&gt;&lt;em&gt;PCA&lt;/em&gt;&lt;/strong&gt;를 활용하여 &lt;strong&gt;차원축소&lt;/strong&gt;를 하거나, &lt;strong&gt;&lt;em&gt;autoencoder&lt;/em&gt;&lt;/strong&gt;를 기반으로 &lt;strong&gt;노이즈를 제거&lt;/strong&gt;하는 등의 방식에 활용이 되는 것이 주 였다고 볼 수 있습니다. &lt;br /&gt;
또한, &lt;strong&gt;&lt;em&gt;CNN, MLP&lt;/em&gt;&lt;/strong&gt;와 같은 딥러닝 모델은 적절한 귀납적 편향(&lt;em&gt;inductive bias&lt;/em&gt;)의 부족으로 지나치게 &lt;strong&gt;&lt;em&gt;Overparametrized&lt;/em&gt;&lt;/strong&gt; 되어 정형 데이터 내 매니폴드에서 일반화된 해결책을 찾는데 어려움을 발생시길 수 있습니다. 그럼에도 불구하고 이런 딥러닝 학습 방법론을 정형 데이터 학습에 사용하고자 하는 이유는 이미지나 다른 종류에 데이터와 정형데이터를 함께 학습(&lt;em&gt;Multi-Modal&lt;/em&gt;)할 수 있으며 트리 기반 모델 성능의 핵심인 Feature Engineering과 같은 작업이 크게 필요하지 않습니다. 또한, 스트리밍 데이터 학습이 용이하고 종단간(&lt;em&gt;end-to-end&lt;/em&gt;) 모델은 Domain adaptation, Generative modeling, Semi-supervised learning과 같은 가치있는 응용 모델과 같은 표현 학습(&lt;em&gt;representation learning&lt;/em&gt;)이 가능합니다.&lt;/p&gt;

&lt;h2 id=&quot;2-motivation&quot;&gt;2. Motivation&lt;/h2&gt;
&lt;p&gt;본 논문에서는 새로운 고성능 및 해석 가능한 표준 심층 테이블 형식 데이터 학습 아키텍처 인 TabNet을 제안합니다. TabNet은 원자료의 다른 전처리 없이 입력할 수 있고 경사하강법 최적화 방법을 통해 유연한 통합(flexible integration)이 가능한 종단간(end-to-end) 학습이 가능합니다. 또한, 순차적인 어텐션(Sequential Attention)을 사용하여 각 의사 결정 단계에서 추론할 feature들을 선택합니다. 이로인해 더 나은 해석 능력과 학습이 가능하며 숨겨진 특징을 예측하기 위해 사전 비지도 학습을 사용하여 정형 데이터에 중요한 성능 향상을 보여줍니다(&lt;em&gt;Self-supervised learning&lt;/em&gt;).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;본 논문에서 제안한 TabNet의 차별화된 아이디어와 contribution은 다음과 같습니다.
 1) 전처리 과정을 거치치 않은 &lt;strong&gt;raw data로도 end-to-end 학습이 가능&lt;/strong&gt;
 2) &lt;strong&gt;sequantial attention&lt;/strong&gt; 과정에서 &lt;strong&gt;각 step마다 중요한 feature를 선별&lt;/strong&gt;하면서 각 과정의 모델 해석과 성능 향상이 가능 
 3) &lt;strong&gt;다양한 도메인의 데이터&lt;/strong&gt;에서 다른 테이블 학습 모델과 비교 했을 때 &lt;strong&gt;분류/회귀 문제에서 우수한 성능&lt;/strong&gt;을 보임 
 4) &lt;strong&gt;masking 된 feature를 예측하는 tabnet decoder 비지도 학습을 통한 우수한 성능&lt;/strong&gt;을 보임&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;3-method&quot;&gt;3. Method&lt;/h2&gt;
&lt;p&gt;본격적으로 TabNet architecture를 살펴보기 전에, TabNet의 feature selection 방법과, encoder, decoder에 대한 설명을 간략히 정리했습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/82039869/232205929-78a7c5b6-f792-4728-8967-47064f1edadb.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;기존의 Feature seletion은 Lasso Regularization, Instance-wise feature selection등의 방법을 사용합니다. 이러한 방식을 본 논문에서는 Hard feature selection으로 표현하고 있으며, TabNet은 Soft feature selection을 구현하여 사용합니다.
간단히 언급하면, &lt;strong&gt;TabNet은 masking&lt;/strong&gt;을 이용하여, 좌측에서 우측으로 &lt;strong&gt;sequential하게 feature selection&lt;/strong&gt;을 하며 피드백을 주고 학습해나가는 구조입니다 (&lt;em&gt;figure 1 그림 참조&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/82039869/232206115-5e02beaf-6cc7-4891-b1a0-3335b0330035.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;더하여 tabnet encoder를 통해서 &lt;strong&gt;feature engineering&lt;/strong&gt;효과를 내고, decision making 부분을 통해 &lt;strong&gt;feature selection&lt;/strong&gt;이 이루어집니다. encoder는 &lt;em&gt;fine-tuning&lt;/em&gt;하면서 task에 맞게 성능을 향상시켜 맞춰갑니다. &lt;br /&gt;
encoder에 decoder 구조 결합하면 autoencoder 같은 자기 학습 구조를 가지고 있고, decoder 에서는 ?(물음표) 로 된 &lt;em&gt;missing value&lt;/em&gt; 를 채워넣는 구조입니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;TabNet encoder와 decoder의 architecture를 좀 더 자세히 살펴보면 아래 그림과 같습니다.&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/82039869/232206212-771ed0b7-abb8-4ec8-a42a-37c9265ea17e.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;(a) : TabNet encoder&lt;/strong&gt; 의 경우, 각 decision step에 대해서 &lt;strong&gt;&lt;em&gt;1)feature transformer, 2)attentive transformer, 3)feature masking&lt;/em&gt;&lt;/strong&gt;으로 구성되어 있습니다.    첫 의사결정 단계에서 부족한 부분을 다음 의사결정 단계에서 보완하는 방식이며, 트리기반 부스팅 모델들과 유사합니다.
모델의 용어를 좀 더 구체적을 설명하면, BN : 최초 Feature에 대해 배치 정규화를 적용
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;&lt;em&gt;Feature transformer&lt;/em&gt;&lt;/strong&gt; : 이후에 아래의 단계 반복.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;em&gt;Attentive transformer&lt;/em&gt;&lt;/strong&gt; : (d)에서 설명&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;em&gt;Mask&lt;/em&gt;&lt;/strong&gt; : Attentive transformer에서 나온 &lt;strong&gt;&lt;em&gt;Mask&lt;/em&gt;&lt;/strong&gt;(&lt;strong&gt;&lt;em&gt;M[i]&lt;/em&gt;&lt;/strong&gt;)에 대해 feature &lt;strong&gt;&lt;em&gt;f&lt;/em&gt;&lt;/strong&gt;를 곱하여 이후 step의 &lt;em&gt;Feature transformer&lt;/em&gt;에 들어가는 input을 조절. Mask를 통해 변수를 soft selection 함.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;em&gt;Feature transformer&lt;/em&gt;&lt;/strong&gt; : (c)에서 설명&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;&lt;em&gt;Split&lt;/em&gt;&lt;/strong&gt; : Feature transformer의 output을 두 개로 복제하여 하나는 &lt;em&gt;relu&lt;/em&gt;로, 하나는 &lt;em&gt;Attentive transformer&lt;/em&gt;로 보냄&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;(b) : TabNet decoder&lt;/strong&gt;는 각 단계에서 feature transformer 블록으로 구성됩니다. 일반 학습에서는 Decoder를 사용하지 않고, self-supervised 학습을 진행할 때, 인코더 다음에 붙여져서 기존의 결측값을 보완하고 표현학습을 진행하게 됩니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;(c) Feature Transformer&lt;/strong&gt;는 4개의 네트워크 묶음으로 구성됩니다.
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;&lt;em&gt;Fully Connected Layer&lt;/em&gt;&lt;/strong&gt; (&lt;strong&gt;FC&lt;/strong&gt;) - &lt;strong&gt;&lt;em&gt;Batch Normalization&lt;/em&gt;&lt;/strong&gt; (&lt;strong&gt;BN&lt;/strong&gt;) - &lt;strong&gt;&lt;em&gt;Gated Linear Unit Activation&lt;/em&gt;&lt;/strong&gt; (&lt;strong&gt;GLU&lt;/strong&gt;)로 구성된 블럭들을 순차적으로 통과하는 구조를 쌓고, 블럭간의 &lt;em&gt;residual skip connecion&lt;/em&gt;을 적용했습니다. 그리고 &lt;em&gt;residual output&lt;/em&gt;의 &lt;em&gt;normalization&lt;/em&gt;을 위해 &lt;strong&gt;&lt;em&gt;sqrt&lt;/em&gt;&lt;/strong&gt;(&lt;strong&gt;&lt;em&gt;0.5&lt;/em&gt;&lt;/strong&gt;)를 곱했습니다. 이때, 두 레이어는 모든 &lt;em&gt;decision&lt;/em&gt; 단계에서 공유되며, 나머지 두 레이어는 &lt;em&gt;decision&lt;/em&gt; 단계에 의존합니다. 따라서 앞 2개의 네트워크는 모든 파라미터를 공유하며 &lt;strong&gt;글로벌 성향&lt;/strong&gt;을 학습하고, 뒤에 2개의 네트워크 그룹은 &lt;strong&gt;로컬 성향&lt;/strong&gt;을 학습합니다. 
이어서 &lt;strong&gt;D&lt;/strong&gt;개의 변수를 갖는 값들을 입력받은 &lt;em&gt;feature transformer&lt;/em&gt;는 &lt;strong&gt;&lt;em&gt;split&lt;/em&gt;&lt;/strong&gt;할 값들을 내보냅니다. 총(&lt;strong&gt;B,N&lt;/strong&gt;)의 &lt;em&gt;output&lt;/em&gt;을 내보냈을 때, 논문에서는 &lt;em&gt;split&lt;/em&gt; 과정을 통해 &lt;em&gt;d&lt;/em&gt;[&lt;em&gt;i&lt;/em&gt;]와 &lt;em&gt;a&lt;/em&gt;[&lt;em&gt;i&lt;/em&gt;]로 나누었습니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;(d) Attnetive transformer&lt;/strong&gt; 블록으로, 현재 의사결정 단계에서 각 변수들이 얼마나 많은 영향을 미쳤는지 사전 정보량(&lt;em&gt;prior scale information&lt;/em&gt;)을 통해 집계합니다. 다시말해, 현재 decision step 전에 각 feature가 얼마나 많이 사용되었는지를 집계한 정보를 나타냅니다. 이것은 단일 레이어에 맵핑하여 사용되며, 계수의 정규화는 각 decision step에서 가장 두드러진 특징을 &lt;strong&gt;&lt;em&gt;sparse&lt;/em&gt;&lt;/strong&gt;하게 선택하고, 계수값들을 일반화(&lt;em&gt;normalization&lt;/em&gt;)하기 위해 &lt;strong&gt;&lt;em&gt;sparsemax&lt;/em&gt;&lt;/strong&gt;를 사용하여 학습되어집니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-experiment&quot;&gt;4. Experiment&lt;/h2&gt;
&lt;p&gt;본 논문에서는 regression, classification task로 성능을 평가하였고, 데이터 셋의 모든 categorical value 들은 임베딩되었고, numerical value들은 전처리 없이 input으로 활용되었습니다. TabNet은 대부분의 &lt;em&gt;hyperparameter&lt;/em&gt;에 대해 그리 예민하지 않다는 특징을 가집니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Instance-wise feature selection&lt;/strong&gt; (synthetic dataset - 임의로 생성한 데이터셋 활용)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/82039869/232207111-24fad62a-aa60-437c-9574-93f21a0c990c.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Table 1의 결과를 간략히 정리하면 다음과 같다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;6개 임의로 생성된 데이터로 성능을 평가&lt;/strong&gt; –&amp;gt; syn1 ~ syn 6 데이터&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;syn 1~3 에서는 각 인스턴스(data row) 별로 중요한 피쳐가 같음&lt;/strong&gt; –&amp;gt; 따라서 syn 1~3 에서는 Tabnet 의 성능이 global feature selection 하는 다른 모델들과 성능이 비슷&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;syn 4~6 에서는 각 인스턴스(data row) 별로 중요한 피쳐가 다름&lt;/strong&gt; –&amp;gt; 따라서 불필요한 feature들을 instance wise로 제거해서 성능을 향상&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Performance on real-world datasets&lt;/strong&gt; (실제 데이터셋 활용)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/82039869/232207175-64f6f48a-1be9-4485-a3d6-afac6b09bc8c.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Forest cover type dataset&lt;/em&gt;&lt;/strong&gt;  : 나무 분류 문제 / &lt;strong&gt;&lt;em&gt;Poker Hand&lt;/em&gt;&lt;/strong&gt; : 카드 분류 문제 / &lt;strong&gt;&lt;em&gt;Sarcos&lt;/em&gt;&lt;/strong&gt; : 로봇 팔 관련 데이터 / &lt;strong&gt;&lt;em&gt;Higgs Boson&lt;/em&gt;&lt;/strong&gt; : 이진 분류 문제 /&lt;strong&gt;&lt;em&gt;Rossman Store Sales&lt;/em&gt;&lt;/strong&gt; : 상점 매출 예측&lt;/p&gt;

&lt;p&gt;다양한 데이터셋을 활용하여 &lt;em&gt;XGBoost, LightGBM, Random forest, MLP&lt;/em&gt; 등 다양한 모델과의 비교 결과, &lt;strong&gt;Test accuracy, MSE 등의 평가지표&lt;/strong&gt;에서 TabNet이 다른 모델들에 비해 &lt;strong&gt;좋은 성능&lt;/strong&gt;을 보이는 것을 확인할 수 있습니다.&lt;/p&gt;

&lt;p&gt;또한 본 논문에서 제안한 그림(&lt;em&gt;Figure 5&lt;/em&gt;)을 통해 알 수 있듯이, 각 스텝에서 활성화된 &lt;strong&gt;feature 를 시각화&lt;/strong&gt;로 확인할 수 있다는 장점이 있고, &lt;strong&gt;instance 별 중요도&lt;/strong&gt; 또한 확인이 가능합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/82039869/232208001-15a85c65-4121-43ad-a254-b2ad92bf81e8.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그림의 하얀색 부분이 모델 학습에 사용 feature라고 해석할 수 있습니다. &lt;br /&gt;
간단히 설명해보자면, &lt;em&gt;Syn2 dataset&lt;/em&gt;에서는 &lt;strong&gt;&lt;em&gt;feature X3-X6&lt;/em&gt;&lt;/strong&gt; 만 활용 되었으며 &lt;strong&gt;&lt;em&gt;Magg&lt;/em&gt;&lt;/strong&gt; 는 각 스텝의 &lt;strong&gt;feature importance를 결합&lt;/strong&gt; 했을 때 나오는 결과를 &lt;strong&gt;시각화&lt;/strong&gt; 한 것입니다.&lt;/p&gt;

&lt;h2 id=&quot;5-conclusion&quot;&gt;5. Conclusion&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;저자는 &lt;em&gt;tabular learning&lt;/em&gt;을 위한 참신한 딥러닝 아키텍처인 &lt;strong&gt;&lt;em&gt;TabNet&lt;/em&gt;&lt;/strong&gt;을 제시했으며, TabNet은 각 결정 단계에서 처리할 의미 있는 변수의 subset을 선택하기 위해 &lt;strong&gt;&lt;em&gt;sequential attention mechanism&lt;/em&gt;&lt;/strong&gt;을 활용했습니다. 선택된 특징들은 &lt;em&gt;representation&lt;/em&gt;으로 처리되어서 다음 결정 단계에서 정보를 보내고 기여하고 있습니다. &lt;strong&gt;&lt;em&gt;Instance-wise feature selection&lt;/em&gt;&lt;/strong&gt;은 &lt;em&gt;model capacity&lt;/em&gt;로써 효율적인 학습을 가능하게 한다고 정리할 수 있을 것입니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;저의 개인적인 소견으로는, 논문에서 언급된 것과 같이 &lt;strong&gt;정형 데이터보다는 비정형 데이터에 많이 집중&lt;/strong&gt;되어 개발되어 있는 상태라고 생각합니다. 따라서 비정형데이터에서 성능을 높인 모델을 정형데이터에 적용하기에는 이론이 맞지 않다고 생각했는데, 본 모델에서 대안으로 &lt;strong&gt;&lt;em&gt;attention mechanism&lt;/em&gt;&lt;/strong&gt;이라고 하는 방법이 제시되어 너무 좋았습니다. 또한 본 모델을 활용한다면 전처리의 필요성이 줄어들고, &lt;strong&gt;&lt;em&gt;tree-based model&lt;/em&gt;&lt;/strong&gt;처럼 활용할 수 있는 것이 또 하나의 장점이라고 생각합니다.  &lt;br /&gt;
마지막으로 보통은 딥러닝을 해석하고 평가하기 위해 &lt;em&gt;surrogate model&lt;/em&gt;로 대체하는데, 본 논문에서는 &lt;strong&gt;해석가능성 방식&lt;/strong&gt;을 활용하여 구현한 것이 인상적이었습니다. 아주 작은 코멘트로는, 신경망모델 치고는 조절해야할 파라미터가 조금 많아 &lt;em&gt;parameter search&lt;/em&gt;하는데 시간이 소요 될 것 같습니다. &lt;br /&gt;
하지만 정형 데이터에 대한 모델 연구나 방법론이 한정적이라고 생각했었는데 끝없이 발전하고 있다는 생각이 드는 논문이었습니다. 특히 &lt;strong&gt;&lt;em&gt;tabular data&lt;/em&gt;&lt;/strong&gt;를 활용한 연구를 많이 진행하고 있는 입장으로 TabNet을 저희 연구분야에 적용하기 위해 코드 및 구조를 더욱더 꼼꼼히 배워야 겠다는 생각을 했습니다.
모두 한번 읽어보세요~! :)&lt;/p&gt;

&lt;h2 id=&quot;6-reference--additional-materials&quot;&gt;6. Reference &amp;amp; Additional materials&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;TabNet 논문파일과, Github - tensorflow, torch로 구현된 코드 링크를 함께 첨부드립니다.
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1908.07442&quot;&gt;TabNet 논문&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/google-research/google-research/tree/master/tabnet&quot;&gt;TabNet tensorflow code&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/microsoft/qlib/blob/main/qlib/contrib/model/pytorch_tabnet.py&quot;&gt;TabNet torch code&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
            <pubDate>Thu, 20 Apr 2023 00:00:00 +0900</pubDate>
            <link>http://localhost:4000/TabNet_Attentive_Interpretable_Tabular_Learning.html</link>
            <guid isPermaLink="true">http://localhost:4000/TabNet_Attentive_Interpretable_Tabular_Learning.html</guid>
            
            <category>reviews</category>
            
            
        </item>
        
        <item>
            <title>[Nature Machine Intelligence 2022] Super-resolution generative adversarial networks of randomly-seeded fields</title>
            <description>&lt;h1 id=&quot;paper-title--super-resolution-generative-adversarial-networks-of-randomly-seeded-fields&quot;&gt;Paper title : Super-resolution generative adversarial networks of randomly-seeded fields&lt;/h1&gt;

&lt;h2 id=&quot;1introdcution&quot;&gt;1.introdcution&lt;/h2&gt;
&lt;h3 id=&quot;11-problem-definition&quot;&gt;1.1 Problem definition&lt;/h3&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&quot;기계&quot;는 종종 삶을 좀 더 편리하고 효율적으로 살아갈 수 있도록 도와주는 방법 혹은 도구로 정의된다. 많은 공학 (engineering) 분야에서 이러한 도구를 최적화하기 위해 많은 연구들이 수행되고 있으며, 특히, 딥러닝 방법론이 공학 분야에 어떤 식으로 더욱 효율적인 도구를 개발하기 위해 적용되는지 본 리뷰 논문을 통해 소개하려고 한다.

본 수업을 수강하는 대부분의 data scienctists의 peer reviewer들 위해, 공학 분야에 사용되는 물리적인 방정식에 대한 세부적인 내용은 최대한 줄이고, &quot;딥러닝 알고리즘의 공학 분야에 적용 가능성&quot;을 중점으로 본 리뷰를 이어가려고 한다. 우선, 현실적으로 가장 와닿는 예시로써, 아래 그림 1과 같이, 대부분의 사람들은 하루를 시작하기 전에 기상, 날씨 혹은 미세먼지가 얼마나 높은지 검색을 해보곤 한다. 이는 특정 지역에서 기계 장비 (sensor)의 측정 (measurement)을 통해 얻어진 풍량, 온도, 기압과 같은 데이터 정보들을 종합한 예보를 통해 인간의 삶을 자연 재해로 부터 대비할 수 있게 만들어 준다.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;center&gt;&lt;img src=&quot;/images_DS503/figure_1.gif&quot; alt=&quot;example image&quot; width=&quot;500&quot; height=&quot;500&quot; /&gt;&lt;/center&gt;

&lt;center&gt;그림 1. 센서 장비에 의해 측정된 데이터 정보 예시 &lt;/center&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;하지만, 이러한 기계 장비는 정확도가 높고 정밀한 데이터를 얻기 위해 다양하고 복잡한 전자 장비를 동반하기 때문에 큰 설치 비용이 발생한다. 이에 따라, 관심 있는 지역에 공간적으로 거리가 먼 (Sparse) 센서 장비 설치를 하고, 특정 위치에서 측정된 값을 넓은 영역을 대표하는 방식을 채택하여 해상도가 적은 (low-resolution) 결과를 제공한다. 즉, 사람이 활동하고 있는 해상도가 높은 (high-resolution) 다양한 공간 내 정보를 조밀하게 배치된 센서 장비를 통해 얻는 것은 현실적으로 큰 비용 문제가 발생한다.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;12-neural-network-methodology&quot;&gt;1.2 Neural network methodology&lt;/h3&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;이러한 비용-공간 해상도 사이 절충 (trade-off) 문제를 해결하기 위해, 인공신경망 (neural-network) 기반 방법론이 저해상도 데이터와 고해상도 데이터 사이 비선형 (nonlinear) 관계성을 연결하는데 효율적이고 성공적인 결과를 보이는 도구로 증명되고 있다. 딥러닝 방법론이 저해상도와 고해상도 사이 모델링에 어떻게 적용되는지 설명하기 위해 아래 그림 2을 통해 나타내고자 한다. 대표적으로, 딥러닝 모델의 입력은 관심 있는 지역에 공간적으로 거리가 먼 (Sparse) 센서 장비 정보이고 딥러닝 모델의 출력은 해상도가 높은 (high-resolution) 다양한 공간 내 정보로 설정하여 두 사이 간 비선형적 관계를 노드와 레이어를 갖는 복잡한 neural network 조합을 통해 학습하고 새로운 저해상도 정보로 부터 고해상도 센서 정보를 예측한다 [1].
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;center&gt;&lt;img src=&quot;/images_DS503/figure_2.jpeg&quot; alt=&quot;example image&quot; width=&quot;700&quot; height=&quot;500&quot; /&gt;&lt;/center&gt;

&lt;center&gt;그림 2. 인공신경망 기법을 기반한 저해상도&lt;/center&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;더욱 구체적으로, 저해상도-고해상도 관계를 모델링하기 위해, 그림과 같이 크게 (1) residual block와 (2) super-resolution block 를 갖는Deep learning architecture [2]를 제안한다. 저해상도 이미지를 deep learning model에 입력으로 사용하고, Residual block을 반복적으로 연결하여 입력 및 출력 사이 공간적 정보 손실을 최소화한다. 이 후, 학습 변수를 갖는 up-sampling convolutional operation을 residual block 을 구조 끝단 활용하여 고해상도 output 차원까지 복구하는 기법이 이용되고 있다. 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;center&gt;&lt;img src=&quot;/images_DS503/figure_3.png&quot; alt=&quot;example image&quot; width=&quot;800&quot; height=&quot;500&quot; /&gt;&lt;/center&gt;

&lt;center&gt;그림 3. 저해상도-고해상도 맵핑을 위한 뉴럴네트워크 구조&lt;/center&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;이는 기존 저해상도에서 고해상도로 복구하는 일반적인 선형 interpolation 기법에 비교하여 convolution 연산을 통해 비선형성을 맵핑할 수 있기 때문에, 놀라운 고해상도 복구 결과를 아래 그림 3과 같이 보인다.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;center&gt;&lt;img src=&quot;/images_DS503/figure_4.png&quot; alt=&quot;example image&quot; width=&quot;800&quot; height=&quot;500&quot; /&gt;&lt;/center&gt;

&lt;center&gt;그림 4. 일반적인 저해상도-고해상도 맵핑 딥러닝 알고리즘 및 기존 보간 알고리즘 예측 결과 비교&lt;/center&gt;

&lt;h3 id=&quot;13-challenges-of-the-exsiting-deep-learning-approachs-for-engineering-application&quot;&gt;1.3 Challenges of the exsiting deep learning approachs for engineering application&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;센서 저해상도-고해상도 문제에 대해 딥러닝 방법론이 성공적으로 적용되고 있지만, 앞서 설명한 기상 예보와 같은 센서 어플리케이션을 위한 딥러닝 방법론을 적용하기엔 다음과 같은 두 가지 한계점이 여전히 존재한다.

 (1) 딥러닝 학습을 위한 고해상도 데이터 획득 문제: 다량의 데이터가 존재한다면, 앞서 설명한 딥러닝 방법론이 성공적으로 다양한 저해상도-고해상도 문제 (미세 현미경 정확도 보정, 해수면 온도 측정 및 유체 난류 해상도 문제 등)에 적용할 수 있지만, 공학 분야에 이용되고 있는 센서들을 통해 조밀한 고해상도 딥러닝 학습 데이터를 확보 하는 것은 매우 큰 비용이 발생한다.
   
 (2) 딥러닝 학습을 위한 고해상도 데이터 관리 문제: (1)의 문제를 해결할 수 있더라도, 고해상도로 구축된 센서들은 예기치 못한 다양한 환경 조건으로 인해 기기가 무작위로 On-off되거나 움직임이 필요한 센서로 인해 일관성 있고 정확한 고해상도 데이터를 확보하는 것이 매우 어려운 문제를 제공한다.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;2-deep-learning-framework-to-solve-these-limitations&quot;&gt;2. Deep learning framework to solve these limitations&lt;/h2&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;본 논문에서는 딥러닝 학습 과정에서 고해상도의 측정 데이터 필요없이 저해상도-고해상도 측정 데이터 관계를 모델링 가능한 획기적인 딥러닝 프레임 워크를 제안한다. 제안된 방법론의 주된 장점은 딥러닝 모델이 학습 과정에서 오직 공간에 무작위로 spase하게 분포한 센서만을 이용하여 저해상도-고해상도 간 관계를 모델링 할 수 있다는 것이다. 해당 프레임 워크는 무작위로 분포한 데이터만을 활용하여 앞서 설명한 데이터 관련 문제를 해결하기 때문에 randomly seeded super-resolution GAN (RaSeedGAN)이라고 명칭되고, 제안하는 프레임 워크의 우수성을 검증하기 위해 3가지 예측 task (유체 유속 시뮬레이션, 지구 해수면 온도 및 유체 유속 측정 문제)를 채택한다. 자세한 딥러닝 구조에 대한 설명, 메커니즘 및 예측 결과들은 아래와 같이 설명하고자 한다.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;21-deep-learning-architecture-for-raseedgan&quot;&gt;2.1 Deep learning architecture for RaSeedGAN&lt;/h3&gt;

&lt;center&gt;&lt;img src=&quot;/images_DS503/figure_5.jpeg&quot; alt=&quot;example image&quot; width=&quot;1000&quot; height=&quot;500&quot; /&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;center&gt;그림 5. 본 리뷰 논문에서 제안된 딥러닝 구조 세부사항&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Main acrchitecture and idea&lt;/strong&gt; : GANs은 “generator” 그리고 “discriminator”라는 두 개의 서로 다른 neural network를 구성하고 있다.”generator”는 참값 (target data)을 모방하여 인공적인 예측 값을 (generated data) 생성하는 network이고, “discriminator”는 인공적으로 생성된 예측 값과 참값 사이 다른 점을 구별하는 network이다. 본 연구에서는 이전 연구들의 한계인 학습 데이터에 high-resolution full fields 획득에 대한 제한점을 해결하는 generator를 제안하여 GAN을 이용하여 보다 우수한 생성 능력을 갖도록 학습시킨다 (그림5)
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Generator network&lt;/strong&gt; : 본 연구에서 이용되는 generator의 입력과 출력은 기존 저해상도-고해상도 관계를 해결하는 딥러닝 연구들과 다르게 할당하여 해당 문제를 해결한다. 더욱 구체적으로, 고해상도의 모든 공간 데이터를 이용하는 것이 아닌 고해상도의 센서 데이터에서 무작위로 추출한 sparse한 데이터를 &lt;strong&gt;generator 출력&lt;/strong&gt;으로 설정하고 (그림 5a), 무작위로 추출된 sparse한 고해상도 데이터 (그림 5b 왼쪽)로 부터 특정 직사각형 크기 픽셀마다 평형하게 이동하여 10개의 센서를 평균화 작업을 통해 &lt;strong&gt;저해상도의 generator 입력 feature&lt;/strong&gt;를 생성한다 (그림 a 왼쪽). 즉, 고가 센서 장비를 sparse하게 분포시켜 데이터를 얻고, 이를 convolutions stride처럼 이동하여 평균 값을 산정하여 low-resolution5 입력을 얻는다. 이러한 접근은 잡음 뿐만 아니라 공간적 해상도에 대한 균일성을 증가하는 장점을 갖는다. generator는 기존 baseline neural network architecture [2]에 비해 변형된 구조를 갖는데 이는 아래와 같다.
&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(1) Initial layer : 먼저 low-resolution fields가 generator로 입력되는데, filter size 9 × 9 그리고 64 feature maps을 갖는 convolutional layer가 이용되고, parametric rectified linear unit (ReLU) 활성화 함수가 convolutional operation에 의해 추출된 정보를 포착한다. &amp;lt;br&amp;gt;
   
(2) Medium layer : 초기 레이어에 의해 추출된 정보는 16개의 residual blocks [2] 을 통해 보다 높은 비선형적 관계에 대해 모델링한다. 이떄 residual block은 3x3 kernel을 동반한 64개의 feature map 을 이용한다. 여기서, 저해상도를 증가시키기 전에, skip-connection sum 레이어가 residual blocks의 출력 부분 그리고 initialization layers의 출력 부분 사이에 적용된다. 이 후애, [2]에 제안된 subpixel convolution layer 가 generator 출력 만큼 증가 시키기 위해 사용된다. 최종적으로 생성되는 spase-high resolution ouput은 비선형 보간 기법을 이용하여 sparse sample과 sample 사이 데이터를 공간적으로 보간하여 관심 있는 최종적 고해상도 데이터를 복구할 수 있다.
&amp;lt;br&amp;gt; &amp;lt;br&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;3. Discriminator network&lt;/strong&gt; : spase-high resolution target 그리고 generated fields 사이 예측 정확도를 향상시키 위해, 아래와 같은 discriminator network를 이용하여 generated fields를 입력으로 연결한다 (그림 5b). Discriminator network는 초기에 filter size 3 × 3 그리고 64 feature maps를 이용하여 중요 공간적 정보를 추출하고, 7개의 discriminator blocks이 연속적으로 적용된다. 이때, 홀수 block 마다 stride 사이즈를 높여 차원을 줄이고, 줄여진 feature-map tensor가 하나의 vector로 변환된다. 이때 1,024개의 fully connected layer를 이용하여 변환된 vector를 discriminator 출력 값으로 예측하고, 그 출력값이 sigmoid function을 통해 확률적으로 참 (0s)인지 거짓 (1s)인지 적대적으로 학습시켜 generator의 생성 능력을 향상시킨다. discriminator의 손실 함수 (loss function)는 다음과 같이 정의된다.
&lt;br /&gt;&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;/images_DS503/eq_1.jpeg&quot; alt=&quot;example image&quot; width=&quot;1000&quot; height=&quot;500&quot; /&gt;&lt;/center&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;여기서 𝔼[]는 mini-batch 내 평균에 대한 연산자이고, H_R 및 L_R은 각각 고해상도 및 저해상도 이미지를 나타내며, D()는 저해상도 입력으로 부터 생성된 고해상도 이미지에 대한 loss를 계산하는 discriminator network를 나타낸다. F_v 는 하나의 연산자 계수로써 고해상도 이미지 내 센서가 존재할때 1 그렇지 않은 경우 0으로 변환하는 역할을 한다. 여기서, generator network의 loss function은 다음과 같이 정의된다.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;center&gt;&lt;img src=&quot;/images_DS503/eq_2_.jpeg&quot; alt=&quot;example image&quot; width=&quot;1000&quot; height=&quot;500&quot; /&gt;&lt;/center&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;여기서 G()는 generator network가 실제 참값과 생성하는 sparse high-resolutional 값 사이 격자별 오차를 나타낸다.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;center&gt;&lt;img src=&quot;/images_DS503/eq_3.jpeg&quot; alt=&quot;example image&quot; width=&quot;1000&quot; height=&quot;500&quot; /&gt;&lt;/center&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;GANs을 학습시키기 위해, 위의 adversarial loss가 생성된 spase-high resolution field를 binary cross-entropy로 레이블링한다. 여기서, discriminator는 생성된 spase-high resolution field가 ‘fake’ 인지 진위 여부를 해당 1값으로 할당한 binary cross-entropy를 통해 결정한다.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;22-prediction-results&quot;&gt;2.2 Prediction results&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;본 논문에서 제안하는 값비싼 고해상도 데이터를 학습과정에서 요구하지 않는 RaSeedGAN의 우수성을 검증하기 위해, 비선형성과 변동성을 포함하고 있는 3가지 실험 및 시뮬레이션 데이터를 이용한다. 우수성 검증을 위해, 아래 그림과 같이 (1) Pinball flow numerical simulation, (2) NOAA sea surface temperature database (3) particle-image velocimetry (PIV) experiment 와 같은 세 가지 예시에 대해 검증하고, 보다 자세한 numerical simulation 조건은 해당 리뷰에서 딥러닝 알고리즘에 대한 high-quality 정보를 집중하기 위해 생략한다. 세 가지 예시는 서로 다른 image 차원을 갖는다. (1),(2),(3)은 각각, 512 x 512, 720 × 1,440, 128 × 128 크기로 서로 다른 이미지 사이즈를 갖고 해당 예측 task에 마다 딥러닝 모델을 독립적으로 학습하여 예측 성능에 대해 검증한다. 또한, 서로 다른 task이지만 본 알고리즘의 일반화 능력을 검증하기 위해 무작위로 추출된 sparse한 고해상도 데이터로 부터 특정 직사각형 크기 픽셀마다 평형하게 이동하여 직사각형 공간 내 10개의 센서를 평균화하는 작업을 통해 32 x 32 bin size를 갖도록 input feature를 가공한다. 

그림 6은 세 가지 예측 task에 대해 딥러닝 모델이 고해상도 데이터를 학습에 이용하지 않고, 저해상도-고해상도 사이 관계를 예측할 수 있는지에 대해 비교한 결과이다. 그림 6의 첫번째 열에는 평균화 작업을 거친 입력 저해상도 필드, 두번째 열은 &quot;Sparse HR reference&quot;라고 하는 특징화된 고해상도 필드, 세번째 그리고 네번째 열은 RaSeed GAN이 이를 예측하고 변환환 결과와 타겟 필드들을 나타낸다. 여기서 주목할 점은, 전체 고해상도 데이터는 딥러닝 모델 능력 테스트를 위해 그림 6의 마지막 열에 포함되어 있지만 훈련 중 직접적으로 사용되지는 않았다는 점이다. 그림 6의 각 task에서 알 수 있듯이, RaSeedGAN은 원기둥 주변과 원기둥 부근의 난류 발달 지역을 정확하게 복구할 수 있을 뿐만 아니라, 높은 수준의 디테일로 온도 실험 데이터를 복구할 수 있다.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;task-1-pinball-flow-numerical-simulation&quot;&gt;Task 1: Pinball flow numerical simulation&lt;/h4&gt;
&lt;center&gt;&lt;img src=&quot;/images_DS503/figure6_1.jpeg&quot; alt=&quot;example image&quot; width=&quot;1000&quot; height=&quot;500&quot; /&gt;&lt;/center&gt;

&lt;h4 id=&quot;task-2-noaa-sea-surface-temperature-database&quot;&gt;Task 2: NOAA sea surface temperature database&lt;/h4&gt;
&lt;center&gt;&lt;img src=&quot;/images_DS503/figure6_2.jpeg&quot; alt=&quot;example image&quot; width=&quot;1000&quot; height=&quot;500&quot; /&gt;&lt;/center&gt;

&lt;h4 id=&quot;task-3-particle-image-velocimetry-piv-experiment&quot;&gt;Task 3: particle-image velocimetry (PIV) experiment&lt;/h4&gt;
&lt;center&gt;&lt;img src=&quot;/images_DS503/figure6_3.jpeg&quot; alt=&quot;example image&quot; width=&quot;1000&quot; height=&quot;500&quot; /&gt;&lt;/center&gt;

&lt;center&gt;그림 6. RaSeedGAN 예측 결과 비교&lt;/center&gt;

&lt;h3 id=&quot;3-conclusion&quot;&gt;3 Conclusion&lt;/h3&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;본 연구에서는 무작위로 공간에 배치된 센서 데이터로부터 관심 있는 영역의 고해상도 센서 측정 결과들을 추정하기 위한 RaSeedGAN 프레임워크를 제안한다. 해당 프레임 워크의 가장 큰 장점이자 방법론적 접근 요약은 (1) 무작위로 공간에 배치된 센서 데이터들을 특정 구간 센서 개수마다 평균화하여 저해상도 입력 feature를 만드는 것. (2) 저해상도 입력 feature로 부터 듬성듬성 배치된 해당 센서 데이터를 복구하는 것. (3) 예측된 결과를 보간하여 다시 공간적으로 매우 세부적으로 기술된 온도/유속 고해상도 데이터를 얻는 것에 있다. 여기서 GAN은 듬성듬성 배치된 해당 센서 데이터들을 예측하는 generator에 대해 실제 결과와 매우 유사한 생성 field를 예측할 수 있게 했다. 특히, 본 연구는 유체 흐름 시뮬레이션, 해양 표면 온도 분포 측정 및 입자-이미지 속도측정 데이터에서 검증되었다. 이러한 프레임 워크는 고해상도의 데이터를 훈련 자체에 필요하지 않기 때문에 비용-효율적으로 많은 다양한 분야에 적용될 수 있을 것으로 생각한다.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;4-reference&quot;&gt;4 Reference&lt;/h3&gt;

&lt;p&gt;[1]Adversarial super-resolution of climatological wind and solar data, &lt;strong&gt;&lt;em&gt;Proceedings of the National Academy of Sciences of the United States of America&lt;/em&gt;&lt;/strong&gt;,July 6, 2020,117 (29) 16805-16815
https://doi.org/10.1073/pnas.1918964117
&lt;br /&gt;
[2]Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network, &lt;strong&gt;&lt;em&gt;arXiv&lt;/em&gt;&lt;/strong&gt;:1609.0480&lt;/p&gt;
</description>
            <pubDate>Thu, 20 Apr 2023 00:00:00 +0900</pubDate>
            <link>http://localhost:4000/Super_resolution_generative_adversarial_networks_of_randomly_seeded_fields.html</link>
            <guid isPermaLink="true">http://localhost:4000/Super_resolution_generative_adversarial_networks_of_randomly_seeded_fields.html</guid>
            
            <category>reviews</category>
            
            
        </item>
        
        <item>
            <title>[NIPS 2021] Subgraph Federated Learning with Missing Neighbor Generation</title>
            <description>&lt;h1 id=&quot;neurips-21-subgraph-federated-learning-with-missing-neighbor-generation-리뷰&quot;&gt;[NeurIPS-21] Subgraph Federated Learning with Missing Neighbor Generation 리뷰&lt;/h1&gt;

&lt;h1 id=&quot;1-motivation&quot;&gt;1. Motivation&lt;/h1&gt;

&lt;p&gt;본 논문은 Graph domain에서 &lt;strong&gt;Subgraph Federating Learning&lt;/strong&gt; 을 처음 시도한 논문이다. 이 연구가 왜 필요한지 알아보려면 우선 Federated Learning이 무엇인지 간략한 개념을 알아야한다. &lt;strong&gt;Federated Learning&lt;/strong&gt;이란 privacy등의 이유로 다양한 local system들로 부터 data를 모을 수 없는 상황에서 모델을 함께 학습시켜 각자의 local system 내의 data로만 학습시켰을 때 보다 더 좋은 성능을 얻기 위한 방법이다. 예를들어 Computer Vision (CV) 도메인 에서는 휴대폰 사진첩내의 사진들을 모두 중앙 서버로 모아서 Machine Learning모델을 학습시키려 한다면 각자의 얼굴 등의 privacy가 침해 될 우려가 있을 수 있기 때문에 데이터를 공유하지 않고 Convolution Neural Network (CNN)을 학습 시키려고 하는 것이다. 이러한 이유로 CV와 NLP도메인에서는 Federated Learning연구가 많이 진행되어 왔는데 Graph 도메인에서는 왜 Federated Learning이 필요할까?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?export=download&amp;amp;id=1SygClT33EqcEuDLVZbLYWeQyjEu7oxHh&quot; alt=&quot;Motivation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Graph 도메인에서 Graph Neural Networks (GNNs)를 이용한 많은 적용사례가 있지만 그 중 하나로 유사한 특성을 가진 환자간에 edge를 연결시켜서 만든 Patient Graph를 이용하여 환자의 질병 유무 등을 예측하는 task가 있다. 이 경우 환자의 개인 정보 및 검사 결과 사진 등을 Medical Center로 보내서 GNN을 학습 시켜야 하는데 real-world에서의 상황을 생각해보면 병원에서 환자 개인정보는 매우 민감한 privacy 문제가 있기 때문에 사실상 하나의 Center에 여러 병원의 환자 정보를 취합하기가 힘들다. 그러다보니 위의 그림처럼 Hospital A, B, C, D에서 데이터를 Moedical Administration Center로 모아서 학습시키는 것이 아니라 &lt;strong&gt;‘Joint training without sharing graph data’&lt;/strong&gt; 가 필요한 것이다. 그렇다면 CV와 NLP에서 사용하는 방법론들을 그대로 적용하지 않고 새로운 연구가 필요한 것일까?&lt;/p&gt;

&lt;p&gt;위의 예시처럼 Hospital간에 data를 share하지 않는 경우 Hospital A와 B에 있는 환자 간에는 edge를 연결 시킬 수 없게 된다. 따라서 GNN에서 결과값을 낼 때 전체 Graph를 다 활용하지 못하고 병원 별 &lt;strong&gt;Subgraph&lt;/strong&gt; 만을 활용할 수 있기 때문에 missing edge가 발생하여 이로 인한 정보 손실이 일어난다. 이러한 현상은 기존 다른 도메인에서는 없는 문제이기 때문에 이를 처리하기 위한 방법이 필요하고 따라서 &lt;strong&gt;Subgraph Federated Learning&lt;/strong&gt; 에 특화된 방법론을 연구할 필요가 있다.&lt;/p&gt;

&lt;h1 id=&quot;2-method&quot;&gt;2. Method&lt;/h1&gt;

&lt;p&gt;본 논문에서는 위의 설명과 같이 여러 개의 Subgraph간에 data를 공유하지 않고 jointly training하는 Federated Learning을 위한 방법론을 제시하였다. 그래서 첫번째로 1) 가장 기본적인 Federated Learning 방법인 FedAvg를 Graphsage 인코더를 활용하여 적용해본 &lt;strong&gt;Fedsage&lt;/strong&gt;을 제시하였고 2) Missing Edge를 문제를 해결하기 위해 이 Link를 다시 복원시켜주는 Generative Model을 결합시킨 &lt;strong&gt;Fedsage+&lt;/strong&gt; 를 추가적으로 제시하였다.&lt;/p&gt;

&lt;h2 id=&quot;2-1-fedsage&quot;&gt;2-1) Fedsage&lt;/h2&gt;

&lt;p&gt;먼저 Fedsage는 일반적인 GNN에서 Node classification model을 학습시키듯이 각각의 label에 대한 prediction 값을 계산한 뒤 아래 Cross Entropy Loss를 계산하고 gradient descent를 통하여 학습시킨다. 
&lt;img src=&quot;https://drive.google.com/uc?export=download&amp;amp;id=1rZ4fhaWnSMNTqxtcpMapl9tWeM6OI2kt&quot; alt=&quot;CE&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그리고 FedAvg를 적용하는데 Fedavg는 1) Round마다 local system을 학습 2) 학습된 local system의 weight를 server에서 average 3) server에서 weight를 각각의 local system으로 배포 의 순서를 반복하는 알고리즘이다. 이는 가장 대표적이고 간단하여 널리 쓰이는 방법론이고 아래 Fedavg논문에서의 수도 코드를 참고하여 쉽게 이해할 수 있다.
&lt;img src=&quot;https://drive.google.com/uc?export=download&amp;amp;id=1TXPt7ESV5qanZHAZpQW8vnrJisC3Wq4s&quot; alt=&quot;Fedavg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-2-fedsage&quot;&gt;2-2) Fedsage+&lt;/h2&gt;

&lt;p&gt;위 Fedsage는 Subgraph Federated Learning 세팅에서 처음으로 Federated Learning을 시도해봤다는 의미가 있지만 이 방법은 이전 Motivation 섹션에서 말한 Missing Link로 인한 문제를 전혀 다루지 않고 있다. 따라서 본 논문에서는 Generative Model을 활용하여 Missing 잃어버린 연결관계를 복원시켜주는 방법을 제안한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?export=download&amp;amp;id=1meNAOF25mBHTH6A0yoztAX62jT6Vy0jo&quot; alt=&quot;Fedavg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그래서 모델 Architecture를 크게 보면 Missing Neighbor Generator (NeighGen)을 통하여 Missing 된 Node들을 다시 만들어서 Observe된 Graph에서 붙여주고 (Graph mending) 복원된 Graph를 통하여 downstream task인 node classification을 수행한다. Node classification 부분은 위 fedsage와 동일하기 때문에 중요한 부분은 NeighGen을 어떻게 구성하는지 인데 우선 missing된  link를 Subgraph내에서 만들어주기 위해서는 각각의 Subgraph 내의 node마다 몇개의 node가 drop되었는지를 알아야 하기 때문에 missing된 node 개수를 예측하는 dGen을 학습시키고 그 예측된 node 개수에 맞게 새로운 node의 feature를 만들어주는 fGen을 학습시킨다.&lt;/p&gt;

&lt;p&gt;구체적으로 보면 학습단계에서는 Ground Truth값을 알아야 model을 학습시킬 수 있기 때문에 관측된 Graph에서 몇몇 node를 더 숨겨 (hide) missing link를 발생시킨다. 그리고 NeighGen을 위한 GNN을 통해 node마다의 representation을 뽑아 낸뒤 dGen을 통하여 몇개의 node가 숨겨졌는지 예측한다. 여기서 dGen는 $\theta^d$라고 표기되는 parameter를 가진 linear regression 모델이다. 그래서 논문에는 아래 수식으로 missing neighbor 수인 $\tilde{n_v}$ 를 prediction을 한다고 하는데 논문의 $n_v$는 typo인 것 같고 node representation을 타나내는 $z_v$로 대체 되어야 할 것 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?export=download&amp;amp;id=1RdS9I6bgqZPOZ-dPq2a7arkXHg2LyS_a&quot; alt=&quot;dGen&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그리고 fGen에서는 예측된 missing neighbor수 만큼 node를 만들어 줘야 하는데 다양성을 위하여 Gaussian noise와 node representatino을 더해주고 fGen의 parameter  $\theta^f$와 곱해주는 방식으로 새로운 node feature를 만들어낸다. 이 때 dGen에서 예측한 missing neighbor 개수인 $\tilde{n_v}$개 만큼 생성하고 수식은 아래와 같다. (여기서 R은 Random Sampler를 의미한다)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?export=download&amp;amp;id=1U8_7sHEpJIO9IsuPGfEku-IjK4kvmeAk&quot; alt=&quot;fGen&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그래서 전반적인 NeighGen에 관한 학습은 위 예측값을 바탕으로 아래 수식과 같이 실제 missing neighbor의 차이를 계산하는 dGen에 관한 loss 그리고 Generate한 Neighbor와 holding된 Neighbor의 feature difference를 계산하는 부분으로 구성된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?export=download&amp;amp;id=1IZIcwK_5nJ3wDtDSLZC3AhglMBENGhfo&quot; alt=&quot;Graph_mending&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Downstream task인 node classification (fedsage와 동일)에 대한 loss와 balance parameter인 $\lambda$와 함께 결합하여 아래 최종 loss function이 도출된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?export=download&amp;amp;id=13YbmbMAiP28gAwae_MA7Q94H8FXlobp5&quot; alt=&quot;overall&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;federated-learning-of-graphsage-and-neighgen&quot;&gt;Federated Learning of GraphSage and NeighGen&lt;/h3&gt;

&lt;p&gt;Federated Learning을 하기 위해서는 위 Loss function을 바탕으로 Graphsage인코더와 NeighGen을 각각의 local system에서 학습시키고 Fedavg를 하는 것이 가장 간단하고 직관적인 방법일 것이다. 하지만 저자는 실험적으로 NeighGen의 weight를 averaging을 했을 경우 diverse한 neighborhood node가 만들어지지 않았다고 한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?export=download&amp;amp;id=1a2BnNEAHL4ZVVqzRdlwh_me5uuJ7fqjQ&quot; alt=&quot;fGen_local&quot; /&gt;&lt;/p&gt;

&lt;p&gt;따라서 NeighGen은 공통된 하나의 model을 학습시키는 것이 아니라 local한 model을 각자 학습시키는 방식을 사용하는 technique을 이용하였다. 하지만 다른 node의 정보를 동시에 이용하기 위해서 위의 수식과 같이 변경하였는데 살펴보면 앞쪽 term은 기존에 local system내에서 minimize하던 loss와 동일한데 뒤쪽 term은 다른 client에 있는 node와의 distance를 minimize하는 term이 있다. 이 loss를 계산하기 위해서는 다른 node의 정보를 받아와야 하는데 이는 Federated Learning의 세팅과 맞지 않기 때문에 각자의 model의 weight와 node representation을 server로 보내서 server에서 해당 term을 계산하고 gradient를 보내주는 방식의 trick을 이용했다고 한다. 그렇다고 해도 node representation을 server로 전송해야 하기 때문에 privacy문제가 전혀 없다고 할 수는 없을 것이고 이 부분이 이 논문의 가장 큰 limitation이라고 생각한다. 설명한 부분에 관한 수도 코드는 아래와 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?export=download&amp;amp;id=1IH_yPZmVmgwwI_AsGckqN7Xgovq5iDZH&quot; alt=&quot;fedsage+&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;3-experiments&quot;&gt;3. Experiments&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?export=download&amp;amp;id=1XHx8BVUdSfdgpmCiKqr77BZjUZyVqweU&quot; alt=&quot;statistics&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 논문에서는 Subgraph Federated Learning실험을 위해 전체 graph를 Louvain method를 통해서 여러 partition으로 나눈 뒤 Silo 수 (Subgraph 수)인 M개 만큼 나눠서 각각의 silo에 배분하는 방식으로 실험 세팅을 하였다. Benchmark dataset인 Cora, Citeseer, Pubmed, MSAcademic을 사용하여 실험을 진행했고 $\Delta E$가 해당 갯수로 partition했을 때 발생하는 missing link개수를 의미하는데 Silo수가 많아 질수록 당연히 더 Missing link가 많아 지는 것을 확인 할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?export=download&amp;amp;id=1fasj0podN9FZ0oU5aG4BomJYim2UyNFY&quot; alt=&quot;statistics&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 Benchmark 데이터셋을 이용하여 실험한 결과 Federated Learning을 이용한 FedSage가 각각의 local model에서 GNN을 학습한 LocSage에 비해서 큰 성능 향상을 보여 Subgraph Federated Learning의 가능성을 보여줬다. 또한 FedSage+ 즉 Missing Neighbor를 만들어 준 모델이 모든 세팅에서 더 좋은 성능을 보여 해당 Missing link의 중요성을 보여줌과 함께 제안된 모델의 우수성을 입증하였다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://drive.google.com/uc?export=download&amp;amp;id=1fJa1HsP9ArosVH0a50_-By6dDtmlao7Z&quot; alt=&quot;statistics&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그리고 모델을 학습시킬 때 다른 Silo의 node 정보를 이용한 portion인 $\alpha$와 학습 시 어느정도의 node를 hide 시킬지에 대한 hyper-parameter인 h에 대한 Sensitivity anlaysis를 하였다. 위 실험을 통해 다른 silo의 정보를 적정량 가져 오는 것이 더 좋은 성능을 보여 왜 위의 loss를 fedavg하지 않고 NeighGen을 각기 학습시켰는지 보여주었다.&lt;/p&gt;

&lt;h1 id=&quot;4-conlcusion&quot;&gt;4. Conlcusion&lt;/h1&gt;

&lt;p&gt;해당 논문을 종합해보면 &lt;strong&gt;Subgraph Federated Learning&lt;/strong&gt;의 필요성을 좋은 예시를 들어 설명해주었고 그에 따라 Missing Link라는 Graph domain만의 문제가 발생할 수 있다는 점을 제기하였다. 이를 해결하기 위해 Neighborhood generator라는 방법론을 제안하였는데 method적으로 아주 획기적인 논문은 아니였지만 연구를 할 때 새로운 문제를 잘 정의해보고 실제로 그런지 실험을 통해서 잘 보여주는 것이 중요하다는 교훈을 주는 논문이라고 생각한다.&lt;/p&gt;
</description>
            <pubDate>Thu, 20 Apr 2023 00:00:00 +0900</pubDate>
            <link>http://localhost:4000/Subraph_Federated_Learning_with_Missing_Neighbor_Generation.html</link>
            <guid isPermaLink="true">http://localhost:4000/Subraph_Federated_Learning_with_Missing_Neighbor_Generation.html</guid>
            
            <category>reviews</category>
            
            
        </item>
        
        <item>
            <title>[ICML 2022] Structure-Aware Transformer for Graph Representation Learning</title>
            <description>&lt;h1 id=&quot;structure-aware-transformer-for-graph-representation-learning&quot;&gt;&lt;strong&gt;**Structure-Aware Transformer for Graph Representation Learning&lt;/strong&gt;**&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Background before reading this review.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;*Notation&lt;/p&gt;

&lt;p&gt;$G = (V, E, \mathbf X)$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;node $u \in V$&lt;/li&gt;
  &lt;li&gt;node attribute $x_u \in \mathcal X \subset \mathbb R^d$&lt;/li&gt;
  &lt;li&gt;$\mathbf X \in \mathbb R^{n \times d}$&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Transformers on Graph&lt;/p&gt;

    &lt;p&gt;GNN에서는 그래프 구조를 explicit하게 활용하는 반면, transformer는 노드의 attribute를 활용하여 노드들 사이 relation을 나타내는데 활용&lt;/p&gt;

    &lt;p&gt;Transformer 구성 요소&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;Self-attention module
        &lt;ul&gt;
          &lt;li&gt;input node feature $\mathbf X$가 linear projection을 통해 Query($\mathbf Q$), Key($\mathbf K$), Value($\mathbf V$)로 투영되고, 이를 활용하여 self-attention을 계산할 수 있음&lt;/li&gt;
          &lt;li&gt;multi-head attention&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;feed-forward NN
        &lt;ul&gt;
          &lt;li&gt;self-attention의 output이 skipconnection이나 FFN등을 거치면 하나의 transforemer layer를 통과하게됨&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Absolute encoding&lt;/p&gt;

    &lt;p&gt;그래프의 위치적/구조적인 representation을 input node feature에 더하거나 concatenate하여 Transformer의 input으로 사용 : Vanilla transformer의 PE&lt;/p&gt;

    &lt;p&gt;Graph Transformer에서 자주 사용되는 Positional encoding method들&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Laplacian PE&lt;/li&gt;
      &lt;li&gt;Random Walk PE&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;(-) 노드와 그 이웃들 사이의 structural similarity를 반영하지는 않음&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Self-attention and kernel smoothing&lt;/p&gt;

    &lt;p&gt;$\operatorname{Attn}\left(x_v\right)=\sum_ {u \in V} \frac{\kappa_ {\exp }\left(x_v, x_u\right)}{\sum_ {w \in V} \kappa_ {\exp }\left(x_v, x_w\right)} f\left(x_u\right), \forall v \in V$&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;linear value function $f(x) = \mathbf W_ {\mathbf V}x$&lt;/li&gt;
      &lt;li&gt;$\kappa_ {\exp }$ (non-symmetric) exponential kernel parameterized by $\mathbf W_ {\mathbf Q}, \mathbf W_ {\mathbf K}$&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;$\kappa_ {\exp }\left(x, x^{\prime}\right):=\exp \left(\left\langle\mathbf{W}_ {\mathbf{Q}} x, \mathbf{W}_ {\mathbf{K}} x^{\prime}\right\rangle / \sqrt{d_ {\text {out }}}\right)$&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;$\langle \cdot, \cdot\rangle$ : dotproduct&lt;/li&gt;
      &lt;li&gt;학습가능한 exponential kernel&lt;/li&gt;
      &lt;li&gt;(-) only position-aware, not structure-aware encoding&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;1-problem-definition&quot;&gt;&lt;strong&gt;1. Problem Definition&lt;/strong&gt;&lt;/h1&gt;

&lt;h2 id=&quot;limitations-of-gnn&quot;&gt;&lt;em&gt;&lt;strong&gt;Limitations of GNN&lt;/strong&gt;&lt;/em&gt;&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;limited expressiveness : 최대 1-WL test의 표현력을 가짐&lt;/li&gt;
  &lt;li&gt;Over-smoothing problem : GNN layer의 수가 충분히 커지면 모든 node representation이 상수로 수렴하게됨&lt;/li&gt;
  &lt;li&gt;Over-squashing problem : 그래프의 수많은 메세지들이 고정된 길이의 벡터 하나로 압축되어 발생하는 그래프 “bottleneck”으로 인해 멀리 위치한 노드의 메세지가 효율적으로 전파되지 않는 문제&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;⇒ Beyond neighborhood aggregation!&lt;/p&gt;

&lt;h2 id=&quot;transformer&quot;&gt;&lt;em&gt;&lt;strong&gt;Transformer&lt;/strong&gt;&lt;/em&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;하나의 self-attention layer를 통해 그래프내의 어떤 노드쌍이든지 그 사이의 상호작용을 확인할 수 있음&lt;/li&gt;
  &lt;li&gt;GNN과 달리 중간 계층에서 structural inductive bias가 발생하지 않아 GNN의 표현력 한계를 해결&lt;/li&gt;
  &lt;li&gt;graph structure info를 얼마나 학습하는지 input node feature에만 structural, positional info를 encoding해 넣음&lt;/li&gt;
  &lt;li&gt;노드에 대한 structural, positional info만 input node feature로 encoding하지만, 그래프 구조 자체에서 학습할 수 있는 정보의 양이 제한됨&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;💡 Goal : 그래프 데이터에 Transformer를 적절히 변형해 적용하여 그래프 구조를 잘 반영하고 높은 표현력을 가지는 Achitecture를 디자인하는 것&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&quot;2-motivation&quot;&gt;&lt;strong&gt;2. Motivation&lt;/strong&gt;&lt;/h1&gt;

&lt;h2 id=&quot;message-passing-graph-neural-networks&quot;&gt;&lt;em&gt;&lt;strong&gt;Message passing graph neural networks.&lt;/strong&gt;&lt;/em&gt;&lt;/h2&gt;

&lt;p&gt;최대 1-WL test로 제한된 표현력, over-smoothing, over-quashing&lt;/p&gt;

&lt;h2 id=&quot;limitations-of-existing-approaches&quot;&gt;&lt;em&gt;&lt;strong&gt;Limitations of existing approaches&lt;/strong&gt;&lt;/em&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;노드들 사이 positional relationship만 encoding하고, strucutral relationship을 직접 encoding하지않음
    &lt;ul&gt;
      &lt;li&gt;노드들 사이 structural similarity를 확인하기가 어렵고, 노드들 사이의 structural interaction을 모델링하는데 실패하게됨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ex.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/69068083/231114064-4063d3f0-f895-4d7c-a932-eddbeb77de34.png&quot; alt=&quot;Untitled&quot; /&gt;&lt;/p&gt;

&lt;p&gt;G1과 G2에서 최단거리를 활용한 positional encoding을 할경우 node u와 v가 다른 노드들에 대해 모두 같은 representation을 가지게되지만, 그래프의 실제 구조는 다름 → strucure aware에 실패&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;💡 Message-passing GNN과 Transformer architecture 각각의 장점을 살려 local, global info를 모두 고려하는 transformer architecture를 제안&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;contribution-of-this-paper&quot;&gt;&lt;em&gt;&lt;strong&gt;Contribution of this paper&lt;/strong&gt;&lt;/em&gt;&lt;/h2&gt;
&lt;p&gt;Q. Transformer구조에 structural info를 어떻게 인코딩할것인가?&lt;/p&gt;

&lt;p&gt;A. Structure-aware self attention를 도입한 Structre-Aware Transformer(SAT)&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;reformulate the self-attention mechanism
    &lt;ul&gt;
      &lt;li&gt;kernel smoother&lt;/li&gt;
      &lt;li&gt;원래 노드 feature에 적용하는 exponential 커널을 확장하여 각 노드가 중심인 subgraph representation을 추출하여 local structure에도 적용&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;subgraph representation들을 자동적으로 만들어내는 방법론 제안
    &lt;ul&gt;
      &lt;li&gt;이를 통해 kernel smoother가 구조적/특성적 유사성을 포착할 수 있게됨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;GNN으로 그래프의 subgraph info를 포함하는 node representation을 만들어 기존 GNN에 추가적인 구조 개선 없이도 더 높은 성능을 냄&lt;/li&gt;
  &lt;li&gt;Transformer의 성능향상이 structure-aware한 측면에서 일어난 것을 증명하고 absolute encoding이 추가된 transfoemr보다 SAT가 얼마나 interpretable한지를 보여줌&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;3-method&quot;&gt;&lt;strong&gt;3. Method&lt;/strong&gt;&lt;/h1&gt;

&lt;h2 id=&quot;structure-aware-transformer&quot;&gt;&lt;em&gt;&lt;strong&gt;Structure-Aware Transformer&lt;/strong&gt;&lt;/em&gt;&lt;/h2&gt;

&lt;h3 id=&quot;1-structure-aware-self-attention&quot;&gt;&lt;em&gt;1. &lt;strong&gt;Structure-Aware Self-attention&lt;/strong&gt;&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;position-aware한 structural encoding에 노드들 사이 structural similarity를 포함하기 위해 각 노드의 local structure에 관한 generalized kernel을 추가&lt;/p&gt;

&lt;p&gt;각 노드가 중심이되는 subgraph set을 추가함으로써 structure-aware attention은 다음과 같이 정의될 수 있음&lt;/p&gt;

&lt;p&gt;$\operatorname{SA-Attn}\left(v\right):=\sum_ {u \in V} \frac{\kappa_ {\text{graph} }\left(S_G(v), S_G(u)\right)}{\sum_ {w \in V} \kappa_ {\text{graph}}\left(S_G(v), S_G(u)\right)} f\left(x_u\right)$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$S_G(v)$ : node feature $\mathbf X$와 연관된 $v$를 중심으로하는 subgraph&lt;/li&gt;
  &lt;li&gt;$\kappa_ {\text{graph} }$ : subgraph쌍을 비교하는 kernel&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;⇒ attribute &amp;amp; structural similarity 모두 표현 가능한 expressive node representation 생성 → table 1&lt;/p&gt;

&lt;p&gt;⇒ 동일한 subgraph 구조를 가지는 경우에만 permutation equivariant한 성질을 갖게됨&lt;/p&gt;

&lt;p&gt;$\kappa_ {\text {graph }}\left(S_G(v), S_G(u)\right)=\kappa_ {\exp }(\varphi(v, G), \varphi(u, G))$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\varphi(v, G)$ : feature $\mathbf X$를 가지는 node $v$가 중심에 있는 subgraph의 vector representation을 만들어내는 structure extractor
    &lt;ul&gt;
      &lt;li&gt;GNN이나 differentiable Graph kernel등 subgraph의 representation을 만들 수 있는 어느 모델이든 될 수 있음&lt;/li&gt;
      &lt;li&gt;Task/data 특성에 따라 Edge attribute을 활용할 필요가 있는 경우 그에 맞는GNN을 선택하면 됨. 따로 edge attribute을 활용하지는 않고 subgraph extractor에서 활용&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;k-subtree GNN extractor.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;$\varphi(u, G) = \operatorname{GNN}_G^{(k)}(u)$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;node u에서 시작하는 k-subtree structure의 representation생성&lt;/li&gt;
  &lt;li&gt;at most 1-WL test&lt;/li&gt;
  &lt;li&gt;작은 k 값이더라도 over-smoothing, over-squashing issue없이 좋은 성능을 내는것을 확인&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;k-subgraph GNN extractor.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;$\varphi(u, G) = \sum_ {v \in \mathcal N_k(u)} \operatorname{GNN}_G^{(k)}(v)$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;node u의 representation만을 사용하는데서 나아가 node u가 중심이 되는 k-hop subgraph전체의 representation을 생성하고 활용&lt;/li&gt;
  &lt;li&gt;node u 의 k-hop이웃 $\mathcal N_k(u)$에 대해 각 노드에 GNN을 적용한 node representation을 pooling(논문에서는 summation)&lt;/li&gt;
  &lt;li&gt;More powerful than 1-WL test&lt;/li&gt;
  &lt;li&gt;original node representation과의 concatenation을 통ㅎ structural similarity뿐만 아니라 attributed similarity도 반영&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Other structure extractors.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;directly learn a number of “hidden graphs” as the “anchor subgraphs” to represent subgraphs&lt;/li&gt;
  &lt;li&gt;domain-specific GNNs&lt;/li&gt;
  &lt;li&gt;non-parametric graph-kernel&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-structure-aware-transformer&quot;&gt;&lt;em&gt;2. Structure-Aware Transformer&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/69068083/231114106-a71006e8-a9e5-44cb-b353-578ec4e09a80.png&quot; alt=&quot;Untitled&quot; /&gt;&lt;/p&gt;

&lt;p&gt;self-attention→ skipconnection → normalization layer → FFN → normalization layer&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Augmentation on skip connection.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;$x’_v = x_c +1/ \sqrt {d_v} \operatorname{SA-Attn}\left(v\right)$&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$d_v$ : node $v$의 degree&lt;/li&gt;
  &lt;li&gt;degree factor를 포함하여 연결이 많은 graph component들이 압도적인 영향을 미치지 않도록함&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;*graph-level task를 진행해야 할 경우 input graph에 다른 노드와의 connectivity없이 virtual &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[cls] &lt;/code&gt;node를 추가하거나, node-level representation을 sum/average 등으로 aggregation&lt;/p&gt;

&lt;h3 id=&quot;3-combination-with-absolute-encoding&quot;&gt;&lt;em&gt;3. Combination with Absolute Encoding&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;위의 structure aware self-attention에 추가로 absolute encoding을 추가하게 되면 postion-aware한 특성이 추가되어 기존의 정보를 보완하는 역할을 하게된다. 이러한 조합을 통해 성능향상을 확인할 수 있었다.&lt;/p&gt;

&lt;p&gt;RandomWalk PE&lt;/p&gt;

&lt;p&gt;Absolute PE만 사용할 경우 structural bias가 과도하게 발생하지 않아서 누개의 노드가 유사한 local structure를 갖고 있더라도 비슷한 node representation이 생성되는것을 보장하기 어렵다!&lt;/p&gt;

&lt;p&gt;→ Structural, positional sign으로 주로 사용되는 distance나 Laplacian-based positional representation이 노드들 사이의 structural simialrity를 포함하지 않기때문&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;📌 Structural aware attenrion은 inductive bias가 더 강하더라도 노드의 strucutral similarity를 측정하는데 적합하여 유사한 subgraph구조를 가진 노드들이 비슷한 embedding을 갖게하고, expressivity가 향상되어 좋은 성능을 보임&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;4-expressivity-analysis&quot;&gt;&lt;em&gt;4. Expressivity Analysis&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;SAT에서는 각노드를 중심으로하는 k-subgraph GNN extractor가 도입되어 적어도 subgraph representation만큼은 expressive하다는 것을 보장&lt;/p&gt;

&lt;h1 id=&quot;4-experiment&quot;&gt;&lt;strong&gt;4. Experiment&lt;/strong&gt;&lt;/h1&gt;

&lt;h3 id=&quot;experiment-setup&quot;&gt;&lt;em&gt;&lt;strong&gt;Experiment setup&lt;/strong&gt;&lt;/em&gt;&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Dataset&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ZINC&lt;/li&gt;
  &lt;li&gt;CLUSTER&lt;/li&gt;
  &lt;li&gt;PATTERN&lt;/li&gt;
  &lt;li&gt;OGBG-PPA&lt;/li&gt;
  &lt;li&gt;OGBG-CODE2&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Baseline&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;&lt;strong&gt;GNNs&lt;/strong&gt;&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;GCN&lt;/li&gt;
      &lt;li&gt;GraphSAGE&lt;/li&gt;
      &lt;li&gt;GAT&lt;/li&gt;
      &lt;li&gt;GIN&lt;/li&gt;
      &lt;li&gt;PNA&lt;/li&gt;
      &lt;li&gt;Deeper GCN&lt;/li&gt;
      &lt;li&gt;ExpC&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;&lt;strong&gt;Transformers&lt;/strong&gt;&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Original Transformer with RWPE&lt;/li&gt;
      &lt;li&gt;Graph Transformer&lt;/li&gt;
      &lt;li&gt;SAN&lt;/li&gt;
      &lt;li&gt;Graphormer&lt;/li&gt;
      &lt;li&gt;GraphTrans&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;results&quot;&gt;&lt;em&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/em&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Table1.&lt;/strong&gt; SAT와 graph regression, classification task의 sota모델과 비교&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ZINC dataset의 경우 작을수록 더 좋은 성능을 의미하는 MAE(Mean Absolute Error), CLUSTER와 PATTERN의 경우 높을수록 더 좋은 성능을 의미하는 Acurracy가 평가지표로 사용되었음.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/69068083/231114155-056893f6-8d16-4a59-b43b-62c76fd482a3.png&quot; alt=&quot;Untitled&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Table2.&lt;/strong&gt; SAT와 OGB데이터셋에서의 sota모델 비교&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;OGB dataset의 경우 높을수록 더 좋은 성능을 의미하는 Acurracy, F1 score가 평가지표로 사용되었음.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/69068083/231114185-23daa0d6-bc32-4838-93e8-0a6d09a17f7e.png&quot; alt=&quot;Untitled&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Table3.&lt;/strong&gt; structure extractor로 사용한 GNN과의 성능비교. Sparse GNN을 모든 경우에서 outperform하는 것을 확인할 수 있음&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/69068083/231114223-e6e32dfd-039b-4caa-b123-14e72e9fc867.png&quot; alt=&quot;Untitled&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Fig3.&lt;/strong&gt; ZINC데이터셋에 SAT의 다양한 variant실험&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;평가지표 : MAE(더 작은 지표가 좋은 성능을 의미)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/69068083/231114263-2ea26465-c8b3-4df8-b7d4-4d329d41d97b.png&quot; alt=&quot;Untitled&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;structure extractor에서의 k의 영향 비교
    &lt;ul&gt;
      &lt;li&gt;k=0일때, Absolute encoding만을 활용하는 vanilla transformer랑 같다고 볼 수 있음&lt;/li&gt;
      &lt;li&gt;k=3일때, optimal performance를 보임을 확인&lt;/li&gt;
      &lt;li&gt;k=4를 넘어서면 성능이 악화되는것을 확인할 수 있었는데, 이는 GNN에서의 알려진 사실인 더 적은 수의 layer를 가지는 network가 더 좋은 성능을 보이는 것과 마찬가지라고 할 수 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Absolute encoding의 영향 비교
    &lt;ul&gt;
      &lt;li&gt;RWPE vs. Laplacian PE&lt;/li&gt;
      &lt;li&gt;Structure-aware attention의 도입으로 인한 성능향상보다는 그 정도가 낮았지만, RWPE를 도입할 경우 성능이 더 좋은것으로 보았을 때, 두가지 encoding이 상호보완적인 역할을 한다고 해석할 수 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Readout method의 영향 비교
    &lt;ul&gt;
      &lt;li&gt;node-level representation을 aggregate할 때 사용하기 위한 readoutd으로 mean과 sum을 비교하였음&lt;/li&gt;
      &lt;li&gt;추가로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[CLS]&lt;/code&gt; 토큰을 통해 graph-level 정보를 pooling하는 방법도 같이 비교하여보았음&lt;/li&gt;
      &lt;li&gt;GNN에서는 readout method의 영향이 매우 컸지만 SAT에서는 매우 약한 영향만을 확인함.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;5-conclusion&quot;&gt;&lt;strong&gt;5. Conclusion&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Strong Points.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;structural info를 graphormer에서처럼 휴리스틱하게 shortest path distance(SPD)를 활용하지 않고, 그러한 local info를 잘 배우는 GNN으로 대체한 점이 novel하다고 할 수 있음&lt;/p&gt;

&lt;p&gt;Transformer의 global receptive field 특성과 GNN의 local structure특성이 상호보완적&lt;/p&gt;

&lt;p&gt;encoding에 있어서도&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;RWPE를 통한 positional encoding&lt;/li&gt;
  &lt;li&gt;k-subtree/subgraph GNN을 통한 structure-aware attention&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;두가지가 상호보완적인 역할을 함&lt;/p&gt;

&lt;p&gt;→ 각자가 잘 배우는 특성을 고려하여 상호보완적인 두가지 방법론을 잘 섞어서 좋은 성능을 내었고, 그 이유가 납득하기 쉬움&lt;/p&gt;

&lt;p&gt;&lt;em&gt;&lt;strong&gt;Weak Points.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;그래프데이터에 Transformer를 적용한 다른 논문의 architecture인 Graphormer에서 사용한 SPD만의 장점 : 직접적으로 연결되어있지 않은, 아주 멀리에 위치한 노드쌍이더라도 shortest path상의 weighted edge aggregation을 하는 만큼 그러한 특성 반영되면 좋은 그래프 구조/ 데이터셋에서는 SAT가 capture하지 못하는 부분이 있을 것&lt;/p&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;author-information&quot;&gt;&lt;strong&gt;Author Information&lt;/strong&gt;&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Dexiong Chen&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Department of Biosystems Science and Engineering, ETH Zurich, Switzerland.&lt;/li&gt;
      &lt;li&gt;SIB Swiss Institute of Bioinformatics, Switzerland.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Leslie O’Bray&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Department of Biosystems Science and Engineering, ETH Zurich, Switzerland.&lt;/li&gt;
      &lt;li&gt;SIB Swiss Institute of Bioinformatics, Switzerland.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Karsten Borgwardt&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Department of Biosystems Science and Engineering, ETH Zurich, Switzerland.&lt;/li&gt;
      &lt;li&gt;SIB Swiss Institute of Bioinformatics, Switzerland.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;6-reference--additional-materials&quot;&gt;&lt;strong&gt;6. Reference &amp;amp; Additional materials&lt;/strong&gt;&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Github Implementation : &lt;a href=&quot;https://github.com/BorgwardtLab/SAT&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://github.com/BorgwardtLab/SAT&quot;&gt;https://github.com/BorgwardtLab/SAT&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Reference : &lt;a href=&quot;https://arxiv.org/abs/2202.03036&quot;&gt;Structure-Aware Transformer for Graph Representation Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
            <pubDate>Thu, 20 Apr 2023 00:00:00 +0900</pubDate>
            <link>http://localhost:4000/Structure_Aware_Transformer_for_Graph_Representation_Learning.html</link>
            <guid isPermaLink="true">http://localhost:4000/Structure_Aware_Transformer_for_Graph_Representation_Learning.html</guid>
            
            <category>reviews</category>
            
            
        </item>
        
        <item>
            <title>[T-ITS 2021] Spatio-Temporal Knowledge Transfer for Urban Crowd Flow Prediction via Deep Attentive Adaptation Networks</title>
            <description>&lt;h1 id=&quot;tits-21spatio-temporal_knowledge_transfer_for_urban_crowd_flow_prediction_via_deep_attentive_adaptation_networks&quot;&gt;[TITS-21]Spatio-Temporal_Knowledge_Transfer_for_Urban_Crowd_Flow_Prediction_via_Deep_Attentive_Adaptation_Networks&lt;/h1&gt;

&lt;h1 id=&quot;0-overview&quot;&gt;0. Overview&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Title : Spatio-Temporal Knowledge Transfer for Urban Crowd Flow Prediction via Deep Attentive Adaptation Networks&lt;/li&gt;
  &lt;li&gt;Authors : Senzhang Wang, Hao Miao, Jiyue Li, Jiannong Cao&lt;/li&gt;
  &lt;li&gt;Year : 2021&lt;/li&gt;
  &lt;li&gt;Publish : TITS (IEEE Transactions on Intelligent Transportation Systems)&lt;/li&gt;
&lt;/ul&gt;

&lt;aside&gt;
💡 **How to transfer spatio-temporal knowledge well, between different two domains?**

&lt;/aside&gt;

&lt;aside&gt;
💡 **We propose the ST-DAAN framework : ConvLSTM + DAN + Attention**

&lt;/aside&gt;

&lt;h1 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h1&gt;

&lt;h2 id=&quot;1-why-do-we-need-it&quot;&gt;1) Why do we need it?&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Deep learning이 다양한 spatio-temporal(시공간) prediction task에 사용되고 있음
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://ojs.aaai.org/index.php/AAAI/article/view/10735&quot;&gt;ST-ResNet(2017, Cit. 1606)&lt;/a&gt; : forecast crowds inflow &amp;amp; outflow in each region of a city&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1803.01254&quot;&gt;STDN(2018, Cit. 521)&lt;/a&gt; : road network based traffic prediction&lt;/li&gt;
      &lt;li&gt;predict passenger pickup/demand demands (Attention+ConvLSTM)&lt;/li&gt;
      &lt;li&gt;DeepTransport : predict the traffic data within a transport network (CNN+RNN)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;aside&gt;
🥲 **하지만, 현실에서 시공간 정보는 그리 풍부하지 않음 → DL 쉽게 적용할 수 없음**

&lt;/aside&gt;

&lt;aside&gt;
🥲 **더불어 앞서 언급한 모델들 = 다른 시공간 정보에도 적용할 만큼 General 하지 않음**

&lt;/aside&gt;

&lt;ul&gt;
  &lt;li&gt;최근에는 transfer learning을 사용해 상기 문제를 풀어보고자 했음
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1802.00386&quot;&gt;RegionTrans(2019, Cit. 88)&lt;/a&gt; : source, target city의 비슷한 지역을 매칭 → 이 작업 하려면 other service data가 또 필요 (data 관점 = region level)&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1901.08518&quot;&gt;MetaST(2019, Cit. 166)&lt;/a&gt; : 여러 도시의 장기적 추세를 뽑아내서 target city에 써보자 → 이걸 automatically 해주는 통합 모델은 없음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;우리는 data 관점 = distribution 수정하고, unified framework를 만들어보겠다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-related-works--core-things&quot;&gt;2) Related works &amp;amp; Core things&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Urban Crowd Flow Prediction : 도시/교통 분야의 큰 주제. 전통적으로는 ARIMA 같은 통계 based methods를 주로 사용했으나, 최근에는 DL methods가 많이 쓰이는 편
    &lt;ul&gt;
      &lt;li&gt;DNN, ST-ResNet, SeqST-GAN, ConvLSTM, MT-ASTN, DCRNN, RegionTrans, MetaST 등&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Transfer Learning : ML의 scarce labeled data problem을 해결하기 위해 제시된 방법론
    &lt;ul&gt;
      &lt;li&gt;TCA, TLDA, JAN, JMMD 등&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1502.02791&quot;&gt;DAN(2015, Cit. 4413)&lt;/a&gt; : CNN을 domain adaptation task에 맞게 일반화, 컴퓨터 비전 분야에서 큰 성공
    &lt;ul&gt;
      &lt;li&gt;Neural Net이 general feature 잘 잡아내고 성능 좋다만, labeled data 별로 없는 target domain에 바로 CNN 쓰니 문제가 많음&lt;/li&gt;
      &lt;li&gt;실제로 &lt;a href=&quot;https://arxiv.org/abs/1411.1792&quot;&gt;Yosinski et al.(2014, Cit. 8740)&lt;/a&gt; 보니 Conv 1-3까진 OK, Conv 4-5부터 이상해지더니, FC 6-8에선 완전히 메롱&lt;/li&gt;
      &lt;li&gt;DAN 저자들은 Conv 1-3은 그대로 두고(freeze), Conv 4-5 단계에 fine-tuning 적용, FC 6-8은 CNN parameter optimizing에 multi-kernel MMD를 regularizer로 넣는 식으로 개선
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1207.6076&quot;&gt;Sejdinovic et al.(2013, Cit. 610)&lt;/a&gt; : two samples의 distribution이 같은지 평가할 만한 통계량으로 MMD(Maximum Mean Discrepancies)를 제시한 바 있음&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;요약하면 CNN parameter를 찾되, FC-layers 단에서 만들어지는 source와 target의 hidden representation이 비슷해지도록 추가 제한을 설정한 것&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1506.04214&quot;&gt;ConvLSTM(2015, Cit. 6876)&lt;/a&gt; : 기존 Fully Connected LSTM은 1차원 time-series → 공간정보(row, column)을 넣어서 3차원 데이터를 다루도록 확장
    &lt;ul&gt;
      &lt;li&gt;홍콩 기상청에서 radar echo images로 강수 예보를 하려니, 기존 LSTM으론 공간성을 담아낼 수 없어선지 성능이 안 좋더라 → image를 LSTM에 넣기 전 CNN으로 초벌구이하는 방식을 제안&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-formulationss&quot;&gt;3) Formulationss&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Spatio-Temporal Data : 2차원 공간 상에서 기록되는, 시간에 따라 변하는 feature를 말한다. 따라서 단일 feature라면 기본적으로 3차원 데이터.&lt;/li&gt;
  &lt;li&gt;본 논문에서는 서로 다른 지역에서 만들어진 데이터를 다루며, 이들을 같은 수의 grid cell로 나눠 작업한다.
    &lt;ul&gt;
      &lt;li&gt;서울, 대전, 뉴욕, … 도시의 크기/형태는 제각각이지만 cell 수가 같도록 격자를 만들어준다.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/67723054/233354355-c106f23c-6012-48d2-8204-c7e78d49f7cd.jpg&quot; alt=&quot;데이터가 cover하는 공간을 m*n개의 grid cell로 나눈다. each cell region이 t시점에 갖는 정보(교통량, 강수 등)가 있을 텐데, 이들이 어떤 값을 갖는지 표현한 게 spatio-temporal image (matrix)라 한다.&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;데이터가 cover하는 공간을 m*n개의 grid cell로 나눈다. each cell region이 t시점에 갖는 정보(교통량, 강수 등)가 있을 텐데, 이들이 어떤 값을 갖는지 표현한 게 spatio-temporal image (matrix)라 한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;격자 형태 matrix를 image라 할 때, 매 시점마다 기록된 image들의 time-series를 모으면 3차원 tensor가 된다.
    &lt;ul&gt;
      &lt;li&gt;서울의 따릉이 통행량(a feature)을 열두 시간쯤 관찰했다면, 해당 데이터는 아래와 같은 spatio-temporal tensor로 묘사할 수 있겠다.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/67723054/233354358-d21b52f2-a3bc-4403-98c5-fd7aeaa93a10.jpg&quot; alt=&quot;image는 시간에 따라 변하며, t시점 기준으로 과거 k개 image를 축적하면, 위와 같은 3차원 tensor를 얻을 수 있다. 이 tensor가 앞으로 전개할 논리의 기본 단위로 자주 쓰인다.&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;image는 시간에 따라 변하며, t시점 기준으로 과거 k개 image를 축적하면, 위와 같은 3차원 tensor를 얻을 수 있다. 이 tensor가 앞으로 전개할 논리의 기본 단위로 자주 쓰인다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;tensor들은 최상단(latest) image를 기준으로 추려낸 최근 k개 images인 셈인데, 이 같은 뭉치를 1-step after 마다 계속 뽑아낸다면, 해당 tensors로 어떤 4차원 리스트를 만들 수 있겠다.
    &lt;ul&gt;
      &lt;li&gt;List with parameters : Row(m) * Column(n) * Accumulation(k) * Time-stamp(t)&lt;/li&gt;
      &lt;li&gt;이 리스트를 tensor set, 길이를 ‘L’이라 하자.&lt;/li&gt;
      &lt;li&gt;데이터가 많은(장기간) domain에서는 집합이 길쭉하게, 반대로 데이터가 부족한 domain에서는 짤막한 집합이 나온다.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/67723054/233354364-0c50754c-04c4-4625-92a9-8dd41f75118b.jpg&quot; alt=&quot;tensor는 정보를 의미하며, domain에 따라 정보량은 다를 테다. 예컨대 여기선 서울의 택시 승객 데이터는 나흘(최종 업데이트 기준) 정도로 길지만, 따릉이 통행량 데이터는 기껏해야 반나절쯤 돼서, 다른 domain인 택시 정보를 어떻게 잘 가져올 수 있을까 고민하게 된다. 그게 이 논문의 핵심 주제.&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;tensor는 정보를 의미하며, domain에 따라 정보량은 다를 테다. 예컨대 여기선 서울의 택시 승객 데이터는 나흘(최종 업데이트 기준) 정도로 길지만, 따릉이 통행량 데이터는 기껏해야 반나절쯤 돼서, 다른 domain인 택시 정보를 어떻게 잘 가져올 수 있을까 고민하게 된다. 그게 이 논문의 핵심 주제.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;2-main-architecture&quot;&gt;2. Main Architecture&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;기본적인 특징은 stacked ConvLSTM 으로 잡아내며, 만들어진 hidden state에 DAN(generalized CNN), 마지막엔 Global Attention 적용 &amp;amp; 기타 features 추가하는 구성이다&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/67723054/233354374-0e4af3ed-40d4-4893-afe7-c0818881f20c.jpg&quot; alt=&quot;논문의 main figure. 크게 1) ConvLSTM, 2) CNN with MMD (DAN), 3) Global spatial attention 구간으로 나뉜다.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;논문의 main figure. 크게 1) ConvLSTM, 2) CNN with MMD (DAN), 3) Global spatial attention 구간으로 나뉜다.&lt;/p&gt;

&lt;h2 id=&quot;1-representaion-learning-convlstm&quot;&gt;1) Representaion Learning (ConvLSTM)&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/67723054/233354368-a5edfec0-af04-4a55-9c56-b00429ccf303.jpg&quot; alt=&quot;convLSTM(CNN+LSTM) 과정을 거쳐 spatio-temporal image tensor set이 4차원 hidden tensor set ‘H’로 변한다. H는 이후 3D Convolution with MMD을 통과해 feature tensor set ‘F’가 된다. 파란색, 살구색 tensor의 경우 CNN을 거쳐 나오는 차원의 수가 불명확해 ?로 적어두었다. (최종 output인 F에선 다시 3*4*12로 맞춰지는 듯하다.)&quot; /&gt;&lt;/p&gt;

&lt;p&gt;convLSTM(CNN+LSTM) 과정을 거쳐 spatio-temporal image tensor set이 4차원 hidden tensor set ‘H’로 변한다. H는 이후 3D Convolution with MMD을 통과해 feature tensor set ‘F’가 된다. 파란색, 살구색 tensor의 경우 CNN을 거쳐 나오는 차원의 수가 불명확해 ?로 적어두었다. (최종 output인 F에선 다시 3&lt;em&gt;4&lt;/em&gt;12로 맞춰지는 듯하다.)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Input = Tensor set(4D) 이지만, 작업은 매 image(2D) 마다 진행 → 한 장씩 CNN을 거쳐 새로운 tensor set을 만들어 낼 수 있음 → 다시 LSTM의 Input gate에 투입 + 이전 hidden state tensor set과 결합 + … (마찬가지로 2D 단위로 진행) → 반복&lt;/li&gt;
  &lt;li&gt;모든 stacked LSTM을 통과해 만들어진 최종 결과물을 ‘H’라 하자&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-knowledge-transfer-dan&quot;&gt;2) Knowledge Transfer (DAN)&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;two different domains’ distributions이 얼마나 다른지, distance로 평가한 것을 MMD라 한다.&lt;/li&gt;
  &lt;li&gt;도메인 별로 hidden state에 CNN을 적용하되, CNN layer 마다 mmd loss를 산출해 평균을 낸다.&lt;/li&gt;
  &lt;li&gt;Parameter set &lt;strong&gt;Θ&lt;/strong&gt; = argmin Loss Function of (GT vs ConvLSTM &amp;amp; CNN &amp;amp; mmd_loss &amp;amp; … )&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-global-spatial-attention&quot;&gt;3) Global Spatial Attention&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;local spatial correlations는 CNN 단계에서 잡히지만, 보다 넓은 범위에서 geographical dependencies는 잘 포착되지 않는다.
    &lt;ul&gt;
      &lt;li&gt;지리상으로는 멀리 떨어진 두 지역이 유사한 Point of Interest distribution을 가지는 경우가 많다&lt;/li&gt;
      &lt;li&gt;이는 taxi-trip, crowd flow 같은 시공간 정보도 마찬가지&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;source domain 데이터를 활용할 때, attention score를 곱해서 가져오면 global relation을 체크하는 효과를 낼 수 있지 않을까&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/67723054/233354371-07961d2f-8a3e-4941-b542-c7b4a2d25b23.jpg&quot; alt=&quot;아침 홍대의 택시 승객(source)은, 같은 시각 홍대와 노원의 자전거 통행량(target)과 닮아있다. domain은 다르지만, ‘출퇴근/통학’ 이라는 요소가 저변에 깔려있음을 attention mechanism을 통해 파악하는 셈. 성수는 노원보다 홍대에 가까이 있지만, 주거/업무/학군 보단 ‘문화예술’ 지역이라 아침에 자전거 타는 사람이 적다고 해석할 수 있겠다.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;아침 홍대의 택시 승객(source)은, 같은 시각 홍대와 노원의 자전거 통행량(target)과 닮아있다. domain은 다르지만, ‘출퇴근/통학’ 이라는 요소가 저변에 깔려있음을 attention mechanism을 통해 파악하는 셈. 성수는 노원보다 홍대에 가까이 있지만, 주거/업무/학군 보단 ‘문화예술’ 지역이라 아침에 자전거 타는 사람이 적다고 해석할 수 있겠다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;구체적으로는 source domain의 2D image의 특정 부분 Region (i, j)가, target domain의 모든 m*n개 region과 얼마나 닮아있는지 체크한다
    &lt;ul&gt;
      &lt;li&gt;본 논문에서 다루는 image는 모두 같은 m*n 사이즈 grid cell로 나눠져 있으니 행렬 계산이 용이하다.&lt;/li&gt;
      &lt;li&gt;dot-product, softmax 취해서 attention matrix 만드는 등 널리 알려진 attention mechanism과 크게 다른 점은 보이지 않았다&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;3-modeling&quot;&gt;3. Modeling&lt;/h1&gt;

&lt;aside&gt;
😞 **아직 이해하지 못해서, 다음 Review에서 다뤄볼까 생각 중입니다**

&lt;/aside&gt;

&lt;h2 id=&quot;1-algorithm&quot;&gt;1) Algorithm&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/67723054/233354356-aaeed10f-eb7a-40fd-83df-02f213efb054.jpg&quot; alt=&quot;algo 1.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-real-code&quot;&gt;2) Real Code&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/MiaoHaoSunny/ST-DAAN&quot;&gt;https://github.com/MiaoHaoSunny/ST-DAAN&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;4-evaluation&quot;&gt;4. Evaluation&lt;/h1&gt;

&lt;aside&gt;
🤷‍♂️ **ST-DAAN is good enough?**

&lt;/aside&gt;

&lt;aside&gt;
🤷‍♂️ **Global Spatial Attention → Performance**

&lt;/aside&gt;

&lt;aside&gt;
🤷‍♂️ **Amount of available data in Target &amp;amp; Source domain → Performance**

&lt;/aside&gt;

&lt;aside&gt;
🤷‍♂️ **Sensitivity to model structure &amp;amp; parameters**

&lt;/aside&gt;

&lt;ul&gt;
  &lt;li&gt;과거 Taxi, Bike 데이터로 Crowd flow prediction 하는 task로 ST-DAAN 성능을 평가해보자&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/67723054/233354337-985678a7-39e6-4abb-9b23-525748e55d12.jpg&quot; alt=&quot;여러 도시에서 수집된 taxi, bike 데이터셋으로, 각각 GPS 경로, 출발/도착지, 시각, ID 등 다양한 variables로 구성돼있다. number of trips, time span을 비교하면 DIDI는 같은 택시 데이터셋인 TaxiNYC보다 data scarce 하다고 볼 수 있다.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;여러 도시에서 수집된 taxi, bike 데이터셋으로, 각각 GPS 경로, 출발/도착지, 시각, ID 등 다양한 variables로 구성돼있다. number of trips, time span을 비교하면 DIDI는 같은 택시 데이터셋인 TaxiNYC보다 data scarce 하다고 볼 수 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Intra-city(TaxiNYC → BikeNYC), Cross-city(BikeChicago → BikeNYC, DIDI → TaxiBJ) transfer case를 모두 다뤄보았다&lt;/li&gt;
  &lt;li&gt;Baseline model은 non-transfer learning, 최근의 transfer leaning based에서 고루 골랐다
    &lt;ul&gt;
      &lt;li&gt;non-transfer learning based : ARIMA, ConvLSTM, DCRNN, DeepST, ST-ResNet&lt;/li&gt;
      &lt;li&gt;transfer learning based : (위 모델들에 fine-tuning), RegionTrans, MetaST&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;1-comparison-with-baselines&quot;&gt;1) Comparison With Baselines&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;ARIMA &amp;lt; non-transfer &amp;lt; non-transfer with fine-tuning &amp;lt; transfer &amp;lt; ST-DAAN 순으로 성능 Good
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;ST-DAAN full version과 Attention &amp;amp; External features을 각각 빼본 variation을 비교해보니, 이들 역시 성능 향상에 도움이 됐음&lt;/p&gt;

        &lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/67723054/233354343-d00945bc-988a-4d10-814a-54d5daf71861.jpg&quot; alt=&quot;Intra-city, Cross-city 무관하게 ST-DAAN이 좋은 성능을 보임. nonAtt, nonExt는 각각 global spatial attention, inserting external feature을 없앤 버전의 ST-DAAN&quot; /&gt;&lt;/p&gt;

        &lt;p&gt;Intra-city, Cross-city 무관하게 ST-DAAN이 좋은 성능을 보임. nonAtt, nonExt는 각각 global spatial attention, inserting external feature을 없앤 버전의 ST-DAAN&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-effect-of-data-amount&quot;&gt;2) Effect of Data Amount&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;데이터가 많을 수록 좋긴 하더라. Source/Target 둘 다 데이터가 많으면 성능 좋음&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/67723054/233356050-9f85199f-d270-4f08-a353-48a055454b34.PNG&quot; alt=&quot;대체로 데이터 length 길수록 예측 성능이 좋아짐. 역시 다다익선&quot; /&gt;&lt;/p&gt;

&lt;p&gt;대체로 데이터 length 길수록 예측 성능이 좋아짐. 역시 다다익선&lt;/p&gt;

&lt;h2 id=&quot;3-parameter-sensitivity-analysis&quot;&gt;3) Parameter Sensitivity Analysis&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Scarce data 다루는 transfer learning, 신경망 깊게 쌓으면 오히려 overfitting 문제가 발생&lt;/li&gt;
  &lt;li&gt;Domain discrepancy에 적당한 penalty 줘야 함. 작게 주면 common knowledge가 전달되지 않고, 너무 크게 주면 only domain-specific feature만 전달됨&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/67723054/233354353-80c061bd-935e-44f6-81a1-b2835f658aa7.jpg&quot; alt=&quot;ConvLSTM, CNN 단계에서 number of layers 너무 많으면 문제, penalty hyper-parameter gamma도 적당히 설정할 필요&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ConvLSTM, CNN 단계에서 number of layers 너무 많으면 문제, penalty hyper-parameter gamma도 적당히 설정할 필요&lt;/p&gt;

&lt;h1 id=&quot;5-others&quot;&gt;5. Others&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;TaxiBJ의 crowd flows를 RegionTrans, ST-DAAN으로 예측해보았는데, 택시 많이 잡는 Rush hour에서 ST-DAAN이 RegionTrans 대비 우수 → 본 모델을 이해하는 데 도움될 만한 직관적 예시?
    &lt;ul&gt;
      &lt;li&gt;기존 모델은 time invariant, 특질을 제대로 구분하지 못하지만, ST-DAAN은 일정 부분 GT에 다가서는 모습을 보였다는 식으로 이해함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/67723054/233354351-b35fb7c7-ded5-4a75-9e53-31e43cb7e7ea.jpg&quot; alt=&quot;택시 많이 안 잡는 심야 시각에는 RegionTrans, ST-DAAN 둘 다 비슷하지만, Rush hour에선 꽤 비슷하게 capture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;택시 많이 안 잡는 심야 시각에는 RegionTrans, ST-DAAN 둘 다 비슷하지만, Rush hour에선 꽤 비슷하게 capture&lt;/p&gt;

</description>
            <pubDate>Thu, 20 Apr 2023 00:00:00 +0900</pubDate>
            <link>http://localhost:4000/Spatio_Temporal_Knowledge_Transfer_for_Urban_Crowd_Flow_Prediction_via_Deep_Attentive_Adaptation_Networks.html</link>
            <guid isPermaLink="true">http://localhost:4000/Spatio_Temporal_Knowledge_Transfer_for_Urban_Crowd_Flow_Prediction_via_Deep_Attentive_Adaptation_Networks.html</guid>
            
            <category>reviews</category>
            
            
        </item>
        
        <item>
            <title>[WWW 2021] SimGRACE: A Simple Framework for Graph Contrastive Learning without Data Augmentation</title>
            <description>&lt;h1 id=&quot;simgrace-a-simple-framework-for-graph-contrastive-learning-without-data-augmentation&quot;&gt;&lt;strong&gt;SimGRACE: A Simple Framework for Graph Contrastive Learning without Data Augmentation&lt;/strong&gt;&lt;/h1&gt;

&lt;h2 id=&quot;1-problem-definition&quot;&gt;&lt;strong&gt;1. Problem Definition&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Please write the problem definition on here&lt;/p&gt;

&lt;p&gt;Graph Contrastive learning(GCL)에서 Graph augmentation이 사용되는데 Graph의 본질적인 의미를 훼손하지 않고 진행하기가 어렵습니다. 따라서 Graph augmentaion은 GCL의 일반적인 적용의 가능성이나 효율성을 제안한다는 한계가 있습니다. 본 논문에서는 Graph Contrastive IEarning을 위한 Simple 프레임워크인 SimGRACE를 제시합니다. 이 프레임워크는 Graph Augmentation이 필요하지 않습니다.&lt;/p&gt;

&lt;h2 id=&quot;2-motivation&quot;&gt;&lt;strong&gt;2. Motivation&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;GCL은 보통 augmentaion으로 4가지 방법을 주로 사용합니다(node dropping, edge perturbation, attribute masking and subgraph). 하지만 이 4가지 방법은 모든 경우에 사용되는 것이 아니라 graph에 따라 차이가 존재합니다. 예를 들어 소셜 네티워크 graph에서는 edge perturbation이 잘되지만 생화학 분자 구조 edge를 변경하는게 분자 구조를 바꾸어 성능이 좋지않다는 것 등이 있습니다.&lt;/p&gt;

&lt;p&gt;이러한 문제를 해결하기 위해 매뉴얼에따라 augmentaion을 선택하는 trail-and-error를 이용하는 방법들이 제시되었습니다. 하지만 trial-and-error를 이용하는 방식들 여전히 GCL의 일반적인 사용과 실용성에 한계가 존재합니다.   JOAO의 경우는 자동적으로 GCL에서 augmentaion pairs를 선택하는 방법을 제시했으나 계산의 복잡도가 올라갔고 augmentaion pool을 구성한느데 인간의 사전 지식을 이용한다는 한계가 존재합니다. 따라서 본 논문에서는 &lt;strong&gt;어떻게하면 manual trial-and-errors를 사용하지 않고 복잡한 계산이나 domain 지식 또한 사용하는 않고 GCL을 사용할 수있을까&lt;/strong&gt;라는 motivation을 제시합니다. 따라서 저자는 graph augumentaion을 사용하지 않고 semantic-preserved data augmentaion을 사용하여 이를 해결하려합니다.&lt;/p&gt;

&lt;h2 id=&quot;3-contribution&quot;&gt;&lt;strong&gt;3. Contribution&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Significance&lt;/em&gt;: 기존의 GCL 방법들에 비해 일반적으로 적용가능하고 manual trail-and-errors를 사용하지 않는 새로운 GCL 방법을 제시합니다.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Framework&lt;/em&gt;: 새롭고 효율적인 framework를 제시하고 SimGRACE가 잘 작동할 수 있는 이유를 설명합니다.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Algorithm&lt;/em&gt;: GCL의 Robustness를 향상시키기 위해 AT-SimGRACE라는 새로운 알고리즘을 제시합니다. 약간의 computational overhead가 존재하지만 더 Robustness한 결과를 제시합니다.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Experiment&lt;/em&gt;: 여러 종류의 dataset에 대해 state-of-the-art 방법들과 비교해 더 뛰어나가너 경재령있는 모습을 보여줍니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-graph-contrastive-learning&quot;&gt;&lt;strong&gt;4. Graph Contrastive Learning&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;GCL은 2가지로 나눌 수 있다. 첫번째는 local과 global representation을 대조하여 encoding을 진행하는 방식이다. DGI과 InfoGraph는 graph-level representaion과 substructure-level representaion의 차이를 최대화하여 graph나 node의 representaion를 encoding한다. 보다 최근에 나온 MVGRL은 node diffusion을 수행하고 contrast learning을 이용해 graph-level과 node-level의 representaion을 얻는 것을 제안한다. 두번째는 data를 변환하는 방법으로 사용되는데 augment하고 이를 shared encoder과 projection head에 넣어 mutual information을 최대화한다. GCA는 node-level task를 위해 제시되었고 DGCL은 false negative 문제를 해결하기 위해 제시되었습니다. Graph-level에서는 GraphCL이 4가지 방법의 augmentaion을 사용하여 제시되었습니다. JOAO는 GraphCL의 manual trail-and-error의 문제를 해결하기 위해 제시되었습니다.&lt;/p&gt;

&lt;h2 id=&quot;5-method&quot;&gt;&lt;strong&gt;5. Method&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/101261577/232289501-42c61afb-f639-473e-b6e1-d8c9b8b5f164.png&quot; alt=&quot;SimGRACE&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(1) Encoder perturbation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;$h$ 와 $h^\prime$ 2개의 graph-level representaion을 추출합니다.&lt;/p&gt;

&lt;p&gt;$h=f(G;\theta),h^\prime=f(G;\theta)$&lt;/p&gt;

&lt;p&gt;$\theta$와 $\theta^\prime$은 GNN 인코더의 l번째 레이어의 weight tensor와 perturbed version이다.
$\Delta\theta_l$ 는 평균이 0이고 분포가 $\sigma^2_l$ 인 가우시안 분포에서 sampling하는 perturbation term이다. 여기서 SimGRACE는 기존의 모델들과 3가지 차별점이 있는데. (1) 모멘텀 업데이트 대신 무작위 가우시안 노이즈로 인코더를 perturbation 시킨다. (2) data augmentaion을 필요로하지 않는다. (3) graph-level representaion에 집중되어 있다.&lt;/p&gt;

&lt;p&gt;$\theta^\prime_l=\theta_l + \eta \cdot \Delta\theta_l$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(2) Projection head&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Projection head라는 non-linear transformation $g(\cdot)$을 이용하여 representaion을 다른 latent space에 매핑 시켜 성능을 향상 시킬 수 있습니다. SimGRACE는 two-layer perceptron(MLP)을 이용합니다.&lt;/p&gt;

&lt;p&gt;$z=g(h), z^\prime = g(h^\prime)$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(3) Contrastive loss&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;SimGRACE에서는 normalized temperature-scaled cross entropy loss (NT-Xent)를 사용하여 postive pairs인 $z$와 $z^\prime$을 negative pairs와 비교하여 그 차이를 줄여갑니다. 본 논문에서 N graph를 randomly sampling하여 GNN인코더를 통해 perturbed version까지 만들었습니다 따라서 2N개의 representaion이 존재하는데 미니배치에서 n번째 graph를 $z_n$라고 표현합니다. Negative pairs는 자신을 제외한 나머지 N-1개의 pertubed representaion을 통해서 나오게됩니다. 따라서 n번째 graph에 대한contrasive loss는 다음같이 나옵니다.&lt;/p&gt;

&lt;p&gt;$l_n = -log {exp(sim(z_n, z^\prime_n)\tau)\over \sum^N_{n^\prime=1,n^\prime\ne n} exp(sim(z_n,z_{n^\prime}))\tau)}$&lt;/p&gt;

&lt;p&gt;sim은 cosine similarity이고, final loss는 모든 postive pairs에 대해서 계산됩니다.&lt;/p&gt;

&lt;h3 id=&quot;at-simgrace&quot;&gt;&lt;strong&gt;AT-SimGRACE&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;GraphCL은 GNN framework를 사용하여 Robustness를 얻을 수 있음을 제시하지만 그 이유까지 제시하지는 않습니다. 또한 GraphCL은 random attack에 대해서는 Robust하지만 adversrial attack에서는 취약한 모습을 보입니다. AT-SimGRACE는 adversarial attack에 Robustness를 향상시켰습니다. 일반적인 AT Framework는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/101261577/232289517-a039b362-723c-4517-8693-95a02562e4e0.png&quot; alt=&quot;AT Framework&quot; /&gt;&lt;/p&gt;

&lt;p&gt;하지만 위의 framework는 graph contrastive learning에 바로 적용할 수 없습니다. 따라서 loss를 위에서 설명한 Contrastive loss로 대체합니다. 또한 효율성을 높히기 위해 다음과 같은 방법을 도입합니다.&lt;/p&gt;

&lt;p&gt;$\Theta$를 GNN의 weight space라고 가정하면 $\theta$를 L2 norm을 이용해 다시 정의할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/101261577/232289522-0ec02f2f-3783-434b-a0b7-da8b33eba60b.png&quot; alt=&quot;AT Framework2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/101261577/232289527-23f9ed5a-23df-4bf0-a551-9c66d0b47ed9.png&quot; alt=&quot;AT Framework3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 AT-SimGRACE는 optimization problem을 다시 정의하는데 inner maximization를 하기위해 contrastive loss를 gradient ascent 방법으로 update합니다. 이를 통해 $\theta$를 미니배치 단위로 SGD를 통해 update합니다&lt;/p&gt;

&lt;h2 id=&quot;6-experiment&quot;&gt;&lt;strong&gt;6. Experiment&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;research-question&quot;&gt;&lt;strong&gt;Research Question&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;RQ1.(Generalizability)&lt;/strong&gt;: SimGRACE는 unsupervised와 semi-supervised에서 다른 모델보다 우수한가?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;RQ2.(Transferabilitry)&lt;/strong&gt;: SimGRACE로 pre-train된 GNN이 다른 모델보다 더 나은 transferability를 보여줄 수 있는가?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;RQ3.(Robustness)&lt;/strong&gt;: AT-SimGRACE는 다양한 adversarial attack에 더 나은 성능을 발휘할 수 있는가?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;RQ4.(Efficiency)&lt;/strong&gt;: SimGRACE의 효율성은 어떻고 다른 모델에 비해 효율적인가?&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;RQ5.(Hyperparameters Sensitivity)&lt;/strong&gt;: SimGRACE가 hyperparameter에 대해 얼마나 Sensitivity한가?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;unsupervised-and-semi-supervised-learning-rq1&quot;&gt;&lt;strong&gt;Unsupervised and semi-supervised learning (RQ1)&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/101261577/232289536-9924cc76-f692-4988-87a3-94246c84aa89.png&quot; alt=&quot;Table2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/101261577/232289546-53f6a789-57c9-4a0a-8933-d8bf2ee8fac4.png&quot; alt=&quot;Table4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Table2를 보면 Unsuperviesd 경우 SimGRACE가 다른 baseline들을 능가하며 모든 dataset에서 상위 3위 안에 듭니다. 또한 Table 4를 보면 semi-superviesde task를 1%와 10%와 label에서 진행하였는데 SOTA 방법론들과 비교했을때 비슷한 성능을 보이거나 더 능가하는 모습을 보였습니다. 10% label에서 JOAO가 조금 더 나은 성능을 보이느데 JOAO의 비효율성을 생각해보면 SimGRACE의 성능 또한 우수하다 볼 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;transferability-rq2&quot;&gt;&lt;strong&gt;Transferability (RQ2)&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/101261577/232289556-a1a9f31d-e33e-45e8-8bc3-ffde2c869d4d.png&quot; alt=&quot;Table3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Pre-training의 transferability를 평가하기 위해 단백질 기능 예측에 대한 transfer learning에 대한 실험을 진행하였습니다. Table 3에 나와 있듯이 SimGRACE는 PPI dataset에서 다른 pre-training scheme에 따라 더 나은 Transferability에 대한 가능성을 보여줍니다.&lt;/p&gt;

&lt;h3 id=&quot;adversarial-robustness-rq3&quot;&gt;&lt;strong&gt;Adversarial robustness (RQ3)&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/101261577/232289574-7fc89115-24ee-439b-8d43-813cc544a57f.png&quot; alt=&quot;Table5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;RandSampling, GradArgmax와 RL-S2V에 대해 AT-SimGRACE의 Robustness를 평가했습니다. Structure2vec를 GNN 인코더를 사용하여 진행했습니다. 3가지 evasion attack에서 AT-SimGRACE는 GNN의 Robustness를 눈에 띄게 향상시켰습니다.&lt;/p&gt;

&lt;h3 id=&quot;efficiency-rq4&quot;&gt;&lt;strong&gt;Efficiency (RQ4)&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/101261577/232289577-68964294-fe00-4165-a094-3f504437c510.png&quot; alt=&quot;Table6&quot; /&gt;&lt;/p&gt;

&lt;p&gt;훈련 시간과 메모리 overhaed 측면에서 결과를 비교해본 결과 SimGRACE는 JOAOv2보다 거의 40-90배 더 빠르고 GCL보다 2.5-4배 더 빠릅니다. GCL의 traial-and-error의 시간까지 고려하면 SimGRACE의 효율성은 더 뛰어나다고 볼 수 있습니다&lt;/p&gt;

&lt;h3 id=&quot;hyper-parameters-sensitivity-analysis-rq5&quot;&gt;&lt;strong&gt;Hyper-parameters sensitivity analysis (RQ5)&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/101261577/232289581-5700dfd6-219f-4ac6-ae13-8f065ab8e54e.png&quot; alt=&quot;Figure4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Magnitude of the pertubation&lt;/strong&gt;
Figure 4에서 볼 수 있듯이 weight pertubation는 SimGRACE에서 매우 중요합니다. $\eta$에 따라 변화하는 성능을 보면 늘 높다고 좋은 결과를 보이지는 않습니다. $\eta$가 0인 경우는 가장 낮은 성능을 내는데 이는 직관적으로 옳은 결과입니다. 적잘한 $\eta$를 설정하는게 중요하다고 할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/101261577/232289585-415c1f7a-42f9-425b-a461-72bf41cb269c.png&quot; alt=&quot;Figure5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Batch-size and traing epochs&lt;/strong&gt;
Figure 5은 다양한 배치 크기와 epoch로 훈련된 결과를 나타냅니다. 일반적으로 더 큰 배치 크기와 epoch일때 좋은 성능이 보여집니다. 왜냐하면 배치 크기가 더 클 수록 더 많은 negative sample을 제공하기 때문입니다.&lt;/p&gt;

&lt;h2 id=&quot;5-conclusion&quot;&gt;&lt;strong&gt;5. Conclusion&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;SimGRACE는 기존 Graph Contarsive Learning model의 data augmentaion의 한계를 계선 시키였고, General한 사용이 가능하게 하였습니다. 또한 AT-simGRACE를 통해 Robustness도 향상되었습니다. 향후에는 (1)인코더의 perubation이 컴퓨터 비전이나 자연어 처리 부분에서 잘 활용될 수 있는지 연구해볼 필요가 있습니다. 또한 (2) Pre-train된 GNN을 여러 real-world task에 적용해볼 필요가 있습니다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;6-github&quot;&gt;&lt;strong&gt;6. Github&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Github Implementation  : https://github.com/junxia97/SimGRACE&lt;/li&gt;
&lt;/ul&gt;
</description>
            <pubDate>Thu, 20 Apr 2023 00:00:00 +0900</pubDate>
            <link>http://localhost:4000/SimGRACE_A_Simple_Framework_for_Graph_Contrastive_Learning_without_Data_Augmentation.html</link>
            <guid isPermaLink="true">http://localhost:4000/SimGRACE_A_Simple_Framework_for_Graph_Contrastive_Learning_without_Data_Augmentation.html</guid>
            
            <category>reviews</category>
            
            
        </item>
        
        <item>
            <title>[NIPS 2022] SHINE: SubHypergraph Inductive Neural nEtwork</title>
            <description>&lt;h1 id=&quot;-shine-subhypergraph-inductive-neural-network&quot;&gt;# SHINE: SubHypergraph Inductive Neural nEtwork&lt;/h1&gt;
&lt;h2 id=&quot;1-problem-definition&quot;&gt;&lt;strong&gt;1. Problem Definition&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/43372696/232283689-d231476b-740e-4938-9446-de7b4a07aa9b.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;생명체는 다양한 단백질들을 활용하여 생명유지에 필요한 여러가지 반응과 작용들을 합니다. 단백질은 여러가지 gene의 정보를 바탕으로 만들어지기에, gene에 문제가 생기면 암을 포함한 여러가지 질병이 생길 수 있습니다. 단백질과 여러가지 물질들의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;기능적 단위&lt;/code&gt;의 집합인 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pathway&lt;/code&gt;를 활용하여 단백질에 대한 효과적인 representation을 구하고, 환자의 유전 정보의 변이를 바탕으로 subgraph를 형성하여 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;환자들의 질병을 예측&lt;/code&gt;하는 모델 SHINE을 제안합니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt; &lt;br /&gt;&lt;/p&gt;
&lt;h2 id=&quot;2-motivation&quot;&gt;&lt;strong&gt;2. Motivation&lt;/strong&gt;&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Preliminary - Graph Neural Network(GNN)&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/43372696/232258261-5b1b0adb-6428-447f-84cb-30974eee4367.PNG&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그래프는 여러가지 Entity 사이의 relation이나 Interaction을 나타내는 방법중 하나로, 보통 $\mathcal{G}=(\mathbb{V},\mathbb{E})$로 나타냅니다. 여기서 $\mathbb{V}$는 node set으로 vertex들을 모은 집합이며, $\mathbb{E}$는 edge set으로 각 edge는 node들을 연결해주는 선들로 표시됩니다. 두 node 또는 Entity가 edge로 연결된 경우에는 이 둘 사이에 유사한 부분이 있거나, interaction을 한다고 봅니다. 보통 각 node와 연결된 이웃 node들의 정보를 aggregation하여 각 node의 hidden representation을 구하는 방식으로 GNN이 작동합니다. 단순히 (node 관점에서) 자기 자신과 이웃을 mean aggregation(smoothing)하는 방식의 대표적인 예시로는 GCN이 있으며, 이웃 node들에 attention을 기반으로 하는 importance 차이를 두어 aggregation하는 GAT도 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt; &lt;br /&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Preliminary - Protein-Protein Interaction graph(PPI)&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/43372696/232283637-9fe695d6-e7cb-4272-8b58-486c04d34cbe.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;단백질에 대한 representation을 잘 뽑기 위하여 흔하게 사용되는 방식중 하나로 PPI network를 사용하는 방법이 있습니다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;protein-protein interaction network(PPI)&lt;/code&gt;는 단백질들을 node로 두고 서로 interaction을 하는 단백질들을 edge로 연결하여 만들어집니다. 만들어진 network에서 GCN, GAT 그리고 heterogeneous GNN등 상황에 따라 적합한 GNN 모델을 사용하게 됩니다. 결과적으로 단백질을 나타내는 각 node는 edge로 연결되어있는 이웃 node들의 정보를 취합하여 단백질에 대한 더 좋은 representation을 얻습니다. 즉, node의 최종 representation에는 해당 단백질의 고유한 정보와 해당 단백질과 관련된 단백질의 정보가 적절히 반영되어있어서, 다양한 downstream task에 효과적으로 사용될 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt; &lt;br /&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Preliminary - Pathway&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/43372696/232283770-0b7e3a2f-7d63-40a4-8a7d-881a54bdf3f8.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;생명체 내부에서 나타나는 다양한 상호작용과 반응은 여러가지 단백질들이 복합적으로 작용하여 이루어집니다. 예를들면, 세포 소기관중 하나인 미토콘드리아에는 에너지를 만들어내기 위한 process가 있으며, 그 과정에서 다양한 단백질들이 사용됩니다. 이렇게 어떤 과정속에서 특정 기능을 하기 위해 사용되는 물질들과 그들의 반응을 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;기능 단위&lt;/code&gt;로 묶어둔 것을 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pathway&lt;/code&gt;라고 합니다. 예를 들면, 위의 그림에는 apoptosis(세포사멸)에 관여하는 pathway와 각 pathway별 단백질들이 나타나있습니다. 생명체는 손상이 생기거나 오래된 세포를 스스로 죽도록 하여 관리를 하는데 이를 apoptosis라고 합니다. 이는 세포 외부에서 오는 여러가지 신호에 의해 조절됩니다. 세포 외부에서 신호가 오면, 이를 유전자가 저장되어있는 세포핵과 기타 조직들에 signaling pathway를 경유하여 전달합니다. 위의 그림에 있는 구체적인 예시로는 이미지 왼쪽의 MAPK signaling pathway가 있습니다. 세포 외부에서 survival factor와 관련된 신호가 오면, Ras, Raf MEK1/2 그리고 EPK1/2라는 단백질들을 순차적으로 거쳐서 세포핵까지 신호를 전달하게 되며, 세포핵에서는 해당 신호에 따른 gene regulation을 하게 됩니다. 이전에 설명한 PPI의 경우에는 단백질 사이의 pairwise interaction을 기준으로 만들었기에, 해당 단백질들이 interaction하는 맥락이 나타나지 않지만, pathway를 사용하게 된다면 각 단백질이 다른 단백질들과 상호작용하는 맥락을 활용할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt; &lt;br /&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Preliminary - Hypergraph&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/43372696/232283807-f608b997-5b74-4ba9-ae6d-024a534c73ca.PNG&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그래프는 Entity 사이의 pairwise relation을 나타내는 구조였습니다. 즉, edge가 2개의 node를 연결하는 구조였습니다. 이와 다르게 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Hypegraph&lt;/code&gt;는 hyperedge가 2개 이상의 node(hypernode)를 연결합니다. 그래서 hyperedge를 3개 이상의 element(node)를 가질수 있는 set으로 해석하기도 합니다. 즉, hypergraph는 graph의 일반화된 형태로, 3개 이상의 entity가 모여서 상호작용하는 경우도 표현 가능한 구조입니다. (Hypernode의 경우 일반적으로 node라고 부르기도 합니다. 이번 리뷰에서는 편의상 node로 부르겠습니다.)&lt;/p&gt;

&lt;p&gt;이렇게 3개 이상의 entity가 모여 interaction하는 경우에는 보통 해당 entity들이 모두 모여야지만 의미가 있다고 보며 이를 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;high order correlation&lt;/code&gt;이 있다고 표현합니다. 또한  hyperedge의 subset으로 이루어진 hyperedge는 존재할 수 없다고 보거나, 만약 있더라도 전혀 다른 의미를 갖는다고 해석합니다. 따라서 hyperedge는 pairwise relation(그래프의 edge)로 분해될 수 없다고 이야기합니다. 
&lt;img src=&quot;https://user-images.githubusercontent.com/43372696/232283841-20055381-7d85-448c-8036-af0224930c59.PNG&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;실제로, 위에서 이야기한 high order correlation이 존재하는 경우가 많이 있습니다. 예를 들어서, 두 단백질이 상호작용하기 위해 보조인자가 필요한 경우가 있습니다. 보조인자가 없으면 두 단백질이 상호작용이 불가능하지만, 보조인자가 있는 경우 단백질의 3차원 구조가 변형되어 두 단백질이 상호작용 할 수 있게 됩니다. 이런 경우에는 3가지 물질이 모두 있는 경우에만 반응이 일어나기 때문에 hyperedge로 나타내는 것이 좋습니다. 해당 hyperedge를 분해하여 pairwise interaction(edge)로 나타내는 경우에는 high order correlation 특성을 나타내기 힘들며, 2개만 존재하는 경우에는 어떠한 interaction도 발생하지 않기 때문에 edge가 존재할 수 없습니다.&lt;/p&gt;

&lt;p&gt;이 논문에서 활용하는 pathway의 경우 여러가지 물질과 단백질들이 순차적으로 상호작용하지만, 수많은 pathway의 작용 순서가 아직 알려지지 않은 상태입니다. 따라서 일반적으로 각 pathway를 단백질들의 집합으로 나타내며, 순서는 나타내지 않습니다. 따라서 이 논문에서는 pathway를 활용하기 위해 hypergraph를 사용합니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Preliminary - Hypergraph Neural Network&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/43372696/232259158-11cec002-cbcd-4502-98fb-e426e2530dfc.PNG&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그래프는 entity 사이의 pairwise relation을 나타내기에 adjacency matrix를 통하여 node 사이의 연결성을 나타내었습니다. 하지만 hypergraph의 경우에는 여러가지 node들이 hyperedge에 동시에 참여하기에 다른 방식으로 연결성을 표현합니다. 이를 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Incidence matrix&lt;/code&gt;라고 하며, node와 hyperedge의 연결성을 나타냅니다. node가 특정hyperedge에 속할 경우에는 1의 값을 갖게되며, 속하지 않는 경우에는 0의 값을 갖게 됩니다. 이를 나타내면 아래와 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/37684658/233369386-0b8eab9a-8ae8-4aaa-b1d3-72f9cebaa780.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;만약 $HH^ {\top}$를 하게 되면 결국 node-node matrix가 나오게 되며, 일반적인 그래프의 adjacency matrix와 같이 node 사이의 연결성을 나타내는 matrix가 나오게 됩니다. HGNN 모델은 이를 활용하여 hypergraph에서의 학습을 합니다.
&lt;img src=&quot;https://user-images.githubusercontent.com/43372696/232259205-ac05f9e6-85f3-4841-a352-ce305873cbfb.PNG&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;HGNN 논문에서 제시하는 hypergraph laplacian은 아래의 식과 같습니다.&lt;/p&gt;

&lt;p&gt;$\Theta = D_ {v}^ {-{1}\over{2}}HW_ {e}D_ {e}^ {-1}H^ {\top}D_ {v}^ {-{1}\over{2}}$&lt;/p&gt;

&lt;p&gt;$W_ {e}$는 hyperedge의 가중치를 의미합니다. 위의 식에서 $D_ {v}$는 node degree로 각 node가 속해있는 hyperedge의 숫자를 의미하며,  $D_ {e}$는 hyperedge degree로 각 hyperedge가 갖는 node의 수를 나타냅니다. $D_ {v}^ {-{1}\over{2}}$와 $D_ {e}^ {-1}$를 곱하여 node와 edge의 degree에 의해 좌우되지 않도록 normalize를 해줍니다. 이를 aggregation 관점에서 본다면 이웃의 정보들을 평균내는 것과 같습니다. 위의 hypergraph laplacian을 활용하여 각 layer는 $activation(\Theta X W)$형태로 구성됩니다. 여기서 $X$는 node feature나 hidden representation이며 W는 학습 가능한 weight matrix입니다. 위의 그림에 나타나있듯이, $H^ {\top}D_ {v}^ {-{1}\over{2}}X$ 부분에서 각 hyperedge는 가지고 있는 node의 정보들을 aggregation하게 됩니다. 그 이후에 $W_ {e}D_ {e}^ {-1}$ 부분을 거쳐 $W_ {e}D_ {e}^ {-1}H^ {\top}D_ {v}^ {-{1}\over{2}}X$가 되면서 hyperedge의 weight와 degree normalization이 진행됩니다. 마지막으로  $D_ {v}^ {-{1}\over{2}}H$ 부분을 거쳐 $D_ {v}^ {-{1}\over{2}}HW_ {e}D_ {e}^ {-1}H^ {\top}D_ {v}^ {-{1}\over{2}}X$가 되면서 hyperedge의 정보들이 각 node들로 모이게 됩니다. 즉, 2단계(node–&amp;gt; hyperedge, hyperedge–&amp;gt;node)로 정보 전달이 이루어집니다. 그리고 학습가능한 weight $W$를 곱하며 해당 layer가 완성됩니다. 
&lt;br /&gt; &lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Motivation&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;기존의 drug discovery나 drug-drug synergy prediction 등 protein을 기반으로하는 다양한 모델들은 PPI network에서 GNN을 학습하는 방식으로 진행이 되었습니다. 하지만 이는 단백질들의 interaction의 유무만 사용하기에 해당 interaction이 어떠한 맥락에서 사용되는지에 대한 정보를 반영할 수 없었습니다. 이를 해결하기 위해 pathway라는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;기능&lt;/code&gt;위주의 정보를 hypergraph를 통하여 나타내고, dual attention 기반 hypergraph neural network를 학습하여 단백질에 대한 효과적인 representation을 학습합니다. 또한 환자의 gene 변이 정보를 subgraph로 나타내어 질병과 암의 종류를 예측하는데 성공합니다.
 &lt;br /&gt; &lt;br /&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Contribution&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;Hypergraph에서의 subgraph representation learning을 학습하는 방식을 최초로 제안합니다.&lt;/li&gt;
    &lt;li&gt;Node와 hyperedge의 representation을 같이 학습하여 pathway 사이의 correlation에 대한 분석이 가능합니다.&lt;/li&gt;
    &lt;li&gt;Genetic Medicine 분야에 적용한 최초의 논문입니다. (pathway 기반 방식으로 접근하는 모델이 bioinformatics 분야에 있기는 했으나 알고리즘 위주의 방식이며 hypergraph neural network를 이 분야에 사용한 것은 이 논문이 처음입니다.)
 &lt;br /&gt; &lt;br /&gt;
      &lt;h2 id=&quot;3-method&quot;&gt;&lt;strong&gt;3. Method&lt;/strong&gt;&lt;/h2&gt;
      &lt;p&gt;&lt;strong&gt;Notation&lt;/strong&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/43372696/232235555-f9e10fcf-af42-436c-bde9-183c6f90c840.PNG&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;해당 논문에서 사용하는 표기는 위의 표와 같습니다. 또한 Hyperedge incidence matrix에 대한 정의는 2번 section의 preliminary에 있었듯이 다음과 같이 정의합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/37684658/233369386-0b8eab9a-8ae8-4aaa-b1d3-72f9cebaa780.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Node set($\mathcal{N}$)은 gene($g_ {i}$)들로 구성되어있습니다. ($\mathcal{N} = {g_ {1}, …, g_ {\vert \mathcal{N} \vert}}$)
Hyperedge set($\mathcal{E}$)은 pathway($p_ {j}$)들로 구성되어있습니다. ($\mathcal{E} = {p_ {1}, …, p_ {\vert \mathcal{E} \vert}}$) 
&lt;br /&gt; &lt;br /&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Hypergraph - Strongly Dual Attention Message Passing&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/43372696/232225281-0aeb8d7c-edf2-4929-8905-cceeb5930ca6.PNG&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;앞서 preliminary에서 설명 드렸지만 hypergraph의 경우에는 2가지 단계를 거쳐 정보를 전달합니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;node to hyperedge aggregation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;새로운 hyperedge representation은 다음과 같이 구합니다.&lt;/p&gt;

&lt;p&gt;$h_ {E}^ {k}(p_ {j}) = \sigma(\sum_ {g_ {i} \in p_ {j}} a_ {E}( p_ {j}, g_ {i})h_ {N}^ {k-1}(g_ {i}))$&lt;/p&gt;

&lt;p&gt;여기서 $a_ {E}$는 hyperedge에 속해있는 node들에 대한 attention score로 다음과 같이 구합니다.&lt;/p&gt;

&lt;p&gt;$a_ {E}( p_ {j}, g_ {i}) = exp(c^ {\top} s(p_ {j}, g_ {i}) / (\sum_ {g_ {i^ {‘}} \in p_ {j}} c^ {\top} s(p_ {j}, g_ {i^ {‘}})))$&lt;/p&gt;

&lt;p&gt;여기서 $s(p_ {j}, g_ {i})$는 아래의 식으로 구하며 $*$는 element wise product입니다.&lt;/p&gt;

&lt;p&gt;$s(p_ {j}, g_ {i}) = LeakyReLU((W_ {N}h_ {N}^ {k-1}(g_ {i}) + b_ {N})*(W_ {E}h_ {E}^ {k-1}(p_ {j}) + b_ {E}))$&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;hyperedge to node aggregation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;새로운 node representation 학습은 전반적으로 위의 경우와 거의 동일합니다만 hyperedge와 node의 위치만 바뀝니다.&lt;/p&gt;

&lt;p&gt;$h_ {N}^ {k}(g_ {i}) = \sigma(\sum_ {p_ {j} \owns g_ {i}} a_ {N}( g_ {i}, p_ {j})h_ {E}^ {k-1}(p_ {j}))$&lt;/p&gt;

&lt;p&gt;여기서 $a_ {N}$는 node가 속해있는 hyperedge들에 대한 attention score로 다음과 같이 구합니다.&lt;/p&gt;

&lt;p&gt;$a_ {N}(  g_ {i}, p_ {j}) = exp(c^ {\top} s(p_ {j}, g_ {i}) / (\sum_ {p_ {j^ {‘}} \owns g_ {i}} c^ {\top} s(p_ {j^ {‘}}, g_ {i})))$&lt;/p&gt;

&lt;p&gt;여기서 $s(p_ {j}, g_ {i})$는 위에서(node to hyperedge aggregation) 구한것과 완벽히 동일합니다. 다른 hypergraph attention 방법들과는 다르게 두 과정에서 완전히 같은 attention matrix를 사용하며 이를 strongly dual attention이라고 저자들은 이야기합니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Regularization&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;GNN과 hypergraph Neural Network에는 공통점이 있습니다. 바로 edge/hyperedge로 연결된 node의 representation이 비슷해진다는 점입니다. 이는 graph/hypergraph 자체가 ‘비슷한 entity끼리 연결된다’는 전제를 가지기 때문입니다. 따라서 aggregation을 통해 점점 비슷해지도록 학습을 하는 것입니다. 따라서 이웃한(연결된) node의 representation이 다를 경우 같아지도록 하기위해 penalize를 할 필요가 있습니다. preliminary에서 설명드린 hypergraph laplacian을 활용하여 저자는 연결된 node끼리 비슷해지도록 regularization term을 추가로 만들었으며 바로 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;$\mathcal{L_ {reg}} = \sum_ {i,j} ((X_ {i}- X_ {j})(X_ {i}- X_ {j})^ {\top}  * \Theta_ {i,j} ) =  \sum_ {i,j} ((X_ {i}X_ {i}^ {\top} -2X_ {i}X_ {j}^ {\top} + X_ {j}X_ {j}^ {\top}) * \Theta_ {i,j} )$
&lt;br /&gt; &lt;br /&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Subgraph - Weighted Subgraph Attention&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Hypergraph Neural Network를 대략 만들었으니, 환자의 변이 gene정보로부터 subgraph를 만들어야합니다. 데이터셋으로부터 통계를 내어 $M$ matrix를 만듭니다. 해당 matrix의 row는 환자를 나타내며, column은 gene을 의미합니다. Matrix에 담겨있는 값들은 해당 환자의 특정 gene의 mutation rate을 의미합니다.&lt;/p&gt;

&lt;p&gt;데이터셋으로부터 얻은 환자의 mutation 관련 gene들을 묶어서 subhypergraph(또는 subgraph)를 만듭니다. $j$번째 환자의 subgraph를 $\mathcal{G}_ {j}$로 표기하고 $i$번째 gene을 $\mathcal{g}_ {i}$로 표기하겠습니다.&lt;/p&gt;

&lt;p&gt;각 환자(subgraph)의 representation은 다음과 같이 구합니다.&lt;/p&gt;

&lt;p&gt;$h(\mathcal{G}_ {j}) = \sigma(\sum_ {\mathcal{g}_ {i} \in \mathcal{G}_ {j}} a(\mathcal{G}_ {j}, \mathcal{g}_ {i}) h_ {N}^ {K}(\mathcal{g}_ {i}))$&lt;/p&gt;

&lt;p&gt;여기서 $a(\mathcal{G}_ {j}, \mathcal{g}_ {i})$는 attention score이며 다음과 같이 구합니다.&lt;/p&gt;

&lt;p&gt;$a(\mathcal{G}_ {j}, \mathcal{g}_ {i}) = exp(M_ {ji}b^ {\top}h_ {N}^ {K}(\mathcal{g}_ {i}) / (\sum_ {\mathcal{g}_ {i^ {‘}} \in \mathcal{G}_ {j}} exp(M_ {ji^ {‘}}b^ {\top}h_ {N}^ {K}(\mathcal{g}_ {i^ {‘}})))$&lt;/p&gt;

&lt;p&gt;이는 환자의 representation을 attention 기반으로 구하지만 여기에 환자의 gene mutation rate정보가 반영되도록 만든것입니다. 이렇게 여러 환자의 subgraph representation을 concatentation으로 합쳐서 $S$를 만듭니다.&lt;/p&gt;

&lt;p&gt;$S = [h(\mathcal{G}_ {1})^ {\top} \vert h(\mathcal{G}_ {2})^ {\top} \vert … \vert  h(\mathcal{G}_ {n})^ {\top}]^ {\top}$&lt;/p&gt;

&lt;p&gt;각 subgraph를 분류하기 위해 다음과같이 classifier를 만듭니다.&lt;/p&gt;

&lt;p&gt;$Z = softmax(W^ {(1)}(ReLU \circ FC)^ {(2)} (S)+W^ {(0)})$&lt;/p&gt;

&lt;p&gt;Loss function으로는 cross-entropy loss를 사용합니다.&lt;/p&gt;

&lt;p&gt;$\mathcal{L} = - \sum_ {j \in \mathcal{Y}_ {D}} {\sum_ {f =1}^ {F} Y_ {jf} \ln Z_ {jf}} + \mathcal{L}_ {reg}$&lt;/p&gt;

&lt;p&gt;&lt;br /&gt; &lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;4-experiment&quot;&gt;4. Experiment&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Task and Dataset&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이 연구에서는 3개의 데이터셋을 사용합니다 :  MSigDB, DisGeNet, TCGA-MC3.&lt;/p&gt;

&lt;p&gt;먼저 Molecular Signature Database(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MSigDB&lt;/code&gt;)를 사용합니다. MsigDB에는 다양한 종류의 pathway와 각 pathway에 참여하는 gene들에 대한 정보가 있습니다. 해당 데이터셋의 경우에는 전문가들에 의해 curated되어있습니다. MSigDB의 경우 단백질의 학습보다는 각 task에 사용되는 데이터셋(DisGeNet, TCGA-MC3)를 전처리하기 위하여 사용합니다. 이 데이터셋을 활용하여 기능이 알려지지 않은 gene과 겹치는 hyperedge를 제거하는 전처리를 합니다.&lt;/p&gt;

&lt;p&gt;그 다음으로는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DisGeNet&lt;/code&gt; dataset을 사용합니다. 해당 데이터는 환자의 유전자 변이와 그로 인한 질병에 대한 정보를 담고 있습니다. 해당 데이터셋을 바탕으로 hypergraph와 subgraph를 구성하게 되며 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Disease Type Prediction&lt;/code&gt; task를 위하여 사용됩니다. 해당 task는 변이된 gene 정보를 바탕으로 환자의 질병을 분류하는 것이며 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Multi class classification&lt;/code&gt; 형태입니다. 질병의 종류는 MeSH code로 정해져있으며, 데이터에 대한 split과 통계는 아래 표에 정리되어있습니다. (위에서 설명하였듯이 DisGeNet 전처리를 위해 MSigDB를 사용합니다.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/43372696/232258276-11cf92fb-da27-491d-8bb7-523c886f4c70.PNG&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막으로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TCGA-MC3&lt;/code&gt; dataset입니다. 해당 데이터는 변이된 gene에 대한 정보와 그에 해당하는 암에 대한 정보를 담고있는 방대한 규모의 데이터입니다. Gene 정보를 바탕으로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Cancer Type Prediction&lt;/code&gt;을 하는 task를 진행하며 이는 마찬가지로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Multi class classification&lt;/code&gt; 형태입니다. TCGA-MC3 dataset은 전문가들에 의하여 7가지의 mutation-calling algorithm을 적용하여 pass 표시를 하였으며, 이 논문에서도 pass 표시가 되어있는 데이터만 활용했습니다. 전처리후 데이터에 대한 통계는 아래의 표와 같습니다. (위에서 설명하였듯이 TCGA-MC3 전처리를 위해 MSigDB를 사용합니다.)
&lt;img src=&quot;https://user-images.githubusercontent.com/43372696/232258277-a495971e-5c29-4643-88ff-0b2ec7365031.PNG&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;아래는 두 task의 데이터셋마다 존재하는 hyperedge의 수와 class, subgraph 수에 대한 통계입니다. hyperedge는 pathway들이며 subgraph는 gene 변이 정보(환자 정보)를 바탕으로 하여 subgraph를 만든 것입니다. 
&lt;img src=&quot;https://user-images.githubusercontent.com/43372696/232258273-adebb37f-7283-417f-a886-d1ae42381c91.PNG&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt; &lt;br /&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Baseline Models&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HGNN&lt;/code&gt; : Hypergraph Neural Network 초기 논문중 하나로 preliminary에 설명한것과 같이 hypergraph laplacian을 제시하며 이를 활용하여 학습합니다.&lt;/li&gt;
    &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HyperGCN&lt;/code&gt; : HyperGCN의 경우 hypergraph를 graph로 변형시킨 다음에 GNN모델을 학습합니다. 매 iteration마다  hyperedge마다 가장 학습이 급한(feature difference가 큰) 두 node를 선별하여 edge로 연결하고 기존의 hyperedge를 삭제합니다. 이렇게 만들어진 graph위에서 GCN을 학습합니다.&lt;/li&gt;
    &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HyperGAT&lt;/code&gt; : text classification을 위해 만들어진 모델입니다. hyperedge를 하나의 가상의 node처럼 취급하여 GAT를 학습하였다고 생각하면 가장 간단합니다. HGNN과는 다르게 $HH^ {\top}$ 대신 hyperedge로 node들의 정보를 attention 기반으로 aggregation하고, node에서 hyperedge정보를 aggregation할때 attention을 사용합니다. SHINE 논문에서는 dual attention으로 같은 attention matrix를 사용한것과 다르게 HyperGAT에서는 두 단계에서 다른 attention matrix를 학습합니다.&lt;/li&gt;
    &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AllSetTransformer&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AllDeepSets&lt;/code&gt; :  Hypergraph Neural Network를 가장 generalize하여 나타낸 모델로 DeepSet 논문으로부터 이야기를 시작합니다. AllDeepSet의 경우 hyperedge/ node의 representation을 얻기 위해 이웃한 node/hyperedge의 representation을 MLP를 거쳐서 aggregation하며, AllSetTransformer는 여기서 attention을 추가하기 위해 Transformer를 기반으로 하여 mean aggregation을 합니다.&lt;/li&gt;
    &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SubGNN&lt;/code&gt; : hyperedge를 node처럼 생각하여 hypergraph를 2가지 type이 존재하는 graph로 봅니다.&lt;/li&gt;
    &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PRS&lt;/code&gt; : polygenic risk score의 약자로 genetic medicine 분야에서 사용하는 방법입니다.(전문 지식)&lt;/li&gt;
    &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NMF&lt;/code&gt; : Non-negative matrix factorization의 약자로 Matrix를 latent vector들로 분해하는 방식입니다.&lt;/li&gt;
    &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;XGBoost&lt;/code&gt; : end-to-end tree boosting system&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt; &lt;br /&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Disease Type Prediction &amp;amp; Cancer Type Prediction&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/43372696/232258270-faa1b12e-668a-43a5-92ea-3fd2c851ed6c.PNG&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;저자는 제안한 모델인 SHINE과 다른 baseline들을 비교하며 SHINE이 다른 모델들에 비하여 확실하게 우월한 성능을 갖고 있음을 보여줍니다. 특히 attention을 사용한 hyperGAT와 AllSetTransformer와의 비교를 통해 node to hyperedge 정보전달 과정(aggregation)과 hyperedge to node 정보전달 과정에서 같은 attention matrix를 사용하는 dual attention 기법이 효과적임을 보여주었습니다. 
&lt;br /&gt; &lt;br /&gt;&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Enrichment Analysis&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/43372696/232273370-e309f525-7444-461c-82e9-3de1f286c22f.PNG&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;저자는 SHINE이 node와 hyperedge의 representation을 같이 학습하기에 결과를 해석하거나 correlation을 분석하는데 있어서 장점이 있다고 주장했습니다. 이를 입증하기 위하여 암의 종류별로, attention 값을 기반으로 관련이 높은 pathway들을 추출했습니다. 위의 표에서 각 column이 암마다 관련있는 pathway들입니다. 첫 column은 breast cancer관련 pathway들이며 두번째 column은 lung cancer 관련 pathway들을 추출한 결과입니다. 글자의 색은 MSigDB를 구축하는 과정에서 사용한 데이터 출처이므로 실험 해석과 관련이 없습니다. 저자는 생명과학쪽 지식을 바탕으로 shine으로 추출한 암 종류별 pathway들이 실제로 해당 암과 관련이 높은것을 설명합니다. 예를 들어 breast cancer의 경우  HER2/4-1BB bispecific molecule(표의 제일 왼쪽 column, 위에서 3번째 row)의 경우 HER2-positive breast cancer의  치료 방법중 하나로 밝혀졌다고 합니다. 이외에도 appendix에서 각 pathway가 어떻게 해당 암과 관련이 있는지 생명과학/의학 지식을 활용하여 설명합니다. 이를 통해 SHINE 모델이 해석을 쉽게 할 수 있도록 도와준다는걸 보여주었습니다. 
&lt;br /&gt; &lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;5-conclusion&quot;&gt;&lt;strong&gt;5. Conclusion&lt;/strong&gt;&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Summary&lt;/strong&gt;
해다 논문은 기존의 PPI 기반 학습보다 효과적으로 기능적인 정보를 반영하기 위해 pathway라는 정보로 구축된 hypergraph를 사용하였습니다. strong dual attention을 기반으로 효과적으로 학습하였으며, subhypergraph를 classification하는 첫 논문으로, 새로운 application으로 질병과 암을 예측하는 논문입니다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt; &lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Discussion &amp;amp; Further Development&lt;/strong&gt;&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;간략한 평가 :  새로운 application에 잘 적용하였고 효과적이지만 논문의 설명 방식이 명확하지 않으며 설명이 깔끔하지는 않았습니다.&lt;/li&gt;
    &lt;li&gt;발전 방향 : pathway들이 여러 단백질의 기능적인 집합이듯이, 여러 pathway들이 모여 어떤 더 큰 기능과 작용을 구성하는것은 분명합니다. 이러한 정보를 활용하여 hypergraph in hypergraph 모델을 만들거나, pathway사이의 관계에 대한 데이터를 활용하면 더 좋은 representation을 뽑는것이 가능해보입니다.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt; &lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;author-information&quot;&gt;&lt;strong&gt;Author Information&lt;/strong&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Yoonho Lee
    &lt;ul&gt;
      &lt;li&gt;Affiliation: &lt;a href=&quot;http://dsail.kaist.ac.kr&quot;&gt;DSAIL@KAIST&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;Research Topic: Graph Nerual Network, Topological Deep Learning, Category Theory.&lt;/li&gt;
      &lt;li&gt;Contact: sml0399benbm@kaist.ac.kr
        &lt;h2 id=&quot;reference--additional-materials&quot;&gt;&lt;strong&gt;Reference &amp;amp; Additional materials&lt;/strong&gt;&lt;/h2&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Github Implementation
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/luoyuanlab/shine&quot;&gt;Code for the paper&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Reference
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1609.02907.pdf&quot;&gt;[ICLR-17] Semi-Supervised Classification with Graph Convolutional Networks&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1710.10903.pdf&quot;&gt;[ICLR-18] Graph Attention Networks&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1809.09401.pdf&quot;&gt;[AAAI-19] Hypergraph Neural Networks&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2011.00387.pdf&quot;&gt;[EMNLP-20] Be More with Less: Hypergraph Attention Networks for Inductive Text Classification&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2106.13264.pdf&quot;&gt;[ICLR-22] You are AllSet: A Multiset Function Framework for Hypergraph Neural Networks&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2210.07309.pdf&quot;&gt;[NeurIPS-22] SHINE: SubHypergraph Inductive Neural nEtwork&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
            <pubDate>Thu, 20 Apr 2023 00:00:00 +0900</pubDate>
            <link>http://localhost:4000/SHINE_SubHypergraph_Inductive_Neural_nEtwork.html</link>
            <guid isPermaLink="true">http://localhost:4000/SHINE_SubHypergraph_Inductive_Neural_nEtwork.html</guid>
            
            <category>reviews</category>
            
            
        </item>
        
    </channel>
</rss>
