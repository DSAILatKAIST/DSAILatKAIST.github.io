<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>DSAILatKAIST.github.io</title>
        <description>Intended as a documentation theme based on Jekyll for technical writers documenting software and other technical products, this theme has all the elements you would need to handle multiple products with both multi-level sidebar navigation, tags, and other documentation features.</description>
        <link>http://dsailatkaist.github.io/</link>
        <atom:link href="http://dsailatkaist.github.io/feed.xml" rel="self" type="application/rss+xml"/>
        <pubDate>Tue, 17 Oct 2023 00:21:24 +0900</pubDate>
        <lastBuildDate>Tue, 17 Oct 2023 00:21:24 +0900</lastBuildDate>
        <generator>Jekyll v3.9.2</generator>
        
        <item>
            <title>[SIGIR 2022] User-controllable Recommendation Against Filter Bubbles</title>
            <description>&lt;h1 id=&quot;sigir-22user-controllable_recommendation_against_filter_bubbles&quot;&gt;[SIGIR-22]User-controllable_Recommendation_Against_Filter_Bubbles&lt;/h1&gt;

&lt;h2 id=&quot;1-problem-definition&quot;&gt;&lt;strong&gt;1. Problem Definition&lt;/strong&gt;&lt;a href=&quot;https://dsailatkaist.github.io/template.html#1-problem-definition&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;ê³¼ê±° ëª‡ ë…„ ë™ì•ˆ ì¶”ì²œì‹œìŠ¤í…œì€ â€˜ê°œì¸í™”ëœ ì •ë³´ í•„í„°ë§â€™ì´ë¼ëŠ” ëª…ëª©ìœ¼ë¡œ ì •ë³´ì˜ ê³¼ì‰ì„ ê°ë‹¹í•˜ê¸° ì–´ë ¤ìš´ ì‚¬ìš©ìë“¤ì—ê²Œ ë¶ˆí•„ìš”í•œ ì •ë³´ë¥¼ í•„í„°ë§ í•´ì£¼ì–´ ë¹ ë¥¸ ë°œì „ì„ ê±°ë‘ì—ˆì§€ë§Œ, â€˜í•„í„°ë²„ë¸”(Filter Bubble)â€™ì— ëŒ€í•œ ë…¼ì˜ëŠ” í•­ìƒ ì´ì–´ì ¸ ì™”ì—ˆë‹¤.&lt;/p&gt;

&lt;p&gt;í•„í„°ë²„ë¸”ì´ë€, ì¶”ì²œì‹œìŠ¤í…œì´ ì‚¬ìš©ì-ì•„ì´í…œ ê°„ ìƒí˜¸ì‘ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ê¸°ì¡´ ì‚¬ìš©ìì˜ ì„ í˜¸ë„ì— ì¼ì¹˜í•˜ëŠ” ì•„ì´í…œë§Œ ê³„ì†í•´ì„œ ë…¸ì¶œì‹œí‚¤ëŠ” í˜„ìƒì„ ê°€ë¦¬í‚¨ë‹¤.  ì´ëŸ° í˜„ìƒì´ ë°˜ë³µë˜ë©´, ìœ ì‚¬í•œ ì•„ì´í…œë§Œ ë…¸ì¶œë˜ëŠ” í™•ë¥ ì´ ê³„ì†í•´ì„œ ì»¤ì§€ê²Œ ë˜ê³ , ì‚¬ìš©ìê°€ ë‹¤ì–‘í•œ ì¹´í…Œê³ ë¦¬ì˜ ì½˜í…ì¸ ë¥¼ ì ‘í•  ìˆ˜ ìˆëŠ” ê¸°íšŒê°€ ì ì  ì¤„ì–´ì§€ê²Œ ëœë‹¤.&lt;/p&gt;

&lt;p&gt;ì¦‰, ì¥ê¸°ì ì¸ ê´€ì ì—ì„œ í•„í„°ë²„ë¸”ì€ ì•„ì´í…œ í˜¹ì€ ì½˜í…ì¸ ì˜ ë‹¤ì–‘ì„±ê³¼ ì˜¤ë¦¬ì§€ë„ë¦¬í‹°ë¥¼ ì¶”ì²œì‹œìŠ¤í…œì—ì„œ ë°°ì œì‹œí‚¤ê²Œ ë˜ê³ , ì´ëŠ” í•„ì—°ì ìœ¼ë¡œ ì •ë³´ì˜ í¸ì‹ìœ¼ë¡œ ì¸í•´ ì‚¬ìš©ìë¡œ í•˜ì—¬ê¸ˆ ì™œê³¡ íš¨ê³¼ë¥¼ ë‚³ê²Œ ëœë‹¤.&lt;/p&gt;

&lt;p&gt;ë”°ë¼ì„œ, í•„í„°ë²„ë¸”ì„ ì™„í™”í•˜ëŠ” ê²ƒ ë˜í•œ ì¶”ì²œì‹œìŠ¤í…œì—ì„œ ì¤‘ìš”í•œ ê³¼ì œë¡œ ìë¦¬ë§¤ê¹€í•˜ê²Œ ë˜ì—ˆë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;2-motivation&quot;&gt;&lt;strong&gt;2. Motivation&lt;/strong&gt;&lt;a href=&quot;https://dsailatkaist.github.io/template.html#2-motivation&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;í•„í„°ë²„ë¸”ì„ ì™„í™”í•˜ëŠ” ë°©ì•ˆìœ¼ë¡œ ê¸°ì¡´ ì—°êµ¬ì—ì„œëŠ” â€˜ë‹¤ì–‘ì„±(Diversity)â€™ê³¼ ê³µì •ì„±(Fairness)â€™ì„ ë†’ì´ëŠ” ë°©ë²•ì„ ì œì•ˆí–ˆì—ˆë‹¤.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;ì¶”ì²œì—ì„œì˜ ë‹¤ì–‘ì„±:
ë‹¤ì–‘ì„±ì€ ì¶”ì²œ ëª©ë¡ì—ì„œ ìœ ì‚¬ì„±ì´ ë‹¤ë¥¸ ì•„ì´í…œì„ ìƒì„±í•˜ë„ë¡ ì¥ë ¤í•˜ëŠ” ë°©ì•ˆì´ë‹¤. ì´ëŠ” í›„ì²˜ë¦¬ ë° ì—”ë“œ-íˆ¬-ì—”ë“œ ë°©ë²•ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤. ì „ìëŠ” ëª‡ëª‡ ëª¨ë¸ì— ì˜í•´ ìƒì„±ëœ ì¶”ì²œ ëª©ë¡ì„ ë‹¤ì‹œ ìˆœìœ„ ì§€ì •ì„ í†µí•´ ë‹¤ì–‘í™”ì‹œí‚¤ë©°, í›„ìëŠ” ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡ ê³¼ì •ì—ì„œ ì •í™•ë„ì™€ ë‹¤ì–‘ì„±ì˜ ê· í˜•ì„ ë§ì¶”ëŠ” ë°©í–¥ìœ¼ë¡œ ì§„í–‰ëœë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ë°©ì‹ë“¤ ì—­ì‹œ ë‹¨ìˆœíˆ ì‚¬ìš©ìì—ê²Œ ë‹¤ì–‘í•œ ì•„ì´í…œì„ ì¶”ì²œí•œ í›„ ì‚¬ìš©ìì˜ í”¼ë“œë°±ì„ í†µí•´ ìƒˆë¡œìš´ ì•„ì´í…œ ì¹´í…Œê³ ë¦¬ë¥¼ ë°œêµ´í•´ê°€ëŠ” ê²ƒìœ¼ë¡œ ë§ì€ ì‹œê°„ì´ í•„ìš”í•˜ë‹¤. ì‹¬ì§€ì–´ ë‹¤ì–‘í•œ ì•„ì´í…œì„ ì¶”ì²œí•˜ëŠ” ë‹¨ê³„ì—ì„œ ì‚¬ìš©ì ì„ í˜¸ë„ì™€ ê´€ë ¨ ì—†ëŠ” ì•„ì´í…œì„ ë§ì´ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ì¶”ì²œì—ì„œì˜ ê³µì •ì„±:
ì¶”ì²œì‹œìŠ¤í…œì—ì„œ ê³µì •ì„±ì„ ë‹¬ì„±í•˜ê¸° ìœ„í•´ì„œëŠ” ë‹¤ì–‘í•œ ì‚¬ìš©ì ê·¸ë£¹ ë˜ëŠ” ì•„ì´í…œ ì¹´í…Œê³ ë¦¬ ê°„ì— ê· í˜•ì„ ë§ì¶”ì–´ì•¼ í•œë‹¤. ì´ê²ƒì€ íŠ¹ì • ê·¸ë£¹ì—ê²Œ ë” ë§ì€ ì¶”ì²œì„ í•˜ë„ë¡ ì¡°ì ˆí•˜ê±°ë‚˜ íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ì•„ì´í…œì„ ë‹¤ë¥¸ ê²ƒë³´ë‹¤ ìì£¼ í¬í•¨ì‹œí‚¤ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ì—¬ í•„í„°ë²„ë¸”ì„ ì–´ëŠ ì •ë„ ì™„í™”í•  ìˆ˜ ìˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ë ‡ê²Œ ê· í˜•ì„ ë§ì¶”ëŠ” ê³¼ì •ì—ì„œ, ì¼ë¶€ ì‚¬ìš©ì ë˜ëŠ” ì•„ì´í…œ ê·¸ë£¹ì— ëŒ€í•œ ì¶”ì²œ ì •í™•ì„±ì„ í¬ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë§Œì•½ íŠ¹ì • ì‚¬ìš©ì ê·¸ë£¹ì—ê²Œ ë” ë§ì€ ê³µì •ì„±ì„ ë¶€ì—¬í•˜ë ¤ë©´ ê·¸ ê·¸ë£¹ì— ëŒ€í•œ ì¶”ì²œ ëª©ë¡ì—ì„œ ë‹¤ë¥¸ ì‚¬ìš©ì ê·¸ë£¹ì˜ ì„ í˜¸ë¥¼ ë¬´ì‹œí•˜ê³  ê·¸ ê·¸ë£¹ì— ë§ì¶°ì•¼ í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ê·¸ ê·¸ë£¹ì— ëŒ€í•œ ì •í™•í•œ ì¶”ì²œì´ í¬ìƒë˜ê³  ì‚¬ìš©ì ê²½í—˜ì´ ì €í•˜ë  ìˆ˜ ìˆë‹¤.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ì´ë ‡ë“¯ ê¸°ì¡´ ì ‘ê·¼ ë°©ì‹ì€ ë‹¤ì–‘ì„±, ê³µì •ì„±ì„ ê³ ë ¤í•˜ì—¬ í•„í„°ë²„ë¸”ì„ ì™„í™”í•˜ì§€ë§Œ ì •í™•ì„±ê³¼ ì‚¬ìš©ì ê²½í—˜ì„ í¬ìƒí•´ì•¼ í•œë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤.&lt;/p&gt;

&lt;p&gt;ë˜í•œ, ì‚¬ìš©ìì˜ í”¼ë“œë°±ì„ í†µí•´ì„œ ì¶”ì²œì‹œìŠ¤í…œì´ ëª¨ë¸ í›ˆë ¨-ì˜ˆì¸¡ì˜ ë¬´í•œ ë£¨í”„ë¥¼ ë„ëŠ” ê³¼ì •ì—ì„œ, ì‚¬ìš©ìëŠ” ì¶”ì²œ ê²°ê³¼ë¥¼ ìˆ˜ë™ì ìœ¼ë¡œë§Œ ë°›ì•„ë“¤ì´ê²Œ ë•Œë¬¸ì— ì§„ì •í•œ â€˜ê°œë³„ë§ì¶¤â€™ ê²°ê³¼ë¥¼ ìƒì„±í•˜ê¸°ê¹Œì§„ ë§ì€ ì‹œê°„ê³¼ ë¹„ìš©ì„ í•„ìš”ë¡œ í•˜ë‹¤. ì¦‰, ë¹„ë¡ ì¶”ì²œì‹œìŠ¤í…œì´ ì‚¬ìš©ìì˜ ì„ í˜¸ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²°ê³¼ë¥¼ ìƒì„±í•˜ê¸´ í•˜ì§€ë§Œ, ë‹¤ì–‘ì„±ê³¼ ê³µì •ì„± í–¥ìƒ ê³¼ì •ì—ì„œ ë‹¤ì‹œ ë¶ˆí•„ìš”í•œ ì •ë³´ê¹Œì§€ í¬í•¨ì‹œí‚¬ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì‚¬ìš©ìê°€ ìì‹ ì—ê²Œ í•„ìš”í•œ ì •ë³´ë§Œì„ ì–»ê¸° ìœ„í•´ì„  ìƒì„±ëœ ì¶”ì²œ ê²°ê³¼ì— ëŒ€í•´ â€˜likeâ€™ í˜¹ì€ â€˜dislikeâ€™ ë“± ì§€ì†ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•˜ê³  í•™ìŠµì„ ì‹œì¼œì•¼ í•œë‹¤.&lt;/p&gt;

&lt;p&gt;ë”°ë¼ì„œ, ë™ ì—°êµ¬ëŠ” ì‚¬ìš©ìê°€ ì§ì ‘ ì»¨íŠ¸ë¡¤ì„ í†µí•´ ìì‹ ì´ ì›í•˜ëŠ” ì¶”ì²œ ê²°ê³¼ë¥¼ ìƒì„±í•  ìˆ˜ ìˆê²Œë” â€˜User-Controllable Recommendation System(UCRS)â€™ ë°©ì•ˆì„ ì œì‹œí–ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.ibb.co/9wXYx34/rsloop.png&quot; alt=&quot;rsloop&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ì‚¬ìš©ìê°€ ì œì–´í•  ìˆ˜ ìˆëŠ” ì¶”ì²œì‹œìŠ¤í…œì¸ UCRSëŠ” ê¸°ì¡´ ì¶”ì²œì‹œìŠ¤í…œ ì™¸ì— ì•„ë˜ 3ê°€ì§€ ê¸°ëŠ¥ì„ ì¶”ê°€ì‹œì¼°ë‹¤.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;í•„í„°ë²„ë¸”ì˜ ìˆ˜ì¤€ì„ ì¸¡ì •í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ì•Œë ¤ì£¼ëŠ” í•„í„°ë²„ë¸” ê²½ê³  ê¸°ëŠ¥&lt;/li&gt;
  &lt;li&gt;4ê°€ì§€ ìˆ˜ì¤€ì˜ ì œì–´ ëª…ë ¹ ê¸°ëŠ¥&lt;/li&gt;
  &lt;li&gt;ì‚¬ìš©ì ì œì–´ì— ë”°ë¼ ì¶”ì²œê²°ê³¼ë¥¼ ì¡°ì •í•˜ëŠ” ì‘ë‹µ ë©”ì»¤ë‹ˆì¦˜&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;ì´ë¡œì¨, ë™ ì—°êµ¬ëŠ” ì‚¬ìš©ìê°€ ì¶”ì²œì‹œìŠ¤í…œì˜ ë™ì‘ì„ ì œì–´í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì œê³µí•˜ì—¬ ì‚¬ìš©ìì˜ ì°¸ì—¬ë¥¼ ì´‰ì§„í•˜ê³ , ì‚¬ìš©ìê°€ ë” ë§ì€ í‘œí˜„ë ¥ê³¼ ì œì–´ê¶Œì„ ê°€ì§€ê²Œ í•¨ìœ¼ë¡œì¨ ì‚¬ìš©ì ê²½í—˜ì„ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ ì‚¼ì•˜ë‹¤.&lt;/p&gt;

&lt;p&gt;ì´ì™€ ë”ë¶ˆì–´, ë™ ì—°êµ¬ëŠ” ì•ì„œ ë§í•œ ì‘ë‹µ ë©”ê±°ë‹ˆì¦˜ ë‹¨ê³„ì—ì„œ ì‚¬ìš©ì Aê°€ 1ë…„ ì „ì—ëŠ” ì•¡ì…˜ ì˜í™”ë¥¼ ë§ì´ ì‹œì²­í–ˆì§€ë§Œ í˜„ì¬ëŠ” ì½”ë¯¸ë”” ì˜í™”ì— ë” ê´€ì‹¬ì´ ìˆì„ ìˆ˜ ìˆëŠ” ê²ƒì²˜ëŸ¼ ì‹œê°„ ì§€ë‚¨ì— ë”°ë¼ ì‚¬ìš©ìì˜ ì„ í˜¸ë„ê°€ ë³€í•  ìˆ˜ ìˆë‹¤ëŠ” ì ê¹Œì§€ ì¸ì‹í•˜ì˜€ë‹¤. ì´ì— ë”°ë¼, ì‚¬ìš©ì í‘œí˜„ì˜ ì˜¤ë˜ëœ ì •ë³´ê°€ ì¶”ì²œì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì™„í™”í•˜ëŠ” ë° ì¤‘ì ì„ ë‘” â€˜User-Controllable Inference(UCI)â€™ í”„ë ˆì„ ì›Œí¬ê°€ ì œì•ˆë˜ì—ˆë‹¤.&lt;/p&gt;

&lt;p&gt;UCIëŠ” ì‚¬ìš©ìê°€ ì œì–´ ëª…ë ¹ì„ ì œê³µí•˜ë©´,  ë°˜ ì‚¬ì‹¤(counterfactual)ì¸ ëŒ€ì¡°ì  ì¶”ë¡ ì„ ì‚¬ìš©í•˜ì—¬ ê³¼ê±°ì— ë‚˜ì˜¨ ì‚¬ìš©ì í‘œí˜„ì˜ ì˜í–¥ì„ ì¤„ì´ëŠ” ë°©ì•ˆì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì—¬ì„±ì¸ ì‚¬ìš©ì BëŠ” ì—¬ì„± ì‚¬ìš©ì ê·¸ë£¹ì´ ì„ í˜¸í•˜ëŠ” ì˜í™” ë¦¬ìŠ¤íŠ¸ì— ì§ˆë ¤ ë‚¨ì„± ì‚¬ìš©ì ê·¸ë£¹ì´ ì„ í˜¸í•˜ëŠ” ì˜í™” ë¦¬ìŠ¤íŠ¸ë¥¼ ì¶”ì²œì‹œí‚¤ëŠ” ì œì–´ ëª…ë ¹ì„ ë‚´ë ¸ë‹¤ê³  ê°€ì •í•˜ì. ì´ ë•Œ, ë°˜ ì‚¬ì‹¤ì¸ ëŒ€ì¡°ì  ì¶”ë¡ ì€ â€˜ì‚¬ìš©ì Bê°€ ë‚¨ì„±ì´ë¼ë©´ ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ëŠ” ì–´ë–»ê²Œ ë³€í• ê¹Œ?â€™ë¼ëŠ” ì§ˆë¬¸ì˜ ëŒ€í•œ ì˜ˆì¸¡ì´ë¼ê³  ìƒê°í•  ìˆ˜ ìˆë‹¤. ì¦‰, UCIëŠ” ì˜¤ë˜ëœ ì‚¬ìš©ì í‘œí˜„ì´ íê¸°ë˜ëŠ” ë°˜ ì‚¬ì‹¤ ì„¸ê³„ë¥¼ ìƒìƒí•˜ê³  ì´ëŸ° ë°˜ ì‚¬ì‹¤ ì¡°ê±´ì— ë§ëŠ” ìƒˆë¡œìš´ ì¶”ì²œ ê²°ê³¼ë¥¼ ìƒì„±í•œë‹¤. ì´ë¡œì¨ ê³¼ê±° ì‚¬ìš©ì-ì•„ì´í…œ ê°„ ìƒí˜¸ì‘ìš© íŒ¨í„´ì— ê·¹í•œë˜ì§€ ì•Šê³  ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ì¶”ì²œì„ ì–»ì„ ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;3-preliminary&quot;&gt;3. Preliminary&lt;/h2&gt;
&lt;p&gt;ë™ ì—°êµ¬ëŠ” Methodë¥¼ í™•ë¦½í•˜ê¸° ì „ì— ë‹¤ì–‘í•œ ì‚¬ìš©ì ê·¸ë£¹ì— ëŒ€í•œ í•„í„°ë²„ë¸” ê²°ê³¼ë¥¼ ë¶„ì„í•˜ëŠ” ì‚¬ì „ ì‹¤í—˜ì„ ìˆ˜í–‰í–ˆë‹¤:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;ëŒ€í‘œì ì¸ ì¶”ì²œ ëª¨ë¸ì¸ Factorization Machine (FM)ì„ DIGIX-Video, Amazon-Book ë° ML-1Mê³¼ ê°™ì€ ì„¸ ê°œì˜ ê³µê°œ ë°ì´í„°ì…‹ì— í›ˆë ¨ì‹œí‚´.&lt;/li&gt;
  &lt;li&gt;ê° ì‚¬ìš©ìì— ëŒ€í•´ ìƒìœ„ 10ê°œì˜ ì¶”ì²œ ì•„ì´í…œì„ ìˆ˜ì§‘í•¨.&lt;/li&gt;
  &lt;li&gt;ì‚¬ìš©ì ê·¸ë£¹ì„ ID, ì„±ë³„ ë° ì—°ë ¹ì„ ê³ ë ¤í•œ ì‚¬ìš©ì íŠ¹ì„±(User Features)ê³¼ ì•„ì´í…œ ì¹´í…Œê³ ë¦¬ì— ëŒ€í•œ ê´€ì‹¬ë„ ë“±ì„ ê³ ë ¤í•œ ì‚¬ìš©ì ìƒí˜¸ì‘ìš©(User Interactions) ë‘ ê°€ì§€ ìš”ì¸ìœ¼ë¡œ ë¶„ë¥˜í•¨.&lt;/li&gt;
  &lt;li&gt;FMì´ ìƒì„±í•œ ì¶”ì²œê²°ê³¼ì™€ ì‚¬ìš©ì ê·¸ë£¹ì— ë”°ë¥¸ ì‚¬ìš©ìì˜ ê³¼ê±° ìƒí˜¸ì‘ìš© íŒ¨í„´ì„ ë¹„êµí•¨.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;ë¶„ì„ ê²°ê³¼ë¡œëŠ”, ì‚¬ìš©ì íŠ¹ì„±ê³¼ ì•„ì´í…œ íŠ¹ì„±(Item Features) 2ê°€ì§€ ì¸¡ë©´ì—ì„œ í•„í„°ë²„ë¸”ì´ ì¡´ì¬í•œë‹¤ëŠ” ì‚¬ì‹¤ì´ ë°œê²¬ë˜ì—ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.ibb.co/0c9Qt8R/fbresult.png&quot; alt=&quot;fbresult&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ì´ë¯¸ì§€ 2(a)ëŠ” DIGIX-Videoì˜ ë‚¨ì„± ë° ì—¬ì„± ì‚¬ìš©ìë³„ ìƒìœ„ 3ê°œ ì•„ì´í…œ ì¹´í…Œê³ ë¦¬ì— ëŒ€í•œ ê³¼ê±° ë¶„í¬ë¥¼ ì‹œê°í™”í•œ ê²°ê³¼ì´ë‹¤. ì—¬ì„± ì‚¬ìš©ì ê·¸ë£¹ì€ ë¡œë§¨ìŠ¤ ì˜í™”ë¥¼ ë” ì„ í˜¸í•œ ë°˜ë©´, ë‚¨ì„± ì‚¬ìš©ì ê·¸ë£¹ì€ ì•¡ì…˜ ì˜í™”ë¥¼ ë” ì„ í˜¸í–ˆë‹¤. ê·¸ ê²°ê³¼, ì´ë¯¸ì§€ 2(b)ì™€ 2(c)ì—ì„œ ì•Œ ìˆ˜ ìˆë“¯ì´, ì¶”ì²œ ê²°ê³¼ì—ì„œë„ í¸í–¥ëœ ë¶„í¬ë¥¼ ìœ ì§€í•˜ê²Œ ë˜ì—ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì´ëŸ¬í•˜ë“¯ì´, ì‚¬ìš©ìëŠ” ê³„ì†í•´ì„œ ìœ ì‚¬í•œ ì•„ì´í…œì„ ì¶”ì²œ ë°›ê²Œ ëœë‹¤. ì¶”ì²œ ëª¨ë¸ì€ ì´ëŸ¬í•œ í¸í–¥ì„ ê°•í™”í•˜ê³  ìƒìœ„ íŠ¹ì • ì¹´í…Œê³ ë¦¬ë¥¼ ë” ë…¸ì¶œì‹œí‚¤ëŠ” ê²½í–¥ìœ¼ë¡œ ì´ì–´ì ¸ ê²°êµ­ ë‚¨ì„±ê³¼ ì—¬ì„± ì‚¬ìš©ì ê·¸ë£¹ ê°„ì˜ ì‹¬ê°í•œ ë¶„ë¦¬ë¥¼ ì•¼ê¸°ì‹œí‚¤ê²Œ ëœë‹¤.&lt;/p&gt;

&lt;p&gt;ë˜í•œ, ì´ë¯¸ì§€ 2(d) ë° 2(e)ëŠ” Amazon-Book ë° ML-1M ë°ì´í„°ì…‹ì— ëŒ€í•´ ì‚¬ìš©ì ìƒí˜¸ì‘ìš©ì— ë”°ë¼ ë‚˜ëˆˆ ê²°ê³¼ì´ë‹¤. ë™ ê²°ê³¼ì—ì„œë„ ê³¼ê±° ì‚¬ìš©ì ìƒí˜¸ì‘ìš© íŒ¨í„´ì— ë”°ë¥¸ ì¹´í…Œê³ ë¦¬ í¸í–¥ ì¦í­ì´ ë°œê²¬ë˜ì—ˆë‹¤. ì¦‰, ì‚¬ìš©ììœ¼ë¡œë¶€í„° ê°€ì¥ í° ê´€ì‹¬ì„ ë°›ì€ ì¹´í…Œê³ ë¦¬ê°€ ì´í›„ ì¶”ì²œ ëª©ë¡ì—ì„œë„ ì¦ê°€ëœ ê²ƒì´ë‹¤. ì´ëŠ” í•„í„°ë²„ë¸”ì˜ ê°•í™”ë¥¼ ì´ˆë˜í•˜ê³  ì‚¬ìš©ìì˜ ê´€ì‹¬ì„ ì¢í˜€ ì‚¬ìš©ì ê·¸ë£¹ ë¶„ë¦¬ë¡œ ì´ì–´ì§€ê²Œ ëœë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;4-method&quot;&gt;&lt;strong&gt;4. Method&lt;/strong&gt;&lt;a href=&quot;https://dsailatkaist.github.io/template.html#3-method&quot;&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;ì „ë°˜ì ìœ¼ë¡œ ë™ ì—°êµ¬ëŠ” ì»¤ë²„ë¦¬ì§€(Coverage), ê²©ë¦¬ì§€ìˆ˜(Isolation Index), ìµœë‹¤ ì¹´í…Œê³ ë¦¬ ì§€ë°°ë„(Majority Category Domination, MCD) ë“± ì§€í‘œë¥¼ í†µí•´ í•„í„°ë²„ë¸”ì˜ ìˆ˜ì¤€ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ê°ì§€í•˜ê³  ì‚¬ìš©ìì—ê²Œ ì•ŒëŒì„ ë³´ë‚´ëŠ” ê²½ê³  ê¸°ëŠ¥ì„ êµ¬í˜„í–ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ë˜í•œ, ì œì–´ ëª…ë ¹ ê¸°ëŠ¥ê³¼ ê´€ë ¨í•´ì„œëŠ” ì•ì„œ ì‚¬ì „ ì‹¤í—˜ì—ì„œ ì‚¬ìš©ì íŠ¹ì„±ê³¼ ì•„ì´í…œ íŠ¹ì„± 2ê°€ì§€ ì¸¡ë©´ì—ì„œ í•„í„°ë²„ë¸”ì´ ì¡´ì¬í•œë‹¤ëŠ” ì‚¬ì‹¤ì„ ë°œê²¬í•¨ì— ë”°ë¼, UCRSë¥¼ ì„¤ê³„í•  ë•Œì—ë„ ì‚¬ìš©ì íŠ¹ì„±ê³¼ ì•„ì´í…œ íŠ¹ì„± ê°ê°ì˜ ë°©ë©´ì—ì„œ ì œì–´ì‹œìŠ¤í…œì„ êµ¬í˜„í–ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ë§ˆì§€ë§‰ìœ¼ë¡œ, ì‚¬ìš©ì ì œì–´ì— ë”°ë¼ ì¶”ì²œê²°ê³¼ë¥¼ ì¡°ì •í•˜ëŠ” ë°˜ ì‚¬ì‹¤ ëŒ€ì¡°ì  ì¶”ë¡  ì‘ë‹µ ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ í•„í„°ë²„ë¸”ì„ ì™„í™”í•˜ëŠ” ë™ì‹œì— ì •í™•ì„±ì€ ìœ ì§€í•˜ê³  ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ” ì¶”ì²œì‹œìŠ¤í…œì„ êµ¬í˜„í–ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.ibb.co/7Nrkm21/ucrs.png&quot; alt=&quot;ucrs&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;41-í•„í„°ë²„ë¸”-ê°ì§€-ì§€í‘œ&quot;&gt;4.1 í•„í„°ë²„ë¸” ê°ì§€ ì§€í‘œ&lt;/h3&gt;
&lt;p&gt;ì¶”ì²œì‹œìŠ¤í…œì—ì„œ í•„í„°ë²„ë¸”ì„ ê°ì§€í•œë‹¤ëŠ” ê²ƒì€ ë‹¤ì–‘ì„±ì„ ì œí•œí•˜ê³  íŠ¹ì • ê·¸ë£¹ ë‚´ì—ì„œ ì‚¬ìš©ìë¥¼ ê³ ë¦½ì‹œí‚¤ëŠ” ê°œì¸í™”ëœ ì¶”ì²œì˜ ì‹¬ê°ë„ë¥¼ ì¸¡ì •í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ì¶”ì²œ ëª©ë¡ì˜ í•­ëª© ì¹´í…Œê³ ë¦¬ ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” Coverageì™€ ì„œë¡œ ë‹¤ë¥¸ ì‚¬ìš©ì íŠ¹ì„± ê·¸ë£¹ ê°„ì˜ ë¶„ë¦¬ë¥¼ í‰ê°€í•˜ëŠ” Isolation Indexì™€ ê°™ì€ ì§€í‘œë¥¼ ì‚¬ìš©í•˜ì—¬ í•„í„°ë²„ë¸”ì˜ ì‹¬ê°ë„ë¥¼ ì •ëŸ‰í™”í•  ìˆ˜ ìˆë‹¤. MCDëŠ” ì•„ì´í…œ íŠ¹ì„±ê³¼ ê´€ë ¨ëœ í•„í„°ë²„ë¸”ì„ ê°ì§€í•˜ëŠ” ìš©ë„ë¡œ ì‚¬ìš©ë˜ì–´ ê°€ì¥ ìì£¼ ì¶”ì²œë˜ëŠ” ì•„ì´í…œ ì¹´í…Œê³ ë¦¬ì˜ ë¹„ìœ¨ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì‹œê°„ì´ ì§€ë‚¨ì— ë”°ë¼ MCDê°€ ì¦ê°€í•˜ë©´ ì•„ì´í…œ ì¹´í…Œê³ ë¦¬ì™€ ê´€ë ¨ëœ í•„í„° ë²„ë¸”ì˜ ì‹¬ê°ì„±ì´ ì»¤ì§€ê³  ìˆìŒì„ ë‚˜íƒ€ë‚¸ë‹¤. ì´ëŸ¬í•œ ì§€í‘œë“¤ì„ í†µí•´ ì‚¬ìš©ìì—ê²Œ ì‹¤ì‹œê°„ìœ¼ë¡œ í•„í„°ë²„ë¸” ê²½ê³ ë¥¼ ë³´ë‚´ê³  ì´ë¥¼ ì œì–´í• ì§€ ì—¬ë¶€ë¥¼ ê²°ì •í•˜ê²Œ ë„ìš¸ ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;h3 id=&quot;42-ì œì–´-ëª…ë ¹-ê¸°ëŠ¥&quot;&gt;4.2 ì œì–´ ëª…ë ¹ ê¸°ëŠ¥&lt;/h3&gt;
&lt;p&gt;ì‚¬ìš©ìì˜ ê³¼ê±° ìƒí˜¸ì‘ìš© ë°ì´í„°ì¸ ğ·ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ê¸°ì¡´ì˜ ì¶”ì²œ ëª¨ë¸ì€ ì¶”ì²œ ğ‘…ì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ ğ‘ƒ(ğ‘…|ğ·)ë¥¼ ì‚¬ìš©í•œë‹¤. ê·¸ëŸ¬ë‚˜ UCRSëŠ” ì¶”ê°€ë¡œ ì‚¬ìš©ì ì œì–´ì¸ ğ¶ë¥¼ ê³ ë ¤í•˜ë©° ì‚¬ìš©ì ê°œì…(ğ‘‘ğ‘œ(ğ¶))ì„ í†µí•´ ğ‘ƒ(ğ‘…|ğ·, ğ‘‘ğ‘œ(ğ¶))ë¥¼ ì¶”ì •í•  ê²ƒì„ ì œì‹œí–ˆë‹¤. ì´ë•Œ, ì‚¬ìš©ì ì œì–´ê³¼ ê´€ë ¨í•´ì„œ UCRSëŠ” ì‚¬ìš©ì ë° í•­ëª©ë³„ë¡œ â€˜Fine-grained controlsâ€™ì™€ â€˜Coarse-grained controlsâ€™ë¥¼ 2ìˆ˜ì¤€ìœ¼ë¡œ ë‚˜ëˆ  ì´ 4ê°€ì§€ ìœ í˜•ì˜ ì‚¬ìš©ì ì œì–´ë¥¼ ì œì‹œí–ˆë‹¤.&lt;/p&gt;

&lt;h4 id=&quot;421-ì‚¬ìš©ì-íŠ¹ì„±-ì œì–´user-feature-controls&quot;&gt;4.2.1 ì‚¬ìš©ì íŠ¹ì„± ì œì–´(User-feature Controls)&lt;/h4&gt;
&lt;p&gt;Nê°€ì§€ ì‚¬ìš©ì íŠ¹ì„±ê³¼ ì‚¬ìš©ì ğ‘¢ê°€ ìˆì„ ë•Œ, ì‚¬ìš©ì ğ‘¢ëŠ” $x &lt;em&gt;{u} = [ x^1 _{u}, \ldots, x^n _{u}, \ldots,  x^N _{u} ], \text{ where }  x^n _{u} \in {0,1 }$ ë¡œ ë‚˜íƒ€ë‚´ë©°, ì—¬ê¸°ì„œ $x^n _{u} \in {0,1 }$
ëŠ” ì‚¬ìš©ì ğ‘¢ê°€ ì‚¬ìš©ì íŠ¹ì„± $x^n$ì„ ê°€ì§€ê³  ìˆëŠ”ì§€ ì—¬ë¶€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, íŠ¹ì„± $x&lt;/em&gt; {1}$ê³¼ $x_ {2}$ê°€ ë‚¨ì„±ê³¼ ì—¬ì„±ì„ ë‚˜íƒ€ë‚¸ë‹¤ë©´, $x_ {u}$ = [0, 1]ì€ ì‚¬ìš©ì ğ‘¢ê°€ ì—¬ì„±ì„ì„ ëœ»í•œë‹¤.&lt;/p&gt;

&lt;p&gt;ì‚¬ìš©ì íŠ¹ì„± ì œì–´ëŠ” â€˜Fine-grained controlsâ€™ì™€ â€˜Coarse-grained controlsâ€™ë¥¼ 2ìˆ˜ì¤€ìœ¼ë¡œ ë˜ ë‚˜ë‰˜ê²Œ ëœë‹¤.  â€˜Fine-grained controlsâ€™ì—ì„œëŠ” ì‚¬ìš©ìê°€ ì„¸ë¶„í™”ëœ ì‚¬ìš©ì íŠ¹ì„±ì„ í™œìš©í•˜ì—¬ êµ¬ì²´ì ì¸ ì œì–´ë¥¼ í†µí•´ ë‹¤ë¥¸ ì‚¬ìš©ì ê·¸ë£¹ì˜ ê´€ì‹¬ì‚¬ì— ë§ëŠ” ì¶”ì²œì„ ë°›ì„ ìˆ˜ ìˆê²Œ ëœë‹¤(ì˜ˆ: 30ëŒ€ ì‚¬ìš©ìê°€ 10ëŒ€ê°€ ì„ í˜¸í•˜ëŠ” ë™ì˜ìƒì„ ì¶”ì²œ ë°›ê¸°).&lt;/p&gt;

&lt;p&gt;ë°˜ë©´ì— â€˜Coarse-grained controlsâ€™ë¥¼ ì‚¬ìš©í•˜ë©´ ì‚¬ìš©ìì˜ ê¸°ì¡´ íŠ¹ì„±ë§Œì„ ì œí•œí•˜ëŠ” ë°©ì•ˆìœ¼ë¡œ ì¶”ì²œì„ ì¤„ì´ê²Œ ëœë‹¤. ì˜ˆë¥¼ ë“¤ì–´, â€˜ì—°ë ¹=30â€™ì²˜ëŸ¼ ì‚¬ìš©ìì˜ ë‚˜ì´ íŠ¹ì„±ì„ ì œê±°í•¨ìœ¼ë¡œì¨ ê¸°ì¡´ ìì‹ ì˜ ì‚¬ìš©ì ê·¸ë£¹ì— ì œì‹œë˜ì—ˆë˜ ì¶”ì²œì„ ì¤„ì„ìœ¼ë¡œì¨ í•„í„°ë²„ë¸”ì„ ì™„í™”í•  ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;h4 id=&quot;422-ì•„ì´í…œ-íŠ¹ì„±-ì œì–´item-feature-controls&quot;&gt;4.2.2 ì•„ì´í…œ íŠ¹ì„± ì œì–´(Item-feature Controls)&lt;/h4&gt;
&lt;p&gt;ì‚¬ìš©ì ê¸°ëŠ¥ ì œì–´ëŠ” ì‚¬ìš©ì íŠ¹ì„±ê³¼ ê´€ë ¨ëœ í•„í„°ë²„ë¸”ì„ í•´ê²°í•˜ì§€ë§Œ ì‚¬ìš©ì ìƒí˜¸ ì‘ìš©ì˜ ì˜í–¥ì€ ê³ ë ¤í•˜ì§€ ì•ŠëŠ”ë‹¤. ì‚¬ìš©ì íŠ¹ì„± ì œì–´ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´ ì•„ì´í…œ íŠ¹ì„± ì œì–´ê°€ ë„ì…ë˜ì–´ ì•„ì´í…œ íŠ¹ì„±ì— ë”°ë¼ ì¶”ì²œ ëª©ë¡ì„ ì¡°ì •í•  ìˆ˜ ìˆë‹¤. ì´ëŸ¬í•œ ì œì–´ ëª…ë ¹ì„ ì‚¬ìš©í•˜ë©´ ì•¡ì…˜ ì˜í™”ì™€ ê°™ì€ íŠ¹ì • ì¹´í…Œê³ ë¦¬ì— ì†í•˜ëŠ”ì§€ ì—¬ë¶€ì™€ ê°™ì€ ì•„ì´í…œì˜ íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ ì¶”ì²œì„ ì§€ì •í•  ìˆ˜ ìˆê²Œ ëœë‹¤.&lt;/p&gt;

&lt;p&gt;Mê°œì˜ ì•„ì´í…œ íŠ¹ì„±ê³¼ ì•„ì´í…œ iëŠ” $h _{i} = [ h^1 _{i}, \ldots, h^m _{i}, \ldots,  h^M _{i} ], \text{ where }  h^n _{i} \in {0,1 }$ ë¡œ ë‚˜íƒ€ë‚´ë©°, ì—¬ê¸°ì„œ $h^m _{i} \in {0,1 }$ ëŠ” ì•„ì´í…œiê°€ ì•„ì´í…œ íŠ¹ì„± $h^m$ ì„ ê°€ì§€ê³  ìˆëŠ”ì§€ ì—¬ë¶€ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.&lt;/p&gt;

&lt;p&gt;â€˜Fine-grained controlsâ€™ì—ì„œëŠ” ì‚¬ìš©ìê°€ íŠ¹ì • ì•„ì´í…œ ì¹´í…Œê³ ë¦¬ì˜ ì¶”ì²œì„ ëŠ˜ë¦¬ë„ë¡ í—ˆìš©í•  ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë¡œë§¨ìŠ¤ ì˜í™”ì™€ ê°™ì€ íŠ¹ì • ì¹´í…Œê³ ë¦¬ì˜ ì•„ì´í…œì„ ë” ë§ì´ ë°›ì„ ìˆ˜ ìˆê²Œë” êµ¬ì²´ì ì¸ ì œì–´ ëª…ë ¹ì„ ë‚´ë¦´ ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;p&gt;â€˜Coarse-grained controlsâ€™ì—ì„œëŠ” ì‚¬ìš©ìì˜ ê³¼ê±° ìƒí˜¸ì‘ìš©ì—ì„œ ê°€ì¥ í° ì•„ì´í…œ ì¹´í…Œê³ ë¦¬ì˜ ì¶”ì²œì„ ì¤„ì´ë„ë¡ ì§„í–‰ëœë‹¤.&lt;/p&gt;

&lt;p&gt;ì¢…í•©ì ìœ¼ë¡œ ë³´ë©´,  UCRSì˜ â€˜Fine-grained controlsâ€™ì—ì„œëŠ” ì‚¬ìš©ìê°€ ì„¸ë¶„í™”ëœ ì‚¬ìš©ì í˜¹ì€ ì•„ì´í…œ íŠ¹ì„±ì„ í™œìš©í•˜ì—¬ êµ¬ì²´ì ì¸ ì œì–´ ëª…ë ¹ì„ í†µí•´ íŠ¹ì • ì¶”ì²œ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ëŠ” ë°©ì•ˆì´ê³ ,  â€˜Coarse-grained controlsâ€™ëŠ” ì„¸ë¶„í™”ëœ íŠ¹ì„±ì— ëŒ€í•´ì„œ ì‚¬ìš©ìê°€ êµ¬ì²´ì ì¸ ì œì–´ ëª…ë ¹ì„ ì§€ì‹œí•  í•„ìš”ì—†ì´ ê°„ë‹¨í•˜ê²Œ í•„í„°ë²„ë¸”ì„ ì™„í™”í•˜ëŠ” ë°©ë²•ì„ êµ¬í˜„í–ˆë‹¤.&lt;/p&gt;

&lt;h3 id=&quot;43-ë°˜-ì‚¬ì‹¤ì counterfactual-ì‘ë‹µ-ë©”ì»¤ë‹ˆì¦˜&quot;&gt;4.3 ë°˜ ì‚¬ì‹¤ì (Counterfactual) ì‘ë‹µ ë©”ì»¤ë‹ˆì¦˜&lt;/h3&gt;
&lt;p&gt;â€˜Fine-grained controlsâ€™ì—ëŠ” ì—°ë ¹ê³¼ ê°™ì´ ë³€ê²½ëœ ì‚¬ìš©ì íŠ¹ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì²œì„ ìƒì„±í•˜ì—¬ ë‹¤ì–‘í•œ ì‚¬ìš©ì ê·¸ë£¹ì—ì„œ ì‚¬ì‹¤ê³¼ ë‹¤ë¥¸ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ì¶”ë¡  ê³¼ì •ì´ í¬í•¨ëœë‹¤.&lt;/p&gt;

&lt;p&gt;UCIëŠ” ì‚¬ìš©ìì˜ ì´ì „ ìƒí˜¸ì‘ìš© íŒ¨í„´ì— ë°˜ëŒ€ë˜ëŠ” ë°˜ ì‚¬ì‹¤ì ì¸ ì¡°ê±´ ì•„ë˜ì—ì„œ ìƒˆë¡œìš´ ì¶”ì²œì„ ìƒì„±í•˜ê²Œ ëœë‹¤. ì´ë¥¼ í†µí•´ ì‚¬ìš©ìê°€ ê³¼ê±° íŒ¨í„´ì— ì œí•œë°›ì§€ ì•Šê³  ì›í•˜ëŠ” ì¶”ì²œì„ ë°›ì„ ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;5-experiment&quot;&gt;&lt;strong&gt;5. Experiment&lt;/strong&gt;&lt;a href=&quot;https://dsailatkaist.github.io/template.html#4-experiment&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;h3 id=&quot;experiment-setup&quot;&gt;&lt;strong&gt;Experiment setup&lt;/strong&gt;&lt;a href=&quot;https://dsailatkaist.github.io/template.html#experiment-setup&quot;&gt;&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Dataset&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;DIGIX-Video&lt;/li&gt;
  &lt;li&gt;ML-1M&lt;/li&gt;
  &lt;li&gt;Amazon-Book&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Baseline&lt;/strong&gt;
 Factorization Machine(FM)ê³¼  Neural Factorization Machine(NFM) 2ê°€ì§€ ì¶”ì²œ ëª¨ë¸ì— ëŒ€í•´ ì•„ë˜ì™€ ê°™ì€ ë² ì´ìŠ¤ë¼ì¸ì„ ì ìš©í•¨&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;woUF: ì‚¬ìš©ì íŠ¹ì„± ì—†ì´ í•™ìŠµëœ í¬ë¡œí† íƒ€ì…&lt;/li&gt;
  &lt;li&gt;maskUF:  ì‚¬ìš©ì íŠ¹ì„±ì„ ì‚­ì œí•˜ëŠ” í¬ë¡œí† íƒ€ì…&lt;/li&gt;
  &lt;li&gt;changeUF: ì‚¬ìš©ì íŠ¹ì„±ì„ ë³€ê²½í•œ í¬ë¡œí† íƒ€ì…&lt;/li&gt;
  &lt;li&gt;Fairco: ê³µì •ì„± ê¸°ë°˜ ranking í¬ë¡œí† íƒ€ì…&lt;/li&gt;
  &lt;li&gt;Diversity: ë‹¤ì–‘ì„± ê¸°ë°˜ re-ranking í¬ë¡œí† íƒ€ì…&lt;/li&gt;
  &lt;li&gt;Reranking: UCIì˜ í•œ ë³€í˜•ìœ¼ë¡œ, counterfactual ì¶”ë¡ ê³¼ target category ì˜ˆì¸¡ì„ ì œì™¸í•œ í¬ë¡œí† íƒ€ì…&lt;/li&gt;
  &lt;li&gt;C-UCI: â€˜Coarse-grained controlsâ€™ UCI í¬ë¡œí† íƒ€ì…&lt;/li&gt;
  &lt;li&gt;F-UCI: â€˜Fined-grained controlsâ€™ UCI í¬ë¡œí† íƒ€ì…&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Evaluation Metric&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Recall: ì •í™•ë„ ì²™ë„&lt;/li&gt;
  &lt;li&gt;NDCG: ì •í™•ë„ ì²™ë„&lt;/li&gt;
  &lt;li&gt;Weighted-NDCG: ì¹´í…Œê³ ë¦¬ ì„ í˜¸ë„ ìš°ì„ ìˆœìœ„ ì²™ë„&lt;/li&gt;
  &lt;li&gt;Coverage: ë‹¤ì–‘ì„± ì²™ë„&lt;/li&gt;
  &lt;li&gt;Isolation Index: ê²©ë¦¬ì§€ìˆ˜ ì²™ë„&lt;/li&gt;
  &lt;li&gt;MCD: ìµœë‹¤ ì¹´í…Œê³ ë¦¬ ë¹„ìœ¨ ì²™ë„&lt;/li&gt;
  &lt;li&gt;DIS-EUC: ìœ í´ë¦¬ë””ì–¸ ê±°ë¦¬ ì²™ë„&lt;/li&gt;
  &lt;li&gt;TCD(Target Category Domination): ëª©í‘œ ì¹´í…Œê³ ë¦¬ ë¹„ìœ¨ ì²™ë„&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;result&quot;&gt;&lt;strong&gt;Result&lt;/strong&gt;&lt;a href=&quot;https://dsailatkaist.github.io/template.html#result&quot;&gt;&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://i.ibb.co/DGLCcZ2/rs1.png&quot; alt=&quot;rs1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.ibb.co/kD7nSjt/rs2.png&quot; alt=&quot;rs2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.ibb.co/WpSs466/rs3.png&quot; alt=&quot;rs3&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;3ê°€ì§€ ë°ì´í„° ì„¸íŠ¸ë¥¼ ëŒ€ìƒìœ¼ë¡œ ì‹¤í—˜í•œ ê²°ê³¼, UCI í”„ë ˆì„ì›Œí¬ê°€ ì‚¬ìš©ì ì œì–´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì›í•˜ëŠ” í•­ëª©ì„ ë” ë§ì´ ì¶”ì²œí•˜ëŠ” íš¨ê³¼ê°€ ì…ì¦ë˜ì—ˆìœ¼ë©°, ì •í™•ì„±ê³¼ ë‹¤ì–‘ì„± ì¸¡ë©´ì—ì„œ ìœ ë§í•œ ì„±ê³¼ë¥¼ ë³´ì˜€ë‹¤.&lt;/li&gt;
  &lt;li&gt;UCI í”„ë ˆì„ì›Œí¬ëŠ” ì‚¬ìš©ì ì œì–´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë” ë§ì€ ì›í•˜ëŠ” í•­ëª©ì„ íš¨ê³¼ì ìœ¼ë¡œ ì¶”ì²œí•˜ì—¬ ì‚¬ìš©ì ë§Œì¡±ë„ì™€ ì¶”ì²œ ìƒíƒœê³„ ì°¸ì—¬ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆëŠ” ê²°ë¡ ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-conclusion&quot;&gt;&lt;strong&gt;6. Conclusion&lt;/strong&gt;&lt;a href=&quot;https://dsailatkaist.github.io/template.html#5-conclusion&quot;&gt;&lt;/a&gt;&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;ë™ ì—°êµ¬ëŠ” ì‚¬ìš©ìê°€ í•„í„°ë²„ë¸” ì™„í™”ë¥¼ ëŠ¥ë™ì ìœ¼ë¡œ ì œì–´í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” UCRS (User-Controllable Recommender System) ë¥¼ ì œì•ˆí•˜ì—¬ ì‚¬ìš©ì íŠ¹ì„± ë° ê³¼ê±° ìƒí˜¸ì‘ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ìœ ì‚¬í•œ í•­ëª©ì„ ê³¼ë„í•˜ê²Œ ì¶”ì²œí•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì‹œë„ë¥¼ ë³´ì˜€ë‹¤.&lt;/li&gt;
  &lt;li&gt;UCRS í”„ë¡œí† íƒ€ì…ì€ í•„í„°ë²„ë¸”ì˜ ì‹¬ê°ë„ë¥¼ ê°ì§€í•˜ê³  ì‚¬ìš©ìì—ê²Œ 4ê°€ì§€ ì œì–´ ëª…ë ¹ì„ ì œê³µí•˜ì—¬ ì‚¬ìš©ìë¡œ í•˜ì—¬ê¸ˆ ì¶”ì²œì‹œìŠ¤í…œì˜ ì œì–´ê¶Œì„ ëŠ¥ë™ì ìœ¼ë¡œ ì‹¤ì‹œí•  ìˆ˜ ìˆë„ë¡ ì¶”ì²œì‹œìŠ¤í…œ ìƒíƒœê³„ì— ë°©í–¥ì„ ì œì‹œí–ˆë‹¤.&lt;/li&gt;
  &lt;li&gt;ì„¸ ê°€ì§€ ë°ì´í„°ì…‹ì„ ëŒ€ìƒìœ¼ë¡œ í•œ ì‹¤í—˜ì„ í†µí•´ UCRS í”„ë¡œí† íƒ€ì…ê³¼ UCI í”„ë ˆì„ì›Œí¬ëŠ” ì •í™•ì„±ê³¼ ë‹¤ì–‘ì„± ì¸¡ë©´ì—ì„œ ìœ ë§í•œ ì„±ê³¼ë¥¼ ë³´ì˜€ìœ¼ë©° ì‚¬ìš©ì ë§Œì¡±ë„ì™€ ì¶”ì²œ ìƒíƒœê³„ì— ëŒ€í•œ ì°¸ì—¬ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ëœë‹¤.&lt;/li&gt;
  &lt;li&gt;ë‹¨, ë™ ì—°êµ¬ëŠ” UCRS í”„ë¡œí† íƒ€ì…ê³¼ UCI í”„ë ˆì„ì›Œí¬ë¥¼ í‰ê°€í•  ë•Œ ì˜¨ë¼ì¸ ì‹¤ì‹œê°„ ë°ì´í„° ëŒ€ì‹  ì˜¤í”„ë¼ì¸ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í–ˆê³ , ì¼ë¶€ ì‚¬ìš©ìê°€ í•„í„°ë²„ë¸”ì„ ì™„í™”í•˜ê³  ì œì–´ ê¸°ëŠ¥ì„ ì œê³µí•  ì˜í–¥ì´ ìˆë‹¤ê³  ê°€ì •ì„ í–ˆê¸° ë•Œë¬¸ì— í˜„ì‹¤ ë°ì´í„°ì™€ ë¹„êµí•˜ë©´ strong assumptionì¼ ê²ƒìœ¼ë¡œ íŒë‹¨ëœë‹¤.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;author-information&quot;&gt;&lt;strong&gt;Author Information&lt;/strong&gt;&lt;a href=&quot;https://dsailatkaist.github.io/template.html#author-information&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Minkyung Choi
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Affiliation:
 &lt;a href=&quot;http://hfel.kaist.ac.kr/&quot;&gt;Human Factors and Ergonomics Lab â€“ Human Factors and Ergonomics Lab (HFEL) (kaist.ac.kr)&lt;/a&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Research Topic:
Data Science, Computer Vision, VR&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference--additional-materials&quot;&gt;&lt;strong&gt;Reference &amp;amp; Additional materials&lt;/strong&gt;&lt;a href=&quot;https://dsailatkaist.github.io/template.html#6-reference--additional-materials&quot;&gt;&lt;/a&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Github Implementation:
https://github.com/WenjieWWJ/UCR&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Reference:
Wang, W., Feng, F., Nie, L., &amp;amp; Chua, T. S. (2022, July). User-controllable recommendation against filter bubbles. In &lt;em&gt;Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval&lt;/em&gt; (pp. 1251-1261).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
            <pubDate>Mon, 16 Oct 2023 00:00:00 +0900</pubDate>
            <link>http://dsailatkaist.github.io/2023-10-16-User-controllable_Recommendation_Against_Filter_Bubbles.html</link>
            <guid isPermaLink="true">http://dsailatkaist.github.io/2023-10-16-User-controllable_Recommendation_Against_Filter_Bubbles.html</guid>
            
            <category>reviews</category>
            
            
        </item>
        
        <item>
            <title>[SIGIR 2023] Uncertainty-aware Consistency Learning for Cold-Start Item Recommendation</title>
            <description>&lt;h1 id=&quot;uncertainty-aware-consistency-learning-for-cold-start-item-recommendation&quot;&gt;&lt;strong&gt;Uncertainty-aware Consistency Learning for Cold-Start Item Recommendation&lt;/strong&gt;&lt;/h1&gt;

&lt;h2 id=&quot;1-problem-definition-and-motivation&quot;&gt;&lt;strong&gt;1. Problem Definition and Motivation&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;In this paper, the central problem at hand is the Cold-Start problem in  GNN-based recommendation systems, particularly when new items with limited interaction data are introduced to a user-item graph.  The cold-start problem especially becomes tricky when the user-item graph constantly changing. This problem arises due to the scarcity of information on these â€œcoldâ€ items, which hinders accurate recommendations. To tackle this issue, existing models primarily rely on auxiliary user and item features which underutilize actual user-item interactions. Cold items have different embeddings due to fewer interactions comparing to warm items. This difference leads to a challenge of improving recommendations for both simultaneously which is called seesaw phenomenon.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;For instance,&lt;/strong&gt; Suppose movie recommender system is using a vector of length 100 for movie embeddings:&lt;/p&gt;

&lt;p&gt;The embedding for â€œSpider-Manâ€ might encompass values such as [0.2, 0.1, -0.3, 0.5, â€¦], capturing its distinct characteristics and traits.&lt;/p&gt;

&lt;p&gt;However, when a new movie, â€œKaist-Man,â€ is introduced, its embedding might initiate with values like [?, ?, ?, â€¦], primarily due to the lack of interaction data.&lt;/p&gt;

&lt;p&gt;in this case, because â€œSpider-Manâ€ is more likely to be recommended to users who are interested in superhero action movies than â€œKaist-Manâ€. because the system has more information about it. The huge gap between their embeddings makes it challenging to improve recommendations for both simultaneously.&lt;/p&gt;

&lt;p&gt;To bridge this gap, in this paper, an Uncertainty-aware Consistency learning framework for Cold-start item recommendation (UCC) was introduced which relies exclusively on user-item interactions.  This framework has two key designs: a) Uncertainty-aware Interaction Generation, and b) Graph-based Teacher-Student Consistency Learning.&lt;/p&gt;

&lt;h2 id=&quot;2-problem-formulation&quot;&gt;&lt;strong&gt;2. Problem formulation&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Notations.&lt;/strong&gt; In this paper $U = {u}$ is the set of users and  $I = {i}$ is set of items and  $O ={ (u, i^+) \vert u  âˆˆ U, i^+ âˆˆ I }$ is user-item interactions, where each pair represents each observed feedback.
&lt;strong&gt;Input:&lt;/strong&gt;  a bipartite graph, where $V = UâˆªI$ is node set and $G = (V, O^+)$ is the edge set. if $M$:= number of users, $N$:=number of items, $D$:= dimension size of embedding
Then in the training process of graph representation learning, $E_ {u}  = [e_ {u_ {1}} , â€¦ , e_ {u_ {M}} ]âˆˆ R^{M\times D}$ is user embedding and $E_ {i}  = [e_ {i_ {1}} , â€¦ , e_ {i_ {N}}] âˆˆR^{N\times D}$ is item embedding.
&lt;strong&gt;Output:&lt;/strong&gt; Recommender models assessing the relationships between unobserved user-item pairs using the dot products of their embeddings. For given user $m$ and given item $n$ the score ${s_ {mn}}$ is calculated  as: $s_ {mn}  = e_ {u_ {m}} {e_ {i_ {n}} }^T$.
A larger value of ${s_ {mn}}$ indicates a stronger preference by the user for the item. The top-k items from the ranking list are recommended to the user.&lt;/p&gt;

&lt;h2 id=&quot;3-method&quot;&gt;&lt;strong&gt;3. Method&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://i.ibb.co/2qjj3J0/figure1.png&quot; alt=&quot;figure1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this paper, they tackle the distribution gap between cold and warm items by employing an uncertainty-aware approach to generate interactions. Then they kept both item-level and model-level similarity with consistency.&lt;/p&gt;

&lt;h3 id=&quot;31-uncertainty-aware-interaction-generation&quot;&gt;3.1. Uncertainty-aware Interaction Generation&lt;/h3&gt;

&lt;p&gt;To determine whether generated interactions are accurate and unbiased enough they introduce the uncertainty degree of each user-item interaction which is calculated by cosine distance.
for user $u_ {m}$ and item $i_ {n}$ cosine distance is
$d_ {mn}= {\vert e_ {u_ {m}} {e_ {i_ {n}} }^T\vert \over \vert \vert e_ {u_ {m}}\vert\vert \ \vert \vert e_ {i_ {n}}\vert \vert }$
${s_ {mn}}_ {n=1}^N$ is ranking scores of item $i_ {n}$ for all users calculated with the pre-trained  recommender, then overall interaction uncertainty of the item $i_ {n}$ can be estimated by the average of all rankings scores:&lt;/p&gt;

&lt;p&gt;$\bar{s_n}= {{1\over M}}\sum_ {k=1}^M s_ {mn}$&lt;/p&gt;

&lt;p&gt;So, high $\bar{s_n}$ suggests low item uncertainty, indicating widespread user acceptance, and whole low $\bar{s_n}$ indicates higher uncertainty.
In order to bridge the gap between cold and warm itemsâ€™ distribution and popularity bias all interactions with $d_ {mn}&amp;lt;\alpha \bar{s_n}$ will be regarded as uncertain interactions and filtered in the generation stage. So the selection would be as follow:
$\hat{O_ {n}}={I(d_ {mn}&amp;gt;\alpha \bar{s_n})}$,
where $\alpha$ is a pre-defined parameter and $I$ is the indicator function.
In other words, they find the average ranking score for an item for all users ($\bar{s_n}$) and then for each user if item-user similarity ($d_ {mn}$) is smaller than $\alpha \bar{s_n}$ they regard that interaction as uncertain.&lt;/p&gt;

&lt;h3 id=&quot;32-teacher-student-consistency-learning&quot;&gt;3.2. Teacher-student Consistency learning&lt;/h3&gt;

&lt;p&gt;To address seesaw phenomena, and to achieve better recommendations for both warm and cold items at the same time, cold-item with generated low-uncertainty interactions should have a similar distribution with the warm items, so in this paper, they trained the teacher model (generator) and student model (recommender) with consistency learning
trained the teacher model (generator) and student model (recommender) with consistency learning
we train the teacher model (generator) and student model (recommender) with consistency learning, to ensure the cold items with additionally generated low-uncertainty interactions can have similar distribution with the warm items.&lt;/p&gt;

&lt;h4 id=&quot;321-item-level-consistency-learning&quot;&gt;3.2.1 Item-level consistency learning&lt;/h4&gt;

&lt;p&gt;This technique employs a contrastive loss to compare item embeddings before and after a generation process.
Two types of augmentations are used:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Weak Augmentation:&lt;/strong&gt;
In this, edges in a graph are dropped out based on a dropout ratio $\rho$ to make the model more robust and less sensitive to specific data points. It can be formulated as follows:
${\mathcal G}^{w}=({\mathcal V},{\mathcal M}\cdot{\mathcal O}^{+})$ where $\mathcal{M}\in{0,1}^{\vert O^{+}}$ is a masking vector.
For example, in the movie recommendation system, if you always include the same userâ€™s connection to a particular movie, the model might become too biased towards that userâ€™s preferences. By using dropout, you randomly exclude some user-movie connections in each training iteration, making the model more balanced and better at recommending movies for a variety of users.
Where M is a masking vector containing binary values {0, 1} for each element in O+.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Strong Augmentation:&lt;/strong&gt;
This type involves adding more edges to the graph based on generated labels and can be formulated as follows:
${\mathcal G}^{s}=({\mathcal V},{\mathcal O}^{+} +{\widehat O})$
For example, if two movies share many common actors or are often rated similarly by users, strong augmentation would introduce more connections between those movies in the recommendation graph. This enriches the information available to the model, allowing it to make more accurate and diverse movie recommendations.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These two augmentation operations create two different views for each node, denoted as $z_i^{\prime},z_i^{\prime\prime}$ ğ‘– (for weak and strong graphs, respectively).
Consistency Regularization: To encourage the similarity between the different views of the same node, a consistency regularization is implemented. This involves the use of a contrastive loss:
$\mathcal{L}_ {cr, item}=\sum_ {i\in I}-\log\frac{\exp(sim(z_iâ€™,z_iâ€™â€™)/\tau)}{\sum_ {j\in I}\exp(sim(z_iâ€™,z_jâ€™â€™)/\tau)}$,
where $\mathcal{L}_ {cr, item}$ represents the item-side consistency regularization loss for both the teacher and the student model, $sim()$ is cosine similarity function and $\tau$ is a predefined hyper-parameter.Similarly, $\mathcal{L}_ {cr, user}$ can be computed.  $\mathcal{L}_ {cr}=\mathcal{L}_ {cr, item}+\mathcal{L}_ {cr, user}$ represents the ultimate consistency loss used for consistency regularization.
Recommendation loss can be calculated as follow:&lt;/p&gt;

&lt;p&gt;$\mathcal{L}_ {\mathbf{rec}}=\sum_ {(u,i^+,i^-)\in O}-\ln\sigma(\hat{y}_ {ui^+}-\hat{y}_ {ui^-})+\lambda\vert \vert \Theta\vert \vert &lt;em&gt;2^2,$ is L2-regularization of modelâ€™s parameters.
And Finally, our total loss is $\mathcal{L}&lt;/em&gt; {total}=\mathcal{L}_ {rec}+\mu\mathcal{L}_ {cr}$ where $\mu$ is a hyper-parameter.&lt;/p&gt;

&lt;h4 id=&quot;322-model-level-consistency-learning&quot;&gt;3.2.2 Model-level consistency learning&lt;/h4&gt;

&lt;p&gt;To keep the consistency between the teacher model and student, they suggested collecting the teacher embedding into the student embedding: 
$\mathbf{E}^s\leftarrow\gamma\mathbf{E}^s+(1-\gamma)\mathbf{E}^t$
where $E^s$ and $E^t$ are the embeddings of the student and student modelâ€™s embedding respectively.&lt;/p&gt;

&lt;h2 id=&quot;4-experiment&quot;&gt;&lt;strong&gt;4. Experiment&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;experiment-setup&quot;&gt;&lt;strong&gt;Experiment setup&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Datasets:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Experiments on Yelp and Amazon-Book benchmark datasets.&lt;/li&gt;
  &lt;li&gt;Follow a 10-core setting as in previous studies (Lightgcn and Neural collaborative filtering)&lt;/li&gt;
  &lt;li&gt;Split user-item interactions into training, validation, and testing sets (ratio 7:1:2).
&lt;strong&gt;Baselines:&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;The foundation for our GNN-based model is LightGCN&lt;/li&gt;
  &lt;li&gt;Focused on modelling user-item interactions, not item features.&lt;/li&gt;
  &lt;li&gt;They used two types of recommendation models, the Generative model and Denoising models for user-item interactions: IRBPR, ADT, and SGL.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Evaluation Metric&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Following previous studies, they evaluate performance using Recall@K and  NDCG@K where K = 20&lt;/p&gt;

&lt;h3 id=&quot;result&quot;&gt;&lt;strong&gt;Result&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;The overall results are shown in Table 1.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.ibb.co/5F54hKF/table1.png&quot; alt=&quot;Table1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This table shows how UCC outperforms previous models.&lt;/p&gt;

&lt;p&gt;The paper conducts a comparative analysis between the proposed method and LightGCN in the context of cold-start recommendation. Items are categorized into ten groups based on popularity, ensuring equal interaction numbers for each group. The last two groups represent cold-start items, with higher GroupID values indicating warmer items. The following figures show how UCC has better performance in all groups&lt;/p&gt;

&lt;p&gt;Unlike other methods that often sacrifice warm item accuracy to improve cold items, the proposed method notably enhances the recall of LightGCN, especially for cold items. For instance, on the Yelp dataset, the proposed method achieves nearly a 7-fold increase in recall compared to LightGCN. The most significant improvement is observed for group-id 1 items in Amazon-Book, with recall improving by 400%, underscoring the methodâ€™s effectiveness in addressing cold-start recommendation challenges and highlighting the â€œseesaw phenomenonâ€ problem.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.ibb.co/j87vzrD/figure2.png&quot; alt=&quot;figure2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In their ablation study, the result shows that the number of generated interactions is adaptive for different item groups. It notes that low-uncertainty interactions, which are more abundant for cold items, help alleviate the distribution difference between warm and cold items. Using item-side-generated interactions significantly improves performance, while user-side-generated interactions exacerbate the distribution gap. This underscores the effectiveness of the uncertainty-aware interaction generation component. Also, as shown in the following figure teacher-student learning outperforms other methods.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.ibb.co/HhYyCx0/figure3.png&quot; alt=&quot;figure3&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;5-conclusion&quot;&gt;&lt;strong&gt;5. Conclusion&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;This paper tackles the Cold-Start problem in recommendation systems by introducing the Uncertainty-aware Consistency Learning framework (UCC). UCCâ€™s Uncertainty-aware Interaction Generation effectively bridges the gap between cold and warm items, resulting in notable improvements in recommendation performance.&lt;/p&gt;

&lt;p&gt;The Teacher-Student Consistency Learning component further enhances recommendation quality, addressing the seesaw phenomenon. Extensive ablation studies and experiments on benchmark datasets showcase the effectiveness of the UCC model, consistently outperforming existing approaches, especially in improving recall for cold items. This paper offers a promising solution to enhance recommendation systems, particularly in dynamic settings.&lt;/p&gt;

&lt;p&gt;However, it may face challenges related to complexity, scalability, and generalizability, and a broader evaluation with diverse datasets and consideration of interpretability is needed to establish its practical applicability. Additionally, a more in-depth analysis of its performance compared to a wider range of state-of-the-art approaches would provide a more comprehensive understanding of its competitiveness.&lt;/p&gt;

&lt;h2 id=&quot;author-information&quot;&gt;&lt;strong&gt;Author Information&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Author Name:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Taichi Liu&lt;/li&gt;
      &lt;li&gt;Chen Gao&lt;/li&gt;
      &lt;li&gt;Zhenyu Wang&lt;/li&gt;
      &lt;li&gt;Dong Li&lt;/li&gt;
      &lt;li&gt;Jianye Hao&lt;/li&gt;
      &lt;li&gt;Depeng Jin&lt;/li&gt;
      &lt;li&gt;Yong Li&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Affiliation:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Tsinghua University&lt;/li&gt;
      &lt;li&gt;Huawei Noahâ€™s Ark Lab&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Research Topic:&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Uncertainty-aware Consistency Learning for Cold-Start Item Recommendation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
            <pubDate>Mon, 16 Oct 2023 00:00:00 +0900</pubDate>
            <link>http://dsailatkaist.github.io/2023-10-16-Uncertainty-aware_Consistency_Learning_for_Cold-Start_Item_Recommendation.html</link>
            <guid isPermaLink="true">http://dsailatkaist.github.io/2023-10-16-Uncertainty-aware_Consistency_Learning_for_Cold-Start_Item_Recommendation.html</guid>
            
            <category>reviews</category>
            
            
        </item>
        
        <item>
            <title>[RecSys 2023] Trending Now: Modeling Trend Recommendations</title>
            <description>&lt;p&gt;&lt;strong&gt;[Recsys 2023] Trending Now: Modeling Trend Recommendations&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Info&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;â–¢   &lt;strong&gt;Authors&lt;/strong&gt; : AWS AI Labs, Amazon USA&lt;/p&gt;

&lt;p&gt;â–¢   &lt;strong&gt;Research topic&lt;/strong&gt; : Trending now (í˜„ì¬ ì¸ê¸°ìˆëŠ” ìƒí’ˆì„ ì¶”ì²œí•˜ëŠ” ì¶”ì²œì‹œìŠ¤í…œ)&lt;/p&gt;

&lt;p&gt;â–¢   &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3604915.3608810&quot;&gt;paper links&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Research Motivation &amp;amp; ë…¼ë¬¸ ì„ ì • ì´ìœ &lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ìµœê·¼ ì¶”ì²œì‹œìŠ¤í…œì—ì„œëŠ” ì¼ë°˜ì ìœ¼ë¡œ â€œí˜„ì¬ ì¸ê¸°ìˆëŠ” ìƒí’ˆ (trending now)â€ ê³¼ ê°™ì€ ë³„ë„ì˜ ì¶”ì²œ í•­ëª©ì„ ì œì‹œí•´ì£¼ì–´ í•´ë‹¹ ìƒí’ˆì˜ ì¸ê¸°ë¥¼ ë†’ì—¬ í™œì„±ìœ ì €ë¥¼ ìœ ì¹˜í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì¼ë°˜ì ìœ¼ë¡œ â€œì‹œê°„ ê°„ê²© ë‚´ interaction ìˆ˜â€ ì™€ ê°™ì€ ë‹¨ìˆœí•œ íœ´ë¦¬ìŠ¤í‹± ë°©ë²•ì„ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì²œí•´ì£¼ê¸° ë•Œë¬¸ì— rich-get-richer (ì¸ê¸°ìˆëŠ” ìƒí’ˆë§Œ ë” ë…¸ì¶œë˜ì–´ ë” ë§ì€ ì¸ê¸°ë¥¼ ì–»ëŠ”) ë¬¸ì œ ë“± ê°œì„ ì˜ ì—¬ì§€ê°€ ë§ì´ ë‚¨ì•„ìˆìœ¼ë©°, ì¶”ì²œì‹œìŠ¤í…œì—ì„œ trend ë¥¼ ëª¨ë¸ë§í•˜ëŠ” ì—°êµ¬ëŠ” ì œí•œì ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ í•´ë‹¹ ë…¼ë¬¸ì€ &lt;strong&gt;ì‹œê³„ì—´ ì˜ˆì¸¡&lt;/strong&gt; ì´ë¼ëŠ” ìƒˆë¡œìš´ ê´€ì ì—ì„œ &lt;strong&gt;trend ë¥¼ ë°˜ì˜í•œ ì¶”ì²œì‹œìŠ¤í…œ ëª¨ë¸ì„ ì œì•ˆ&lt;/strong&gt;í•©ë‹ˆë‹¤. item trendiness ì— ëŒ€í•œ ì •ì˜ë¥¼ í†µí•´ trend recommendation task ë¥¼ &lt;strong&gt;one-step time series forecasting&lt;/strong&gt; ë¬¸ì œë¡œ ê³µì‹í™”í•©ë‹ˆë‹¤. item ì˜ ë¯¸ë˜ trend ë¥¼ ì˜ˆì¸¡í•˜ê³  ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” deep latent variable ëª¨ë¸ì¸ &lt;strong&gt;TrendRec&lt;/strong&gt; ì„ ì œì•ˆí•©ë‹ˆë‹¤.
í˜„ì¬ ì‹œì ì—ì„œì˜ trend ë¥¼ íŒŒì•…í•˜ê³  ìœ ì €ì—ê²Œ ê´€ë ¨ item ì„ ì¶”ì²œí•˜ëŠ” ê²ƒì„ ì‹œê³„ì—´ ì˜ˆì¸¡ìœ¼ë¡œ ì ‘ê·¼í•œ ê´€ì ì´ ì‹ ì„ í–ˆê³ , â€œaccelerationâ€ ì´ë¼ëŠ” ê°œë…ì„ ì •ì˜í•˜ì—¬ ì¸ê¸°ê°€ â€œë¹ ë¥´ê²Œ ìƒìŠ¹â€ í•˜ê³  ìˆê³  ê°€ê¹Œìš´ ë¯¸ë˜ì— ì¸ê¸°ë¥¼ ì–»ì„ ê°€ëŠ¥ì„±ì´ ìˆëŠ” item ì„ ì¶”ì²œí•œë‹¤ëŠ” ì•„ì´ë””ì–´ê°€ ì°¸ì‹ í•œ ê²ƒ ê°™ì•„ í•´ë‹¹ ë…¼ë¬¸ì„ ì„ ì •í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;1-introudction&quot;&gt;1. Introudction&lt;/h3&gt;

&lt;h4 id=&quot;1--definition&quot;&gt;1-â‘ . Definition&lt;/h4&gt;

&lt;h5 id=&quot;-ìš©ì–´ì •ì˜&quot;&gt;â–¶ ìš©ì–´ì •ì˜&lt;/h5&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;ìš©ì–´&lt;/th&gt;
      &lt;th&gt;ë‚´ìš©&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Popularity&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;íŠ¹ì • ì‹œê°„ ê°„ê²© ë‚´ì— ë°œìƒí•œ interaction ìˆ˜&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;trend in the recommendation context&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;ì¸ê¸°ë„ (popularity) ë˜ëŠ” ê°€ì†ë„ (acceleration)ì˜ &lt;strong&gt;ë³€í™”ìœ¨&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;trending now&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;í˜„ì¬ ì‹œì ì—ì„œ ì ì  ë” ë§ì€ interaction ì´ ë°œìƒí•˜ëŠ” item list ì´ì§€ë§Œ, í•´ë‹¹ item ë“¤ì€ ë°˜ë“œì‹œ ê°€ì¥ ì¸ê¸°ìˆëŠ” item ì„ ëœ»í•˜ì§„ ì•ŠëŠ”ë‹¤. ì•„ì§ ì¸ê¸°ê°€ ë†’ì§€ ì•Šì€ ìœ ë§í•œ trending up item ì— ëŒ€í•œ íƒìƒ‰ì´ ê°€ëŠ¥í•˜ë¯€ë¡œ popularity bias ì—†ì´ íš¨ê³¼ì ìœ¼ë¡œ item ì„ ì¶”ì²œí•œë‹¤.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h5 id=&quot;-trending-now-ì˜-target-item-ì˜ˆì‹œ&quot;&gt;â–¶ trending now ì˜ target item ì˜ˆì‹œ&lt;/h5&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Example&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;â€¢  ìµœê·¼ ì¶œì‹œëœ ì¢‹ì€ í’ˆì§ˆì˜ cold items (ex. ì™•ì¢Œì˜ ê²Œì„ ìƒˆë¡œìš´ ì—í”¼ì†Œë“œ) = Recently released cold items of good quality&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;â€¢  ê°‘ì‘ìŠ¤ëŸ¬ìš´ ë³€í™”ê°€ ë°œìƒí•˜ëŠ” í•­ëª© (ex. ì˜í™”ê°€ ì˜¤ìŠ¤ì¹´ìƒì„ ìˆ˜ìƒí•˜ì—¬ ê°‘ìê¸° ìœ í–‰í•˜ëŠ” ê²½ìš°) = Items experiencing sudden changes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;â€¢  ì£¼ê¸°ì ìœ¼ë¡œ ìœ í–‰í•˜ëŠ” ì˜¤ë˜ ì§€ì†ë˜ëŠ” í’ˆëª© (ex. ê²¨ìš¸ì˜ë¥˜ì™€ ê°™ì´ ê³„ì ˆì ì¸ ì˜í–¥ì„ ë°›ëŠ” í’ˆëª©) = Long lasting items with periodic up-trend&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;1--challenge&quot;&gt;1-â‘¡. Challenge&lt;/h4&gt;

&lt;p&gt;â–¢  &lt;strong&gt;Problem&lt;/strong&gt;: í˜„ì¬ trend ë¥¼ ì•ˆì •ì ì´ê³  ì‹ ë¢°ì„± ìˆê²Œ íŒŒì•…í•˜ê¸° ìœ„í•´ì„œëŠ” ì¶©ë¶„í•œ interaction ì„ ìˆ˜ì§‘í•˜ëŠ” ë° ì¼ì • ì‹œê°„ì´ í•„ìš”í•˜ë‚˜, trend ëŠ” ì •ì˜ íŠ¹ì„±ìƒ ì—­ë™ì ìœ¼ë¡œ ë³€í™”í•˜ê³  ë°ì´í„° ìˆ˜ì§‘ ê¸°ê°„ ë™ì•ˆ ë³€ë™ì´ ìˆì„ ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;p&gt;â–¢  &lt;strong&gt;Solution&lt;/strong&gt; : trend recommendation ì„ one-step forecasting problem ìœ¼ë¡œ ê³µì‹í™”í•œë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;1--problem-setting--one-step-forecasting-problem&quot;&gt;1-â‘¢. Problem setting : One-step forecasting problem&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/hopebii/kaist_ds535/blob/main/fig1.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;â–¢ (ì™¼ìª½ ê·¸ë˜í”„) ê³¼ê±° item ì˜ trend ë³€í™”ê°€ ì£¼ì–´ì§€ë©´, &lt;strong&gt;ë‹¤ìŒ ì‹œê°„ ë‹¨ê³„ì— ì–´ë–¤ item ì´ ìœ í–‰í• ì§€ ì˜ˆì¸¡ (â†’ One-step forecasting)&lt;/strong&gt; í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•œë‹¤. ëª¨ë¸ì´ ë‹¤ìŒ ì‹œê°„ ë‹¨ê³„ì—ì„œ ìœ í–‰í•˜ëŠ” item ì„ ì˜ˆì¸¡í•˜ë©´, ë°±ì—”ë“œì—ì„œ ë°ì´í„°ë¥¼ ë²„í¼ë§í•˜ë©´ì„œ ë‹¤ìŒ ì‹œê°„ ë‹¨ê³„ ë‚´ user ì—ê²Œ í•´ë‹¹ item ì„ í‘œì‹œí•œë‹¤. ë‹¤ìŒ ì‹œê°„ ë‹¨ê³„ê°€ ëë‚˜ë©´ ëª¨ë¸ì€ ìƒˆë¡œ ì¶•ì ëœ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë°”ë¡œ ë‹¤ìŒ ì‹œê°„ ë‹¨ê³„ì— ëŒ€í•œ ìƒˆë¡œìš´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•œë‹¤. ë˜ ë‹¤ë¥¸ ë°ì´í„° ìˆ˜ì§‘ì˜ ì£¼ê¸°ë¥¼ ì‹œì‘í•˜ëŠ” ê²ƒìœ¼ë¡œ ì¶”ì²œì„ ë°˜ë³µí•œë‹¤.&lt;/p&gt;

&lt;p&gt;â–¢ (ì˜¤ë¥¸ìª½ ê·¸ë˜í”„) íŠ¹ì • use case ì— ëŒ€í•œ ìµœì í™”ëœ íŠ¸ë Œë“œ ì¶”ì²œì„ ì„¤ê³„í•˜ë ¤ë©´ ê¸°ì¡´ì˜ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¶•í•  ìˆ˜ ìˆë‹¤. ë”ë¶ˆì–´ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ ì™¸ì—ë„ ì¶”ì²œ ìƒí™©ì˜ ê³ ìœ í•œ ì†ì„±ì´ í™œìš©ë  ìˆ˜ ìˆë‹¤. ì‹œê³„ì—´ì—ì„œ ê° item ì— ëŒ€í•œ ëŒ€ëµì ì¸ ëˆ„ì  interaction ìˆ˜ ì™¸ì—ë„ ë” ì„¸ë¶„í™”ëœ user-item ê°„ interaction ì´ ì¡´ì¬í•œë‹¤. íŠ¹ì • item ì— ëŒ€í•´ ì–¼ë§ˆë‚˜ ë§ì€ user ê°€ í•´ë‹¹ item ê³¼ ìƒí˜¸ì‘ìš© í–ˆëŠ”ì§€ ì•Œ ìˆ˜ ìˆì„ ë¿ ì•„ë‹ˆë¼ ì´ëŸ¬í•œ user ê°€ ì •í™•íˆ ëˆ„êµ¬ì¸ì§€ë„ ì•Œ ìˆ˜ ìˆë‹¤. ì´ëŠ” item ê°„ ê·¼ë³¸ì ì¸ ìƒê´€ê´€ê³„ë¥¼ íŒŒì•…í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ì¶”ê°€ì ì¸ ì •ë³´ë¥¼ ì œê³µí•˜ë©° &lt;strong&gt;user ê°€ ë§ì´ ê²¹ì¹˜ëŠ” item ì€ ê³µí†µ trend íŒ¨í„´ì„ ê³µìœ  í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ trend ì˜ˆì¸¡ì˜ ì •í™•ë„ë¥¼ ë†’ì¼&lt;/strong&gt; ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;2-preliminaries&quot;&gt;2. Preliminaries&lt;/h3&gt;

&lt;h4 id=&quot;2--term-definition&quot;&gt;2-â‘ . Term Definition&lt;/h4&gt;

&lt;h5 id=&quot;-ìš©ì–´ì •ì˜-1&quot;&gt;â–¶ ìš©ì–´ì •ì˜&lt;/h5&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;ìš©ì–´&lt;/th&gt;
      &lt;th&gt;ë‚´ìš©&lt;/th&gt;
      &lt;th&gt;í‘œê¸°&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Time Step&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;ë¯¸ë¦¬ ì •ì˜í•œ ì‹œê°„ ê°„ê²© (ex. í•œì‹œê°„)&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Î”ğ‘¡&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Velocity&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;item j ì— ëŒ€í•´ì„œ, time step t ë™ì•ˆ ìˆ˜ì§‘ëœ interaction ìˆ˜ë¥¼ time step t ì—ì„œì˜ velocity ë¡œ ì •ì˜ (unit time Î”ğ‘¡ ë‹¹ item j ì˜ popularity)&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Wğ‘—ğ‘¡&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Acceleration&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;time step t ì—ì„œì˜ item j ì— ëŒ€í•œ acceleration. item j ì˜ velocity ê°€ ë‹¨ìœ„ ì‹œê°„ Î”ğ‘¡ë‹¹ Î”Wğ‘—ğ‘¡ì”© &lt;strong&gt;ë³€í™”&lt;/strong&gt;í•˜ê³  ìˆìŒì„ ë‚˜íƒ€ë‚¸ë‹¤ (&lt;strong&gt;ë³€í™”ëŸ‰&lt;/strong&gt;).&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;Ağ‘—ğ‘¡&lt;/strong&gt; = &lt;strong&gt;Î”Wğ‘—ğ‘¡&lt;/strong&gt; = Wğ‘—(ğ‘¡) - Wğ‘—(ğ‘¡-1)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h5 id=&quot;-acceleration--trend&quot;&gt;â–¶ &lt;strong&gt;Acceleration = Trend&lt;/strong&gt;&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;item ğ‘— ì˜ ì‹œê°„ ë‹¨ê³„ ğ‘¡ ì—ì„œì˜  acceleration Ağ‘—ğ‘¡ ê°€ &lt;strong&gt;ëª¨ë“  item ì˜ acceleration ì¤‘ ê°€ì¥ ë†’ìœ¼ë©´&lt;/strong&gt; í•´ë‹¹ item ì€ ì‹œê°„ ë‹¨ê³„ ğ‘¡ì—ì„œ &lt;strong&gt;trendy í•œ ê²ƒìœ¼ë¡œ ê°„ì£¼&lt;/strong&gt;í•œë‹¤.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;2--problem-definition&quot;&gt;2-â‘¡. Problem Definition&lt;/h4&gt;

&lt;h5 id=&quot;-ì ì ˆí•œ-time-interval-ex-í–¥í›„-1ì‹œê°„-trend-item-í–¥í›„-í•˜ë£¨ë™ì•ˆ-trend-item-ì´-ê´€ê±´ì´ë‹¤&quot;&gt;â–¶ ì ì ˆí•œ Time interval (ex. í–¥í›„ 1ì‹œê°„ trend item, í–¥í›„ í•˜ë£¨ë™ì•ˆ trend item) ì´ ê´€ê±´ì´ë‹¤.&lt;/h5&gt;

&lt;p&gt;í˜„ì¬ trend ë¥¼ ë¹ ë¥´ê²Œ ê°ì§€í•˜ê³  ì‹¤ì‹œê°„ìœ¼ë¡œ ì¸ê¸° ìˆëŠ” item ì„ ì¶”ì²œí•˜ëŠ” ê²ƒì´ ì´ìƒì ì´ë‹¤. ì´ë¥¼ ìœ„í•´ì„  ì¶©ë¶„í•œ interaction ì„ ì¶•ì í•˜ëŠ”ë° ì¼ì • ì‹œê°„ì´ í•„ìš”í•˜ë©´ì„œë„, trend ëŠ” ë™ì ìœ¼ë¡œ ë³€í™”í•˜ë©´ì„œ (dynamic variations) ë°ì´í„° ìˆ˜ì§‘ ê³¼ì •ì—ì„œ ì‹œê°„ì  ë³€ë™ (temporal drift) ìˆëŠ” ê²ƒì´ íŠ¹ì§•ì´ê¸° ë•Œë¬¸ì— ì ì ˆí•œ time interval Î”t ë¥¼ ì„¤ì •í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤. Î”t ê°€ ë„ˆë¬´ ì‘ë‹¤ë©´ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ë¶ˆì¶©ë¶„í•˜ì—¬ noisy ê°€ ë°œìƒí•  ìˆ˜ ìˆê³ , ë„ˆë¬´ í¬ë‹¤ë©´ ì‹œê°„ì  ë³€ë™ì„±ì´ ë°œìƒí•´ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ë‚®ì•„ì§ˆ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ë°ì´í„°ì— ë§ëŠ” feasible í•˜ê³  short í•œ ì ì ˆí•œ time interval ì„ ì°¾ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.&lt;/p&gt;

&lt;h5 id=&quot;-ì‹œê°„-ë‹¨ê³„-ê¸¸ì´-time-step-length-ì™€-ì‘ì—…-ì‹¤í–‰-ê°€ëŠ¥ì„±-task-feasibility-ê°„ì˜-ìƒê´€-ê´€ê³„ì—-ëŒ€í•œ-ê°€ì„¤&quot;&gt;â–¶ ì‹œê°„ ë‹¨ê³„ ê¸¸ì´ (time step length) ì™€ ì‘ì—… ì‹¤í–‰ ê°€ëŠ¥ì„± (task feasibility) ê°„ì˜ ìƒê´€ ê´€ê³„ì— ëŒ€í•œ ê°€ì„¤&lt;/h5&gt;

&lt;p&gt;&lt;strong&gt;bias-variance tradeoff&lt;/strong&gt; : ì‹œê°„ ë‹¨ê³„ ê¸¸ì´ê°€ ì§§ìœ¼ë©´(ì˜ˆ: 1ì‹œê°„) ë°ì´í„° í¬ì†Œì„± (data sparsity) ìœ¼ë¡œ ì¸í•´ variance ê°€ ë°œìƒí•˜ê³ , ì‹œê°„ ë‹¨ê³„ ê¸¸ì´ê°€ ê¸¸ë©´(ì˜ˆ: í•˜ë£¨) ì‹œê°„ì  ë“œë¦¬í”„íŠ¸ (temporal drift ì‹œê°„ì— ë”°ë¥¸ ë³€ë™) ë¡œ ì¸í•´ í¸í–¥ì´ ë°œìƒí•œë‹¤. ë”°ë¼ì„œ ë‘˜ ì‚¬ì´ì˜ ê· í˜•ì„ ì˜ ë§ì¶œ ìˆ˜ ìˆëŠ” sweet spot ì„ ì°¾ì•„ì•¼ í•œë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/hopebii/kaist_ds535/blob/main/fig3.png&quot; alt=&quot;fig3&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;-one-step-time-series-forecasting-task&quot;&gt;â–¶ one-step time series forecasting task&lt;/h5&gt;

&lt;p&gt;trend recommendation task ë¥¼ one-step time series forecasting ë¬¸ì œë¡œ ì •ì˜í•œë‹¤. ê° item ëŒ€í•´ ì£¼ì–´ì§„ &lt;strong&gt;historical acceleration&lt;/strong&gt; [Ağ‘—0, Ağ‘—1, . . . , Ağ‘—ğ‘¡] := &lt;strong&gt;Ağ‘—,0:ğ‘¡&lt;/strong&gt; ê³¼, covariates ì™€ ê°™ì€ ì¶”ê°€ì ì¸ &lt;strong&gt;contextual information&lt;/strong&gt; [Cğ‘—0, Cğ‘—1, . . . , Cğ‘—ğ‘¡] := &lt;strong&gt;Cğ‘—,0:t&lt;/strong&gt; ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ë‹¤ìŒ step ì¸ (t+1) ì—ì„œì˜ acceleration ì„ ì˜ˆì¸¡í•˜ê¸°ë¥¼ ì›í•œë‹¤. ê·¸ë¦¬ê³  trend prediction ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒìœ„ k ê°œì˜ ì•„ì´í…œì„ ì¶”ì²œí•œë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/hopebii/kaist_ds535/blob/main/fig2.png&quot; alt=&quot;fig2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;2--baseline-model&quot;&gt;2-â‘¢. Baseline model&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;ë„ë¦¬ ì±„íƒëœ ë‘ ê°€ì§€ íœ´ë¦¬ìŠ¤í‹± ëª¨ë¸ì„ ì„¤ëª…í•œ ë‹¤ìŒ ì¼ë°˜ì ì¸ í˜•íƒœì˜ ë”¥ëŸ¬ë‹ ê¸°ë°˜ í™•ë¥ ë¡ ì  ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ì„ baseline model ë¡œ ì‚¬ìš©í•œë‹¤.&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;-1-markov-heuristic-model&quot;&gt;â–¶ (1) Markov Heuristic Model&lt;/h5&gt;

&lt;p&gt;ì„ì˜ì˜ item j ì˜ next time step ì˜ acceleration Ağ‘— (ğ‘¡+1) ëŠ” â€œì˜¤ì§â€ í˜„ì¬ time step ì˜ acceleration Ağ‘—t ì— ì˜ì¡´í•œë‹¤ê³  ê°€ì •í•œë‹¤. ì‹¤ì œë¡œ acceleration ëŠ”  ì§§ì€ ì‹œê°„ ë™ì•ˆ ë™ì¼í•˜ê²Œ ìœ ì§€ë˜ëŠ” ê²½í–¥ì´ ìˆìœ¼ë¯€ë¡œ ë§ˆë¥´ì½”í”„ íœ´ë¦¬ìŠ¤í‹± ëª¨ë¸ì„ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•œë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/hopebii/kaist_ds535/blob/main/fig4.png&quot; alt=&quot;fig4&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;AË†ğ‘— (ğ‘¡+1) = next time step ì—ì„œ ì˜ˆì¸¡ëœ acceleration&lt;/li&gt;
  &lt;li&gt;Auto regressive model (AR) ì˜ special case ë¡œë„ ë³¼ ìˆ˜ ìˆë‹¤.&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;-2-exponential-moving-average-ema-heuristic-model&quot;&gt;â–¶ (2) Exponential Moving Average (EMA) Heuristic Model&lt;/h5&gt;

&lt;p&gt;ë§ˆë¥´ì½”í”„ íœ´ë¦¬ìŠ¤í‹± ëª¨ë¸ì˜ ê°€ì¥ í° ë‹¨ì ì€ ë‹¤ìŒ ì‹œê°„ ë‹¨ê³„ì˜ item acceleration ì´ í˜„ì¬ ì‹œê°„ ë‹¨ê³„ì˜ ì˜í–¥ì„ ë°›ê¸° ë•Œë¬¸ì— ë°ì´í„° í¬ì†Œì„± ë“±ì˜ ë¬¸ì œë¡œ ì¸í•´ ë…¸ì´ì¦ˆê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. ë”°ë¼ì„œ ì—¬ëŸ¬ ê°œì˜ ìµœì‹  ì‹œê°„ ë‹¨ê³„ë¥¼ ê³ ë ¤í•˜ì—¬ ë” ìµœê·¼ì˜ ì‹œê°„ ë‹¨ê³„ì— ë” ë§ì€ ê°€ì¤‘ì¹˜ë¥¼ í• ë‹¹í•˜ëŠ” ì§€ìˆ˜ì´ë™í‰ê·  íœ´ë¦¬ìŠ¤í‹± ëª¨ë¸ì„ ì •ì˜í•  ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/hopebii/kaist_ds535/blob/main/fig5.png&quot; alt=&quot;fig5&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;AË†ğ‘— (ğ‘¡+1) = next time step ì—ì„œ ì˜ˆì¸¡ëœ acceleration&lt;/li&gt;
  &lt;li&gt;T : ëª¨ë¸ì´ ê³ ë ¤í•˜ê³  ìˆëŠ” ìµœê·¼ ì‹œê°„ ë‹¨ê³„ ìˆ˜&lt;/li&gt;
  &lt;li&gt;wk : í˜„ì¬ ì‹œê°„ step ì—ì„œ ë©€ì–´ì§ˆìˆ˜ë¡ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ê°ì†Œí•˜ëŠ” ë¯¸ë¦¬ ì •ì˜ëœ ê°€ì¤‘ì¹˜&lt;/li&gt;
  &lt;li&gt;ARIMA model ì˜ special case ë¡œ ë³¼ ìˆ˜ ìˆë‹¤.&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;-3-deep-learning-based-time-series-forecasting-model&quot;&gt;â–¶ (3) Deep Learning based Time Series Forecasting Model&lt;/h5&gt;

&lt;p&gt;íœ´ë¦¬ìŠ¤í‹± ëª¨ë¸ì€ ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ì— ì ì‘í•  ìˆ˜ ìˆëŠ” ìœ ì—°ì„±ì´ ë¶€ì¡±í•œ ì¼ë°˜ì ì¸ ê°€ì • (general assumptions) ì„ ì¸ì½”ë”©í•œë‹¤. ê·¸ëŸ¬ë‚˜ &lt;strong&gt;acceleration íŒ¨í„´ì€ ë„ë©”ì¸(ë¦¬í…Œì¼, ë¯¸ë””ì–´, ë‰´ìŠ¤ ë“±)ì— ë”°ë¼ ë‹¤ë¥´ë‹¤&lt;/strong&gt;. ì˜ˆë¥¼ ë“¤ì–´, ë§¤ì£¼ ìˆ˜ìš”ì¼ë§ˆë‹¤ TV ì‹œë¦¬ì¦ˆì˜ ìƒˆ ì—í”¼ì†Œë“œê°€ ê³µê°œë˜ëŠ” ê²ƒê³¼ ê°™ì´ ë¦¬í…Œì¼ê³¼ ë¯¸ë””ì–´ ì˜ì—­ ëª¨ë‘ì—ì„œ ë‹¤ì–‘í•œ ì£¼ê¸°(ì¼ë³„, ì£¼ë³„, ê³„ì ˆë³„ ë“±)ì˜ ì£¼ê¸°ì  acceleration  íŒ¨í„´ì´ í’ë¶€í•˜ê²Œ ì¡´ì¬í•œë‹¤. ë°˜ëŒ€ë¡œ ë‰´ìŠ¤ëŠ” ì‹œê°„ì— ë¯¼ê°í•˜ê³  ì‚¬ëŒë“¤ì€ ê°€ì¥ ìµœê·¼ ë‰´ìŠ¤ë¥¼ íŒ”ë¡œìš°í•˜ëŠ” ê²½í–¥ì´ ìˆê¸° ë•Œë¬¸ì— ë‰´ìŠ¤ ì˜ì—­ì—ì„œëŠ” ì´ëŸ¬í•œ ê·œì¹™ì ì¸ acceleration  íŒ¨í„´ì´ ê±°ì˜ ê´€ì°°ë˜ì§€ ì•ŠëŠ”ë‹¤. ë˜í•œ ê°™ì€ ë„ë©”ì¸ ë‚´ì—ì„œë„ ë‹¤ì–‘í•œ acceleration  íŒ¨í„´ì´ ê³µì¡´í•  ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, íŠ¹ì • ì˜í™” í”Œë«í¼ì—ì„œ ìƒˆë¡œ ê°œë´‰í•œ ì•¡ì…˜ ì˜í™”ì˜ acceleration ê³¡ì„ ì€ í•´ë‹¹ í”Œë«í¼ ì‚¬ìš©ì ì»¤ë®¤ë‹ˆí‹°ì˜ ì„ í˜¸ë„ì— ë”°ë¼ ìƒˆë¡œ ê°œë´‰í•œ ë‹¤íë©˜í„°ë¦¬ ì˜í™”ì˜ acceleration ê³¡ì„ ì— ë¹„í•´ ì§€ì†ì ìœ¼ë¡œ ê°€íŒŒë¥¸ ì¦ê°€ì„¸ë¥¼ ë³´ì¼ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ &lt;strong&gt;íŠ¸ë Œë“œ ì¶”ì²œì„ ìœ„í•œ ë³´ë‹¤ ì¼ë°˜ì ì¸ ì†”ë£¨ì…˜ì€ ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ì— ì ì‘í•  ìˆ˜ ìˆëŠ” í•™ìŠµ ê°€ëŠ¥í•œ ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ì„ ì„¤ê³„&lt;/strong&gt;í•˜ëŠ” ê²ƒì´ë‹¤. ëª¨ë¸ì„ ê³µì‹í™”í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/hopebii/kaist_ds535/blob/main/fig6.png&quot; alt=&quot;fig6&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ğ‘“seq (Â·) : ê³¼ê±° acceleration  ë¥¼ ì§‘ê³„í•˜ê³  ë‹¤ìŒ ì‹œê°„ ë‹¨ê³„ì—ì„œ acceleration  ì˜ í™•ë¥ ì  ë¶„í¬ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ìˆœì°¨ì  ëª¨ë¸ë¡œ DeepAR, RNN, MQCNN, TFT ë“±ì´ ìˆë‹¤.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;3-model--collaborative-time-series-forecasting-model-with-user-item-interactions-trendrec&quot;&gt;3. Model : collaborative time series forecasting model with user-item interactions (TRENDREC)&lt;/h3&gt;

&lt;h4 id=&quot;3--two-phase-framework&quot;&gt;3-â‘ . Two-phase framework&lt;/h4&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;phase&lt;/th&gt;
      &lt;th&gt;objective&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;1. ë‹¤ìŒ item ì¶”ì²œ&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;ë³´ì¡°ì ì¸ ëª©í‘œë¡œ í•˜ì—¬, TrendRecì€ user-item ê°„ interactive signal ì„ í™œìš©í•˜ì—¬ item ê°„ì˜ ê¸°ë³¸ ìƒê´€ê´€ê³„ë¥¼ ê°ì§€í•˜ê³  ì´ëŸ¬í•œ ì§€ì‹ì„ item embedding ìœ¼ë¡œ ì¸ì½”ë”©í•œë‹¤.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;2. ë‹¤ìŒ time step ì˜ trend (=acceleration) ì˜ˆì¸¡&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;í•™ìŠµëœ item embedding ì„ ì‚¬ìš©í•˜ì—¬ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª©í‘œë¥¼ ìœ„í•´ ê° ì‹œê³„ì—´ì— ëŒ€í•œ ì¶”ê°€ context ë¥¼ ì œê³µí•œë‹¤.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;3--model-overview&quot;&gt;3-â‘¡. Model overview&lt;/h4&gt;

&lt;blockquote&gt;
  &lt;p&gt;TrendRec : RecSys + Time series forecasting&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;ì¶”ì²œ ëª¨ë¸ì€ user-item interaction ì„ í†µí•´ ë‹¤ìŒ ì•„ì´í…œ ì¶”ì²œ objective ë¥¼ í•™ìŠµí•œë‹¤.
    &lt;ul&gt;
      &lt;li&gt;item feature ì— ëŒ€í•œ representation learning ì„ í†µí•´ dense latent item embedding ì„ ìƒì„±í•œë‹¤. ì´ë¥¼ í†µí•´ item ê°„ correlation ì„ íŒŒì•…í•˜ì—¬ ì‹œê³„ì—´ ì˜ˆì¸¡ì— ëŒ€í•œ ì¶”ê°€ì ì¸ context ë¥¼ ì œê³µí•œë‹¤. Item correlation ì„ ì¸ì½”ë”©í•˜ëŠ” shared latent item embeddings ì„ í†µí•´ ë‘ objectives ê°€ ì—°ê²°ëœë‹¤.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ì€ itemì˜ accelerations ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ ë‹¨ê³„ acceleration ì˜ˆì¸¡ objective ì— ëŒ€í•´ í•™ìŠµí•œë‹¤.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;probabilistic graphical model (PGM)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/hopebii/kaist_ds535/blob/main/fig7.png&quot; alt=&quot;fig7&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;-ë…¸ë“œ&quot;&gt;â–¸ &lt;strong&gt;ë…¸ë“œ&lt;/strong&gt;&lt;/h5&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;í‘œê¸°&lt;/th&gt;
      &lt;th&gt;ë‚´ìš©&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Vğ‘—t âˆˆ R(D)&lt;/td&gt;
      &lt;td&gt;âˆ˜  &lt;strong&gt;item ğ‘—â€™s properties&lt;/strong&gt; till time step t  both static properties and dynamic properties &lt;br /&gt; âˆ˜  ğ· is the hidden dimension of the embedding &lt;br /&gt; âˆ˜ &lt;strong&gt;latent item embedding&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ğœ†v&lt;/td&gt;
      &lt;td&gt;âˆ˜  &lt;strong&gt;hyperparameters&lt;/strong&gt; related to distribution variance of latent item embedding&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Ağ‘—,0:t âˆˆ R(ğ‘ğ‘—t)&lt;/td&gt;
      &lt;td&gt;âˆ˜  &lt;strong&gt;item ğ‘—â€™s historical acceleration&lt;/strong&gt; till time step ğ‘¡ which is [Ağ‘—0, Ağ‘—1, . . . ,Ağ‘—ğ‘¡]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Ağ‘—(ğ‘¡+1) âˆˆ R&lt;/td&gt;
      &lt;td&gt;âˆ˜  the acceleration of item ğ‘— at the next time step ğ‘¡ + 1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ğ‘ğ‘—t&lt;/td&gt;
      &lt;td&gt;âˆ˜  the number of historical time steps of item ğ‘— till time step t&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Uğ‘–t âˆˆ R(D)&lt;/td&gt;
      &lt;td&gt;âˆ˜  &lt;strong&gt;user ğ‘–â€™s interests&lt;/strong&gt; till time step t &lt;br /&gt; âˆ˜  &lt;strong&gt;latent user embedding&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ğœ†u&lt;/td&gt;
      &lt;td&gt;âˆ˜  hyperparameters related to distribution variance of latent user embedding&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Sğ‘–t âˆˆ R(Nğ‘–t x D)&lt;/td&gt;
      &lt;td&gt;âˆ˜  &lt;strong&gt;user ğ‘–â€™s historical interaction sequence&lt;/strong&gt; till time step ğ‘¡ &lt;br /&gt; âˆ˜  embedding matrix and each row of it represents an item embedding&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ğ‘ğ‘–t&lt;/td&gt;
      &lt;td&gt;âˆ˜  the number of interactions from user ğ‘– till time step t&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Rğ‘–ğ‘—t âˆˆ {0, 1}&lt;/td&gt;
      &lt;td&gt;âˆ˜  &lt;strong&gt;interaction label&lt;/strong&gt; denoting whether user ğ‘– interacted with item ğ‘— at time step t&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h5 id=&quot;-ì—£ì§€&quot;&gt;â–¸ &lt;strong&gt;ì—£ì§€&lt;/strong&gt;&lt;/h5&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;í‘œê¸°&lt;/th&gt;
      &lt;th&gt;ë‚´ìš©&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Edge Sğ‘–ğ‘¡ â†’ Uğ‘–t&lt;/td&gt;
      &lt;td&gt;âˆ˜  user ì˜ ì´ì „ interactions ëŠ” user ì˜ interest ë¥¼ í‘œí˜„í•˜ë©°, ì´ëŠ” userì˜ ë‹¤ìŒ í–‰ë™ì— ì˜í–¥ì„ ë¯¸ì¹œë‹¤. &lt;br /&gt; âˆ˜  ex. íœ´ëŒ€í°ì„ êµ¬ë§¤í•œ ì‚¬ìš©ìê°€ ë‹¤ìŒì— íœ´ëŒ€í° ì•¡ì„¸ì„œë¦¬ë¥¼ êµ¬ë§¤í•  ìˆ˜ ìˆë‹¤.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Edge {Uğ‘–ğ‘¡, Vğ‘—ğ‘¡} â†’ Rğ‘–ğ‘—ğ‘¡&lt;/td&gt;
      &lt;td&gt;âˆ˜  Interaction ì€ user interests Uğ‘–ğ‘¡ ì™€ item properties Vğ‘—t ì— ë”°ë¼ ë‹¬ë¼ì§„ë‹¤.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Edge {Vğ‘—ğ‘¡, Ağ‘—,0:ğ‘¡ } â†’ Ağ‘— (ğ‘¡+1)&lt;/td&gt;
      &lt;td&gt;âˆ˜ ë‹¤ìŒ ì‹œê°„ ë‹¨ê³„ ğ‘¡ +1ì—ì„œ ì•„ì´í…œ ğ‘–ì˜ acceleration ëŠ” item feature ì™€ item ì˜ ê³¼ê±° acceleration ì— ì˜í–¥ì„ ë°›ëŠ”ë‹¤. &lt;br /&gt; âˆ˜ ex. ì•¡ì…˜ ì˜í™”ëŠ” íŠ¹ì • ì›¹ì‚¬ì´íŠ¸ì˜ ì‚¬ìš©ì ì»¤ë®¤ë‹ˆí‹°ì—ì„œ íŠ¸ë Œë“œê°€ ë  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤. &lt;br /&gt; âˆ˜ ex. ì£¼ê°„ trend íŒ¨í„´ì´ ì£¼ê¸°ì ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” item&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h5 id=&quot;-generative-process&quot;&gt;â–¸ &lt;strong&gt;Generative process&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;í‰ê· ì´ ğ ê·¸ë¦¬ê³  ë¶„ì‚°ì´ diagonal covariance Î»&lt;sup&gt;-1&lt;/sup&gt;â… D ì¸ ê°€ìš°ì‹œì•ˆ ë¶„í¬ì—ì„œ latent offset vector ë¥¼ ì„¤ì •í•˜ì—¬ latent item embedding ê³¼ latent user embedding ì„ ê³„ì‚°í•œë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/hopebii/kaist_ds535/blob/main/fig8.png&quot; alt=&quot;fig8&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Rğ‘–ğ‘—t ë¥¼ êµ¬í•˜ê¸° ìœ„í•´ì„œ softmax function ì„ latent user embedding ì™€ latent item embedding ì„ ë‚´ì í•œ ê°’ì— ì ìš©í•˜ì—¬ recommendation score ë¥¼ ê³„ì‚°í•œë‹¤.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Yğ‘–ğ‘—ğ‘¡ = ğ‘“softmax(Uâ€™ğ‘–ğ‘¡â€¢Vğ‘—ğ‘¡)&lt;/li&gt;
  &lt;li&gt;Rğ‘–âˆ—ğ‘¡ ~ ğ¶ğ‘ğ‘¡([Yğ‘–ğ‘—ğ‘¡]), j: 1,,..,J , ğ¶ğ‘ğ‘¡ is categorical distribution.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;3--training&quot;&gt;3-â‘¢. Training&lt;/h4&gt;

&lt;h5 id=&quot;-maximum-a-posteriori-map-estimation&quot;&gt;â–¸ &lt;strong&gt;Maximum a Posteriori (MAP) Estimation&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;maximum a posteriori (MAP) estimation&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/hopebii/kaist_ds535/blob/main/fig11.png&quot; alt=&quot;fig11&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ë‹¤ìŒ item ì„ ì¶”ì²œí•˜ëŠ” ê²ƒì— ìˆì–´ì„œ interaction Rğ‘–ğ‘—t ì— ëŒ€í•œ ì¡°ê±´ë¶€ í™•ë¥ ì„ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•œë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/hopebii/kaist_ds535/blob/main/fig9.png&quot; alt=&quot;fig9&quot; /&gt;&lt;/p&gt;

&lt;p&gt;item accelerations Ağ‘—(ğ‘¡+1) ì— ëŒ€í•œ ì¡°ê±´ë¶€ í™•ë¥ ì„ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•œë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/hopebii/kaist_ds535/blob/main/fig10.png&quot; alt=&quot;fig10&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ğ‘“ğ‘¡ğ‘  (Â·) : ë‹¤ìŒ ì‹œê°„ ë‹¨ê³„ ğ‘¡ì—ì„œ acceleration ì˜ í™•ë¥ ì  ë¶„í¬ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ item ì˜ ê³¼ê±° acceleration ì™€ latent item embedding ì„ ëª¨ë‘ ì‚¬ìš©í•˜ëŠ” ëª¨ë“  ìœ í˜•ì˜ í™•ë¥ ë¡ ì  ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;-negative-log-likelihood-nll&quot;&gt;â–¸ &lt;strong&gt;Negative Log Likelihood (NLL)&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;posterior probability ë¥¼ ìµœëŒ€í™” í•˜ëŠ” ê²ƒì€ negative log likelihood ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤. NLL ì€ ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/hopebii/kaist_ds535/blob/main/fig12.png&quot; alt=&quot;fig12&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;(10) : Next Item Recommendation Loss â†’ ì´ë¥¼ ìµœì†Œí™”í•˜ë©´ í•™ìŠµ ì„¸íŠ¸ì—ì„œ ë‹¤ìŒ í•­ëª© ì¶”ì²œ ì„±ëŠ¥ì´ í–¥ìƒëœë‹¤.&lt;/li&gt;
  &lt;li&gt;(11) : Time Series Forecasting Loss â†’ ì´ term ì„ ìµœì†Œí™”í•˜ë©´ í›ˆë ¨ ì„¸íŠ¸ì—ì„œ acceleration ì˜ˆì¸¡ì´ í–¥ìƒëœë‹¤.&lt;/li&gt;
  &lt;li&gt;(12) : Regularizing Latent Item Embedding Vğ‘—t and Latent User Embedding Uğ‘–t  â†’  Vğ‘—t ë¥¼ zero-mean Gaussian prior ì— ê·¼ì ‘í•˜ê²Œ ì •ê·œí™”í•˜ê³  Uğ‘–ğ‘¡ ë¥¼ ìœ ì €ì˜ ê³¼ê±°ì´ë ¥ì´ ìœ ì €ì˜ í¥ë¯¸ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤ê³  ê°€ì •í•˜ê³ , ê³„ì‚°ëœ  ğ‘“seq (Sğ‘–ğ‘¡) likelihood function adopted by the probabilistic time series forecasting modelì— ê·¼ì ‘í•˜ê²Œ ì •ê·œí™”í•œë‹¤.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;3--inference&quot;&gt;3-â‘£. Inference&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/hopebii/kaist_ds535/blob/main/fig13.png&quot; alt=&quot;fig13&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;V*jt : the posterior of item jâ€™s latent item embedding&lt;/li&gt;
  &lt;li&gt;ğ‘“âˆ—ts (Â·) : the trained sequential time series forecasting model&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;3--model-architecture&quot;&gt;3-â‘¤. Model architecture&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/hopebii/kaist_ds535/blob/main/fig14.png&quot; alt=&quot;fig14&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ì™¼ìª½ê·¸ë¦¼ : overview network structure&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ì˜¤ë¥¸ìª½ê·¸ë¦¼ : figure visualizes the full details of the TrendRec implementation&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;the model contains two principal components
    &lt;ul&gt;
      &lt;li&gt;(1) a sequential recommender :   &lt;strong&gt;Rğ‘–ğ‘—t&lt;/strong&gt; â‡¨  recommendation score ëŠ” latent user embedding Uğ‘–t ê³¼ latent item embedding Vğ‘—t ì‚¬ì´ì˜ ë‚´ì  ê³±ì„ ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°ëœë‹¤.&lt;/li&gt;
      &lt;li&gt;(2) collaborative time series forecasting model :   &lt;strong&gt;Ağ‘—(ğ‘¡+1)&lt;/strong&gt; â‡¨  ë‹¤ìŒ item ì¶”ì²œ (1) ê³¼ì •ì—ì„œ pre-trained ëœ latent item embedding ì„ ê°€ì ¸ì™€ item historical acceleration ì™€ í•¨ê»˜ í™œìš©í•˜ì—¬ ë‹¤ìŒ ì‹œê°„ ë‹¨ê³„ì—ì„œì˜ acceleration ë¥¼ ì˜ˆì¸¡í•œë‹¤.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;â†’ 2ê°€ì§€ ìš”ì†ŒëŠ” í•™ìŠµê°€ëŠ¥í•œ latent item embedding ì„ í†µí•´ join ëœë‹¤.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;4-experiments&quot;&gt;4. Experiments&lt;/h3&gt;

&lt;p&gt;â–¢ &lt;strong&gt;Research question&lt;/strong&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Research question&lt;/th&gt;
      &lt;th&gt;ë‚´ìš©&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Q1&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;task feasibility ì™€ time step length ì˜ correlations ì— ëŒ€í•´ ì œì•ˆí•œ ê°€ì„¤ (ì ì ˆí•œ Î”ğ‘¡ ì˜ ì¡´ì¬) ì´ ì ìš©ë˜ëŠ”ì§€, ê° dataset ì˜ time step length ëŠ” ì–´ë–»ê²Œ ì„ íƒí•´ì•¼ í•˜ëŠ”ì§€&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Q2&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;TrendRecì´ íœ´ë¦¬ìŠ¤í‹± ëª¨ë¸ê³¼ ê¸°ë³¸ì ì¸ ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ì„ í¬í•¨í•œ ëª¨ë“  ê¸°ì¤€ ëª¨ë¸ë³´ë‹¤ ì„±ëŠ¥ì´ ìš°ìˆ˜í•œì§€&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;4--datasets&quot;&gt;4-â‘ . Datasets&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/hopebii/kaist_ds535/blob/main/fig15.png&quot; alt=&quot;fig15&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ë¦¬í…Œì¼ (TaoBao), ë¯¸ë””ì–´(Netflix), ë‰´ìŠ¤(MIND)ë¥¼ í¬í•¨í•œ ë‹¤ì–‘í•œ ë„ë©”ì¸ì˜ ë°ì´í„°ë¥¼ ì´ìš©
    &lt;ul&gt;
      &lt;li&gt;TaoBao ì˜ ê²½ìš°  ì•„ì´í…œ ì¹´í…Œê³ ë¦¬ê°€ í¬ê¸° ë•Œë¬¸ì— , 3ê°œì˜ êµ¬ë¶„ëœ ë°ì´í„°ì…‹ì„ êµ¬ì¡°í™”í•˜ê¸° ìœ„í•´ ì¸í„°ë™ì…˜ ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒìœ„ 3ê°œì˜ ì•„ì´í…œ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•œë‹¤ â†’ TaoBao Cat1, TaoBao Cat2, TaoBao Cat3&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ë‹¤ìŒ item ì¶”ì²œ objective ì™€ ì‹œê³„ì—´ ì˜ˆì¸¡ objective ì‚¬ì´ì— ì‹œê°„ì  ëˆ„ìˆ˜ (temporal leakage) ê°€ ë°œìƒí•˜ì§€ ì•Šë„ë¡ ì—„ê²©í•œ ì‹¤í—˜ì„¤ì •ì„ ì ìš© : ëª¨ë“  training interactions ì´ ëª¨ë“  testing interactions ë³´ë‹¤ ë¨¼ì € ë°œìƒí•˜ë„ë¡ ë°ì´í„°ë¥¼ ì‹œê°„ì ìœ¼ë¡œ ë¶„í• í•˜ê³ , training ë‹¨ê³„ì—ì„œ ë‘ objective ì— ëŒ€í•´ ì •í™•í•˜ê²Œ ë™ì¼í•œ í›ˆë ¨ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œë‹¤.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;4--evaluated-methods&quot;&gt;4-â‘¡. Evaluated methods&lt;/h4&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Mtehods&lt;/th&gt;
      &lt;th&gt;ì„¤ëª…&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Oracle&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;âˆ˜  ë‹¤ìŒì‹œê°„ ë‹¨ê³„ì—ì„œ ì‹¤ì œ ì •ë‹µ (ground truth) ë¯¸ë˜ acceleration ì— ì ‘ê·¼í•  ìˆ˜ ìˆë‹¤. &lt;br /&gt; âˆ˜  í•­ìƒ acceleration ì„ ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•˜ê³  ìƒìœ„ k ê°œì˜ íŠ¸ë Œë“œ ì•„ì´í…œì„ ì¶”ì²œí•œë‹¤.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Random&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;âˆ˜  ì „ì²´ ì•„ì´í…œ ì¹´íƒˆë¡œê·¸ì—ì„œ replacement ì—†ì´ ì „ì²´ ì•„ì´í…œìœ¼ë¡œë¶€í„° random selection ì„ í•˜ì—¬ ì•„ì´í…œì„ ì¶”ì²œí•´ì¤€ë‹¤.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Exponential moving average (EMA)&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;âˆ˜  ì§€ìˆ˜ì´ë™í‰ê· ì€ ìµœê·¼ m ì‹œê°„ ë‹¨ê³„ (latest ğ‘š time steps) ì˜ acceleration ì˜ ê°€ì¤‘ì¹˜ í•©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ì‹œê°„ ë‹¨ê³„ì˜ acceleration ì„ ì˜ˆì¸¡í•˜ëŠ” ê·œì¹™ê¸°ë°˜ ëª¨ë¸ì´ë‹¤. (m=8) &lt;br /&gt;  âˆ˜  ê°€ì¤‘ì¹˜ëŠ” í˜„ì¬ ì‹œê°„ ë‹¨ê³„ë¡œë¶€í„° ë©€ì–´ì§€ëŠ” ì‹œê°„ ë‹¨ê³„ìˆ˜ê°€ ì¦ê°€í•¨ì— ë”°ë¼ 0.75ì˜ ê³„ìˆ˜ë¡œ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ê°ì†Œí•œë‹¤.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;DeepAR&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;âˆ˜  auto-regressive RNN ì— ê¸°ë°˜í•œ SOTA ì‹œê³„ì—´ ëª¨ë¸ ì¤‘ í•˜ë‚˜ì´ë‹¤.&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;TrendRec&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;âˆ˜  ë³¸ ì—°êµ¬ì—ì„œ ì œì•ˆí•œ ëª¨ë¸ë¡œ two-phase ë¡œ ì´ë£¨ì–´ì ¸ìˆë‹¤. &lt;br /&gt; âˆ˜ ë‹¤ìŒ ì•„ì´í…œ ì¶”ì²œì„ ìœ„í•´ GRU4Rec ì„ ì±„íƒí•´ latent item embedding ì„ í•™ìŠµí•œë‹¤. &lt;br /&gt; âˆ˜ ì‹œê³„ì—´ ì˜ˆì¸¡ì„ ìœ„í•´ì„  DeepAR ëª¨ë¸ì„ ì‚¬ìš©í•œë‹¤. ìµœì‹  ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ ì¤‘ í•˜ë‚˜ì´ê³ , ë„ë¦¬ ì±„íƒë˜ê³  ìˆê¸° ë•Œë¬¸ì´ë‹¤.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;4--evaluation-metrics&quot;&gt;4-â‘¢. Evaluation metrics&lt;/h4&gt;

&lt;p&gt;ì‹œê³„ì—´ ì˜ˆì¸¡ ì„¤ì •ì—ì„œ RMSEì™€ ê°™ì€ í‰ê°€ ì§€í‘œë¥¼ ì±„íƒí•˜ëŠ” ëŒ€ì‹ , ë‹¤ìŒ ì‹œê°„ ë‹¨ê³„ì—ì„œ íŠ¸ë Œë“œ ì•„ì´í…œì„ ì¶”ì²œí•˜ëŠ” íŠ¸ë Œë“œ ì¶”ì²œ ì§‘í•©ì˜ ëª©í‘œì— ë°€ì ‘í•˜ê²Œ ë¶€í•©í•˜ëŠ” í‰ê°€ ì§€í‘œë¥¼ ì„¤ê³„í•œë‹¤.&lt;/p&gt;

&lt;h5 id=&quot;-1-acceleration-metric&quot;&gt;â–¸ (1) Acceleration metric&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/hopebii/kaist_ds535/blob/main/fig16.png&quot; alt=&quot;fig16&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ë‹¤ìŒ ë‹¨ê³„ ì‹œê°„ t ì˜accelerations ì— ê¸°ë°˜í•˜ì—¬ ìƒìœ„ k ê°œ item ì„ ì„ íƒ&lt;/li&gt;
  &lt;li&gt;ê·¸ëŸ° ë‹¤ìŒ ì„ íƒí•œ ğ‘˜ ì•„ì´í…œì„ ë‹¤ìŒ ì‹œê°„ ë‹¨ê³„ ğ‘¡ì—ì„œ í•´ë‹¹ ground truth acceleration ì— ë‹¤ìŒê³¼ ê°™ì´ ë§µí•‘&lt;/li&gt;
  &lt;li&gt;acceleration ì´ trend ì˜ ì •ëŸ‰ì ì¸ ì¸¡ì • (quantitative measurement) ì´ê¸° ë•Œë¬¸ì— item ì˜ ë‹¤ìŒ ì‹œê°„ ë‹¨ê³„ì˜ ì˜ˆì¸¡í•œ acceleration ì˜ ì´í•© (sum) ìœ¼ë¡œ ê³„ì‚°í•˜ê³  ëª¨ë¸ì˜ trendiness score ë¡œ ì‚¬ìš©
    &lt;ul&gt;
      &lt;li&gt;ê°’ì´ ë†’ì„ìˆ˜ë¡ ëª¨ë¸ì€ ë‹¤ìŒ ì‹œê°„ ë‹¨ê³„ì—ì„œì˜ íŠ¸ë Œë“œí•œ ì•„ì´í…œì— ëŒ€í•œ ì˜ˆì¸¡ì„ ë” ì˜í•œë‹¤.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;[0,1] ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§ì„ í•˜ê¸° ìœ„í•´ trendiness score ì˜ top ì— ëŒ€í•´ min-max normalization ì„ ì ìš©í•œë‹¤. trendiness score ì˜ upper bound ëŠ” Oracle ëª¨ë¸ì—ì„œ, lower bound ëŠ” Random ëª¨ë¸ì—ì„œ ì˜¨ë‹¤.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/hopebii/kaist_ds535/blob/main/fig18.png&quot; alt=&quot;fig18&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;-2-tndcg-metric&quot;&gt;â–¸ (2) TNDCG Metric&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/hopebii/kaist_ds535/blob/main/fig17.png&quot; alt=&quot;fig17&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Trendiness-Normalized-DCG (TNDCG) metric : ì•„ì´í…œì˜ rank position ì„ logarithmic reduction factor ë¡œ ê³ ë ¤í•œë‹¤.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/hopebii/kaist_ds535/blob/main/fig19.png&quot; alt=&quot;fig19&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;r : index the rank position&lt;/li&gt;
  &lt;li&gt;A&lt;sup&gt;p&lt;/sup&gt;&lt;sub&gt;r&lt;/sub&gt; : acceleration of item ranked at position r based on order from &lt;strong&gt;model prediction (í‘œê¸° p)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;A&lt;sup&gt;O&lt;/sup&gt;&lt;sub&gt;r&lt;/sub&gt; : acceleration of item ranked at position r based on order from &lt;strong&gt;ground truth (í‘œê¸° O)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;-3-evaluation-protocol&quot;&gt;â–¸ (3) Evaluation protocol&lt;/h5&gt;

&lt;p&gt;timestamp ë¥¼ ê¸°ì¤€ìœ¼ë¡œ training ê³¼ test step ì„ ë‚˜ëˆˆë‹¤. ê·¸ë¦¬ê³  testing ì„ ìœ„í•´ ê°€ì¥ ìµœê·¼ì˜ 20% time span ì„ ë‚¨ê¸´ë‹¤. (ì˜ˆ. eight hour training window, two-hour testing window)&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;4---hypothesis-validation-q1--ì ì ˆí•œ-Î´t-ì„ íƒí•˜ê¸°&quot;&gt;4-â‘£.  Hypothesis validation Q1 : ì ì ˆí•œ Î”t ì„ íƒí•˜ê¸°&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/hopebii/kaist_ds535/blob/main/fig20.png&quot; alt=&quot;fig20&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Markov heuristic model ì„ í™œìš©í•´ ì„±ëŠ¥ì„ í‰ê°€í•œë‹¤. ê°„ë‹¨í•˜ì§€ë§Œ generic í•œ ê°€ì •ì— ê¸°ë°˜í•œ ê¸°ì´ˆì ì¸ ëª¨ë¸ì´ê³ , ë”°ë¼ì„œ í•´ë‹¹ ëª¨ë¸ì˜ ì„±ëŠ¥ì€ task feasibility ë¥¼ ë°˜ì˜í•œë‹¤. 
ê²°ê³¼ë¥¼ ë³´ë©´, TaoBao ì™€ MIND ë°ì´í„° ì„¸íŠ¸ì˜ ê³¡ì„ ì€ ë°ì´í„° í¬ì†Œì„± ì™„í™”ë¡œ ì¸í•´ ì‹œê°„ ê°„ê²©ì´ ê¸¸ì–´ì§ˆìˆ˜ë¡ acc ì§€í‘œê°€ ë¨¼ì € ê°œì„ ëœ ë‹¤ìŒ temporal drift ë¡œ ì¸í•´ ê°ì†Œí•˜ëŠ” Q1 ê°€ì„¤ê³¼ ì¼ì¹˜í•˜ëŠ” ê²°ê³¼ë¥¼ ë³´ì¸ë‹¤. ë°˜ë©´ Netflix ë°ì´í„°ì…‹ì˜ ê²½ìš° ê³¡ì„ ì´ ê³„ì† ê°ì†Œí•˜ê³  ìˆëŠ”ë°, ì´ëŠ” time stamp ë‹¨ìœ„ê°€ í•˜ë£¨ë¡œ, ì¶©ë¶„í•œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•  ìˆ˜ ìˆì„ ë§Œí¼ ê¸¸ì§€ë§Œ temporal drift ê°€ ë°œìƒí•˜ê¸° ë•Œë¬¸ì´ë‹¤. ì „ë°˜ì ìœ¼ë¡œ ìœ„ì˜ ê²°ê³¼ëŠ” ê°€ì„¤ì„ ì…ì¦í•˜ê³  ìˆë‹¤. ê° ë°ì´í„°ì…‹ì˜ ì‹œê°„ ê°„ê²© &lt;strong&gt;Î”ğ‘¡ì„ ê° ê³¡ì„ ì˜ peak ì— ë”°ë¼ ì„ íƒ&lt;/strong&gt;í•œë‹¤. ì¼ê´€ì„±ì„ ìœ„í•´ 3ê°œì˜ TaoBao dataset ì€ ëª¨ë‘ 3ì‹œê°„, Netflix ëŠ” í•˜ë£¨, MIND ëŠ” 30ë¶„ ì‹œê°„ê°„ê²©ìœ¼ë¡œ ì„¤ì •í•œë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;4---experimental-results-q2--trendrec-ëª¨ë¸ì˜-ìš°ìˆ˜í•¨-ì¦ëª…&quot;&gt;4-â‘¤.  Experimental results Q2 : TrendRec ëª¨ë¸ì˜ ìš°ìˆ˜í•¨ ì¦ëª…&lt;/h4&gt;

&lt;p&gt;TrendRec ëª¨ë¸ì„ 3ê°œ ë„ë©”ì¸ì˜ ë°ì´í„°ì— ëŒ€í•œ ë‹¤ì–‘í•œ ë² ì´ìŠ¤ë¼ì¸ëª¨ë¸ì— ëŒ€í•´ í‰ê°€í•œë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/hopebii/kaist_ds535/blob/main/fig21.png&quot; alt=&quot;fig21&quot; /&gt;&lt;/p&gt;

&lt;p&gt;TrendRec ì´ ê°€ì¥ ì¢‹ì€ performance ë¥¼ ë³´ì¸ë‹¤. TrendRec ì˜ ì‹œê³„ì—´ ì˜ˆì¸¡ ë¶€ë¶„ì´ DeepAR ë¡œ êµ¬ì„±ë˜ì–´ ìˆëŠ”ë°, DeepAR ëŒ€ë¹„ TrendRec ì˜ ì„±ëŠ¥ í–¥ìƒì€, ë‹¤ìŒ item ì¶”ì²œ íŒŒíŠ¸ì—ì„œ ì–»ì€ pre-trained ëœ latent item embedding ì„ í™œìš©í•œ ê²ƒì´ íš¨ê³¼ì ì´ì—ˆìŒì„ ë³´ì—¬ì¤€ë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;4---findings&quot;&gt;4-â‘¥.  Findings&lt;/h4&gt;

&lt;h5 id=&quot;-deep-learning-based-models-significantly-outperform-heuristic-models&quot;&gt;â–¸ Deep learning based models significantly outperform heuristic models&lt;/h5&gt;

&lt;p&gt;ë”¥ëŸ¬ë‹ ê¸°ë°˜ì˜ ëª¨ë¸ì€ íœ´ë¦¬ìŠ¤í‹± ëª¨ë¸ë³´ë‹¤ ì„±ëŠ¥ì´ ë†’ë‹¤. íŠ¹íˆ TaoBao ê³¼ Netflix ë°ì´í„°ì…‹ì— ëŒ€í•´ DeepAR ê³¼ TrendRec ê³¼ ê°™ì€ ë”¥ëŸ¬ë‹ ê¸°ë°˜ì˜ ëª¨ë¸ì€ íœ´ë¦¬ìŠ¤í‹± ëª¨ë¸ì„ í° ì°¨ì´ë¡œ ë” ì„±ëŠ¥ì´ ë†’ê²Œ ë‚˜ì˜¨ë‹¤. í•´ë‹¹ ê²°ê³¼ëŠ” trend ì¶”ì²œì„ ìœ„í•´ í•™ìŠµê°€ëŠ¥í•œ ëª¨ë¸ì„ ì±„íƒí•˜ëŠ” ê²ƒì˜ ì¤‘ìš”ì„±ì„ ê°•ì¡°í•œë‹¤.&lt;/p&gt;

&lt;h5 id=&quot;-the-ema-model-is-worse-than-the-markov-model-in-most-cases&quot;&gt;â–¸ The EMA model is worse than the Markov model in most cases&lt;/h5&gt;

&lt;p&gt;EMA ëª¨ë¸ì€ ëŒ€ë¶€ë¶„ì˜ ê²½ìš°ì—ì„œ ë§ˆë¥´ì½”í”„ ëª¨ë¸ë³´ë‹¤ ì„±ëŠ¥ì´ ë” ì €í•˜ë˜ëŠ” ê²°ê³¼ë¥¼ ë³´ì˜€ë‹¤. ì´ëŠ” trend ê°€ ë™ì ìœ¼ë¡œ ë³€í•˜ê³   ì´ëŸ¬í•œ ê²°ê³¼ëŠ”, recency bias ë¥¼ ê°€ì§„ ê°„ë‹¨í•œ ê°€ì¤‘ì¹˜í•© (weighted sum) ë³´ë‹¤, ë‹¤ìŒ ì‹œê°„ ë‹¨ê³„ì—ì„œì˜ íŠ¸ë Œë“œì™€ ê³¼ê±° íŠ¸ë Œë“œ (historical trends) ì‚¬ì´ì— ì˜ì¡´ì ì¸ ê´€ê³„ê°€ ë³´ë‹¤ ë” ë³µì¡í•˜ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.&lt;/p&gt;

&lt;h5 id=&quot;-performance-gain-from-deep-learning-based-models-is-relatively-small-in-the-news-domain&quot;&gt;â–¸ Performance gain from deep learning based models is relatively small in the News domain&lt;/h5&gt;

&lt;p&gt;ë‰´ìŠ¤ë„ë©”ì¸ì´ ë¦¬í…Œì¼ì´ë‚˜ ë¯¸ë””ì–´ ë„ë©”ì¸ê³¼ ë¹„êµí–ˆì„ ë•Œ, ë”¥ëŸ¬ë‹ ê¸°ë°˜ì˜ ëª¨ë¸ê³¼ íœ´ë¦¬ìŠ¤í‹± ëª¨ë¸ ì‚¬ì´ì˜ ì„±ëŠ¥ ì°¨ì´ëŠ” ìƒëŒ€ì ìœ¼ë¡œ ë¯¸ë¯¸í•˜ë‹¤. (relatively marginal) ë‰´ìŠ¤ ë„ë©”ì¸ì˜ item ì‹œê³„ì—´ì„ ë¶„ì„í•´ ë³´ë©´ ì¼ë°˜ì ìœ¼ë¡œ ì•„ì´í…œì´ ì¶œì‹œë˜ë©´ ë‹¨ê¸°ê°„ì— ìµœëŒ€ acceleration ì— ë„ë‹¬í•œ í›„ ê¸‰ê²©íˆ í•˜ë½í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚œë‹¤.  ì´ëŠ” ì£¼ë¡œ ì‹œê°„ì— ë¯¼ê°í•œ ë‰´ìŠ¤ì˜ íŠ¹ì„± ë•Œë¬¸ì´ë©°, ë”¥ëŸ¬ë‹ ê¸°ë°˜ ëª¨ë¸ì— í° ë„ì „ ê³¼ì œì´ë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;5-conclusion&quot;&gt;5. Conclusion&lt;/h3&gt;

&lt;h4 id=&quot;5--summary&quot;&gt;5-â‘ . Summary&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;ì´ ì—°êµ¬ì—ì„œëŠ” ì¶”ì²œ ì‹œìŠ¤í…œì—ì„œ ì˜ ë‹¤ë£¨ì–´ì§€ì§€ ì•Šì€ ì£¼ì œì¸ trend recommender ë¥¼ ì—°êµ¬í•œë‹¤. ì„ í–‰ ì—°êµ¬ê°€ ì œí•œì ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆê¸° ë•Œë¬¸ì— trend ë¼ëŠ” ê°œë…ì„ ê³µì‹ì ìœ¼ë¡œ ì •ì˜í•˜ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘í•œë‹¤. ì´í›„ ì ì‹œì— ì•ˆì •ì ìœ¼ë¡œ trend ë¥¼ ì‹ë³„í•˜ëŠ”ë° ë¬¸ì œê°€ ë˜ëŠ” bias-variance tradeoff í˜„ìƒì„ ê´€ì°°í•˜ì—¬ ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ trend recommendation ì„ one-step time series forecasting ë¡œ ê³µì‹í™”í•œë‹¤.&lt;/li&gt;
  &lt;li&gt;ë°©ë²•ë¡  ì¸¡ë©´ì—ì„œ user-item interactive signal ì„ í™œìš©í•˜ì—¬ item ê°„ correlation ì„ íŒŒì•…í•˜ê³  ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ trend ì˜ˆì¸¡ì„ ìš©ì´í•˜ê²Œ í•˜ëŠ” TrendRec ì´ë¼ëŠ” two phase model ì„ ê°œë°œí•˜ì˜€ë‹¤.&lt;/li&gt;
  &lt;li&gt;Recommendation context ì—ì„œ trend ì˜ ê°œë…ì„ ê³µì‹ì ìœ¼ë¡œ ì •ì˜í•˜ê³  ê·¸ì— ë§ëŠ” í‰ê°€ì§€í‘œì™€ í‰ê°€ í”„ë¡œì„¸ìŠ¤ë¥¼ ìˆ˜ë¦½í–ˆë‹¤.&lt;/li&gt;
  &lt;li&gt;ë¦¬í…Œì¼, ë¯¸ë””ì–´, ë‰´ìŠ¤ ë“± ë‹¤ì–‘í•œ ì˜ì—­ì˜ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì‹¤í—˜ìœ¼ë¡œ í†µí•´ TrendRec ëª¨ë¸ì˜ íš¨ê³¼ë¥¼ ì…ì¦í–ˆë‹¤.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;5--opinion&quot;&gt;5-â‘¡. Opinion&lt;/h4&gt;

&lt;p&gt;í•´ë‹¹ ë…¼ë¬¸ì€ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ì„ ì ìš©í•˜ì—¬ trend í•œ item set ì„ ì¶”ì²œí•´ì£¼ëŠ” ë°©ë²•ë¡ ì„ ì œì•ˆí•˜ê³  ìˆìŠµë‹ˆë‹¤. user-item ê°„ interaction ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ item ì •ë³´ë¥¼ embedding í•˜ì—¬, ë” íš¨ê³¼ì ìœ¼ë¡œ ì‹œê³„ì—´ ì˜ˆì¸¡ì´ ê°€ëŠ¥í•˜ë„ë¡ ëª¨ë¸ êµ¬ì¡°ë¥¼ êµ¬ì„±í•˜ì˜€ìœ¼ë©° íŠ¹íˆ trend ë¼ëŠ” ë§¥ë½ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì ì ˆí•œ time interval ì„ ì„¤ì •í•˜ëŠ” ë° ìˆì–´ bias-variance tradeoff ë¬¸ì œë¥¼ ëª…ì‹œí•˜ê³  ê´€ë ¨ëœ í•´ê²°ì±…ì„ ì œì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤. interaction ìˆ˜ì˜ ë³€í™”ìœ¨ (acceleration) ì„ ê¸°ì¤€ìœ¼ë¡œ trend ë¥¼ ê°ì§€í•˜ë ¤ê³  í•œ ì‹œë„ê°€ ì‹ ì„ í•˜ê²Œ ë‹¤ê°€ì™”ìœ¼ë©°, ìˆ˜ì—…ì—ì„œ ë°°ì› ë˜ sequence í•œ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¶”ì²œí•´ì£¼ëŠ” ì¶”ì²œì‹œìŠ¤í…œ ëª¨ë¸ë“¤ê³¼ëŠ” ë˜ ë‹¤ë¥¸ ë§¥ë½ì˜ ì¶”ì²œ ë°©ë²•ë¡ ì¸ ê²ƒ ê°™ì•„ ì „ë°˜ì ìœ¼ë¡œ ì¸ìƒê¹Šì—ˆë˜ ë…¼ë¬¸ì´ì—ˆìŠµë‹ˆë‹¤. ë˜í•œ dataset ë§ˆë‹¤ trend ê°€ ë°œìƒí•˜ëŠ” ìƒì´í•œ íŠ¹ì§•ì— ë”°ë¼ optimal í•œ time interval ì„ ì„¤ì •í•˜ëŠ” ì ‘ê·¼ ë°©ì‹ì´ ë…¼ë¬¸ì—ì„œ ë‰´ìŠ¤ë‚˜ ì˜í™” ì˜ˆì‹œë¥¼ ë“¤ì—ˆë˜ ê²ƒ ì²˜ëŸ¼ domain-based í•œ ë¶€ë¶„ì´ë¼, ì¶”ì²œ ë©”ì»¤ë‹ˆì¦˜ì— ëŒ€í•œ í•´ì„ì´ ë” í¥ë¯¸ë¡­ê²Œ ë‹¤ê°€ì™”ë˜ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ TrendRec ì—ì„œ ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ë¡œ DeepAR ì„ ì„ íƒí•œ ê²ƒ, ë‹¤ìŒ ì•„ì´í…œ ì˜ˆì¸¡ ëª¨ë¸ì— ì„ë² ë”© ë°©ì‹ìœ¼ë¡œ GRU4Recì„ ì±„íƒí•œ ê²ƒ ëŒ€í•œ ê·¼ê±°ê°€ ì¡°ê¸ˆ ë¶€ì¡±í•˜ë‹¤ê³  ëŠê¼ˆìŠµë‹ˆë‹¤. ì¶”ê°€ì ì¸ ë‹¤ë¥¸ ëª¨ë¸ ì±„íƒ êµ¬ì„±ë°©ì‹ì˜ ì‹¤í—˜ê²°ê³¼ë„ ë¹„êµí•´ì£¼ì—ˆìœ¼ë©´ ì¢‹ì„ ê²ƒ ê°™ë‹¤ëŠ” ìƒê°ì´ ë“¤ì—ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ trend recommendation ì´ë¼ëŠ” ë¶„ì•¼ì—ì„œ í•´ë‹¹ ë…¼ë¬¸ì´ ê°€ì§€ê³  ìˆëŠ” ê°€ì¹˜ëŠ” ë§¤ìš° í¬ë‹¤ê³  ìƒê°í•˜ë©° ì•ìœ¼ë¡œ í•´ë‹¹ ë¶„ì•¼ê°€ ë°œì „í•¨ì— ìˆì–´ì„œ ì¤‘ìš”í•œ ì—°êµ¬ê°€ ë  ê²ƒì´ë¼ ìƒê°í•©ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;-review-writer-information&quot;&gt;ğŸ‘©ğŸ» Review writer information&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;ì´ë‹¤í˜„ (Lee Dahyeon)
    &lt;ul&gt;
      &lt;li&gt;Master student, Department of Data science, KAIST&lt;/li&gt;
      &lt;li&gt;contact : isdawell@kaist.ac.kr&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
            <pubDate>Mon, 16 Oct 2023 00:00:00 +0900</pubDate>
            <link>http://dsailatkaist.github.io/2023-10-16-Trending_Now_Modeling_Trend_Recommendations.html</link>
            <guid isPermaLink="true">http://dsailatkaist.github.io/2023-10-16-Trending_Now_Modeling_Trend_Recommendations.html</guid>
            
            <category>reviews</category>
            
            
        </item>
        
        <item>
            <title>[AAAI 2023] Simple and Efficient Heterogeneous Graph Neural Network</title>
            <description>&lt;h1 id=&quot;simple-and-efficient-heterogeneous-graph-neural-network&quot;&gt;Simple and Efficient Heterogeneous Graph Neural Network&lt;/h1&gt;

&lt;h2 id=&quot;1-problem-definition&quot;&gt;1. Problem Definition&lt;/h2&gt;

&lt;p&gt;Heterogeneous Graph Neural Networks(HGNN)ì€ ê¸°ì¡´ Graph Neural Network(GNN)ì—ì„œ ì‚¬ìš©í•˜ëŠ” attentionì´ë‚˜ multi-layer êµ¬ì¡° ë“±ì˜ ë§¤ì»¤ë‹ˆì¦˜ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ì™”ë‹¤. í•˜ì§€ë§Œ homogeneous graphë¥¼ ìœ„í•´ ë””ìì¸ëœ GNNì—ì„œ ì‚¬ìš©í•˜ëŠ” ë§¤ì»¤ë‹ˆì¦˜ì„ Heterogeneous graphì— ì ìš©í–ˆì„ ë•Œ, ì •ë§ íš¨ê³¼ê°€ ìˆëŠ”ì§€ì— ëŒ€í•œ ë¶„ì„ì€ ì´ë£¨ì–´ì§€ì§€ ì•Šì•˜ë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ì´ëŸ¬í•œ ë§¤ì»¤ë‹ˆì¦˜ë“¤ì˜ íš¨ê³¼ì„±ì— ëŒ€í•œ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ, Heterogeneous graphë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ëª¨ë¸ë§í•  ìˆ˜ ìˆëŠ” Simple and Efficient Heterogeneous Graph Neural Networks(SeHGNN)ì„ ì œì•ˆí•œë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;2-motivation&quot;&gt;2. Motivation&lt;/h2&gt;

&lt;p&gt;ì´ì „ì˜ Heterogeneous Graph Neural Network(HGNN)ì€ GNNì—ì„œ ì‚¬ìš©í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì´ heterogeneous ê·¸ë˜í”„ì— íš¨ê³¼ê°€ ìˆëŠ”ì§€ì— ëŒ€í•œ ë¶„ì„ì€ ê±°ì˜ í•˜ì§€ ì•Šì€ ì±„, ì´ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë©´ì„œ heterogeneous ê·¸ë˜í”„ì˜ representation learningì„ ìˆ˜í–‰í•´ì™”ë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” attentionì´ë‚˜ multi-layer êµ¬ì¡°ê°€ heterogeneous ê·¸ë˜í”„ë¥¼ ëª¨ë¸ë§í•˜ëŠ”ë° íš¨ê³¼ì ì¸ì§€ì— ëŒ€í•´ ë¶„ì„í•˜ëŠ” ê³¼ì •ì—ì„œ ì¤‘ìš”í•œ ë‘ ê°€ì§€ ë°œê²¬ì„ í•˜ì˜€ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ SeHGNN ì•„í‚¤í…ì²˜ë¥¼ ì„¤ê³„í•˜ì˜€ë‹¤. Heterogeneous ê·¸ë˜í”„ì— ëŒ€í•œ ê¸°ì¡´ ë§¤ì»¤ë‹ˆì¦˜ì˜ íš¨ê³¼ì„±ì— ëŒ€í•œ ë¶„ì„ ê³¼ì •ê³¼ ê·¸ì— ë”°ë¥¸ ë°œê²¬ì€ ì•„ë˜ì™€ ê°™ë‹¤.&lt;/p&gt;

&lt;li&gt; attentionì— ëŒ€í•œ ì—°êµ¬ &lt;/li&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://imgbb.com/&quot;&gt;&lt;img src=&quot;https://i.ibb.co/MDVHbCW/figure1.png&quot; alt=&quot;figure1&quot; border=&quot;0&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;HGNNì€ Figure 1ì— ë‚˜íƒ€ë‚œ ê²ƒì²˜ëŸ¼ ì„œë¡œ ë‹¤ë¥¸ ëª¨ë“ˆì´ë‚˜ íŒŒë¼ë¯¸í„°ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°ë˜ëŠ” ì—¬ëŸ¬ attentionì„ ì‚¬ìš©í•œë‹¤. ì´ëŸ¬í•œ attentionë“¤ì€ ë‘ ê°€ì§€ ìœ í˜•ìœ¼ë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆëŠ”ë°, ì²« ë²ˆì§¸ëŠ” ê°™ì€ relationì˜ neighborë“¤ ì‚¬ì´ì—ì„œ ê³„ì‚°ë˜ëŠ” neighbor attentionì´ê³ , ë‘ ë²ˆì§¸ëŠ” ì„œë¡œ ë‹¤ë¥¸ relation ì‚¬ì´ì—ì„œ ê³„ì‚°ë˜ëŠ” semantic attentionì´ë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” attentionì˜ íš¨ê³¼ì„±ì„ ì‚´í´ë³´ê¸° ìœ„í•´ attentionì„ ì‚¬ìš©í•œ ê²½ìš°ì™€ ì‚¬ìš©í•˜ì§€ ì•Šì€ ê²½ìš°ì— ëŒ€í•œ ë¹„êµë¥¼ ìˆ˜í–‰í•˜ì˜€ë‹¤.&lt;br /&gt;
ì´ë•Œ, attentionì˜ ì‚¬ìš© ì–‘ìƒì€ Heterogeneous graphë¥¼ ëª¨ë¸ë§í•˜ëŠ” ìœ í˜•ì— ë”°ë¼ ë‚˜ë‰˜ì–´ ì§€ëŠ”ë°, HANê³¼ ê°™ì´ metapath ê¸°ë°˜ ë°©ë²•ì€ neighbor aggregation ë‹¨ê³„ì™€ semantic fusion ë‹¨ê³„ ê°ê°ì—ì„œ ë‘ ê°€ì§€ attentionì„ ëšœë ·í•˜ê²Œ êµ¬ë¶„í•˜ì—¬ ì‚¬ìš©í•œë‹¤. ë°˜ë©´, HGBì™€ ê°™ì´ metapathë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ë°©ë²•ì€ relation-specificí•œ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ 1-hop neighborì˜ attentionì„ ê³„ì‚°í•´ì„œ, ë‘ ê°€ì§€ attention ìœ í˜•ì„ êµ¬ë¶„í•˜ëŠ” ê²ƒì´ ì–´ë ¤ìš¸ ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, attentionì˜ ì˜í–¥ì„ ì œê±°í•˜ê¸° ìœ„í•´ ì¶”ê°€ ê³„ì‚°ì„ ìˆ˜í–‰í•´ì•¼ í•œë‹¤. êµ¬ì²´ì ìœ¼ë¡œ ê° ë…¸ë“œì˜ ì´ì›ƒì˜ attention ê°’ì„ relationë³„ë¡œ í‰ê· í™”í•˜ì—¬ neighbor attentionì„ ì œê±°í•˜ê±°ë‚˜, ê° relation ë‚´ì—ì„œ ì •ê·œí™”í•˜ì—¬ ê° relationì´ ìµœì¢… ê²°ê³¼ì— ë™ì¼í•˜ê²Œ ê¸°ì—¬í•˜ë„ë¡ ì¡°ì •í•  ìˆ˜ ìˆëŠ”ë° ì´ê²ƒì€ semantic attentionì„ ì œê±°í•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://imgbb.com/&quot;&gt;&lt;img src=&quot;https://i.ibb.co/Ns6wjmB/table1.png&quot; alt=&quot;table1&quot; border=&quot;0&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ HANê³¼ HGBì— ëŒ€í•´ ê° ìš”ì†Œë¥¼ ì œê±°í•˜ë©´ì„œ DBLP ë°ì´í„°ì™€ ACM ë°ì´í„°ì— ëŒ€í•´ node classificationì„ ìˆ˜í–‰í•´ ì‹¤í—˜í•˜ì˜€ê³  ê·¸ ê²°ê³¼ë¥¼ Table 1ì— ì •ë¦¬í•˜ì˜€ë‹¤. ì—¬ê¸°ì„œ â€˜*â€˜ëŠ” neighbor attentionë¥¼ ì œê±°í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ê³  â€˜â€ â€™ëŠ” semantic attentionë¥¼ ì œê±°í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. Table 1ì˜ ê²°ê³¼ì—ì„œ, semantic attentionì´ ì—†ëŠ” ëª¨ë¸ì€ ì„±ëŠ¥ì´ ê°ì†Œí•˜ëŠ” ê²ƒì„ ë‚˜íƒ€ë‚´ëŠ” ë°˜ë©´, neighbor attentionì´ ì—†ëŠ” ëª¨ë¸ì€ ê·¸ë ‡ì§€ ì•ŠìŒì„ ë³´ì—¬ì¤€ë‹¤. ì´ë¥¼ í†µí•´ semantic attentionì€ HGNNì—ì„œë„ í•„ìˆ˜ì ì´ë©°, neighbor attentionì€ í•„ìš”í•˜ì§€ ì•Šë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆê³ , ì¶”ê°€ì ìœ¼ë¡œ neighbor attentionì˜ ê²½ìš° ë‹¤ì–‘í•œ SGC(Stochastic Gradient Community)ê¸°ë°˜ì˜ ì—°êµ¬ì—ì„œ ë‹¨ìˆœ mean aggregtionì´ attention ëª¨ë“ˆì„ ì‚¬ìš©í•œ aggregationê³¼ ë™ì¼í•œ íš¨ê³¼ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ í™•ì¸í•˜ì˜€ë‹¤ê³  í•˜ë©´ì„œ, mean aggregationìœ¼ë¡œ ëŒ€ì²´í•  ìˆ˜ ìˆìŒì„ ì–¸ê¸‰í•œë‹¤.&lt;/p&gt;

&lt;li&gt; multi-layer êµ¬ì¡°ì— ëŒ€í•œ ì—°êµ¬ &lt;/li&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;neighbor attentionì´ ì—†ëŠ”, metapathë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ë°©ë²•ì€ ê° relation ë‚´ì—ì„œ neighborì˜ featureë¥¼ ë¨¼ì € í‰ê· í™”í•œ ë‹¤ìŒ, ë‹¤ë¥¸ relationì˜ ê²°ê³¼ë¥¼ fusioní•˜ëŠ” í˜•íƒœë¥¼ ì§€ë‹Œë‹¤. ë”°ë¼ì„œ ì´ë“¤ì€ multi-layer êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©°, ê° ë ˆì´ì–´ì—ì„œ 1-hop metapathë§Œ ì‚¬ìš©í•˜ëŠ” metapath ê¸°ë°˜ ë°©ë²•ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” metapath ê¸°ë°˜ ë°©ë²•ì—ì„œì˜ ë ˆì´ì–´ ìˆ˜ì™€ metapath ìˆ˜ì˜ ì˜í–¥ì— ì¤‘ì ì„ ë‘ê³  ì‹¤í—˜ì„ ìˆ˜í–‰í•˜ì˜€ë‹¤. metapath ê¸°ë°˜ ë°©ë²•ì¸ HANì— ëŒ€í•œ ì‹¤í—˜ì„ ìˆ˜í–‰í•˜ë©´ì„œ ê° variantì˜ êµ¬ì¡°ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìˆ«ì listë¥¼ ì‚¬ìš©í•˜ì˜€ëŠ”ë°, ì˜ˆë¥¼ ë“¤ì–´ ACM ë°ì´í„°ì…‹ì—ì„œ (1,1,1,1)ì€ ê° ë ˆì´ì–´ì—ì„œ 1-hop metapath â€œPAâ€ ë° â€œPSâ€ë¥¼ ì‚¬ìš©í•˜ëŠ” ë„¤ ê°œì˜ ë ˆì´ì–´ ë„¤íŠ¸ì›Œí¬ë¥¼ ë‚˜íƒ€ë‚´ë©°, (4)ëŠ” 4-hop ì´ìƒì˜ metapathê°€ ì—†ëŠ” single-layer ë„¤íŠ¸ì›Œí¬ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ì´ëŸ¬í•œ listëŠ” receptive fieldì˜ í¬ê¸°ë„ ë³´ì—¬ì¤€ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ (1,1,1,1), (2,2), (4)ëŠ” ë™ì¼í•œ receptive fieldì˜ í¬ê¸°ë¥¼ ê°€ì§€ë©° 4-hop neighborë¥¼ í¬í•¨í•œë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë§ˆì°¬ê°€ì§€ë¡œ DBLP ë°ì´í„°ì™€ ACM ë°ì´í„°ì— ëŒ€í•´ ì‹¤í—˜ì„ ìˆ˜í–‰í•˜ì—¬ Table 2ì— ì •ë¦¬í•˜ì˜€ê³  ì´ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‘ ë²ˆì§¸ ë°œê²¬ì„ ë„ì¶œí–ˆë‹¤.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://imgbb.com/&quot;&gt;&lt;img src=&quot;https://i.ibb.co/ZJp6f07/table2.png&quot; alt=&quot;table2&quot; border=&quot;0&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Table 2ì— ë‚˜íƒ€ë‚œ ê²ƒì²˜ëŸ¼ single-layer êµ¬ì¡°ì™€ ê¸´ metapathë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ì´, multi-layer êµ¬ì¡°ì™€ ì§§ì€ metapathë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. single-layerì™€ ê¸´ metapathë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ì€ ë™ì¼í•œ receptive field í¬ê¸°ì—ì„œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ëŠ”ë°, ì´ëŠ” multi-layer ë„¤íŠ¸ì›Œí¬ê°€ ê° ë ˆì´ì–´ë§ˆë‹¤ semanticë“¤ì„ fusioní•˜ê¸° ë•Œë¬¸ì— ê³ ìˆ˜ì¤€ ì˜ë¯¸ë¥¼ êµ¬ë³„í•˜ê¸° ì–´ë µê²Œ ë§Œë“ ë‹¤ëŠ” ì‚¬ì‹¤ë¡œ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ACM ë°ì´í„°ì—ì„œ network êµ¬ì¡°ë¡œ (4)ì™€ ê°™ì€ í˜•íƒœë¥¼ ê°€ì§„ ëª¨ë¸ì—ì„œ multi-hop metapathë¥¼ ì‚¬ìš©í•˜ë©´, ë™ì¼í•œ ì €ìë¡œë¶€í„° ì“°ì—¬ì§„ (PAP) ë˜ëŠ” ìµìˆ™í•œ ì €ì (PAPAP)ì™€ ê°™ì€ ê³ ìˆ˜ì¤€ ì˜ë¯¸ë¥¼ êµ¬ë³„í•  ìˆ˜ ìˆì§€ë§Œ, ëª¨ë“  ì¤‘ê°„ ë²¡í„°ê°€ ì„œë¡œ ë‹¤ë¥¸ semanticì˜ í˜¼í•©ì„ ë‚˜íƒ€ë‚´ë¯€ë¡œ 4ê°œ layer ë„¤íŠ¸ì›Œí¬ (1,1,1,1)ì—ì„œëŠ” ì´ëŸ¬í•œ ì°¨ì´ë¥¼ êµ¬ë¶„í•  ìˆ˜ ì—†ë‹¤. ë” ë‚˜ì•„ê°€, ìµœëŒ€ metapath ê¸¸ì´ë¥¼ ì¦ê°€ì‹œí‚´ìœ¼ë¡œì¨ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ”ë° ë„ì›€ì´ ë˜ë©°, ë‹¤ì–‘í•œ ì˜ë¯¸ë¥¼ ê°–ëŠ” ë” ë§ì€ metapathë¥¼ ë„ì…í•  ìˆ˜ ìˆë‹¤ê³  ì„¤ëª…í•œë‹¤.&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;ìœ„ì˜ ë‘ ê°€ì§€ ë°œê²¬ì„ ë°”íƒ•ìœ¼ë¡œ ì œì•ˆëœ SeHGNNì—ì„œëŠ” ê° metapath ë²”ìœ„ì—ì„œ mean aggregationì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í¬ìƒì‹œí‚¤ì§€ ì•Šìœ¼ë©´ì„œ ì¤‘ë³µë˜ëŠ” neighbor attentionë¥¼ í”¼í•  ìˆ˜ ìˆê³ , single-layer ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ë©´ì„œ ë‹¨ìˆœí•˜ì§€ë§Œ ë” ê¸´ metapathë¥¼ ì‚¬ìš©í•˜ì—¬ receptive fieldë¥¼ í™•ì¥í•¨ìœ¼ë¡œì¨ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ì–»ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤. ë”ë¶ˆì–´ attention ëª¨ë“ˆì´ ì—†ëŠ” neighbor aggregation ë¶€ë¶„ì€ linear ì—°ì‚°ë§Œ í¬í•¨í•˜ê³  í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ê°€ ì—†ìœ¼ë¯€ë¡œ, neighbor aggregationì„ ë§¤ íŠ¸ë ˆì´ë‹ ì—í­ë§ˆë‹¤ ìˆ˜í–‰í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì „ì²˜ë¦¬ ë‹¨ê³„ì—ì„œ í•œ ë²ˆë§Œ ì‹¤í–‰í•  ìˆ˜ ìˆë„ë¡ í•˜ì—¬ í›ˆë ¨ ì‹œê°„ì„ í¬ê²Œ ì¤„ì¼ ìˆ˜ ìˆë‹¤. ì¦‰, ì´ëŸ¬í•œ ìµœì í™”ë¥¼ í†µí•´ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ë¥¼ ê°„ì†Œí™”í•˜ê³  íš¨ìœ¨ì ìœ¼ë¡œ ë§Œë“œëŠ” ê²ƒì´ SeHGNNì˜ í•µì‹¬ í¬ì¸íŠ¸ì´ë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;methodology&quot;&gt;Methodology&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://ibb.co/cYBCPdK&quot;&gt;&lt;img src=&quot;https://i.ibb.co/PDL9R8s/figure2.png&quot; alt=&quot;figure2&quot; border=&quot;0&quot; /&gt;&lt;/a&gt;&lt;br /&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://usefulwebtool.com/&quot;&gt;writing keyboard&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;SeHGNNì˜ ì•„í‚¤í…ì²˜ëŠ” Simplified Neighbor Aggregationê³¼ Multi-layer Feature Projection, ê·¸ë¦¬ê³  Transformer-based Semantic Fusionì˜ ì„¸ ê°€ì§€ ì£¼ìš” ìš”ì†Œë¡œ êµ¬ì„±ëœë‹¤. Figure 2ì—ì„œ SeHGNNê³¼ ë‹¤ë¥¸ metapath ê¸°ë°˜ HGNN ê°„ì˜ ì°¨ì´ë¥¼ ë³¼ ìˆ˜ ìˆëŠ”ë°, SeHGNNì€ &lt;b&gt;neighbor aggregationì„ ì „ì²˜ë¦¬ ë‹¨ê³„ì—ì„œ ì‚¬ì „ ê³„ì‚°&lt;/b&gt;í•˜ë¯€ë¡œ, ë§¤ íŠ¸ë ˆì´ë‹ ì—í­ì—ì„œ ë°˜ë³µì ì¸ neighbor aggregationì˜ ê³¼ë„í•œ ë³µì¡ì„±ì„ í”¼í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì´ ì£¼ìš”í•œ íŠ¹ì§•ì´ë‹¤. ê° êµ¬ì„± ìš”ì†Œë¥¼ ì„¸ë¶€ì ìœ¼ë¡œ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.&lt;/p&gt;

&lt;li&gt;Simplified Neighbor Aggregation&lt;/li&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;ê°„ì†Œí™”ëœ neighbor aggregationì€ ì „ì²˜ë¦¬ ë‹¨ê³„ì—ì„œ ë‹¨ í•œ ë²ˆ ìˆ˜í–‰ë˜ëŠ”ë°, ì£¼ì–´ì§„ ëª¨ë“  metapathì˜ ì§‘í•© $\Phi_X$ì— ëŒ€í•œ ë‹¤ë¥¸ semanticì˜ feature matrixë“¤ì˜ listë¥¼ ì•„ë˜ì™€ ê°™ì´ ìƒì„±í•œë‹¤.&lt;/p&gt;

&lt;p&gt;$M = {X_P : P \in \Phi_X}$&lt;/p&gt;

&lt;p&gt;ì¼ë°˜ì ìœ¼ë¡œ ê° ë…¸ë“œ $v_i$ì— ëŒ€í•´, ê° ì£¼ì–´ì§„ metapathë¡œë¶€í„° metapath ê¸°ë°˜ ì´ì›ƒì˜ featureë¥¼ aggregateí•˜ê¸° ìœ„í•´ mean aggregationì„ ì‚¬ìš©í•˜ë©° semantic feature vectorë“¤ì˜ listë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì¶œë ¥í•˜ëŠ”ë°,&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$mi = {z_P^i = \frac{1}{&lt;/td&gt;
      &lt;td&gt;Â &lt;/td&gt;
      &lt;td&gt;SP&lt;/td&gt;
      &lt;td&gt;Â &lt;/td&gt;
      &lt;td&gt;} \sum_{p(i,j)\in SP} x_j : P \in \Phi_X}$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;ë³¸ ë…¼ë¬¸ì—ì„œëŠ” metapath ê¸°ë°˜ neighbor collectionì„ ê°„ì†Œí™”í•˜ê¸° ìœ„í•´ ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì•ˆí•œë‹¤. HANê³¼ ê°™ì€ ê¸°ì¡´ì˜ metapath ê¸°ë°˜ ë°©ë²•ì€ ê° metapathì— ëŒ€í•´ ëª¨ë“  metapath ê¸°ë°˜ ì´ì›ƒì„ ì—´ê±°í•˜ëŠ” metapath neighbor ê·¸ë˜í”„ë¥¼ êµ¬ì¶•í•˜ë©°, ì´ëŠ” metapathì˜ ê¸¸ì´ì— ë”°ë¼ metapath ì¸ìŠ¤í„´ìŠ¤ì˜ ìˆ˜ê°€ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•˜ë¯€ë¡œ ë†’ì€ ë¶€í•˜ë¥¼ ì´ˆë˜í–ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” GCNì˜ ë ˆì´ì–´ë³„ ì „íŒŒì—ì„œ ì˜ê°ì„ ì–»ì–´ ê° ë…¸ë“œì˜ ìµœì¢… ê¸°ì—¬ ê°€ì¤‘ì¹˜ë¥¼ ì¸ì ‘ í–‰ë ¬ì˜ ê³±ì…ˆì„ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°í•œë‹¤.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$X_c = {x_0^{cT}; x_1^{cT}; \ldots; x_{&lt;/td&gt;
      &lt;td&gt;Â &lt;/td&gt;
      &lt;td&gt;V_c&lt;/td&gt;
      &lt;td&gt;Â &lt;/td&gt;
      &lt;td&gt;-1}^{cT}} \in \mathbb{R}^{&lt;/td&gt;
      &lt;td&gt;Â &lt;/td&gt;
      &lt;td&gt;V_c&lt;/td&gt;
      &lt;td&gt;Â &lt;/td&gt;
      &lt;td&gt;\times d_c}$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;ì—¬ê¸°ì„œ $d_c$ëŠ” feature dimensionì´ê³ , $X_c$ëŠ” $c$ ìœ í˜•ì— ì†í•˜ëŠ” ëª¨ë“  ë…¸ë“œì˜ ì´ˆê¸° feature matrixë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ê·¸ëŸ° ë‹¤ìŒ ê°„ì†Œí™”ëœ neighbor aggregation ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ë  ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;p&gt;$XP = \hat{A}&lt;em&gt;{c,c1}\hat{A}&lt;/em&gt;{c1,c2}\ldots \hat{A}_{cl-1,cl}X^{cl}$&lt;/p&gt;

&lt;p&gt;ì—¬ê¸°ì„œ $P = c1c2 â€¦ cl$ì€ $l$-hop metapathì´ë©°, $\hat{A}&lt;em&gt;{ci,ci+1}$ì€ ë…¸ë“œ ìœ í˜• $c_i$ì™€ $c&lt;/em&gt;{i+1}$ ê°„ì˜ ì¸ì ‘ í–‰ë ¬ $A_{ci,ci+1}$ì˜ row-normalizedëœ í˜•íƒœì´ë‹¤.&lt;/p&gt;

&lt;p&gt;ì—¬ê¸°ì— ë ˆì´ë¸”ì„ ì¶”ê°€ ì…ë ¥ìœ¼ë¡œ í†µí•©í•˜ë©´ ëª¨ë¸ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì…ì¦í•œ ì´ì „ ì—°êµ¬(Wang and Leskovec 2020; Wang et al. 2021b; Shi et al. 2021)ë¥¼ í™œìš©í•˜ê¸° ìœ„í•´, raw featureë“¤ì„ aggregationí•˜ëŠ” ê²ƒê³¼ ìœ ì‚¬í•˜ê²Œ, ë ˆì´ë¸”ì„ one-hot í˜•ì‹ìœ¼ë¡œ í‘œí˜„í•˜ê³  ë‹¤ì–‘í•œ metapathë¥¼ í†µí•´ ì „íŒŒí•œë‹¤. ì´ ê³¼ì •ì€ ì¼ë ¨ì˜ í–‰ë ¬ ${Y_P : P \in \Phi_Y}$ ì„ ìƒì„±í•˜ë©°, ì´ëŸ¬í•œ í–‰ë ¬ì€ í•´ë‹¹ metapath neighbor ê·¸ë˜í”„ì˜ ë ˆì´ë¸” ë¶„í¬ë¥¼ ë°˜ì˜í•œë‹¤. metapath $P \in \Phi_Y$ ì˜ ë‘ ëì ì€ ë…¸ë“œ ë¶„ë¥˜ ì‘ì—…ì—ì„œ ëŒ€ìƒ ë…¸ë“œ ìœ í˜• $c$ì—¬ì•¼ í•œë‹¤. metapath $P = cc_1c_2 \ldots c_{l-1}c \in \Phi_Y$ ê°€ ì£¼ì–´ì§€ë©´, ë ˆì´ë¸” ì „íŒŒ ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ë  ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;p&gt;$Y^P = rm _ diag(\hat{A}^P)Y^c, \, \hat{A}^P = \hat{A}&lt;em&gt;{c,c1}\hat{A}&lt;/em&gt;{c1,c2}\ldots \hat{A}_{cl-1,c}\,$&lt;/p&gt;

&lt;p&gt;ì—¬ê¸°ì„œ $Y^c$ëŠ” raw label matrixì´ë‹¤. $Y^c$ì—ì„œ training setì— ì†í•˜ëŠ” ë…¸ë“œì— í•´ë‹¹í•˜ëŠ” í–‰ì€ one-hot í˜•íƒœì˜ label ê°’ì„ ê°€ì§€ë©°, ë‹¤ë¥¸ í–‰ì€ 0ìœ¼ë¡œ ì±„ì›Œì§„ë‹¤. ë ˆì´ë¸” ìœ ì¶œì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ê° ë…¸ë“œê°€ ìì‹ ì˜ ì‹¤ì œ ë ˆì´ë¸” ì •ë³´ë¥¼ ë°›ì§€ ì•Šë„ë¡ í•˜ê¸° ìœ„í•´ ì¸ì ‘ í–‰ë ¬ì˜ ê³±ì…ˆ ê²°ê³¼ì—ì„œ ëŒ€ê°ì„ ì— ìˆëŠ” ê°’ì„ ì œê±°í•œë‹¤. ë ˆì´ë¸” ì „íŒŒëŠ” neighbor aggregation ë‹¨ê³„ì—ì„œ ì‹¤í–‰ë˜ë©° ë‚˜ì¤‘ì— í•™ìŠµì„ ìœ„í•œ ì¶”ê°€ ì…ë ¥ìœ¼ë¡œ semantic í–‰ë ¬ì„ ìƒì„±í•œë‹¤.&lt;/p&gt;

&lt;li&gt;Multi-layer Feature Projection&lt;/li&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;feature projection ë‹¨ê³„ëŠ” ì„œë¡œ ë‹¤ë¥¸ metapathì˜ semantic ë²¡í„°ê°€ ë‹¤ë¥¸ ì°¨ì›ì„ ê°€ì§€ê±°ë‚˜ ë‹¤ì–‘í•œ ë°ì´í„° ê³µê°„ì— ìœ„ì¹˜í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, ì´ëŸ¬í•œ semantic ë²¡í„°ë¥¼ ë™ì¼í•œ ë°ì´í„° ê³µê°„ìœ¼ë¡œ projectioní•˜ëŠ” ê³¼ì •ì´ë‹¤. ì¼ë°˜ì ìœ¼ë¡œ, ê° metapath $P$ì— ëŒ€í•œ semantic-specificí•œ transformation matrix $W^P$ë¥¼ ì •ì˜í•˜ê³  ${H^â€²P = W^PX^P}$ ë¥¼ ê³„ì‚°í•œë‹¤. ë” ë‚˜ì€ representationì„ ìœ„í•´, ê° metapath $P$ì— ëŒ€í•´ multi-layer perception ë¸”ë¡ $MLP_P$ë¥¼ ì‚¬ìš©í•˜ë©°, ì´ ë¸”ë¡ì€ ë‘ ê°œì˜ ì—°ì†ì ì¸ linear layer ì‚¬ì´ì— normalization layer, non-linear layer ë° dropout layerë¥¼ í¬í•¨í•œë‹¤. ì´ ê³¼ì •ì„ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¸ë‹¤.&lt;/p&gt;

&lt;p&gt;$Hâ€™_P = \text{MLP}_P(X_P)$&lt;/p&gt;

&lt;li&gt;Transformer-based Semantic Fusion&lt;/li&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;semantic fusion ë‹¨ê³„ëŠ” semantic feature ë²¡í„°ë¥¼ ìœµí•©í•˜ê³  ê° ë…¸ë“œì— ëŒ€í•œ ìµœì¢… ì„ë² ë”© ë²¡í„°ë¥¼ ìƒì„±í•œë‹¤. ë‹¨ìˆœí•œ weighted sum í˜•ì‹ ëŒ€ì‹ , ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ê° semantic ìŒ ê°„ì˜ ìƒí˜¸ ê´€ê³„ë¥¼ ë” íƒìƒ‰í•˜ê¸° ìœ„í•´ íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ì˜ semantic fusion ëª¨ë“ˆì„ ì œì•ˆí•˜ì˜€ë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ì˜ semantic fusion ëª¨ë“ˆì€ ë¯¸ë¦¬ ì •ì˜ëœ metapath list $\Phi = {P_1, \ldots, P_K}$ ì™€ ê° ë…¸ë“œì— ëŒ€í•œ projectedëœ semantic ë²¡í„° ${hâ€™&lt;em&gt;{P1}, \ldots, hâ€™&lt;/em&gt;{PK}}$ ì„ ê³ ë ¤í•˜ì—¬ semantic ë²¡í„° ìŒ ê°„ì˜ ìƒí˜¸ attentionë¥¼ í•™ìŠµí•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆë‹¤. ê° semantic ë²¡í„° 
$h^{â€˜Pi}$ ì— ëŒ€í•´ ì´ ëª¨ë“ˆì€ ì´ ë²¡í„°ë¥¼ query ë²¡í„° $q^{Pi}$, key ë²¡í„° $k^{Pi}$ ë° value ë²¡í„° $v^{Pi}$ë¡œ ë§¤í•‘í•œë‹¤. ìƒí˜¸ attention ê°€ì¤‘ì¹˜ $\alpha(P_i, P_j)$ ëŠ” ì†Œí”„íŠ¸ë§¥ìŠ¤ normalization í›„ì˜ query ë²¡í„° $q^{Pi}$ì™€ key ë²¡í„° $k^{Pi}$ì˜ dot product ê²°ê³¼ì´ë‹¤. current semantic $P_i$ì˜ ì¶œë ¥ ë²¡í„° $h^{Pi}$ëŠ” ëª¨ë“  value ë²¡í„° $v^{Pj}$ì˜ weighted sumê³¼ residual connectionì„ í¬í•¨í•œë‹¤. semantic fusion ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ë  ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;p&gt;$q^{Pi} = W_Q hâ€™^{Pi}$ , $k^{Pi} = W_K hâ€™^{Pi}$ , $v^{Pi} = W_V hâ€™^{Pi}$ , $P_i \in \Phi$ &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;$\alpha(Pi,Pj) = \frac{exp(q^{Pi} \cdot k^{{Pj}^T})}{\sum_{Pt\in\Phi} exp(q^{Pi} \cdot k^{{Pt}^T})}$ &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;$h^{Pi} = \beta \sum_{P_j\in\Phi} \alpha(P_i,P_j) v^{P_j} + hâ€™^{P_i}$ &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;ì—¬ê¸°ì„œ $W_Q$, $W_K$, $W_V$, Î²ëŠ” ëª¨ë“  metapath ê°„ì— ê³µìœ ë˜ëŠ” í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ì´ë‹¤.&lt;/p&gt;

&lt;p&gt;ê° ë…¸ë“œì˜ ìµœì¢… ì„ë² ë”© ë²¡í„°ëŠ” ëª¨ë“  ì¶œë ¥ ë²¡í„°ì˜ ì—°ê²°ë¡œ ì´ë£¨ì–´ì§€ëŠ”ë°, node classificationê³¼ ê°™ì€ downstream ì‘ì—…ì„ ìœ„í•´ ë˜ ë‹¤ë¥¸ MLPê°€ ì‚¬ìš©ë˜ì–´ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìƒì„±í•˜ë©°, ì´ëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ë  ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$Pred = \text{MLP}([h^{P1}&lt;/td&gt;
      &lt;td&gt;Â &lt;/td&gt;
      &lt;td&gt;h^{P2}&lt;/td&gt;
      &lt;td&gt;Â &lt;/td&gt;
      &lt;td&gt;\ldots&lt;/td&gt;
      &lt;td&gt;Â &lt;/td&gt;
      &lt;td&gt;h^{P&lt;/td&gt;
      &lt;td&gt;\Phi&lt;/td&gt;
      &lt;td&gt;}])$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;experiment&quot;&gt;Experiment&lt;/h2&gt;

&lt;p&gt;ë³¸ ë…¼ë¬¸ì—ì„œëŠ” DBLP, ACM, IMDB ë° Freebaseì™€ ê°™ì€ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” heterogeneous ê·¸ë˜í”„ 4ê°œì™€ OGB ì±Œë¦°ì§€ì—ì„œ ê°€ì ¸ì˜¨ í° ê·œëª¨ì˜ ogbn-mag ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤í—˜ì„ ì§„í–‰í–ˆê³ , node classificationì˜ ì„±ëŠ¥ ë¹„êµë¥¼ í†µí•´ ì œì•ˆí•œ ë°©ë²•ì˜ íš¨ê³¼ì„±ì„ ê²€ì¦í–ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://ibb.co/dpyvcbK&quot;&gt;&lt;img src=&quot;https://i.ibb.co/fN7WS80/table3.png&quot; alt=&quot;table3&quot; border=&quot;0&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;li&gt;Results on HGB Benchmark&lt;/li&gt;
&lt;p&gt;&lt;br /&gt;
Table 3ì€ ë„¤ ê°€ì§€ ë°ì´í„°ì…‹ì—ì„œ SeHGNNì˜ ì„±ëŠ¥ì„ HGB ë²¤ì¹˜ë§ˆí¬ì˜ ì—¬ëŸ¬ baselineë“¤ê³¼ ë¹„êµí•œ ê²°ê³¼ë¥¼ ì œì‹œí•˜ë©°, 1st í–‰ì€ ë„¤ ê°€ì§€ metapath ê¸°ë°˜ ë°©ë²•, 2nd í–‰ì€ metapathë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ë„¤ ê°€ì§€ ë°©ë²•ì„ ë‚˜íƒ€ë‚¸ë‹¤. SeHGNNì´ Freebase ë°ì´í„°ì…‹ì˜ micro-f1ì„ ì œì™¸í•˜ê³  ëª¨ë“  baseline ëŒ€ë¹„ ìµœìƒì˜ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ì˜€ë‹¤.&lt;/p&gt;

&lt;p&gt;ì¶”ê°€ì ìœ¼ë¡œ Motivation íŒŒíŠ¸ì—ì„œ ì–¸ê¸‰í•œ ë‘ ê°€ì§€ ë°œê²¬ì„ ê²€ì¦í•˜ê³ , ë‹¤ë¥¸ ëª¨ë“ˆì˜ ì¤‘ìš”ì„±ì„ ê²°ì •í•˜ê¸° ìœ„í•´ ablation studyë„ ìˆ˜í–‰í•˜ì˜€ëŠ”ë°, Table 3ì˜ 4th í–‰ì€ SeHGNNì— ë„¤ ê°€ì§€ ë³€í˜•ì„ ê°€í•œ ê°ê°ì˜ ê²½ìš°ì— ëŒ€í•œ ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. Variant#1ì€ neighbor aggregation ë‹¨ê³„ì—ì„œ HANê³¼ ê°™ì´ ê° metapathì— ëŒ€í•´ GATë¥¼ ì‚¬ìš©í•œ ê²½ìš°ì´ë‹¤. Variant#2ëŠ” ê° ë ˆì´ì–´ê°€ ë…ë¦½ì ì¸ neighbor aggregation ë° semantic fusion ë‹¨ê³„ë¥¼ ê°–ëŠ” ë‘ ê°œì˜ ë ˆì´ì–´ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ë©°, ê° ë ˆì´ì–´ì˜ metapathì˜ ìµœëŒ€ hopì´ SeHGNNì˜ ì ˆë°˜ìœ¼ë¡œ ë†“ê³  SeHGNNê³¼ Variant#2ê°€ ë™ì¼í•œ ìˆ˜ìš© ì˜ì—­ í¬ê¸°ë¥¼ ê°–ë„ë¡ í•œ ê²½ìš°ì´ë‹¤. SeHGNNê³¼ Variant#1ê³¼ Variant#2 ì‚¬ì´ì˜ ì„±ëŠ¥ ì°¨ì´ë¥¼ í†µí•´ Motivationì—ì„œ ì–¸ê¸‰í•œ ë‘ ê°€ì§€ ë°œê²¬ì— ëŒ€í•œ ë‚´ìš©ì´ SeHGNNì—ë„ ì ìš©ëœë‹¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. Variant#3ëŠ” ì¶”ê°€ ì…ë ¥ìœ¼ë¡œ ë ˆì´ë¸”ì„ í¬í•¨í•˜ì§€ ì•ŠëŠ” ê²½ìš°ì´ê³ , Variant#4ëŠ” HANê³¼ ê°™ì´ weighted sum fusionìœ¼ë¡œ íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ì˜ semantic fusionì„ ëŒ€ì²´í•œ ê²½ìš°ì´ë‹¤. íŠ¹íˆ, SeHGNNì— ë’¤ì³ì§€ì§€ë§Œ, Variant#3ì€ Freebase ë°ì´í„°ì…‹ì˜ micro-f1ì„ ì œì™¸í•œ ëŒ€ë¶€ë¶„ì˜ baselineë“¤ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤. ì´ëŸ¬í•œ ê²°ê³¼ëŠ” ë ˆì´ë¸” ì „íŒŒì™€ íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ fusionì˜ í™œìš©ì´ ëª¨ë¸ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¨ë‹¤ëŠ” ê²ƒì„ ì…ì¦í•˜ëŠ” ì¦ê±°ë¡œ ë³¼ ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://imgbb.com/&quot;&gt;&lt;img src=&quot;https://i.ibb.co/TmM0Ldh/table4.png&quot; alt=&quot;table4&quot; border=&quot;0&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;li&gt;Results on  Ogbn-mag&lt;/li&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë‹¤ì„¯ ë²ˆì§¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ogbn-magì„ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ì„ ë¹„êµí•˜ì˜€ë‹¤. ogbn-mag ë°ì´í„°ì…‹ì€ ì¼ë¶€ ìœ í˜•ì˜ ë…¸ë“œì˜ ì´ˆê¸° featureê°€ ë¶€ì¡±í•˜ê³ , target type ë…¸ë“œê°€ ì—°ë„ì— ë”°ë¼ ë¶„í• ë˜ì–´ training ë…¸ë“œì™€ test ë…¸ë“œê°€ ë‹¤ë¥¸ ë°ì´í„° ë¶„í¬ë¥¼ ê°–ê²Œ ëœë‹¤ëŠ” ë¬¸ì œì ì„ ê°–ê³  ìˆë‹¤. ê¸°ì¡´ì˜ ë‹¤ë¥¸ ë°©ë²•ë“¤ì€ ì¼ë°˜ì ìœ¼ë¡œ ì´ëŸ¬í•œ ì–´ë ¤ì›€ì„ ë‹¤ë£¨ê¸° ìœ„í•´ ComplEx (Trouillon et al. 2016)ì™€ ê°™ì€ ë¹„ì§€ë„ í‘œí˜„ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ê°€ ì„ë² ë”©ì„ ìƒì„±í•˜ê³ , multi-stage learningì„ í™œìš©í•˜ì—¬ ë§ˆì§€ë§‰ í•™ìŠµ ë‹¨ê³„ì—ì„œ í™•ì‹  ìˆëŠ” ì˜ˆì¸¡ì„ ê°€ì§„ test ë…¸ë“œë¥¼ ì„ íƒí•˜ê³  ì´ëŸ¬í•œ ë…¸ë“œë¥¼ training setì— ì¶”ê°€í•˜ì—¬ ìƒˆë¡œìš´ ë‹¨ê³„ì—ì„œ ëª¨ë¸ì„ ë‹¤ì‹œ í›ˆë ¨í•œë‹¤ê³  í•œë‹¤(Li, Han, and Wu 2018; Sun, Lin, and Zhu 2020; Yang et al. 2021). ë³¸ ë…¼ë¬¸ì˜ ì €ìëŠ” ì´ëŸ¬í•œ ë°©ë²•ë“¤ì„ ì‚¬ìš©í•˜ê±°ë‚˜ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²°ê³¼ë¥¼ ì´ìš©í•˜ì—¬ ë¹„êµí•˜ì˜€ë‹¤. ì¶”ê°€ ì„ë² ë”©ì´ ì—†ëŠ” ë°©ë²•ì˜ ê²½ìš° ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”ëœ ì´ˆê¸° feature ë²¡í„°ë¥¼ ì‚¬ìš©í–ˆë‹¤ê³  í•œë‹¤.&lt;/p&gt;

&lt;p&gt;Table 4ëŠ” ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ ogbn-magì— ëŒ€í•´ baselineê³¼ ë¹„êµí•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤. ê²°ê³¼ëŠ” SeHGNNì´ ë™ì¼í•œ ì¡°ê±´ì—ì„œ ë‹¤ë¥¸ ë°©ë²•ì„ ëŠ¥ê°€í•œë‹¤. ë¬´ì‘ìœ„ë¡œ ì´ˆê¸°í™”ëœ íŠ¹ì§•ì„ ê°€ì§„ SeHGNNì´ ì¶”ê°€ í‘œí˜„ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì—ì„œ ì˜ í›ˆë ¨ëœ ì„ë² ë”©ì„ ê°€ì§„ ë‹¤ë¥¸ ë°©ë²•ë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì´ëŠ”ë°, ì´ëŠ” SeHGNNì´ ê·¸ë˜í”„ êµ¬ì¡°ë¡œë¶€í„° ë” ë§ì€ ì •ë³´ë¥¼ í•™ìŠµí•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ëŠ” ì¦ê±°ì´ë‹¤.&lt;/p&gt;

&lt;li&gt;Time Analysis&lt;/li&gt;
&lt;p&gt;&lt;br /&gt;
ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë˜í•œ ì‹¤í–‰ ì‹œê°„ì— ëŒ€í•œ ë¹„êµ ë¶„ì„ë„ ìˆ˜í–‰í•˜ì˜€ë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://imgbb.com/&quot;&gt;&lt;img src=&quot;https://i.ibb.co/f8wwp0k/table5.png&quot; alt=&quot;table5&quot; border=&quot;0&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ë¨¼ì €, Table 5ì—ì„œ ë³´ì—¬ì§€ë“¯ì´ SeHGNNì˜ ì‹œê°„ ë³µì¡ë„ë¥¼ HANê³¼ HGBì™€ ë¹„êµí•˜ëŠ” ì´ë¡ ì  ë¶„ì„ì„ ìˆ˜í–‰í•œë‹¤. SeHGNNê³¼ HANì€ kê°œì˜ metapathì™€ single-layer êµ¬ì¡°ë¥¼ ê°€ì •í•˜ê³ , HGBëŠ” $l$ê°œì˜ ë ˆì´ì–´ êµ¬ì¡°ë¥¼ ê°–ëŠ” ê²ƒìœ¼ë¡œ ê°€ì •í•˜ì—¬ ë¶„ì„í•œë‹¤. metapathì˜ ìµœëŒ€ hopë„ $l$ë¡œ ì„¤ì •í•˜ì—¬ receptive fieldì˜ í¬ê¸°ë¥¼ ë™ì¼í•˜ê²Œ ìœ ì§€í•œë‹¤. ëŒ€ìƒ ìœ í˜• ë…¸ë“œì˜ ìˆ˜ëŠ” nì´ë©° ì…ë ¥ ë° hidden ë²¡í„°ì˜ ì°¨ì›ì€ $d$ì´ë‹¤. HANì—ì„œ metapath neighbor ê·¸ë˜í”„ì˜ í‰ê·  neighbor ìˆ˜ëŠ” $e_1$ì´ê³ , HGBì—ì„œ multi-layer aggregation ì¤‘ì— ê´€ë ¨ëœ neighbor ìˆ˜ëŠ” $e_2$ ì´ë‹¤. $e_1$ ê³¼ $e_2$ëŠ” metapathì˜ ê¸¸ì´ì™€ ë ˆì´ì–´ ìˆ˜ì¸ $l$ê³¼ í•¨ê»˜ ì§€ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•œë‹¤. ìœ„ ë‹¤ì„¯ ê°œì˜ ë°ì´í„°ì…‹ì—ì„œ ë³¸ ë…¼ë¬¸ì€ ìµœëŒ€ metapath ìˆ˜ì‹­ ê°œë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, ë ˆì´ì–´ $l$ â‰¥ 3ì— ëŒ€í•´ ê° ë…¸ë“œëŠ” í‰ê·  ìˆ˜ì²œ ê°œì˜ neighborë“¤ë¡œë¶€í„° ì •ë³´ë¥¼ aggregationí•œë‹¤. ì¼ë°˜ì ìœ¼ë¡œ $e_1$ â‰« $k^2$, $e_2$ â‰« $k^2$ì´ë‹¤. ë”°ë¼ì„œ SeHGNNì˜ ì´ë¡ ì  ë³µì¡ì„±ì€ HANê³¼ HGBë³´ë‹¤ í›¨ì”¬ ë‚®ì€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://imgbb.com/&quot;&gt;&lt;img src=&quot;https://i.ibb.co/9GcRN9c/figure3.png&quot; alt=&quot;figure3&quot; border=&quot;0&quot; /&gt;&lt;/a&gt;&lt;br /&gt;&lt;a target=&quot;_blank&quot; href=&quot;https://imgbb.com/&quot;&gt;image hosting without registration&lt;/a&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;ì´ë¡ ì  ë¶„ì„ì„ ê²€ì¦í•˜ê¸° ìœ„í•´ SeHGNNì˜ ì‹œê°„ì„ ì´ì „ì— ë‚˜ì˜¨ HGNNë“¤ê³¼ ë¹„êµí•˜ëŠ” ì‹¤í—˜ì„ ìˆ˜í–‰í•˜ì˜€ê³ , Figure 3ì€ ê° ëª¨ë¸ì˜ í‰ê·  ì‹œê°„ ë‹¨ìœ„ë¡œ í•™ìŠµ ì‹œê°„ì— ë”°ë¥¸ micro-f1 ì ìˆ˜ ë‹¬ì„± ì •ë„ë¥¼ ë³´ì—¬ì¤€ë‹¤. ì´ëŠ” SeHGNNì´ í•™ìŠµ ì†ë„ì™€ ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‘ì—ì„œ ìš°ìˆ˜í•¨ì„ ë‚˜íƒ€ë‚¸ë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;ë³¸ ë…¼ë¬¸ì€ heterogeneouos ê·¸ë˜í”„ representation learningì„ ìœ„í•œ SeHGNNì´ë¼ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì„ ì œì•ˆí•œë‹¤. ì´ ë°©ë²•ì€ attention ì‚¬ìš© ì—¬ë¶€ì™€ ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ì— ë”°ë¥¸ íš¨ê³¼ì„±ì— ëŒ€í•œ ë‘ ê°€ì§€ ì£¼ìš” ë°œê²¬ì„ ê¸°ë°˜ìœ¼ë¡œ ì œì•ˆë˜ì—ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” light mean aggregationì„ ì‚¬ìš©í•˜ì—¬ neighbor aggregationì„ ì‚¬ì „ì— ê³„ì‚°í•¨ìœ¼ë¡œì¨ êµ¬ì¡° ì •ë³´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í¬ì°©í•˜ë©´ì„œ, neighbor attentionì´ ê³¼ë„í•˜ê²Œ ì‚¬ìš©ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê³  ë°˜ë³µì ì¸ neighbor aggregationë„ í”¼í•  ìˆ˜ ìˆë„ë¡ í•˜ì˜€ë‹¤. ì´ì™€ í•¨ê»˜ receptive fieldë¥¼ í™•ì¥í•˜ê³  semantic ì •ë³´ë¥¼ ë” ì˜ í™œìš©í•˜ê¸° ìœ„í•´ ê¸´ metapathë¥¼ ì‚¬ìš©í•˜ëŠ” single-layer êµ¬ì¡°ì™€, íŠ¸ëœìŠ¤í¬ë¨¸ ê¸°ë°˜ì˜ semantic fusion ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ íš¨ê³¼ì„±ì„ í–¥ìƒì‹œì¼°ë‹¤.&lt;/p&gt;

</description>
            <pubDate>Mon, 16 Oct 2023 00:00:00 +0900</pubDate>
            <link>http://dsailatkaist.github.io/2023-10-16-Simple_and_Efficient_Heterogeneous_Graph_Neural_Network.html</link>
            <guid isPermaLink="true">http://dsailatkaist.github.io/2023-10-16-Simple_and_Efficient_Heterogeneous_Graph_Neural_Network.html</guid>
            
            <category>reviews</category>
            
            
        </item>
        
        <item>
            <title>[RecSys 2023] STRec: Sparse Transformer for Sequential Recommendations</title>
            <description>&lt;h1 id=&quot;title&quot;&gt;&lt;strong&gt;Title&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;STRec: Sparse Transformer  for  Sequential Recommendations&lt;/p&gt;

&lt;h2 id=&quot;1-problem-definition&quot;&gt;&lt;strong&gt;1. Problem Definition&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Transformer êµ¬ì¡°ê°€ ê¸‰ì†ë„ë¡œ ë°œì „í•¨ì— ë”°ë¼ researcherë“¤ì€ SRS(sequential recommender systems)ì—ì„œ Transformer êµ¬ì¡°ë¥¼ ì ìš©í•˜ê³  ì´ì „ SRS modelë“¤ì— ë¹„í•´ SRS taskì— ëŒ€í•˜ì—¬ ë°œì „ëœ ì„±ëŠ¥ì„ ë‚˜íƒ€ë‚´ëŠ” modelì„ ì œì‹œí•˜ê³  ìˆë‹¤. &lt;br /&gt;
ì´ ë…¼ë¬¸ì—ì„œ user-item interaction historyëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤. &lt;br /&gt;
 $\begin{align}S = {\left{\left(v_ 1, t_ 1 \right), \ldots, \left(v_ n, t_ n \right), \ldots, \left(v_ N, t_ N \right)  \right}} \end{align}$&lt;/p&gt;

&lt;p&gt;ì—¬ê¸°ì„œ $v_ n \in V$ëŠ” timestamp $t_ n$ì—ì„œ sequence $S$ì˜ $n$ë²ˆì§¸ interacted itemì´ê³  $N$ì€ sequenceì˜ ìµœëŒ€ ê¸¸ì´ì´ë‹¤. ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ user ë° ì‹¤ì œ ê¸¸ì´ì— ëŒ€í•œ í‘œê¸°ëŠ” ìƒëµë˜ì—ˆê³  interacted timestamp $t_ n$ì„ ê³ ë ¤í•œë‹¤. &lt;br /&gt;
SRSëŠ” ì œê³µëœ ê¸¸ì´ê°€ $N$ì¸ interaction sequence ${\left{\left(v_ 1, t_ 1 \right), \ldots, \left(v_ N, t_ N \right)  \right}}$ë¥¼ í™œìš©í•´ì„œ ë‹¤ìŒ interacted item $v_ {N+1}$ì„ ì˜ˆì¸¡í•´ì•¼ í•˜ëŠ” ë¬¸ì œì´ë‹¤. &lt;br /&gt;
ê·¸ëŸ¬ë‚˜ ëŒ€ë¶€ë¶„ì˜ ê¸°ì¡´ transformer ê¸°ë°˜ SRS modelë“¤ì€ ëª¨ë“  item-item pair ê°„ì˜ attention scoreë¥¼ ê³„ì‚°í•˜ëŠ” vanilla attention mechanismì„ ì‚¬ìš©í•˜ê³  ìˆë‹¤.
ì´ ê²½ìš° ì¤‘ë³µë˜ëŠ” item interactionìœ¼ë¡œ ì¸í•´ model ì„±ëŠ¥ì´ ì €í•˜ë˜ê³  ë§ì€ ê³„ì‚° ì‹œê°„ê³¼ ë©”ëª¨ë¦¬ë¥¼ í•„ìš”ë¡œ í•  ìˆ˜ ìˆë‹¤ëŠ” ë¬¸ì œì ì´ ë°œìƒí•œë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;2-motivation&quot;&gt;&lt;strong&gt;2. Motivation&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;vanilla self-attentionì„ transformer ê¸°ë°˜ SRS modelì— í™œìš©í•˜ë©´ ëª¨ë“  item interactionì„ scaní•  ìˆ˜ ìˆì§€ë§Œ ëª¨ë“  interactionì„ scaní•  ê²½ìš° ë§‰ëŒ€í•œ ê³„ì‚° ì‹œê°„ê³¼ ë©”ëª¨ë¦¬ ë¹„ìš©ì´ ë°œìƒí•˜ì—¬ SRS ëª¨ë¸ì˜ inference íš¨ìœ¨ì„±ì´ ì €í•˜ëœë‹¤.
ê²Œë‹¤ê°€ ìµœì ì´ ì•„ë‹Œ item interactionì„ ê³ ë ¤í•  ìˆ˜ ìˆì–´ ì¶”ì²œ ì„±ëŠ¥ì´ ì €í•˜ë  ìˆ˜ ìˆë‹¤.&lt;br /&gt;
ë”°ë¼ì„œ inference íš¨ìœ¨ì„±ê³¼ ì¶”ì²œ ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•´ í•„ìš”í•œ item interactionì„ êµ¬ë³„í•  ìˆ˜ ìˆëŠ” íš¨ìœ¨ì ì¸ transformer êµ¬ì¡°ë¥¼ ì„¤ê³„í•˜ëŠ” ê²ƒì´ í•„ìš”í•˜ë‹¤. &lt;br /&gt;
íš¨ìœ¨ì ì¸ Transformer êµ¬ì¡°ë¥¼ ì„¤ê³„í•˜ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ ë…¸ë ¥ì´ ì´ë£¨ì–´ì¡Œë‹¤.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2004.05150&quot;&gt;Longformer&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2007.14062&quot;&gt;Big Bird&lt;/a&gt; : sparce attention ì „ëµì„ ì‚¬ìš©í•˜ì—¬ í•„ìˆ˜ì ì¸ token pairì— ëŒ€í•´ì„œë§Œ attention scoreë¥¼ ê³„ì‚°&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2006.04768&quot;&gt;Linformer&lt;/a&gt; : low-rank approximation ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ attention score ê³„ì‚°&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2106.13008&quot;&gt;Autoformer&lt;/a&gt; : ì‹œê³„ì—´ ì˜ˆì¸¡ì„ ìœ„í•´ sequenceë¥¼ ë¶„í•´&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2202.10447&quot;&gt;FLASH&lt;/a&gt; : vanilla attentionì„ gated attention unitìœ¼ë¡œ ëŒ€ì²´&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2001.04451&quot;&gt;Reformer&lt;/a&gt; : locality-sensitive hashing(lsh) module ì ìš©&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;í•˜ì§€ë§Œ ìœ„ì˜ ë°©ë²•ë“¤ì€ SR(Sequential recommendation)ì„ ìœ„í•œ ëª©ì ìœ¼ë¡œ ì„¤ê³„ë˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì—(NLPë‚˜ ì‹œê³„ì—´ ì˜ˆì¸¡ì„ ìœ„í•œ ëª©ì ìœ¼ë¡œ ì„¤ê³„ë¨) SR taskì— ì§ì ‘ ì ìš©í•˜ë©´ ì¶”ì²œ ì„±ëŠ¥ì´ ì €í•˜ë  ìˆ˜ ìˆë‹¤. &lt;br /&gt;
ìƒˆë¡œìš´ transformer êµ¬ì¡° ì„¤ê³„ê°€ í•„ìš”í•œ ì´ìœ ë¥¼ ì•„ë˜ Figure 1ì„ í†µí•´ ì„¤ëª…í•˜ê³  ìˆë‹¤.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://velog.velcdn.com/images/yst3147/post/da1298ed-6331-42bf-89b6-527befee79d0/image.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 1ì˜ Attention weight matrixë¥¼ í†µí•´ SRS taskì—ì„œì˜ transformer ê¸°ë°˜ modelì´ ë†’ì€ sparsityë¥¼ ë³´ì´ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.
ë…¼ë¬¸ì—ì„œëŠ” í•´ë‹¹ sparse attentionì—ì„œ ë³´ì´ëŠ” low-rank phenomenonì„ ë‘ ê°€ì§€ ì¸¡ë©´ì—ì„œ ì„¤ëª…í•˜ê³  ìˆë‹¤.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;ê·¹íˆ ì¼ë¶€ì˜ interactionì´ outputì— ì°¨ì´ë¥¼ ë§Œë“ ë‹¤. (heatmap column level)&lt;/li&gt;
  &lt;li&gt;attention weight vectorê°€ ìœ ì‚¬í•˜ê³  ì´ê²ƒì´ low-rank phenomenonì„ ê°€ì¤‘ì‹œí‚¨ë‹¤. (heatmap row level)&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://velog.velcdn.com/images/yst3147/post/c910d64c-6c51-4c47-bbfc-0a11b206a0db/image.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;low-rank phenomenonì€ attention weight matrixì˜ í–‰ì— ëŒ€í•œ SVD ë¶„í•´ë¥¼ í†µí•´ ëª…í™•íˆ ë“œëŸ¬ë‚œë‹¤. Figure 2ë¥¼ í†µí•´ eigenvalueì˜ ë¶„í¬ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤. &lt;br /&gt;
xì¶•ì€ eigenvalue, yì¶•ì€ valueì˜ ë¹„ìœ¨ì´ë‹¤.
ëŒ€ë¶€ë¶„ì˜ eigenvalueëŠ” ìƒëŒ€ì ìœ¼ë¡œ ì‘ë‹¤. ì´ë¥¼ í†µí•´ low-rank matrixë¥¼ ì‚¬ìš©í•˜ì—¬ original attention weight matrixë¥¼ ê·¼ì‚¬í™” í•  ìˆ˜ ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤. &lt;br /&gt;
ì´ëŸ¬í•œ í˜„ìƒì„ ë°”íƒ•ìœ¼ë¡œ ì´ ë…¼ë¬¸ì—ì„œëŠ” íš¨ìœ¨ì„±ì„ ìœ„í•´ ì¼ë¶€ interaction ìŒë§Œ transformer layerì—ì„œ ê³„ì‚°í•˜ëŠ” sparse transformer ëª¨ë¸(&lt;strong&gt;STRec&lt;/strong&gt;)ì„ ì œì•ˆí•˜ì˜€ë‹¤.&lt;br /&gt;
&lt;strong&gt;S&lt;/strong&gt;parse &lt;strong&gt;T&lt;/strong&gt;ransformer model for sequenctial &lt;strong&gt;Rec&lt;/strong&gt;ommendation tasks(&lt;strong&gt;STRec&lt;/strong&gt;)ëŠ” cross-attentionì™€ í•™ìŠµ ê°€ëŠ¥í•œ parameterë¥¼ í™œìš©í•œ sampling ì „ëµì„ ê¸°ë°˜ìœ¼ë¡œ í•œë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;3-method&quot;&gt;&lt;strong&gt;3. Method&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;STRec&lt;/strong&gt;ì€ transformer ê¸°ë°˜ backbone modelì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì„±ë˜ì—ˆë‹¤. Figure 3ë¥¼ í†µí•´ &lt;strong&gt;STRec&lt;/strong&gt; ëª¨ë¸ êµ¬ì¡°ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://velog.velcdn.com/images/yst3147/post/18dfd67a-8704-4238-b825-762bff970ac5/image.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Modelì€ Embedding layer, ì—¬ëŸ¬ Transformer layer, Prediction layerë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤. 
Backbone modelê³¼ ë¹„êµí–ˆì„ ë•Œ TransformerLayerì—ì„œ ì°¨ì´ê°€ ìˆëŠ”ë°, ë…¼ë¬¸ì—ì„œëŠ” cross-attentionê³¼ í•™ìŠµ ê°€ëŠ¥í•œ parameterë¥¼ í™œìš©í•œ íš¨ìœ¨ì ì¸ sampling ì „ëµì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” sparse transformerë¥¼ í™œìš©í•œë‹¤. &lt;br /&gt;
Modelì˜ ê° layerë¥¼ ì„¤ëª…í•˜ë˜ ì´ ë…¼ë¬¸ì˜ í•µì‹¬ì¸ Cross Attention Transformer Layerì™€ Sampling ì „ëµ ë¶€ë¶„ì„ ì¢€ ë” ìì„¸íˆ ì„¤ëª…í•  ì˜ˆì •ì´ë‹¤.&lt;/p&gt;

&lt;h3 id=&quot;31-embedding-layer&quot;&gt;3.1 Embedding Layer&lt;/h3&gt;
&lt;p&gt;ID embeddingê³¼ positional embeddingì„ í†µí•©í•œ input itemì˜ ì´ˆê¸° í‘œí˜„ì„ ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.&lt;/p&gt;

&lt;p&gt;$ \begin{align}h_ {n}^{0} = e_ n + p_ n \end{align}$&lt;/p&gt;

&lt;p&gt;ì—¬ê¸°ì„œ $e_ n$ì€ item $v_ n$ì— ëŒ€í•œ ID embeddingì´ê³ , $p_ n$ì€ sequenceì˜ item index $n$ì— ëŒ€í•œ positional embeddingì´ë‹¤. $h_ {n}^{0}$ì˜ ìœ„ ì²¨ì index 0ëŠ” embedding layerì„ì„ ë‚˜íƒ€ë‚¸ë‹¤.&lt;/p&gt;

&lt;h3 id=&quot;32-sparse-transformer-in-strec&quot;&gt;3.2 Sparse Transformer in STRec&lt;/h3&gt;

&lt;h4 id=&quot;321-cross-attention-transformer-layer&quot;&gt;3.2.1 Cross Attention Transformer Layer&lt;/h4&gt;
&lt;p&gt;Attention layerì˜ ê³„ì‚° ë¹„ìš©ì„ ì¤„ì´ê¸° ìœ„í•´ vanilla self-attentionì„ cross-attentionìœ¼ë¡œ ëŒ€ì²´í•˜ì˜€ë‹¤. cross-attentionì€ input sequenceë¥¼ key, valueë¡œ ìƒ˜í”Œë§ëœ item sequenceë¥¼ queryë¡œ ì‚¬ìš©í•œë‹¤.
samplingëœ query matrixëŠ” ê¸°ì¡´ query matrixì— ë¹„í•´ í¬ê²Œ ì¶•ì†Œë˜ê¸° ë•Œë¬¸ì— ê³„ì‚°ì´ ë” íš¨ìœ¨ì ì´ë‹¤.
$H^ {l-1}$ì— ëŒ€í•´ì„œ cross-attentionì€ ë‹¤ìŒê³¼ ê°™ì€ ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. &lt;br /&gt;
$ \begin{align} \tilde{H}^ {l-1} = Add\&amp;amp;Norm\left(Attention\left(H_{I}^ {l-1}, H^ {l-1}, H^ {l-1}\right) \right)  \end{align}$
$\tilde{H}^ {l-1}$ì€ ì‚¬ì „ ì •ì˜ëœ $k_l$ì˜ ê¸¸ì´ë¥¼ ê°–ìœ¼ë©° sampling index $I_l$ì— ì˜í•´ $H_ {l-1}$ì— ìˆëŠ” item representationì´ samplingëœ ë¶€ë¶„ì§‘í•©ì´ë‹¤.
$\tilde{H}^ {l-1}$ì€ $H_{I}^ {l-1}$ê³¼ ë˜‘ê°™ì€ shapeë¥¼ ê°–ëŠ”ë‹¤.&lt;br /&gt;
FFN layerëŠ” ì§§ì•„ì§„ $\tilde{H}^ {l-1}$ë¥¼ inputìœ¼ë¡œ í•˜ì—¬ output hidden stateë¥¼ ë§Œë“¤ì–´ ë‚¸ë‹¤. &lt;br /&gt;
$ \begin{align} {H}^ {l} = Add\&amp;amp;Norm\left(FFN\left(\tilde{H}^ {l-1} \right)\right) \end{align}$
output hidden state $H^ {l}$ì˜ ê¸¸ì´ëŠ” ì—¬ì „íˆ $k_l$ì´ë©°, $H_{I}^ {l-1}$, $\tilde{H}^ {l-1}$ê³¼ ë˜‘ê°™ì€ shapeë¥¼ ê°–ëŠ”ë‹¤.&lt;br /&gt;
vanila self-attention transformer layerì™€ ë¹„êµí–ˆì„ ë•Œ cross-attention layerëŠ” attentionê³¼ feed-forward network ëª¨ë‘ì—ì„œ sampled itemì— ëŒ€í•´ì„œë§Œ ê³„ì‚°í•œë‹¤.&lt;br /&gt;
ì‹œê°„ ë³µì¡ë„ì™€ ê³µê°„ ë³µì¡ë„ ëª¨ë‘ $O(n^ 2)$ì—ì„œ $O(nk_ l)$ë¡œ ê°ì†Œí•œë‹¤.&lt;/p&gt;

&lt;h4 id=&quot;322-sampling-strategy&quot;&gt;3.2.2 Sampling strategy&lt;/h4&gt;
&lt;p&gt;Figure 1ì„ í†µí•´ í›„ë°© itemì´ SR taskì—ì„œ ì¤‘ìš”í•  ê°€ëŠ¥ì„±ì´ ë†’ìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.
ë”°ë¼ì„œ ë…¼ë¬¸ì—ì„œëŠ” ë§ˆì§€ë§‰ itemê³¼ì˜ time intervalì„ ë°”íƒ•ìœ¼ë¡œ í•™ìŠµ ê°€ëŠ¥í•œ parameterë¥¼ ì‚¬ìš©í•´ì„œ sampling ì „ëµì„ ìˆ˜í–‰í•œë‹¤. time intervalì€ $T = \left{\tilde{t}_ {i}\right}_ {1 \le i \le N}$ë¡œ í‘œí˜„í•œë‹¤.&lt;/p&gt;

&lt;p&gt;$ \begin{align} \tilde{t}_ {i} = t_ i - t_ N  \end{align}$
$t_ i, 1 \le i \le N$ì€ interaction $v_ i$ì— ëŒ€í•´ ê¸°ë¡ëœ timestampì´ë‹¤.&lt;br /&gt;
ì²«ë²ˆì§¸ layerì˜ ê²½ìš° MLP(Multi-layer Perceptron)ì„ í™œìš©í•´ì„œ time interval $T$ë¥¼ sampling densityë¡œ mappingí•œë‹¤. ë¬´ì‘ìœ„ ìƒ˜í”Œë§ì„ ìœ„í•´ uniform distributionì„ ê°–ëŠ” random matrix $R$ ì„ ì¶”ê°€í•œë‹¤. sampling index $I$ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ìƒì„±ëœë‹¤.&lt;/p&gt;

&lt;p&gt;$ \begin{align} I_ {1} = Top_k\left(MLP(T) + R, k_ 1\right)  \end{align}$
$ \begin{align} r_ {i} \sim Uniform\left(0, 1\right) \nonumber \end{align}$&lt;/p&gt;

&lt;p&gt;$Top_k\left(\cdot \right)$ëŠ” ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬ëœ indexë“¤ì˜ setì„ ìƒì„±í•œë‹¤.
$k_ 1$ì€ hyperparameterë¡œì„œ ì²«ë²ˆì§¸ layerì˜ pre-defineëœ sample sizeì´ë‹¤.&lt;/p&gt;

&lt;p&gt;ì´í›„ layerë“¤ì— ëŒ€í•´ì„œ sampling indexë¥¼ layerë³„ë¡œ ìƒì„±í•˜ëŠ”ë°ëŠ” ë§ì€ ì‹œê°„ì´ ê±¸ë¦°ë‹¤. ë”°ë¼ì„œ $MLP\left(T \right) + R$ ë¶€ë¶„ì€ ëª¨ë“  layerì— ëŒ€í•´ fine-tuning ë° inference ê³¼ì •ì—ì„œ ë™ì¼í•˜ê²Œ ìœ ì§€ëœë‹¤. &lt;br /&gt;
ë…¼ë¬¸ì—ì„œëŠ” ì •ë ¬ëœ index $I_ 1$ë¥¼ ì…ë ¥í•˜ê³  ì²« $k_ l$ê°œì˜ indexë¥¼ $I_ l$ë¡œ ì‚¬ìš©í•œë‹¤.&lt;/p&gt;

&lt;p&gt;$ \begin{align} I_ {l} = I_ 1\left[1: k_ l\right]  \end{align}$
$ \begin{align} {H}_ {I}^ {l-1} = \left[{h}_ {I_ l \left[1\right]}^ {l-1}, \ldots, {h}_ {I_ l \left[k_ l\right]}^ {l-1} \right]
â€âˆ€ 2 \le l \le L  \end{align}$
$I$ëŠ” ë¯¸ë¶„ ê°€ëŠ¥í•œ ë°©ì‹ìœ¼ë¡œ ê·¼ì‚¬ëœ(í•˜ì§€ë§Œ ë¯¸ë¶„ ê°€ëŠ¥í•œ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬í•˜ê¸° ì–´ë ¤ìš´) hard decisionì„ ìƒì„±í•˜ëŠ” random processë¥¼ ìš”êµ¬í•˜ëŠ” $Top_k$ ì—°ì‚° ìœ¼ë¡œ ì¸í•´ ë¯¸ë¶„ ë¶ˆê°€ëŠ¥í•˜ë‹¤. &lt;br /&gt;
ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ pre-train ê³¼ì •ì—ì„œ &lt;a href=&quot;https://arxiv.org/abs/1611.01144&quot;&gt;Gumbel-Softmax&lt;/a&gt;ë¥¼ ì ìš©í•˜ì—¬ sampling ê³¼ì •ì„ attention mask $M$ìœ¼ë¡œ ëŒ€ì²´í•œë‹¤.
ì—¬ê¸°ì„œ $m_ {ij} \approx 0$ì€ $i$ë²ˆì§¸ queryì™€ $j$ë²ˆì§¸ key ê°„ì˜ attention weightê°€ ê³„ì‚°ë˜ì§€ ì•Šì•˜ìŒì„ ì˜ë¯¸í•˜ê³  ê·¸ ë°˜ëŒ€ì˜ ê²½ìš°(ê³„ì‚°ëœ ê²½ìš°)ëŠ” $m_ {ij} \approx 1$ì´ë‹¤.&lt;/p&gt;

&lt;p&gt;$ \begin{align} S_ {l} = Sigmoid\left(MLP\left(T\right) + R + \alpha _ l\right) â€âˆ€ 1 \le l \le L  \end{align}$
$ \begin{align} S_ {0} = \left[ 1, 1, \ldots, 1\right] \end{align}$
$ \begin{align} M_ {l} = S_ {l-1} \otimes S_ {l} â€âˆ€ 1 \le l \le L  \end{align}$
$ \begin{align} r_ {i} \sim Uniform\left(0, 1\right) \nonumber \end{align}$&lt;/p&gt;

&lt;p&gt;$MLP\left(\cdot\right)$ëŠ” normalizationì´ í¬í•¨ëœ multi-layer perceptronì´ë‹¤. &lt;br /&gt;
$\alpha$ëŠ” samplingë  interaction ìˆ˜(mask matrix $S_l$ì˜ 1 ê°œìˆ˜)ë¥¼ ì œì–´í•˜ëŠ”ë° ì‚¬ìš©ëœë‹¤.
$\alpha_ {l}$ì´ ì»¤ì§€ë©´ í•´ë‹¹ layerì—ì„œ ë” ë§ì€ sampleì´ ìƒì„±ëœë‹¤. &lt;br /&gt;
layer $S_ {l}$ê³¼ ê·¸ ì´ì „ layer $S_ {l-1}$ì„ í™œìš©í•˜ì—¬ attention mask matrixëŠ” outer product $\otimes$ë¡œ ê³„ì‚°ëœë‹¤. 
ì´ë•Œ query-key pairë¥¼ ë½‘ìœ¼ë©´ queryëŠ” $S_ {l}$ì—ì„œ ë‚˜ì˜¤ê³  keyëŠ” $S_ {l-1}$ì—ì„œ ë‚˜ì˜¨ë‹¤&lt;/p&gt;

&lt;p&gt;attention mask $M$ì´ ìˆëŠ” ë¯¸ë¶„ ê°€ëŠ¥í•œ attention layerëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„ëœë‹¤.&lt;/p&gt;

&lt;p&gt;$ \begin{align} \tilde{H}^ {l-1} = Add\&amp;amp;Norm\left(\sigma \left(H_{I}^ {l-1}H^ {l-1^ {T}} + \left(M - 1\right) * \infty \right)H^ {l-1} \right)  \end{align}$&lt;/p&gt;

&lt;h3 id=&quot;33-prediction-layer&quot;&gt;3.3 Prediction layer&lt;/h3&gt;
&lt;p&gt;ë…¼ë¬¸ì—ì„œëŠ” ë§ˆì§€ë§‰ item embeddingì— ëŒ€í•œ ìµœì¢… output prediction scoreë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ &lt;a href=&quot;https://ieeexplore.ieee.org/document/5197422&quot;&gt;Matrix Factorization(MF)&lt;/a&gt;ë¥¼ ìˆ˜í–‰í•œë‹¤.&lt;/p&gt;

&lt;p&gt;$\begin{align} \hat{y} = \sigma\left(h_ {N}^ {L}E^ {T} \right)  \end{align}$&lt;/p&gt;

&lt;p&gt;ìœ„ ì‹ì— ë‚˜ì˜¨ ê¸°í˜¸ ì •ë¦¬ë¥¼ í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$h_ {N}^ {L} \in R^ d$: ë§ˆì§€ë§‰ transformer layerì—ì„œ ë‚˜ì˜¨ ë§ˆì§€ë§‰ item representation&lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;$E \in R^ {&lt;/td&gt;
          &lt;td&gt;V&lt;/td&gt;
          &lt;td&gt;\times d}$: candidate item $V$ì— ëŒ€í•œ embedding matrix&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;$\sigma\left(\cdot\right)$: softmax&lt;/li&gt;
  &lt;li&gt;$d$: embedding ì°¨ì›&lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;$\hat{y} \in R^ {&lt;/td&gt;
          &lt;td&gt;V&lt;/td&gt;
          &lt;td&gt;}$: prediction ê²°ê³¼ë¡œì„œ item set $V$ì— ëŒ€í•œ ë‹¤ìŒ itemì˜ probability distribution&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;34-optimization&quot;&gt;3.4 Optimization&lt;/h3&gt;
&lt;p&gt;ë…¼ë¬¸ì—ì„œëŠ” &lt;strong&gt;STRec&lt;/strong&gt;ì„ pre-trainê³¼ fine-tuningì˜ ë‘ ë‹¨ê³„ë¡œ ë‚˜ëˆ„ì–´ì„œ trainí•œë‹¤.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;pre-train ë‹¨ê³„ì—ì„œëŠ” ì‹ $(9)-(12)$ë¥¼ í™œìš©í•´ samplingì„ êµ¬í˜„í•˜ê³  ëª¨ë“  parameterë¥¼ ìµœì í™”í•œë‹¤.&lt;/li&gt;
  &lt;li&gt;fine-tuning ë‹¨ê³„ì—ì„œëŠ” MLPì˜ fixëœ ê·¼ì‚¬ hash í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê³  ë‹¤ë¥¸ parameterë¥¼ fine-tuningí•˜ë©´ì„œ ì¶”ê°€ë¡œ ìµœì í™”ë¥¼ ì§„í–‰í•˜ëŠ” ëŒ€ì‹  ì‹ $(6)$ì„ í™œìš©í•˜ì—¬ sampling index $I$ë¥¼ ì§ì ‘ ìƒì„±í•œë‹¤.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ìµœì í™” í•  parameterì—ëŠ” ë‹¤ìŒê³¼ ê°™ì€ 2ê°€ì§€ ì¢…ë¥˜ê°€ ìˆë‹¤.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$W$: backbone model parameter&lt;/li&gt;
  &lt;li&gt;$A$: ì‹$(9) -(12)$ì— í¬í•¨ëœ sampling ì „ëµ parameter&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ìµœì í™” ë¬¸ì œë¥¼ ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Pre-training stage
  $\begin{align} \min _{\boldsymbol{W}, \mathcal{A}} \mathcal{L}(\hat{\boldsymbol{y}}, \boldsymbol{y}) \nonumber \end{align}$&lt;/li&gt;
  &lt;li&gt;Fine-tuning Stage
  $\begin{align} \min _{\boldsymbol{W}} \mathcal{L}(\hat{\boldsymbol{y}}, \boldsymbol{y}) \nonumber \end{align}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;candidate itemì€ ëª¨ë“  itemì´ê³  &lt;br /&gt;
$\hat{y}$ëŠ” ë‹¤ìŒ ë°©ë¬¸í•˜ëŠ” itemì— ëŒ€í•œ ì˜ˆì¸¡ í™•ë¥ , $y$ëŠ” ground truthì¸ ë‹¤ìŒ itemì„ ì˜ë¯¸í•œë‹¤. &lt;br /&gt;
item embeddingê³¼ ë§ˆì§€ë§‰ transformer layerì˜ output vector ì‚¬ì´ì˜ ë‚´ì ì„ ìˆ˜í–‰í•˜ì—¬ ë‹¤ìŒ ë°©ë¬¸ itemì— ëŒ€í•œ í™•ë¥ ì„ ì–»ëŠ”ë‹¤.&lt;/p&gt;

&lt;p&gt;$\mathcal{L}(\hat{\boldsymbol{y}}, \boldsymbol{y})$ loss functionì€ SRS ì‘ì—…ì—ì„œ í™œìš©ë˜ëŠ” binary Cross-Entropy lossì´ë‹¤.
 $\begin{align} \mathcal{L}(\hat{\boldsymbol{y}}, \boldsymbol{y})  = \boldsymbol{y}log(\hat{\boldsymbol{y}}) + (1 - \boldsymbol{y})log(1 - \hat{\boldsymbol{y}})  \end{align}$&lt;/p&gt;

&lt;p&gt;ìƒì„¸ ìµœì í™” ê³¼ì •ì€ ì•„ë˜ Algorithm 1ì— ì„¤ëª…ë˜ì–´ ìˆë‹¤.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://velog.velcdn.com/images/yst3147/post/b18318d2-596a-48c1-ae56-ef5195ee8095/image.png&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;(line 3) flag c ì´ˆê¸°í™”, cë¥¼ í™œìš©í•´ train epoch ê³„ì‚°&lt;/li&gt;
  &lt;li&gt;(line 4-9) pretrain ë‹¨ê³„ì—ì„œ ëª¨ë“  parameterë¥¼ ë™ì‹œì— train&lt;/li&gt;
  &lt;li&gt;(line 10-14) parameter $A$ë¥¼ ê³ ì •í•˜ê³  $W$ë¥¼ train ë‹¨ê³„ì—ì„œ ìˆ˜ë ´í•˜ë„ë¡ ê³„ì† train&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;35-model-inference&quot;&gt;3.5 Model Inference&lt;/h3&gt;
&lt;p&gt;inference ê³¼ì •ì„ ìˆœì„œëŒ€ë¡œ ì‘ì„±í•˜ì˜€ë‹¤.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ì‹ $(2)$ í™œìš© ê° interactionì˜ ì´ˆê¸° representationì„ ë§Œë“¤ê³  $H^ 0$ê³¼ ì—°ê²°&lt;/li&gt;
  &lt;li&gt;ì‹ $(5)$ í™œìš© visiting time interval $T$ ê³„ì‚°&lt;/li&gt;
  &lt;li&gt;ì‹ $(6)$ í™œìš© ì²« layerì— ëŒ€í•œ index $I_ 1$ ìƒì„±&lt;/li&gt;
  &lt;li&gt;$H^ 0$ê°€ $L$ê°œì˜ transformer layerì— ì˜í•´ ì‹ $(3, 4)$ì™€ ê°™ì´ ë³€í™˜ë¨&lt;/li&gt;
  &lt;li&gt;ì‹ $(7)$ í™œìš© ê° layer $l$ì˜ index $I_ l$ì´ í¬í•¨ëœ sampling query ì§ì ‘ ìƒì„±&lt;/li&gt;
  &lt;li&gt;ëª¨ë“  candidate itemê³¼ sparse transformerì˜ outputì„ ë‚´ì &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ëª¨ë“  candidate item similarity ì ìˆ˜ $\hat{y}$ì„ í†µí•´ next itemì— ëŒ€í•œ prediction ê²°ê³¼ë¥¼ íšë“ ê°€ëŠ¥í•˜ë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;4-experiment&quot;&gt;&lt;strong&gt;4. Experiment&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;experiment-setup&quot;&gt;&lt;strong&gt;Experiment setup&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Dataset
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://grouplens.org/datasets/movielens/1m/&quot;&gt;ML-20M&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://snap.stanford.edu/data/loc-gowalla.html&quot;&gt;Gowalla&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://jmcauley.ucsd.edu/data/amazon/&quot;&gt;Amazon-Electronics&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;baseline
    &lt;ul&gt;
      &lt;li&gt;classical SRS models
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1511.06939&quot;&gt;GRU4Rec&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1711.04725&quot;&gt;NARM&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1808.09781&quot;&gt;SASRec&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1904.06690&quot;&gt;Bert4Rec&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://www.ijcai.org/proceedings/2019/0600.pdf&quot;&gt;FDSA&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/doi/10.1145/3336191.3371786&quot;&gt;Ti-SASRec&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;transformer architecture
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2006.04768&quot;&gt;Linformer&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2007.14062&quot;&gt;Big Bird&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2012.07436&quot;&gt;Informer&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2001.04451&quot;&gt;Reformer&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2202.10447&quot;&gt;FLASH&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2106.13008&quot;&gt;Autoformer&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Evaluation Metric
    &lt;ul&gt;
      &lt;li&gt;mean reciprocal rank(MRR)&lt;/li&gt;
      &lt;li&gt;normalized discounted cumulative gain(NDCG)&lt;/li&gt;
      &lt;li&gt;hit ratio(HR)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;result&quot;&gt;&lt;strong&gt;Result&lt;/strong&gt;&lt;/h3&gt;
&lt;h3 id=&quot;rq1-how--the--proposed--strec--performs--in--accuracy--while--it-can-reduce-the-time-and-spatial--complexity&quot;&gt;RQ1: How  the  proposed  STRec  performs  in  accuracy  while  it can reduce the time and spatial  complexity?&lt;/h3&gt;

&lt;p&gt;RQ1ì— ëŒ€í•œ ë‹µë³€ì„ ìœ„í•´ accuracy ì„±ëŠ¥ì„ ê³„ì‚°í•˜ì—¬ ë¹„êµí•œ Table 2ë¥¼ ì œì‹œí•˜ì˜€ë‹¤.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://velog.velcdn.com/images/yst3147/post/66fdeb97-55de-4231-a59d-e4bdafc217c5/image.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ë¶„ì„ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;ëª¨ë“  datasetì—ì„œ Transformer ê¸°ë°˜ ë°©ë²•ì´ RNN ê¸°ë°˜ ë°©ë²•ë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ë‹¤.(ê¸´ sequenceë¥¼ ë” ì˜ ëª¨ë¸ë§í•˜ê¸° ë•Œë¬¸)&lt;/li&gt;
  &lt;li&gt;FDSAëŠ” side informationì´ ë¶€ì¡±í•˜ê¸° ë•Œë¬¸ì— ì„±ëŠ¥ì´ ì¢‹ì§€ ì•Šë‹¤.(ê³µì •í•œ ë¹„êµë¥¼ ìœ„í•´ side information ì œì™¸í•˜ê³  ì‹¤í—˜)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;STRec&lt;/strong&gt;ì€  65%ì˜ sparsityë¥¼ ê°€ì§„ ML-20M ë° Gowalla datasetì—ì„œ ë‹¤ë¥¸ baselineë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ë‹¤.&lt;/li&gt;
  &lt;li&gt;TiSASRecê³¼ &lt;strong&gt;STRec&lt;/strong&gt; ëª¨ë‘ SRSì— ì‹œê°„ informationì„ ì‚¬ìš©í•˜ì˜€ë‹¤. TiSASRecì€ time intervalì— ë”°ë¼ item embeddingì„ ê°•í™”í•˜ê¸° ë•Œë¬¸ì— ì„±ëŠ¥ì´ ì¢‹ì§€ ì•Šìœ¼ë‚˜ &lt;strong&gt;STRec&lt;/strong&gt;ì€ ì‹œê°„ informationì„ ì‚¬ìš©í•˜ì—¬ itemì˜ potentioal importanceë¥¼ í•™ìŠµí•œë‹¤.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;rq2--compared--with--the--efficient--transformer--methods--how-strec-performs--in-the-aspect-of-efficiency&quot;&gt;RQ2:  Compared  with  the  efficient  transformer  methods,  how STRec performs  in the aspect of efficiency?&lt;/h3&gt;

&lt;p&gt;RQ2ì— ëŒ€í•œ ë‹µë³€ì„ ìœ„í•´ efficiency performanceë¥¼ ì¸¡ì •í•˜ì—¬ ë¹„êµí•œ Table 3ë¥¼ ì œì‹œí•˜ì˜€ë‹¤.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://velog.velcdn.com/images/yst3147/post/7b669acf-9769-40b8-be0b-4c766ddb5483/image.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ë¶„ì„ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Linformerì™€ InformerëŠ” backbone modelë³´ë‹¤ íš¨ìœ¨ì ì´ê³ , InformerëŠ” down-sampling settingì„ ì ìš©í–ˆê¸° ë•Œë¬¸ì— ê°€ì¥ ì¢‹ì€ memory íš¨ìœ¨ì„ ë³´ì¸ë‹¤.
   ê·¸ëŸ¬ë‚˜ samplingì— ë§ì€ operationì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì— &lt;strong&gt;STRec&lt;/strong&gt;ì— ë¹„í•´ inference timeì´ í›¨ì”¬ ê¸¸ë‹¤. ê²Œë‹¤ê°€ ì„±ëŠ¥ë„ &lt;strong&gt;STRec&lt;/strong&gt;ì— ë¹„í•´ ì¢‹ì§€ ì•Šë‹¤.&lt;/li&gt;
  &lt;li&gt;Big birdì˜ ê²°ê³¼ê°€ N/Aì¸ ì´ìœ ëŠ” sparse patternì— ëŒ€í•œ ë†’ì€ complexityë¡œ ì¸í•´ íš¨ìœ¨ì„±ì´ ë–¨ì–´ì ¸ êµ¬í˜„í•  ìˆ˜ ì—†ì—ˆê¸° ë•Œë¬¸ì´ë‹¤.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;STRec&lt;/strong&gt;ì€ ì‹œê°„ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¤‘ìš”í•œ queryë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— accuracyì™€ time-space íš¨ìœ¨ì„± ëª¨ë‘ì—ì„œ ë‹¤ë¥¸ transformer baselineë“¤ì„ ëŠ¥ê°€í•œë‹¤.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;rq3-how-does-the-sparsity-and-pre-training-process-of-strec-affect-the--accuracy--performance&quot;&gt;RQ3: How does the sparsity and pre-training process of STRec affect the  accuracy  performance?&lt;/h3&gt;

&lt;h4 id=&quot;sparcity&quot;&gt;Sparcity&lt;/h4&gt;

&lt;p&gt;Figure 4ëŠ” sparsity ì¸¡ë©´ì—ì„œì˜ parameter study ê²°ê³¼ì´ë‹¤.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://velog.velcdn.com/images/yst3147/post/16cc41d6-273b-4ed3-9b81-06009932aa8a/image.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 5ëŠ” sparsity ì¸¡ë©´ì—ì„œì˜ efficieny performance ë¹„êµ ê²°ê³¼ì´ë‹¤.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://velog.velcdn.com/images/yst3147/post/cc72f6ee-dbc8-478a-9cc8-e02ae18c8767/image.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;xì¶•ì€ ëª¨ë“  layerì• ì„œì˜ sample size $k_ l$ì— ì˜í•´ ê³„ì‚°ëœ sparsityë¥¼ ì˜ë¯¸í•œë‹¤.&lt;br /&gt;
e.g. sequence ê¸¸ì´ê°€ 50ì´ê³  $k_ l$ì´ 5ì¼ ë•Œ sparsityëŠ” (50 - 5) / 50 = 90% &lt;br /&gt;
yì¶•ì€ accuracy performanceì™€ efficiency performance(backbone modelê³¼ì˜ inference timeê³¼ memory costì˜ persentage ë¹„êµ)ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.&lt;/p&gt;

&lt;p&gt;Figure 4ì™€ 5ì— ëŒ€í•œ ë¶„ì„ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;optimal sparsityëŠ” 69%ì´ë‹¤.&lt;/li&gt;
  &lt;li&gt;sparsityê°€ 42%ë³´ë‹¤ ë‚®ì„ ë•Œ sparsityì™€ model ì„±ëŠ¥ì€ ë¹„ë¡€í•œë‹¤. ê·¸ ì´ìœ ëŠ” ì¤‘ìš”í•˜ì§€ ì•Šì€ periodì˜ interactionì— ëŒ€í•œ ì˜í–¥ì„ ì œê±°í•˜ì—¬ transformerê°€ sequential user preferenceë¥¼ ì˜ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ì¤‘ë³µë˜ëŠ” interaction ê³„ì‚°ì„ ìƒëµí•˜ê¸° ë•Œë¬¸ì´ë‹¤.&lt;/li&gt;
  &lt;li&gt;ë„ˆë¬´ ë†’ì€ sparsityëŠ” ì„±ëŠ¥ì„ ê°ì†Œì‹œí‚¨ë‹¤. 77%ë³´ë‹¤ sparsityê°€ ì»¤ì§ˆ ë•Œ ëª¨ë¸ ì„±ëŠ¥ì€ ì ì  ê°ì†Œëœë‹¤. ê·¸ ì´ìœ ëŠ” sparsityê°€ ë„ˆë¬´ ì‹¬í•˜ë©´ ë§ì€ key informationì„ ìƒì–´ë²„ë¦¬ê³  predictionì„ ì¶©ë¶„íˆ í•™ìŠµí•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì´ë‹¤.&lt;/li&gt;
  &lt;li&gt;(42%-77%)ì˜ sparsity ë²”ìœ„ì—ì„œ STRecì€ SASRec(ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ baseline)ì˜ ì„±ëŠ¥ì„ ëŠ¥ê°€í•œë‹¤. ê·¸ ì´ìœ ëŠ” sequenceì—ì„œ representative interactionì„ ì„ íƒí•˜ëŠ” ì„±ê³µì ì¸ sampling ì „ëµ ë•ë¶„ì´ë‹¤.(denoisingê³¼ ë¹„ìŠ·)&lt;/li&gt;
  &lt;li&gt;100%ì— ê°€ê¹Œìš´ sparsityì—ì„œë„ inferenceì„ ìœ„í•œ backbone modelì˜ costìœ¼ë¡œ ì¸í•´ I/O ë° embedding layerì—ë„ ì•½ 15%ì˜ ì‹œê°„ì´ ì†Œìš”ëœë‹¤. Memory costì€ ì£¼ë¡œ transformer layerì— ì˜í•´ ë°œìƒí•˜ë¯€ë¡œ sparsityì´ 100%ì— ê°€ê¹Œì›Œì§€ë©´ memory costê°€ ê±°ì˜ 0%ê°€ ë  ìˆ˜ ìˆë‹¤.
    &lt;h4 id=&quot;training-pipeline-analysis&quot;&gt;Training Pipeline Analysis&lt;/h4&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Figure 6ëŠ” pretrain epoch $C$ë¥¼ ë³€í™”ì‹œì¼œ ê°€ë©° ì‹¤í—˜ì„ ì§„í–‰í•œ ê²°ê³¼ì´ë‹¤.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://velog.velcdn.com/images/yst3147/post/c0d0e1ed-e2ec-4536-85a3-bce4a0178c80/image.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;xì¶•ì€ epoch $C$, yì¶•ì€ performance(NDCG@10)ë¥¼ ì˜ë¯¸í•˜ë©° &lt;br /&gt;
í‘¸ë¥¸ ì„ ì€ fine-tuning ë‹¨ê³„ë¥¼ skipí•˜ê³  ë°”ë¡œ pre-training ë‹¨ê³„ë§Œì„ ê±°ì¹œ ëª¨ë¸ë¡œ ì˜ˆì¸¡ì„ ì§„í–‰í•œ ê²°ê³¼ì´ë‹¤.&lt;/p&gt;

&lt;p&gt;Figure 6ë¥¼ í†µí•´ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$C$ê°€ 60ì¼ ë•Œ ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ë‹¤. pre-training epochì´ ê·¸ ì´ìƒìœ¼ë¡œ ëŠ˜ì–´ë‚˜ë©´ overfittingì´ ë°œìƒí•˜ì—¬ ì„±ëŠ¥ì´ ê°ì†Œí•œë‹¤.&lt;/li&gt;
  &lt;li&gt;$C$ë¥¼ 60ì—ì„œ 10ìœ¼ë¡œ ê°ì†Œì‹œí‚¤ë©´ ì„±ëŠ¥ì€ í¬ê²Œ ê°ì†Œí•œë‹¤. ì´ë¥¼ í†µí•´ pre-training ë‹¨ê³„ë¥¼ ìƒëµí•˜ë©´ underfitting ë¬¸ì œê°€ ë°œìƒí•¨ì„ ì•Œ ìˆ˜ ìˆë‹¤.&lt;/li&gt;
  &lt;li&gt;fine-tuning ë‹¨ê³„ë¥¼ skipí•˜ë©´ ìµœì ì˜ performanceë¥¼ ì–»ì„ ìˆ˜ ì—†ë‹¤.(blue line ì°¸ê³ )&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;rq4-what--is--the--influence--on--the--performance--of--the--core--com--ponents-in-strecablation-study&quot;&gt;RQ4: What  is  the  influence  on  the  performance  of  the  core  com- ponents in STRec?(Ablation Study)&lt;/h3&gt;

&lt;p&gt;Figure 7ì€ &lt;strong&gt;STRec&lt;/strong&gt;ì— ëŒ€í•œ Ablation Study ê²°ê³¼ì´ë‹¤.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://velog.velcdn.com/images/yst3147/post/1a18be99-5cce-4eba-8355-ab3835f66651/image.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;STRec&lt;/strong&gt;ì— ëŒ€í•œ ì„¸ ê°€ì§€ ë³€í˜•ìœ¼ë¡œ ì‹¤í—˜ì„ ì§„í–‰í•˜ì˜€ë‹¤.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;STRec-1 : trainê³¼ inference ë‘˜ ë‹¤ì—ì„œ ì‹ (6)ì˜ random matrix $R$ ì œê±°(random sampling ì•ˆí•¨)&lt;/li&gt;
  &lt;li&gt;STRec-2 : ì‹ (6)ì—ì„œ index $I$ë¥¼ randomìœ¼ë¡œ ë§Œë“¦(ì²«ë²ˆì§¸ layerì—ì„œ itemì„ randomìœ¼ë¡œ samplingí•˜ê³  sortí•¨)&lt;/li&gt;
  &lt;li&gt;STRec-3 : visiting time interval matrix $T$ë¥¼ item ë°©ë¬¸ ìˆœì„œë¥¼ ë‚˜íƒ€ë‚´ëŠ” position index matrixë¡œ ëŒ€ì²´&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ë¶„ì„ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;STRec-1ì€ random samplingì˜ ë¶€ì¬ë¡œ ì„±ëŠ¥ì´ ê°ì†Œí•˜ì˜€ë‹¤. ëª¨ë“  layerì— ëŒ€í•œ queryëŠ” sequenceì˜ ë§ˆì§€ë§‰ ëª‡ê°œì˜ itemìœ¼ë¡œ ì œí•œë˜ë©°, ì´ë¡œ ì¸í•´ user interaction sequenceì˜ ì´ˆê¸° ì •ë³´ë¥¼ ë¬´ì‹œí•˜ê²Œ ë˜ê¸° ë•Œë¬¸ì´ë‹¤.&lt;/li&gt;
  &lt;li&gt;STRec-2ëŠ” interactionì„ queryë¡œ randomìœ¼ë¡œ samplingí•˜ë©° &lt;strong&gt;STRec&lt;/strong&gt;ì— ë¹„í•´ ì„±ëŠ¥ ì €í•˜ë¥¼ ë³´ì´ëŠ” ê²ƒì„ í†µí•´ samplingëœ queryê°€ interaction sequenceì˜ random queryë³´ë‹¤ í›¨ì”¬ ìš°ìˆ˜í•˜ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ ì¤€ë‹¤.&lt;/li&gt;
  &lt;li&gt;STRec-3ì˜ ì„±ëŠ¥ ì €í•˜ëŠ” SRSì—ì„œì˜ ë°©ë¬¸ ìˆœì„œê°€ NLPì˜ ë‹¨ì–´ ìˆœì„œë§Œí¼ì´ë‚˜ ì¤‘ìš”í•˜ë‹¤ëŠ” ë…¼ë¬¸ì˜ ì£¼ì¥ì„ ì…ì¦í•œë‹¤. ë˜í•œ ì´ë¥¼ í†µí•´ SRS taskì—ì„œ sampling ì „ëµì´ time interval ì´ì™¸ì— ë‹¤ë¥¸ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•  ìˆ˜ ìˆìŒì„ ì˜ë¯¸í•œë‹¤.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;rq5-why-the-proposed-method-can-elevate-performance-and-shrink-computation-simultaneously-case-study&quot;&gt;RQ5: Why the proposed method can elevate performance and shrink computation simultaneously? (Case Study)&lt;/h3&gt;

&lt;p&gt;Figure 8ì€ ì‹ (9)ì—ì„œì˜ MLPì— ëŒ€í•œ sampling density functionì„ ì‹œê°í™”í•œ ê²°ê³¼ì´ë‹¤.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://velog.velcdn.com/images/yst3147/post/cb1e4f53-b5ce-4d90-872c-3ba21cf0edcb/image.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Time intervalì˜ ì ˆëŒ€ê°’ì´ ì‘ì„ìˆ˜ë¡ MLPì˜ outputì´ ë†’ìœ¼ë©° ì´ëŠ” sequence ë’¤ìª½ì— ê°€ê¹Œìš´ interactionì´ ë” ì¤‘ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. &lt;br /&gt;
ê²°ê³¼ì ìœ¼ë¡œ í˜„ì¬ ì‹œì ì—ì„œ ê°€ê¹Œìš´ interactionì´ samplingë  ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë©° ì´ˆê¸° periodì—ì„œëŠ” ì†Œìˆ˜ì˜ itemë§Œ samplingëœë‹¤.&lt;/p&gt;

&lt;p&gt;Figure 9ëŠ” userì˜ ì£¼ìš” ê´€ì‹¬ì‚¬ì™€ queryë¡œ samplingëœ ì˜í™”ë¡œë§Œ êµ¬ì„±ëœ ì²« ë²ˆì§¸ layerì˜ attention weight matrixì˜ heatmapì„ ì‹œê°í™”í•˜ì˜€ë‹¤.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://velog.velcdn.com/images/yst3147/post/7e7b47d0-ddc8-4cb7-a25f-cc1aafabd5b5/image.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 9ì˜ Case StudyëŠ” ë…¼ë¬¸ì˜ ëª¨ë¸ì´ ì„œë¡œ ë‹¤ë¥¸ periodì— ëŒ€í•´ ëŒ€í‘œ itemì„ ì¶”ì¶œí•  ìˆ˜ ìˆìŒì„ ë‚˜íƒ€ë‚¸ë‹¤. ì´ë¥¼ í†µí•´ ì‹œê°„ì— ë”°ë¼ ë‹¬ë¼ì§€ëŠ” ì‚¬ìš©ìì˜ ë‹¤ì–‘í•œ ê´€ì‹¬ì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. ì´ëŸ¬í•œ samplingëœ itemì€ ëª¨ë¸ì´ ë‹¤ì–‘í•œ time periodì— ë” ì¤‘ìš”í•œ itemì— ì§‘ì¤‘í•˜ë„ë¡ í•˜ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;5-conclusion&quot;&gt;&lt;strong&gt;5. Conclusion&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;ì´ ë…¼ë¬¸ì—ì„œëŠ” í•™ìŠµ ê°€ëŠ¥í•œ sparse transformerì¸ &lt;strong&gt;s&lt;/strong&gt;parce &lt;strong&gt;t&lt;/strong&gt;ransformer for the seqeuntial &lt;strong&gt;rec&lt;/strong&gt;ommendation(&lt;strong&gt;STRec&lt;/strong&gt;)ì„ ì„¤ê³„í•˜ì˜€ë‹¤. &lt;br /&gt;
ëŒ€í‘œ itemì„ ì„ íƒí•˜ê¸° ìœ„í•´ ëª¨ë“  sequenceì— ëŒ€í•´ ë¨¼ì € sampling indexë¥¼ 
ìƒì„±í•˜ëŠ” ìƒˆë¡œìš´ sampling ì „ëµì„ ì œì‹œí•˜ì˜€ë‹¤. í•œí¸ìœ¼ë¡œëŠ” Cross-attention ê¸°ë°˜ sparse transformerë¥¼ main frameworkë¡œ ì„¤ê³„í•˜ì˜€ë‹¤.&lt;br /&gt;
Sampling ì „ëµì„ ìµœì í™”í•˜ê³  ì •í™•ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ &lt;strong&gt;STRec&lt;/strong&gt;ì„ pre-trainê³¼ fine-tuningì˜ ë‘ ë‹¨ê³„ë¡œ trainí•œë‹¤. &lt;br /&gt;
ê·¸ ê²°ê³¼ &lt;strong&gt;STRec&lt;/strong&gt;ì€ inference timeì„ 54% ë‹¨ì¶•í•˜ê³  GPU memory ë¹„ìš©ì„ 70%ë¥¼ ì¤„ì´ë©´ì„œë„ ë‹¤ë¥¸ state-of-the-art ë°©ë²•ë“¤ë³´ë‹¤ ë” ë‚˜ì€ accuracy ì„±ëŠ¥ì„ ë³´ì¸ë‹¤. &lt;br /&gt;
ì¶”ì²œì‹œìŠ¤í…œì—ì„œ ë°œìƒí•˜ëŠ” sparsity íŠ¹ì„±ì„ í™œìš©í•˜ì—¬ ë” ë¹ ë¥´ê³  memoryë¥¼ ì ê²Œ ì°¨ì§€í•˜ë©´ì„œë„ ì„±ëŠ¥ì´ ì¢‹ì€ transformer ê¸°ë°˜ ì¶”ì²œ ëª¨ë¸ì„ ì œì‹œí•˜ì˜€ë‹¤ëŠ” ì ì´ ì¸ìƒê¹Šì—ˆë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;6-reference--additional-materials&quot;&gt;&lt;strong&gt;6. Reference &amp;amp; Additional materials&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Github Implementation
    &lt;ul&gt;
      &lt;li&gt;https://github.com/ChengxiLi5/STRec&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Reference
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://grouplens.org/datasets/movielens/1m/&quot;&gt;ML-20M&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://snap.stanford.edu/data/loc-gowalla.html&quot;&gt;Gowalla&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://jmcauley.ucsd.edu/data/amazon/&quot;&gt;Amazon-Electronics&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1511.06939&quot;&gt;GRU4Rec&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1711.04725&quot;&gt;NARM&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1808.09781&quot;&gt;SASRec&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1904.06690&quot;&gt;Bert4Rec&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.ijcai.org/proceedings/2019/0600.pdf&quot;&gt;FDSA&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://dl.acm.org/doi/10.1145/3336191.3371786&quot;&gt;Ti-SASRec&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2004.05150&quot;&gt;Longformer&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2006.04768&quot;&gt;Linformer&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2007.14062&quot;&gt;Big Bird&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2012.07436&quot;&gt;Informer&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2001.04451&quot;&gt;Reformer&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2202.10447&quot;&gt;FLASH&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2106.13008&quot;&gt;Autoformer&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1611.01144&quot;&gt;Gumbel-Softmax&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://ieeexplore.ieee.org/document/5197422&quot;&gt;Matrix Factorization(MF)&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
            <pubDate>Mon, 16 Oct 2023 00:00:00 +0900</pubDate>
            <link>http://dsailatkaist.github.io/2023-10-16-STRec_Sparse_Transformer_for_Sequential_Recommendations.html</link>
            <guid isPermaLink="true">http://dsailatkaist.github.io/2023-10-16-STRec_Sparse_Transformer_for_Sequential_Recommendations.html</guid>
            
            <category>reviews</category>
            
            
        </item>
        
        <item>
            <title>[NIPS 2021] Robustness of Graph Neural Networks at Scale</title>
            <description>&lt;h1 id=&quot;robustness-of-gnn-at-scale&quot;&gt;Robustness of GNN at Scale&lt;/h1&gt;

&lt;h1 id=&quot;0-background&quot;&gt;0. Background&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Adversarial Attack on GNN?&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Adversarial Pertubationì„ ì ìš©í•´ ê¸°ì¡´ ë¶„ë¥˜ê¸° ë˜ëŠ” GNN ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë‚®ì¶”ëŠ” ê²ƒì„ ë§í•¨.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;ì´ëŠ” í˜„ì‹¤ì— ì¡´ì¬í•  ìˆ˜ ìˆëŠ” ì´ë¯¸ì§€ì— ëŒ€í•´ì„œëŠ” ì •ìƒì ìœ¼ë¡œ ë™ì‘í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆì§€ë§Œ ì‚¬ëŒì´ ì¼ë¶€ ë³€í™”ë¥¼ ì·¨í•œ Adversarial Examples(Perturbedëœ ê²°ê³¼ë¬¼ë“¤)ì— ëŒ€í•´ì„œëŠ” ì·¨ì•½í•  ìˆ˜ ìˆìŒ.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Testing Phase Attack : ë¶„ë¥˜ ëª¨ë¸ì— ëŒ€í•´ì„œ ê°„ì„­ì„ ì£¼ì§„ ì•Šì§€ë§Œ ëª¨ë¸ì´ ì •í™•í•˜ê²Œ ë™ì‘í•˜ì§€ ì•Šë„ë¡ ì˜¤ë™ì‘ì„ ìœ ë°œí•˜ëŠ” ê³µê²©. ê³µê²©ì„ ìˆ˜í–‰í•  ì‹œ ì‚¬ìš© ê°€ëŠ¥í•œ ì§€ì‹ì˜ ì–‘ì— ë”°ë¼ ê³µê²©ì„ ë¶„ë¥˜&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;White Box Attacks : ë¶„ë¥˜ì— ì‚¬ìš©ë˜ëŠ” modelì— ëŒ€í•œ ëª¨ë“  ì§€ì‹ì„ ê°€ì§€ê³  ìˆëŠ” ê³µê²©&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Black Box Attacks : modelì— ëŒ€í•œ ì •ë³´ ëª¨ë¥´ëŠ” ê³µê²©ìê°€ í–‰í•˜ëŠ” ê³µê²©. ì¡°ì‘ëœ inputê³¼ ì´ê²ƒì´ ì£¼ëŠ” output ê´€ì°°í•¨ìœ¼ë¡œì¨ ì´ìš©ë  ìˆ˜ ìˆìŒ.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;1-problem-definition&quot;&gt;&lt;strong&gt;1. Problem Definition&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;Graph Neural Networksê°€ Adversarial Perturbationsì— Robustí•˜ì§€ ì•Šë‹¤ëŠ” ì ì€ ë°œê²¬ì´ ë˜ë‚˜, ì´ë¥¼ ì‹¤í—˜í•˜ê³  í•´ê²°í•˜ê³ ì í•œ ì—°êµ¬ë“¤ì€ ê±°ì˜ ì—†ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ PubMedë¼ëŠ” GNN Datasetì˜ ê²½ìš°, ì¸ì ‘ í–‰ë ¬ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ê³µê²©ì— ìˆì–´ì„œ 20GBì˜ ë©”ëª¨ë¦¬ê°€ í•„ìš”í•˜ë©° ì´ëŸ¬í•œ ê²½ìš° ê³µê²©ì„ ì‹¤í–‰í•˜ê¸° ì–´ë µë‹¤. ì´ëŠ” GNNì˜ Robustnessë¥¼ í™•ì¸í•´ë³´ê³ ì í•˜ëŠ” ì—°êµ¬ê°€ ì§„ì „ë˜ëŠ” ê²ƒì„ ì–´ë µê²Œ í•œë‹¤.&lt;/p&gt;

&lt;h1 id=&quot;2-motivation&quot;&gt;&lt;strong&gt;2. Motivation&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;í•´ë‹¹ ë…¼ë¬¸ì€ GNNì˜ Adversarial robustnessì— ëŒ€í•œ ê¸°ì¡´ ì—°êµ¬ì˜ í•œê³„ì ë“¤ì„ ë‹¤ìŒê³¼ ê°™ì´ ëª…ì‹œí–ˆë‹¤.: (1) ê¸°ì¡´ì˜ LossëŠ” Global Attackì— ì í•©í•˜ì§€ ì•Šë‹¤ëŠ” ì ê³¼ (2) GNN Attackì— ì†Œìš”ë˜ëŠ” ë¹„ìš©ì´ $O(n^2)$ì´ìƒìœ¼ë¡œ ë§¤ìš° í° ì  ê·¸ë¦¬ê³  (3) ê¸°ì¡´ì˜ Robust GNNì€ scalableí•˜ì§€ ì•Šë‹¤ëŠ” ì ì´ë‹¤.&lt;/p&gt;

&lt;p&gt;ë”°ë¼ì„œ, ìœ„ì˜ í•œê³„ì ë“¤ì„ (1) &lt;strong&gt;Surrogate loss&lt;/strong&gt;ë¥¼ ì œì‹œí•¨ìœ¼ë¡œì¨, (2) R-BCDë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ì—¬ &lt;strong&gt;ì‹œê°„ë³µì¡ë„ë¥¼ $O(\Delta)$ë¡œ ì¤„ì´ëŠ” ë°©ë²•ì„ ì œì‹œí•¨&lt;/strong&gt;ìœ¼ë¡œì¨, ë§ˆì§€ë§‰ìœ¼ë¡œ (3) &lt;strong&gt;Soft Medianìœ¼ë¡œ íš¨ê³¼ì ìœ¼ë¡œ GNNì„ ë°©ì–´í•˜ëŠ” ë°©ë²•ì„ ê´€ì°°í•¨&lt;/strong&gt;ìœ¼ë¡œì¨ í•´ê²°í•˜ê³ ì í–ˆë‹¤.&lt;/p&gt;

&lt;h1 id=&quot;3-method-and-experiment&quot;&gt;&lt;strong&gt;3. Method and Experiment&lt;/strong&gt;&lt;/h1&gt;

&lt;h2 id=&quot;31-surrogate-losses&quot;&gt;3.1 Surrogate Losses&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Why previous loss invalid?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;ë§ì€ GNNì˜ Global Attacksë“¤ì€ í‰ê·  Cross Entropy Lossë¥¼ ì¦ê°€ì‹œí‚¨ë‹¤. ê·¸ëŸ¬ë‚˜ nodeê°€ ë§ì€ Large Graphë“¤ì˜ ê²½ìš° ìœ„ì˜ lossëŠ” íš¨ìœ¨ì ì´ì§€ ì•Šë‹¤.&lt;/p&gt;

&lt;h3 id=&quot;surrogate-loss&quot;&gt;Surrogate loss&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;(ì •ì˜ 1)&lt;/strong&gt; Global Attackì— ëŒ€í•œ Surrogate Loss $L^\prime$ì€&lt;/p&gt;

&lt;p&gt;(1) ì˜³ê²Œ ë¶„ë¥˜ëœ pertubing nodeë“¤ì— ëŒ€í•´ì„œë§Œ incentiveë¥¼ ì£¼ê³ ,&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$\frac{\partial L^\prime}{\partial z_c^*}&lt;/td&gt;
      &lt;td&gt;\psi_0 = 0$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;(2) Decision Boundary ê·¼ì²˜ì— ìˆëŠ” nodeë“¤ì„ ì„ í˜¸í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ Lossë¥¼ êµ¬ì„±í•œë‹¤.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$\frac{\partial L^\prime}{\partial z_c^*}&lt;/td&gt;
      &lt;td&gt;\psi_1 &amp;lt; \frac{\partial L^\prime}{\partial z_c^*}&lt;/td&gt;
      &lt;td&gt;\psi_2 \; for \, any \,0&amp;lt;\psi_1&amp;lt;\psi_2$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;ìœ„ ì •ì˜ì—ì„œ ë„ì¶œëœ ëª…ì œì— ë”°ë¥´ë©´, Cross Entropy LossëŠ” (1)ì„ ìœ„ë°˜í•˜ì—¬ Global Optimumì„ ê°€ì§ˆ ìˆ˜ ì—†ê¸°ì— Surrogate lossê°€ ë  ìˆ˜ ì—†ë‹¤. ë˜í•œ, Carlini-Wagner(CW) LossëŠ” Decision boundaryì— ìˆëŠ” nodeë“¤ì„ ê³ ë ¤í•˜ì§€ ëª»í•˜ê¸°ì— (2)ì„ ìœ„ë°˜í•œë‹¤.&lt;/p&gt;

&lt;p&gt;ë…¼ë¬¸ì€ Surrogate lossì— ë§ëŠ” lossë¡œì¨ Masked Cross Entropy(MCE)ë¥¼ ì œì•ˆí•œë‹¤.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$MCE = \frac{1}{&lt;/td&gt;
      &lt;td&gt;V^+&lt;/td&gt;
      &lt;td&gt;}  \sum_{i \in V^+}  -\log(p^{(i)}_{c^*})$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;MCEëŠ” Projected Gradient Descent attack(PR-BCD)ì— ëŒ€í•´ì„œëŠ” Cross Entropyì™€ í° ì°¨ì´ë¥¼ ë³´ì´ì§€ ì•Šì§€ë§Œ, Greedy Gradient-Based attackì— ìˆì–´ì„œëŠ” ê°•ì ì„ ë³´ì´ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.ibb.co/gJFYDfd/2023-10-15-17-52-14.png&quot; alt=&quot;2023-10-15-17-52-14.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(ì •ì˜ 2)&lt;/strong&gt; ì •ì˜ 1ì„ ë” í™•ì¥í•˜ì—¬ Surrogate Loss $L^\prime$ëŠ”&lt;/p&gt;

&lt;p&gt;(1) í™•ì‹¤í•˜ê²Œ ì˜ëª» ë¶„ë¥˜ëœ ë…¸ë“œ($\psi$ê°€ -1ì— ê°€ê¹Œìš´ ë…¸ë“œ)ë“¤ì— ëŒ€í•´ì„œ í™•ì‹¤í•˜ê²Œ êµ¬ë¶„í•˜ê³ ,&lt;/p&gt;

&lt;p&gt;$\lim_{\psi  \to  -1^+} L^\prime &amp;lt; \infty$&lt;/p&gt;

&lt;p&gt;(2) ì •ì˜ 1ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ Decision Boundary ê·¼ì²˜ì— ìˆëŠ” pointë“¤ì„ ì„ í˜¸í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ Lossë¥¼ êµ¬ì„±í•œë‹¤.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$\frac{\partial L^\prime}{\partial z_c^*}&lt;/td&gt;
      &lt;td&gt;\psi_1 &amp;lt; \frac{\partial L^\prime}{\partial z_c^*}&lt;/td&gt;
      &lt;td&gt;\psi_2&amp;lt;0 \; \ for \, any \; 0&amp;lt;\psi_1&amp;lt;\psi_2&amp;lt;1 \; \ or \; -1&amp;lt;\psi_2&amp;lt;\psi_1&amp;lt;0$&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;32-scalable-attacks&quot;&gt;3.2 Scalable Attacks&lt;/h2&gt;

&lt;p&gt;Gradient-based attacksë“¤ì€ ì¸ì ‘í–‰ë ¬ $A$ì— ëŒ€í•´ì„œ ëª¨ë‘ ìµœì í™”í•˜ê¸°ì— $\Theta(n^2)$ì˜ ì‹œê°„ ë³µì¡ë„ë¥¼ ë³´ì—¬ Large Graphì— ëŒ€í•œ robustnessëŠ” ëŒ€ê°œ ì¸¡ì •í•˜ê¸° ì–´ë ¤ì› ë‹¤.&lt;/p&gt;

&lt;p&gt;Large-scale Optimizationì„ ìœ„í•´ R-BCD(Randomized Block Coordinate Descent)ë¥¼ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ ë³€ìˆ˜ë“¤ì˜ ë¶€ë¶„ì§‘í•©ì— ëŒ€í•´ì„œë§Œ gradientsë¥¼ êµ¬í•˜ê¸° ë•Œë¬¸ì— ì‚¬ìš©ë˜ëŠ” ë©”ëª¨ë¦¬ì™€ ìˆ˜í–‰ ì‹œê°„ì„ ì¤„ì¼ ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;p&gt;Perturbation $P\in{0,1}^{n*n}$ ëŠ” ì•„ë˜ì™€ ê°™ì´ ëª¨ë¸ë˜ì—ˆë‹¤.&lt;/p&gt;

&lt;p&gt;$\max_{P\; \text{s.t.}\; P \in {0, 1}^{n \times n},\; \sum P \leq  \Delta} L(f_\theta(A \otimes P, X)) \ \Delta = \text{edge budget}$&lt;/p&gt;

&lt;p&gt;ê·¸ëŸ¬ë‚˜ $O(n^2)$ ë§Œí¼ì˜ ê³µê°„ë³µì¡ë„ì´ê¸°ì— ë…¼ë¬¸ì€ Projected Randomized Block Coordinate Descent(PR-BCD)ë¥¼ ì œì•ˆí•œë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.ibb.co/XsZ1v5K/img2.png&quot; alt=&quot;img2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;PëŠ” ì´ì‚° Edge í–‰ë ¬ë¡œ, ê° element(p)ëŠ” edgeë¥¼ ë’¤ì§‘ì„ í™•ë¥ ì„ ë‚˜íƒ€ë‚¸ë‹¤. ìš°ì„ , epochë§ˆë‹¤ Pì—ì„œ ë¬´ì‘ìœ„ë¡œ ì¶”ì¶œëœ Blockì„ ë°”íƒ•ìœ¼ë¡œ íŠ¹ì • ë¶€ë¶„ì˜ edgeë“¤ë§Œì„ ë³€ê²½í•œë‹¤. ì—…ë°ì´íŠ¸ í›„ pì— ëŒ€í•œ í™•ë¥  ì§ˆëŸ‰í•¨ìˆ˜ë¥¼ ìˆ˜ì •í•˜ì—¬ ë² ë¥´ëˆ„ì´ ë¶„í¬ì— ëŒ€í•œ ê¸°ëŒ“ê°’ì´ Budgetì„ ë„˜ì§€ ì•Šë„ë¡ í•œë‹¤. ê·¸ë¦¬ê³  ë…¼ë¬¸ì€ PR-BCDì— ëŒ€í•œ ë˜ ë‹¤ë¥¸ ëŒ€ì•ˆìœ¼ë¡œ &lt;strong&gt;GR-BCD&lt;/strong&gt;ë¥¼ í•¨ê»˜ ì œì•ˆí•œë‹¤. PR-BCDì—ì„œ Block ì¶”ì¶œ ì‹œ ê°€ì¥ í° gradientë¥¼ ê°€ì§„ entryë§Œ &lt;strong&gt;greedy&lt;/strong&gt;í•˜ê²Œ ë³€ê²½í•˜ëŠ” ê²ƒìœ¼ë¡œ Eë²ˆì˜ epoch í›„ì— budgetì´ ì¶©ì¡±ë˜ë„ë¡ í•˜ëŠ” ë°©ë²•ì´ë‹¤. ê·¸ëŸ¬ë‚˜ ìœ„ ë°©ë²•ë“¤ì€ ì‹¤ì œ ìµœì í™” ë¬¸ì œë¥¼ ì–¼ë§ˆë‚˜ íš¨ê³¼ì ìœ¼ë¡œ ê·¼ì‚¬í•˜ëŠ”ì§€ì— ëŒ€í•œ ë³´ì¥ì€ ì œê³µí•˜ì§€ ì•Šê³ , ê³µê²©ì˜ íš¨ê³¼ì˜ Upper boundë§Œ ë³´ì—¬ì¤€ë‹¤ëŠ” ì ì—ì„œ í•œê³„ê°€ ìˆë‹¤.&lt;/p&gt;

&lt;p&gt;ìœ„ ë°©ë²•ë¡ ì—ì„œ í™•ì¥í•˜ì—¬ ë” í° ê·¸ë˜í”„ë“¤ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´, PPRGoë¥¼ ì‚¬ìš©í–ˆë‹¤. ì´ëŠ” Personalized Page Rank(PPR) Matrix($\Pi$)ë¥¼ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ explicit message passing steps ìˆ˜ë¥¼ 1ë¡œ ì¤„ì—¬ constant complexityë¥¼ ê°€ì§ˆ ìˆ˜ ìˆê²Œ í•œë‹¤.&lt;/p&gt;

&lt;p&gt;$p = softmax \big[\text{AGG}{\Pi_{uv}(A_{uv}, f_\text{enc}(x_u)\big), \;  \forall u \in  \mathbb{N}^\prime(v)}\big]$&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.ibb.co/s6chCC0/img3.png&quot; alt=&quot;img3.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;33-scalable-defense&quot;&gt;3.3 Scalable Defense&lt;/h2&gt;

&lt;p&gt;GNNì˜ ë©”ì„¸ì§€ íŒ¨ì‹± í”„ë ˆì„ì›Œí¬ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;p&gt;$h^{(l)}&lt;em&gt;v = \phi^{(l)}  \big[(  \text{AGG}^{(l)}{\big(A&lt;/em&gt;{uv}, h^{(l-1)}_uW^{(l)}\big), \quad  \forall u \in  \mathbb{N}^\prime(v)}\big] \ \ \text{where} \; \text{neighborhood} \; \mathbb{N}^\prime(v) = \mathbb{N}(v)  \cup v \\\text{and} \; AGG = \text{l-th layer message passing aggregation} \ \text{and} \; h^{(l)}_v = \text{embedding}, \sigma^{(l)}  \text{activation function}$&lt;/p&gt;

&lt;p&gt;Geisler et al. (2020)ì—ì„œëŠ” Aggregate Functionìœ¼ë¡œ Soft Medoidë¥¼ ì œì•ˆí–ˆë‹¤. Soft MedoidëŠ” ì´ì›ƒ ë…¸ë“œë“¤ì˜ embeddingì— ëŒ€í•œ distance matrixì— ëŒ€í•´ í–‰/ì—´ summationì„ ìš”êµ¬í•˜ê¸°ì— neighborhood sizeì— ëŒ€í•´ ì´ì°¨ ë³µì¡ë„ë¥¼ ì§€ë‹Œë‹¤.&lt;/p&gt;

&lt;p&gt;ì´ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ ë…¼ë¬¸ì€ Soft Medianì„ ì œì‹œí•œë‹¤.&lt;/p&gt;

&lt;p&gt;$\mu_\text{SoftMedian}(X) = \text{softmax}(\frac{-c}{T\sqrt{d}}  )^\intercal  \cdot  \mathbf{X} = \mathbf{s}^\intercal  \cdot  \mathbf{X}  \approx  \arg  \min_{x^\prime  \in  \mathbf{X}} |x_{\bar{}}  - x^\prime|$&lt;/p&gt;

&lt;p&gt;ì´ë¡œì¨ Dimension($d$)ì—ë§Œ ì˜ì¡´í•¨ìœ¼ë¡œì¨ ê³„ì‚°ì„ íš¨ìœ¨ì ìœ¼ë¡œ í•  ìˆ˜ ìˆë‹¤. $d$ë¥¼ ì¶©ë¶„íˆ ì‘ê²Œ í•œë‹¤ë©´ Soft Median ìŠ¤ì¼€ì¼ì€ $\mathbb{N}$ì— linearí•œ scaleì„ ë³´ì¸ë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.ibb.co/5vY3Fdk/img4.png&quot; alt=&quot;img4.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ìœ„ ì™¼í¸ì˜ ê·¸ë˜í”„ëŠ” ì›ë³¸ ê·¸ë˜í”„ì™€ Pertubedëœ ê·¸ë˜í”„ë¥¼ ì²« ë²ˆì§¸ message passingì„ í•œ í›„ì˜ latent spaceì˜ $L_2$ Distanceë¥¼ ë‚˜íƒ€ë‚¸ ê·¸ë˜í”„ì´ë‹¤. Soft Medianì´ Weighted Sumë³´ë‹¤ 20% ê°€ê¹Œì´ ë‚®ì€ errorë¥¼ ë³´ì¸ë‹¤. ë°˜ë©´, ê·¸ë˜í”„ì—ëŠ” Soft Medoidê°€ Soft Medianë³´ë‹¤ Robustí•œ ê²°ê³¼ë¥¼ ê°€ì§€ëŠ” ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. ê·¸ëŸ¬ë‚˜ ì˜¤ë¥¸ìª½ í‘œê°€ ë‚˜íƒ€ë‚´ë“¯ Adversarial accuracyì—ì„œëŠ” Soft Medoidê°€ Soft Medianë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ì§€ ëª»í–ˆë‹¤.&lt;/p&gt;

&lt;p&gt;ì´ì²˜ëŸ¼ Soft Medianì€ (1)ì°¨ì›ì— ëŒ€í•œ ê³ ë ¤ ì—†ì´ ë‹¨ìˆœíˆ Summation í•œë‹¤ë©´ computation ë¹„ìš©ì´ ë§ì´ ë“ ë‹¤ëŠ” ì ê³¼ (2)Soft Medianì— ë¹„í•´ robustí•˜ì§€ ì•Šì€ ê²°ê³¼ì™€ ë§ˆì°¬ê°€ì§€ë¡œ ì˜ëª»ëœ robustnessì˜ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë‹¤ëŠ” ì ì„ í•œê³„ì ìœ¼ë¡œ ì‚¼ì„ ìˆ˜ ìˆë‹¤. ê·¸ëŸ¬ë‚˜ PPRGoì™€ ê²°í•©í•˜ì—¬ ì‚¬ìš©í•œë‹¤ë©´ ìœ„ í•œê³„ë¥¼ ì™„í™”í•  ìˆ˜ ìˆë‹¤ê³  ì„¤ëª…í•œë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;34-empirical-evaluation&quot;&gt;3.4 Empirical Evaluation&lt;/h2&gt;

&lt;h3 id=&quot;robustness-wrt-global-attacks&quot;&gt;Robustness w.r.t global attacks.&lt;/h3&gt;

&lt;p&gt;ì•„ë˜ ì‹¤í—˜ ê²°ê³¼ëŠ” Pubmed, arXiv, Products Dataset ê°ê°ì— ëŒ€í•´ $\text{budget} = \Delta$ë§Œí¼ì˜ ê³µê²© í›„ì— adversarial accuracyë¥¼ ì¸¡ì •í•œ ê²°ê³¼ì´ë‹¤. ì•½ 2%ê°€ëŸ‰ Pertubationì„ ê°€í–ˆì„ ë•Œ, ëŒ€ëµ 60%ì˜ ì •í™•ë„ë¡œ ë–¨ì–´ì§€ëŠ” ëª¨ìŠµì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤ê³  ë°í˜”ë‹¤. ê° ê²½ìš°ì—ì„œ Soft Median GDC, Soft Median PPRGo, PPRGo Defenseê°€ íƒ€ ëª¨ë¸ë³´ë‹¤ ë¹„êµì  ì¢‹ì€ ê²°ê³¼ê°€ ë³´ì„ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.ibb.co/x18qbsH/img5.png&quot; alt=&quot;img5.png&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;robustness-wrt-local-attacks&quot;&gt;Robustness w.r.t local attacks.&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://i.ibb.co/Y2pV6Lp/img6.png&quot; alt=&quot;img6.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Cora ML, Citeseer, arXivì—ì„œ PR-BCDê°€ SGAë³´ë‹¤ íš¨ê³¼ì ìœ¼ë¡œ ê³µê²©ì„ í•˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆì—ˆë‹¤. ê·¸ë¦¬ê³  ì•„ë˜ ê·¸ë˜í”„ê°€ ë³´ì´ë“¯ Soft Median PPRGoê°€ ìœ„ ê³µê²©ì— ëŒ€í•´ì„œ Vanila PPRGoì™€ Vanila GCNë³´ë‹¤ ëŒ€ì²´ë¡œ ì˜ ê²¬ë””ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤. (b)ì˜ Papers100Mì—ì„œ $\Delta_i = 0.25$ì¼ ë•Œ, Soft Median PPRGoëŠ” ê³µê²©ìì˜ ì„±ê³µë¥ ì„ 90%ì—ì„œ 30%ë¡œ ì¤„ì¸ ê²ƒì„ ê·¸ë˜í”„ì—ì„œ ë³¼ ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;h1 id=&quot;4-conclusion&quot;&gt;4. Conclusion&lt;/h1&gt;

&lt;p&gt;ê·œëª¨ê°€ ìˆëŠ” GNNì—ì„œì˜ Adversarial Robustnessë¥¼ ì‚´í´ë³´ì•˜ë‹¤. ë…¼ë¬¸ì€ ì´ì „ê¹Œì§€ ë°ì´í„°ì…‹ì˜ ê·œëª¨ë¡œ ì¸í•´ ì˜ ë‹¤ë¤„ì§€ì§€ ì•Šì•˜ë˜ ë¬¸ì œë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ Attackê³¼ Defenseì— ëŒ€í•´ ì§ì ‘ ë°©ë²•ì„ ì œì‹œí•˜ê³  ì‹¤í—˜í•œ ê²°ê³¼ë¥¼ ë³´ì˜€ë‹¤.&lt;/p&gt;

&lt;p&gt;ë”°ë¼ì„œ, Complexityë¥¼ ì¤„ì´ê¸° ìœ„í•´ PPRGoì™€ Soft Medianì´ë¼ëŠ” ë°©ë²•ì„ ë„ì…í•˜ê³  Attackê³¼ Defenseì— ë°˜ì˜í•˜ì—¬ ì‹¤ì œë¡œ í° ë°ì´í„°ì…‹ì— ëŒ€í•œ ì‹¤í—˜ê²°ê³¼ë¥¼ ë‚¸ ê²ƒì„ ë³´ë©° ë¬¸ì œìƒí™©ì— ëŒ€í•œ í•´ê²° ì˜ì§€ë¥¼ ëŠë‚„ ìˆ˜ ìˆì—ˆë‹¤. ê³µê²©ìê°€ Modelì„ ë‹¤ ì•ˆë‹¤ëŠ” ê°€ì •(White-box attack)ì„ í•˜ê³  Budget($\Delta$)ë¥¼ ì¤‘ì ìœ¼ë¡œ Complexityë¥¼ í•´ê²°í–ˆë˜ ê²ƒì´ í˜„ì‹¤ì—ì„œ ì–´ë–»ê²Œ ì ìš©ì´ ë  ìˆ˜ ìˆì„ì§€ë¥¼ ì°¾ì•„ë³´ê³  ì‹¶ì–´ì¡Œë‹¤.&lt;/p&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Geisler, Simon, et al. â€œRobustness of graph neural networks at scale.â€ &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 34 (2021): 7637-7649.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Bojchevski, Aleksandar, et al. â€œScaling graph neural networks with approximate pagerank.â€ &lt;em&gt;Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp;amp; Data Mining&lt;/em&gt;. 2020.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
            <pubDate>Mon, 16 Oct 2023 00:00:00 +0900</pubDate>
            <link>http://dsailatkaist.github.io/2023-10-16-Robustness_of_Graph_Neural_Networks_at_Scale.html</link>
            <guid isPermaLink="true">http://dsailatkaist.github.io/2023-10-16-Robustness_of_Graph_Neural_Networks_at_Scale.html</guid>
            
            <category>reviews</category>
            
            
        </item>
        
        <item>
            <title>[ICML 2022] Rethinking Graph Neural Networks for Anomaly Detection</title>
            <description>&lt;h1 id=&quot;icml-22-rethinking-graph-neural-networks-for-anomaly-detection&quot;&gt;[ICML-22] Rethinking Graph Neural Networks for Anomaly Detection&lt;/h1&gt;

&lt;h1 id=&quot;title&quot;&gt;Title&lt;/h1&gt;

&lt;hr /&gt;

&lt;p&gt;Rethinking Graph Neural Networks for Anomaly Detection&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Author&lt;/th&gt;
      &lt;th&gt;Booktitle&lt;/th&gt;
      &lt;th&gt;Year&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Tang, Jianheng and Li, Jiajin and Gao, Ziqi and Li, Jia&lt;/td&gt;
      &lt;td&gt;International Conference on Machine Learning&lt;/td&gt;
      &lt;td&gt;2022&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;1-problem-definition&quot;&gt;&lt;strong&gt;1. Problem Definition&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;11-background&quot;&gt;1.1 Background&lt;/h3&gt;

&lt;p&gt;ì´ìƒì¹˜ (An anomaly or an outlier) ëŠ” ëŒ€ë¶€ë¶„ì˜ ê°œì²´ì—ì„œ í¬ê²Œ ë²—ì–´ë‚œ ë°ì´í„° ê°ì²´ë¥¼ ëœ»í•˜ë©°, ì´ìƒì¹˜ íƒì§€ (Anomaly Detection) ë¬¸ì œë¡œëŠ” ì‚¬ì´ë²„ ë³´ì•ˆ, ì‚¬ê¸° íƒì§€, ì¥ì¹˜ ì˜¤ë¥˜ íƒì§€ ë“±ì´ ìˆìŠµë‹ˆë‹¤. ê¸°ìˆ ì˜ ë°œì „ìœ¼ë¡œ ì¸í•˜ì—¬ ê·¸ë˜í”„ ë°ì´í„°ê°€ ë³´í¸í™”ë˜ë©´ì„œ structural dataì— ëŒ€í•œ ë¶„ì„ìœ¼ë¡œ Graph Neural Networks (ì´í•˜ â€œGNNâ€)ì´ ê°ê´‘ë°›ì•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ê·¸ë˜í”„ ì´ìƒ íƒì§€ ì‘ì—… (Graph Anomaly Detection Task)ì— ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ, vanilla GNNì€ ì§€ë‚˜ì¹œ í™•ì¼í™” ë¬¸ì œë¡œ ì¸í•˜ì—¬ ì´ìƒì¹˜ íƒì§€ì— ì í•©í•˜ì§€ ì•Šì•˜ê³  ì´ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•˜ì—¬ attention ë§¤ì»¤ë‹ˆì¦˜ì„ ì ìš©í•˜ëŠ” ë°©ë²•, resampling ì „ëµì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•, ê·¸ë¦¬ê³  ë³´ì¡°ì˜ lossesë¥¼ ì„¤ê³„í•˜ëŠ” ë°©ë²•ì´ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ì„¸ ê°€ì§€ ë°©ë²•ì€ ëª¨ë‘ spatial domainì—ì„œì˜ ë¶„ì„ì´ë©°, spectral domainì—ì„œì˜ ë¶„ì„ì€ ê±°ì˜ ì´ë£¨ì–´ì§€ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. GNNì„ ì„¤ê³„í•  ë•Œ ì•Œë§ëŠ” spectral filterì„ ì ìš©í•˜ëŠ” ê²ƒ ë˜í•œ ì¤‘ìš”í•˜ê¸°ì— í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” â€˜ì´ìƒì¹˜ íƒì§€ë¥¼ ìœ„í•œ GNNì—ì„œ ì ì ˆí•œ spectral filterì„ ì–´ë–»ê²Œ ê³ ë¥¼ ê²ƒì¸ê°€?â€™ ì— ëŒ€í•´ ë‹µë³€í•˜ê³ ì í•©ë‹ˆë‹¤.&lt;/p&gt;

&lt;h3 id=&quot;12-overview&quot;&gt;1.2 Overview&lt;/h3&gt;

&lt;p&gt;â€˜ì´ìƒì¹˜ íƒì§€ë¥¼ ìœ„í•œ GNNì—ì„œ ì ì ˆí•œ spectral filterì„ ì–´ë–»ê²Œ ê³ ë¥¼ ê²ƒì¸ê°€?â€™ì— ëŒ€í•´ ë‹µë³€í•˜ê¸° ìœ„í•´ ë‘ ê°€ì§€ ê³¼ì •ì„ ê±°ì¹©ë‹ˆë‹¤. ì²« ë²ˆì§¸ë¡œ, ê·¸ë˜í”„ ì´ìƒ íƒì§€ì—ì„œ spectral localized band-pass filtersì˜ ì¤‘ìš”ì„±ì„ í™•ì¸í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ë…¼ë¬¸ì˜ ì €ìëŠ” ì´ìƒì˜ ì •ë„(degree)ê°€ ì»¤ì§ˆìˆ˜ë¡, ì €ì£¼íŒŒ ì—ë„ˆì§€ê°€ ì ì§„ì ìœ¼ë¡œ ê³ ì£¼íŒŒ ì—ë„ˆì§€ë¡œ ì „í™˜ë¨ì„ í™•ì¸í•˜ì˜€ê³  ì´ë¥¼ spectral ì—ë„ˆì§€ ë¶„í¬ì˜ â€˜ì˜¤ë¥¸ìª½ í¸ì´ (right-shift)â€™ í˜„ìƒìœ¼ë¡œ ì •ì˜í•˜ì˜€ìŠµë‹ˆë‹¤. â€˜ì˜¤ë¥¸ìª½ í¸ì´â€™ í˜„ìƒì— ëŒ€í•œ ìˆ˜ë¦¬ì  ì¦ëª… ë° ë°ì´í„°ë¥¼ í†µí•œ ê²€ì¦ìœ¼ë¡œ ì ì ˆí•œ spectral filterì˜ í•„ìš”ì„±ì„ ë³´ì…ë‹ˆë‹¤. ë‘ ë²ˆì§¸ë¡œ, ê·¸ë˜í”„ ì´ìƒì¹˜ì—ì„œì˜ â€˜ì˜¤ë¥¸ìª½ í¸ì´â€™ í˜„ìƒì„ ì˜ ë‹¤ë£¨ëŠ” ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜, Beta Wavelet Graph Neural Network (ì´í•˜ â€œBWGNNâ€)ì„ ì œì•ˆí•©ë‹ˆë‹¤. Hammondâ€™s graph wavelet theoryì—ì„œ ì°©ì•ˆí•˜ì—¬ Heat kernalì´ ì•„ë‹Œ Beta kernalì„ ì‚¬ìš©í•¨ìœ¼ë¡œì¨, ë§¤ìš° ìœ ë™ì ì´ê³  spatial/spectral-localized í•˜ë©° band-passí•œ filterì„ í†µí•´ ê³ ì£¼íŒŒ ì´ìƒ í˜„ìƒì„ í•´ê²°í•©ë‹ˆë‹¤.&lt;/p&gt;

&lt;h3 id=&quot;13-preliminaries&quot;&gt;1.3 Preliminaries&lt;/h3&gt;

&lt;p&gt;ì†ì„± ê·¸ë˜í”„ (Attributed graph)ëŠ”  $G = { V,E,X }$ ë¡œ ì •ì˜ë˜ë©°, $V$ ëŠ” node, $E$ëŠ” edge, $X$ëŠ” node featuresì„ ì˜ë¯¸í•©ë‹ˆë‹¤. $A$ë¥¼ adjacency matrix, $D$ë¥¼ degree matrixë¡œ í‘œí˜„í•©ë‹ˆë‹¤. $V_ {a}$ ì™€ $V_ {n}$ì„ ë‘ ê°œì˜ ë¶„ë¦¬ëœ í•˜ìœ„ì§‘í•©ì´ë¼ í• ë•Œ, $V_ {a}$ ëŠ” ëª¨ë“  ì´ìƒ ë…¸ë“œë¥¼ ë‚˜íƒ€ë‚´ê³ , $V_ {n}$ ì€ ëª¨ë“  ì •ìƒ ë…¸ë“œë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ê·¸ë˜í”„ ê¸°ë°˜ ì´ìƒ íƒì§€ëŠ” ì£¼ì–´ì§„ ê·¸ë˜í”„ êµ¬ì¡° $E$, node features $X$, ê·¸ë¦¬ê³  ë¶€ë¶„ì ì¸ ë…¸ë“œ ë¼ë²¨ ${V_ {a}, V_ {n}}$ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ $G$ ë‚´ì˜ ë¼ë²¨ë§ ë˜ì§€ ì•Šì€ ë…¸ë“œë¥¼ ì´ìƒ ë˜ëŠ” ì •ìƒìœ¼ë¡œ ë¶„ë¥˜í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. í•´ë‹¹ ë…¼ë¬¸ì€ node ì´ìƒì¹˜ì— ì§‘ì¤‘í•˜ë©°, ëª¨ë“  edgeëŠ” ì‹ ë¢°ëœë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤. ë³´í†µ, ì •ìƒ ë…¸ë“œê°€ ì´ìƒ ë…¸ë“œë³´ë‹¤ í›¨ì”¬ ë§ê¸°ì— ê·¸ë˜í”„ ê¸°ë°˜ ì´ìƒ íƒì§€ëŠ” ë¶ˆê· í˜•í•œ ì´ì§„ ë…¸ë“œ ë¶„ë¥˜ ë¬¸ì œë¡œ ì—¬ê²¨ì§‘ë‹ˆë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;2-motivation&quot;&gt;&lt;strong&gt;2. Motivation&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;ê·¸ë˜í”„ ì´ìƒ íƒì§€ì—ì„œ spatial domainì— ëŒ€í•œ ë¶„ì„ì€ ì´ë£¨ì–´ì¡Œìœ¼ë‚˜ spectral domainì— ëŒ€í•œ ë¶„ì„ì´ ê±°ì˜ ì´ë£¨ì–´ì§€ì§€ ì•Šì•˜ìŒì´ í•´ë‹¹ ë…¼ë¬¸ì˜ ë™ê¸°ì…ë‹ˆë‹¤. ê·¸ë˜í”„ ì´ìƒ íƒì§€ë¥¼ ìœ„í•œ spectral domain ë¶„ì„ì´ ìœ íš¨í•œì§€ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•˜ì—¬ â€˜ì˜¤ë¥¸ìª½ í¸ì´â€™ í˜„ìƒì„ ì •ì˜í•˜ê³  Gaussian anomaly modelë¡œ ì¦ëª…í•˜ë©° ì¸ì¡°ì ì¸ ë°ì´í„°ì™€ ì‹¤ì œ ë°ì´í„°ì—ì„œ ìœ íš¨í•œì§€ í™•ì¸í•˜ì˜€ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;h3 id=&quot;21-theoretical-insights-of-the-right-shift-phenomenon&quot;&gt;2.1 Theoretical insights of the â€˜right-shiftâ€™ phenomenon&lt;/h3&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;$G$ ì—ì„œ $x= (x_1, x_2, â€¦ , x_N)^T \in R^N$ ì„ signal, $\hat{x}= (\hat{x}&lt;em&gt;1, \hat{x}_2, â€¦ , \hat{x}_N)^T = U^T$ ë¥¼ $x$ ì˜ graph Fourier transform ì´ë¼ ê°€ì •í•´ë´…ì‹œë‹¤. ì´ë•Œ $\hat{x}^2_k / \sum&lt;/em&gt; {i=1}^N \hat{x}^2_i$ ë¥¼ $\lambda_k (1 \leq k \leq N)$ ì—ì„œì˜ spectral energy distribution ì´ë¼ í•©ë‹ˆë‹¤. ë…¼ë¬¸ì˜ ì €ìëŠ” ì´ìƒì¹˜ì˜ ì¡´ì¬ê°€ ì¡´ì¬í•˜ë©´ spectral energy ì—ì„œì˜ â€˜ì˜¤ë¥¸ìª½ í¸ì´â€™ í˜„ìƒì´ ë‚˜íƒ€ë‚¨ì„ í™•ì¸í•˜ì˜€ìœ¼ë©°, ì´ëŠ” spectral energy distributionì´ ë‚®ì€ ì£¼íŒŒìˆ˜ì—ëŠ” ì ê²Œ ì§‘ì¤‘ë˜ì–´ ìˆê³  ë†’ì€ ì£¼íŒŒìˆ˜ì—ëŠ” ë§ì´ ì§‘ì¤‘ë˜ì–´ ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ë³¸ë¬¸ì€ ì¦ëª…ì„ ìœ„í•´ $x$ê°€ Gaussian distributionì„ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤. (i.e. $x \sim N(\mu e_N,\sigma^2 I_N)$ ì´ë•Œ, $x$ì˜ ì´ìƒì¹˜ ì •ë„ëŠ” $\sigma /&lt;/td&gt;
      &lt;td&gt;\mu&lt;/td&gt;
      &lt;td&gt;$ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. $x$ì˜ ì´ìƒì¹˜ ì •ë„ì— ë”°ë¼ spectral energy distributionì´ ì–¼ë§ˆë‚˜ ë°”ë€ŒëŠ”ì§€ë¥¼ energy ratioë¼ í• ë•Œ, ì–´ë– í•œ $1 \leq k \leq N-1$ì— ëŒ€í•˜ì—¬ kë²ˆì§¸ ë‚®ì€ ì£¼íŒŒìˆ˜ energy ratio ë¥¼ $\eta_k(x,L) = \frac{\sum_ {i=1}^k \hat{x}^2&lt;em&gt;i } {\sum&lt;/em&gt; {i=1}^N \hat{x}^2&lt;em&gt;i}$ ë¡œ ì •ì˜í•©ë‹ˆë‹¤. xì˜ ì´ìƒì¹˜ ì •ë„ê°€ ë°”ë€œì— ë”°ë¼ $\eta_k(x,L)$ ê°€ ì–´ë–»ê²Œ ë°”ë€ŒëŠ”ì§€ë¥¼ ì•Œê¸° ìœ„í•˜ì—¬ eigen-decompositionì„ ìˆ˜í–‰í•˜ë©´ ì‹œê°„ì´ ë§ì´ ì†Œìš”ë©ë‹ˆë‹¤. ë”°ë¼ì„œ ë³¸ë¬¸ì€ high-frequency area $S&lt;/em&gt; {high} = \frac{\sum_ {i=1}^k \lambda_k \hat{x}^2&lt;em&gt;i } {\sum&lt;/em&gt; {i=1}^N \hat{x}^2_i} = \frac {x^T Lx} {x^Tx}$ ë¥¼ ì •ì˜ë‚´ë ¤ ì„¤ëª…í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ëª¨ë“  ìŠ¤í™íŠ¸ëŸ¼ì—ì„œì˜ â€˜ì˜¤ë¥¸ìª½ í¸ì´â€™ í˜„ìƒì„ ì¦ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;22-validation-on-datasets&quot;&gt;2.2 Validation on Datasets&lt;/h3&gt;

&lt;p&gt;í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” $x$ê°€ Gaussian distributionì„ ë”°ë¥´ëŠ” ë°ì´í„°ì…‹ê³¼ ë”°ë¥´ì§€ ì•Šì€ ë°ì´í„°ì…‹ ê°ê°ì— ëŒ€í•˜ì—¬ â€˜ì˜¤ë¥¸ìª½ í¸ì´â€™ í˜„ìƒì„ ê²€ì¦í•©ë‹ˆë‹¤. ì²« ë²ˆì§¸ë¡œ, ì¸ì¡°ì ì¸ ë°ì´í„°ì…‹ì¸ Barabasi-Albert graph (Figure 1 (a)-(b))ì™€ Minnesota road graph (Figure 1 (c)-(d)) ì—ì„œ ì´ìƒì¹˜ì˜ íš¨ê³¼ë¥¼ ë³´ì…ë‹ˆë‹¤. ì•„ë˜ ê·¸ë¦¼ì—ì„œ íŒŒë€ìƒ‰ ì›ì€ spatial domainì—ì„œì˜ ì´ìƒ ë…¸ë“œë¥¼ ë‚˜íƒ€ë‚´ë©° ì›ì˜ í¬ê¸°ê°€ í´ìˆ˜ë¡ ì´ìƒì¹˜ì˜ ì •ë„ê°€ ì‹¬í•¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ê·¸ë˜í”„ë¥¼ í•´ì„í•˜ë©´, ì´ìƒì¹˜ì˜ ì •ë„, ì¦‰, $\sigma$ ì™€ $\alpha$ ê°€$\alpha\(\lambda$ $\lambda\)\alpha$ê°€ í¼ì„ í™•ì¸í•  ìˆ˜ ìˆìœ¼ë©° ì´ëŠ” 2.1ì—ì„œ ì„¤ëª…í•œ â€˜ì˜¤ë¥¸ìª½ í¸ì´â€™ í˜„ìƒì´ ë³´ì„ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ibb.co/fXmJ8Gf&quot; alt=&quot;Figure1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ë‘ ë²ˆì§¸ë¡œ, node featureê°€ Gaussian distributionì„ ì—„ê²©í•˜ê²ŒëŠ” ë”°ë¥´ì§€ ì•ŠëŠ” í˜„ì‹¤ì˜ ë°ì´í„°ì…‹ì—ì„œì˜ â€˜ì˜¤ë¥¸ìª½ í¸ì´â€™ í˜„ìƒì„ ì…ì¦í•©ë‹ˆë‹¤. ì•„ë˜ëŠ” í•´ë‹¹ 4ê°€ì§€ ë°ì´í„°ì…‹, Amazon, YelpChi, T-Finance, T-Social ì˜ íŠ¹ì§•ê³¼ ì´ìƒì¹˜ íš¨ê³¼ì— ëŒ€í•˜ì—¬ ì •ë¦¬í•œ ë„í‘œì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;%5BICML-22%5D%20Rethinking%20Graph%20Neural%20Networks%20for%20Ano%2021e4e1d2410f4095b1a56d4b069a2342/Table1.png&quot; alt=&quot;Table1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ì•„ë˜ì˜ í‘œëŠ” Amazon datasetì—ì„œ (1) ê¸°ì¡´ ê·¸ë˜í”„, (2) ëª¨ë“  ì´ìƒì¹˜ë¥¼ ì—†ì•¤ ê·¸ë˜í”„, (3) ì„ì˜ì˜ ê°™ì€ ë…¸ë“œì˜ ìˆ˜ë¥¼ ì—†ì•¤ ê·¸ë˜í”„ì˜ spectral energyë¥¼ ë¹„êµí•œ í‘œì…ë‹ˆë‹¤. Figure 3ì˜ ì™¼ìª½ ê·¸ë˜í”„ì—ì„œ, ë‚®ì€ ì£¼íŒŒìˆ˜ì¼ë•Œ, ì¦‰, $\lambda$ ê°’ì´ ì‘ì„ ë•Œ, Drop-Anomaly ê°€ Drop-Random ë³´ë‹¤ í° spectral energy distributionì„ ê°€ì§ì„ í™•ì¸í•  ìˆ˜ ìˆìœ¼ë©° ì´ëŠ”  â€˜ì˜¤ë¥¸ìª½ í¸ì´â€™ í˜„ìƒì´ ìˆìŒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://ibb.co/nrYcYtq&quot; alt=&quot;Figure3.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-method&quot;&gt;&lt;strong&gt;3. Method&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;ëŒ€ë¶€ë¶„ì˜ í•´ë‹¹ ë…¼ë¬¸ ì´ì „ì˜ GNNì€ low-pass filter ë˜ëŠ” adaptive filterì„ ì‚¬ìš©í•˜ì˜€ìœ¼ë©° ì´ëŠ” band-pass ì™€ spectral-localized ë¥¼ ë³´ì¥í•˜ì§€ ëª»í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë‹¨ì ì„ ê·¹ë³µí•˜ê¸° ìœ„í•˜ì—¬ í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” Hammondâ€™s graph wavelet theoryë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ìƒˆë¡œìš´ GNN architectureì¸ BWGNNë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. Hammondâ€™s Graph Wavelet ì€ graph signal $x \in R^N$ ì— wavelets $W = (W_ {\psi_1}, W_ {\psi_2},â€¦)$ ë¥¼ ì ìš©í•˜ì—¬ ë³€í˜•ì‹œí‚¤ëŠ” ê²ƒì´ë©° ì´ë•Œ, $\psi$ ëŠ” â€œmotherâ€ wavelet ì…ë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;Beta distributionì€ ì¢…ì¢… wavelet basisì˜ ì—­í• ì„ í•©ë‹ˆë‹¤. ì´ì „ì— Beta distributionì„ ì‚¬ìš©í•œ ê¸°ë¡ì´ ì—†ì–´ í•´ë‹¹ ë…¼ë¬¸ì—ì„œëŠ” Graph kernal functionë¡œ Beta distributionì„ ì„ íƒí•˜ì—¬ Beta graph waveletë¥¼ ë§Œë“¤ì—ˆê³  íŠ¹ì§•ì„ ë¶„ì„í•˜ì˜€ìŠµë‹ˆë‹¤. Heat Waveletê³¼ Beta Wavelet ë¥¼ ë¹„êµí•´ë³´ë©´, Figure 4ì˜ ì™¼ìª½ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´ Beta Waveletì€ low-pass filterë§Œ ìˆëŠ” Heat Wavelet ê³¼ ë‹¬ë¦¬ low-pass ì™€ band-passë¥¼ í¬í•¨í•œ ë‹¤ì–‘í•œ filter typeì„ í¬í•¨í•©ë‹ˆë‹¤. Figure 4ì˜ ì˜¤ë¥¸ìª½ì—ì„œëŠ” Beta Waveletì€ ê¸ì •ì˜ ë°˜ì‘ë§Œ ë³´ì´ëŠ” Heat Wavelet ê³¼ ë‹¬ë¦¬ ì„œë¡œ ë‹¤ë¥¸ ì±„ë„ì— ëŒ€í•´ ê¸ì •ê³¼ ë¶€ì •ì˜ íš¨ê³¼ë¥¼ ë‘˜ë‹¤ ë³´ì„ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;%5BICML-22%5D%20Rethinking%20Graph%20Neural%20Networks%20for%20Ano%2021e4e1d2410f4095b1a56d4b069a2342/Figure4.png&quot; alt=&quot;Figure4.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ìœ„ì—ì„œ ì„¤ëª…í•œ Beta graph waveletì„ í™œìš©í•˜ì—¬ BWGNNì€ ë³‘ë ¬ì ìœ¼ë¡œ ì„œë¡œ ë‹¤ë¥¸ wavelet kernelì„ ì‚¬ìš©í•œ í›„ í•´ë‹¹ filteringì˜ ê²°ê³¼ë¥¼ ë³‘í•©í•©ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ BWGNNì€ ì•„ë˜ì˜ propagation ê³¼ì •ì„ ì±„íƒí•©ë‹ˆë‹¤.&lt;/p&gt;

\[Z_i = W_ {i,C-i} (MLP(X))\]

\[H = AGG([Z_0, Z_1, ..., Z_C])\]

&lt;h2 id=&quot;4-experiment&quot;&gt;&lt;strong&gt;4. Experiment&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;41-experiment-setup&quot;&gt;4.1 &lt;strong&gt;Experiment setup&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;4 Dataset : T-Finance, T-Social , YelpChi, Amazon
    &lt;ul&gt;
      &lt;li&gt;T-Finance : ê±°ë˜ ë„¤íŠ¸ì›Œí¬ì—ì„œì˜ ì´ìƒ ê³„ì¢Œë¥¼ ì°¾ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•˜ëŠ” ë°ì´í„°ì…‹&lt;/li&gt;
      &lt;li&gt;T-Social : ì†Œì…œ ë„¤íŠ¸ì›Œí¬ì—ì„œ ì´ìƒ ê³„ì •ì„ ì°¾ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•˜ëŠ” ë°ì´í„°ì…‹&lt;/li&gt;
      &lt;li&gt;YelpChi : &lt;a href=&quot;http://Yelp.com&quot;&gt;Yelp.com&lt;/a&gt; ì— ì˜¬ë¼ì˜¨ ì•…ì„± ë¦¬ë·°ë¥¼ ì°¾ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•˜ëŠ” ë°ì´í„°ì…‹&lt;/li&gt;
      &lt;li&gt;Amazon : &lt;a href=&quot;http://Amazon.com&quot;&gt;Amazon.com&lt;/a&gt; ì˜ ìŒì•… ì•…ê¸° ì¹´í…Œê³ ë¦¬ì— ì˜¬ë¼ì˜¨ ê°€ì§œ ì œí’ˆ ë¦¬ë·°ë¥¼ ì°¾ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•˜ëŠ” ë°ì´í„°ì…‹&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Evaluation Metric : F1-macro, AUC&lt;/li&gt;
  &lt;li&gt;Baselines
    &lt;ul&gt;
      &lt;li&gt;First group : MLP, SVM&lt;/li&gt;
      &lt;li&gt;Second group : GCN, ChebyNet, GAT, GIN, GraphSAGE, GWNN&lt;/li&gt;
      &lt;li&gt;Third group : GraphConsis, CAREGNN, PC-GNN&lt;/li&gt;
      &lt;li&gt;Fourth group : BWGNN (hetero), BWGNN (homo)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;42-result&quot;&gt;4.2 &lt;strong&gt;Result&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;ì•„ë˜ì˜ í‘œì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë“¯ì´ BWGNNì€ Amazonì„ ì œì™¸í•œ dataset ì—ì„œì˜ ìµœê³ ì˜ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;%5BICML-22%5D%20Rethinking%20Graph%20Neural%20Networks%20for%20Ano%2021e4e1d2410f4095b1a56d4b069a2342/Table2.png&quot; alt=&quot;Table2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;%5BICML-22%5D%20Rethinking%20Graph%20Neural%20Networks%20for%20Ano%2021e4e1d2410f4095b1a56d4b069a2342/Table3.png&quot; alt=&quot;Table3.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ë¯¼ê°ë„ ë¶„ì„ì„ ì§„í–‰í•œ ê²°ê³¼ëŠ” ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. ì¤‘ìš”í•œ hyperparameterì¸ order Cì™€ ì´ìƒì¹˜ ì •ë„ì˜ ì˜í–¥ì— ëŒ€í•˜ì—¬ ë¯¼ê°ë„ ë¶„ì„ì„ ì§„í–‰í•˜ì˜€ê³  ê°ê° ì™¼ìª½ê³¼ ì˜¤ë¥¸ìª½ì˜ ê²°ê³¼ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;%5BICML-22%5D%20Rethinking%20Graph%20Neural%20Networks%20for%20Ano%2021e4e1d2410f4095b1a56d4b069a2342/Figure5.png&quot; alt=&quot;Figure5.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;%5BICML-22%5D%20Rethinking%20Graph%20Neural%20Networks%20for%20Ano%2021e4e1d2410f4095b1a56d4b069a2342/Figure6.png&quot; alt=&quot;Figure6.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;5-conclusion&quot;&gt;&lt;strong&gt;5. Conclusion&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;í•´ë‹¹ ë…¼ë¬¸ì€ ê·¸ë˜í”„ ì´ìƒ íƒì§€ì— ëŒ€í•˜ì—¬ ì„¤ëª…í•œ í›„ â€˜ì´ìƒì¹˜ íƒì§€ë¥¼ ìœ„í•œ GNNì—ì„œ ì ì ˆí•œ spectral filterì„ ì–´ë–»ê²Œ ê³ ë¥¼ ê²ƒì¸ê°€?â€™ì— ëŒ€í•´ ë‹µë³€í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•˜ì—¬ í•µì‹¬ íŠ¹ì§•ì¸ â€˜ì˜¤ë¥¸ìª½ í¸ì´â€™ì— ëŒ€í•´ ê·¸ë˜í”„ë¡œ ì¦ëª…í•˜ì—¬ ì•Œê³ ë¦¬ì¦˜ì˜ í•„ìš”ì„±ì„ ì´ì•¼ê¸°í•œ í›„ Beta Wavelet Graphë¥¼ í™œìš©í•œ ìƒˆë¡œìš´ ì•Œê³ ë¦¬ì¦˜ì¸ BWGNNë¥¼ ìˆ˜ì‹ì ìœ¼ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤. ì‹¤í—˜ì—ì„œëŠ” 4ê°€ì§€ datasetsì„ í™œìš©í•˜ì—¬ ì•Œê³ ë¦¬ì¦˜ ë¹„êµ ì‹¤í—˜ì„ ì§„í–‰í•˜ì˜€ê³  BWGNNì€ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤. Future workë¡œ noe anomaliesì—ì„œ ë” ë‚˜ì•„ê°„ edge anomaliesë¥¼ ë¶„ì„í•´ë³¼ ìˆ˜ ìˆë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. ìˆ˜ë¦¬ì ìœ¼ë¡œ energy distribution ì„ í‘œí˜„í•œ ê²ƒì´ ì¸ìƒê¹Šì—ˆìŠµë‹ˆë‹¤.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;author-information&quot;&gt;&lt;strong&gt;Author Information&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;ì‹¬ìœ¤ì£¼ (Yoonju Sim)
    &lt;ul&gt;
      &lt;li&gt;Master Student, Department of Industrial &amp;amp; Systems Engineering, KAIST&lt;/li&gt;
      &lt;li&gt;Computational Optimization, Reinforcement Learning, Transportation system&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-reference--additional-materials&quot;&gt;&lt;strong&gt;6. Reference &amp;amp; Additional materials&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Github : https://github.com/squareRoot3/Rethinking-Anomaly-Detection&lt;/li&gt;
  &lt;li&gt;Datasets :  &lt;a href=&quot;https://drive.google.com/drive/folders/1PpNwvZx_YRSCDiHaBUmRIS3x1rZR7fMr?usp=sharing&quot;&gt;google drive&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
            <pubDate>Mon, 16 Oct 2023 00:00:00 +0900</pubDate>
            <link>http://dsailatkaist.github.io/2023-10-16-Rethinking_Graph_Neural_Networks_for_Anomaly_Detection.html</link>
            <guid isPermaLink="true">http://dsailatkaist.github.io/2023-10-16-Rethinking_Graph_Neural_Networks_for_Anomaly_Detection.html</guid>
            
            <category>reviews</category>
            
            
        </item>
        
        <item>
            <title>[WSDM 2020] RecVAE: a New Variational Autoencoder for Top-N Recommendations with Implicit Feedback</title>
            <description>&lt;h1 id=&quot;recvae-a-new-variational-autoencoder-for-top-n-recommendations-with-implicit-feedback&quot;&gt;&lt;strong&gt;RecVAE: a New Variational Autoencoder for Top-N Recommendations with Implicit Feedback&lt;/strong&gt;&lt;/h1&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;&lt;strong&gt;1. Introduction&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;í–‰ë ¬ ë¶„í•´ (Matrix Factorization)ì€ í˜‘ì—… í•„í„°ë§ (Collaborative Filtering)ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì¶”ì²œì‹œìŠ¤í…œì˜ ê¸°ë³¸ì ì¸ ë°©ë²• ì¤‘ í•˜ë‚˜ì´ë‹¤. í•˜ì§€ë§Œ ì´ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë¬¸ì œì ì„ ê°€ì§€ê³  ìˆë‹¤.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;í–‰ë ¬ì˜ í¬ê¸°ê°€ ì‚¬ìš©ì (User)ì™€ í•­ëª© (Item)ì˜ ìˆ˜ì— ì„ í˜•ì ìœ¼ë¡œ ë¹„ë¡€í•˜ê¸° ë•Œë¬¸ì— ë§¤ìš° ë§ì€ íŒŒë¼ë¯¸í„°ë¥¼ í•„ìš”ë¡œ í•œë‹¤.&lt;/li&gt;
  &lt;li&gt;ì½œë“œ ìŠ¤íƒ€íŠ¸ (Cold Start) ë¬¸ì œê°€ ë°œìƒí•œë‹¤. ì¦‰, ìƒˆë¡œìš´ ì‚¬ìš©ìë‚˜ í•­ëª©ì´ ì¶”ê°€ë  ë•Œ, ì •í™•í•œ ì¶”ì²œì„ í•˜ê¸°ê°€ ì–´ë µë‹¤.&lt;/li&gt;
  &lt;li&gt;ëª‡ëª‡ ì‚¬ìš©ìë‚˜ í•­ëª©ì— ëŒ€í•´ì„œ ì£¼ì–´ì§„ ë°ì´í„°ê°€ ë§¤ìš° ì ì„ ìˆ˜ ìˆë‹¤. ì´ëŠ” ê³¼ì í•©ì„ ì´ˆë˜í•  ìˆ˜ ìˆìœ¼ë©° ì¼ë°˜ì ìœ¼ë¡œ ê°•ë ¥í•œ ì •ê·œí™”ê°€ í•„ìš”í•˜ë‹¤.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ìµœê·¼ì— ì´ëŸ¬í•œ ê·¹ë³µí•˜ê¸° ìœ„í•´ ì˜¤í† ì¸ì½”ë” ê¸°ë°˜ì˜ ì ‘ê·¼ì´ ì§€ì†ì ìœ¼ë¡œ ì—°êµ¬ë˜ê³  ìˆë‹¤. Collaborative Denoising Autoencoder (CDAE)&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;ëŠ” ì‚¬ìš©ì í”¼ë“œë°±ì„ ì„ë² ë”© (Embedding)í•˜ì—¬ ì½œë“œ ìŠ¤íƒ€íŠ¸ ë¬¸ì œë¥¼ í•´ê²°í•˜ì˜€ë‹¤. Variational Autoencoder (Mult-VAE)&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;ëŠ” ì ì ˆí•œ ìš°ë„ (Likelihood)ë¥¼ ë„ì…í•˜ì—¬ í–¥ìƒëœ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì—ˆë‹¤. ì´ ì—°êµ¬ì—ì„œëŠ” Mult-VAEì˜ í™•ì¥ìœ¼ë¡œì„œ ì•”ì‹œì  í”¼ë“œë°± (Implicit Feedback)ì„ í™œìš©í•˜ëŠ” Recommender VAE (RecVAE) ë¥¼ ì œì•ˆí•˜ê³  ì´ê²ƒì˜ ê¸°ì—¬ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;ì‚¬ìš©ì ì„ë² ë”©ì„ í–¥ìƒì‹œí‚¤ëŠ” ì¸ì½”ë” ë„¤íŠ¸ì›Œí¬ ì„¤ê³„ ì œì•ˆ&lt;/li&gt;
    &lt;li&gt;ì¶”ì²œ ì‹œìŠ¤í…œì— ì•Œë§ëŠ” ì ì ˆí•œ ì‚¬ì „ë¶„í¬ ì œì•ˆ&lt;/li&gt;
    &lt;li&gt;ì•”ì‹œì  í”¼ë“œë°±ìœ¼ë¡œ ì¸í•œ ìƒˆë¡œìš´ $\beta$-VAE ì œì•ˆ&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;2-preliminary&quot;&gt;&lt;strong&gt;2. Preliminary&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;21-variational-autoencoders-and-their-extensions&quot;&gt;&lt;strong&gt;2.1 Variational autoencoders and their extensions&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;ë³€ë¶„ ì˜¤í† ì¸ì½”ë”ëŠ” ë³µì¡í•œ ë¶„í¬ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ì ì¬ ë³€ìˆ˜ ëª¨ë¸ì´ë‹¤. ì´ì— ëŒ€í•œ ê°„ëµí•œ ìš”ì•½ìœ¼ë¡œ ì‹œì‘í•˜ì. ì£¼ì–´ì§„ ë°ì´í„°ê°€ $p_ {true}(x)$ë¥¼ ë”°ë¥´ê³  ëª¨ë¸ì„ $p_ {\theta}(x)$ë¼ê³  í•˜ì. ì ì¬ë³€ìˆ˜ $z$ë¥¼ í†µí•´ì„œ ëª¨ë¸ì„ ë‹¤ì‹œ í‘œí˜„ í•  ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;$p_ {\theta}(x) = \int p_ {\theta}(x \vert z)p(z)dz$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ì´ ì ë¶„ì„ ê³„ì‚°í•˜ëŠ” ê²ƒì€ ì–´ë ¤ìš°ë¯€ë¡œ ì´ê²ƒì˜ í•˜ê³„ (Lower Bound)ë¥¼ ìµœëŒ€í™” í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ëª¨ë¸ì´ í›ˆë ¨ëœë‹¤. ì´ë¥¼ ELBO (Evidence Lower Bound)ë¼ê³  ë¶€ë¥´ë©° ë‹¤ìŒê³¼ ê°™ë‹¤.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;$p_ {\theta}(x) \geq \mathcal{L}_ {\text{VAE}} = \mathbb{E}_ {q_ {\phi}(z \vert x)} \left[ \log p_ {\theta}(x \vert z) - \text{KL}({q_ {\phi}(z \vert x)} \parallel p(z) )\right],$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ì—¬ê¸°ì„œ $\text{KL}$ì€ KL-divergenceë¥¼ ì˜ë¯¸í•˜ê³  $p(z), q_ {\phi}(z \vert x)$ëŠ” ê°ê° ì‚¬ì „ë¶„í¬ì™€ ë³€ë¶„ ë¶„í¬ë¥¼ ì˜ë¯¸í•œë‹¤. $p_ {\theta}(z,x)$ë¥¼ í•™ìŠµí•¨ìœ¼ë¡œì¨ ë³€ë¶„ ì˜¤í† ì¸ì½”ë”ëŠ” ìƒì„±ëª¨ë¸ë¡œì„œ í™œìš©ë  ìˆ˜ ìˆë‹¤. ë˜í•œ, $q_ {\phi}(z \vert x)$ë¥¼ ì´ìš©í•œë‹¤ë©´ ì„ë² ë”©ì„ í†µí•œ ì¬í‘œí˜„ (Representation)ì„ ì–»ì„ ìˆ˜ ìˆë‹¤. $\beta$-VAE&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;ëŠ” ë”ìš± í–¥ìƒëœ ì¬í‘œí˜„ì„ ì–»ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì´ ë³€í˜•ëœ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì œì•ˆ í•˜ì˜€ë‹¤.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;$\mathcal{L}_ {\beta\text{-VAE}} = \mathbb{E}_ {q_ {\phi}(z \vert x)} \left[ \log p_ {\theta}(x \vert z) - \beta \text{KL}({q_ {\phi}(z \vert x)} \parallel p(z) )\right]$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Denoising variational autoencoders (DVAE)&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;ëŠ” ë°ì´í„°ì— ë…¸ì´ì¦ˆë¥¼ ê°•ì œë¡œ ì£¼ì…í•˜ì—¬ ì¬í‘œí˜„ì„ í•™ìŠµí•˜ê¸° ìœ„í•œ ë°©ë²•ì´ë‹¤. ë…¸ì´ì¦ˆ ë¶„í¬ $p(\tilde{x} \vert x)$ì— ëŒ€í•˜ì—¬ (ì˜ˆë¥¼ ë“¤ì–´, ê°€ìš°ì‹œì•ˆ í˜¹ì€ ë² ë¥´ëˆ„ì´ ë¶„í¬) ë³€í˜•ëœ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì •ì˜í•œë‹¤.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;$\mathcal{L}_ {\text{DVAE}} = \mathbb{E}_ {q_ {\phi}(z \vert x)} \mathbb{E}_ {p(\tilde{x} \vert x)} \left[ \log p_ {\theta}(x \vert z) - \text{KL}({q_ {\phi}(z \vert \tilde{x})} \parallel p(z) )\right]$,&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ê·¸ ê²°ê³¼ë¡œì„œ ë…¸ì´ì¦ˆê°€ ìˆëŠ” ë°ì´í„°ì— ëŒ€í•´ì„œë„ ì˜ë¯¸ìˆëŠ” ì¬í‘œí˜„ì„ ì–»ì„ ìˆ˜ ìˆë‹¤. Conditional Variational Autoencoder (CVAE)&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;ëŠ” ë³€ë¶„ ì˜¤í† ì¸ì½”ë”ì˜ ë˜ ë‹¤ë¥¸ í™•ì¥ì´ë‹¤. ì£¼ì–´ì§„ ë°ì´í„°ê°€ $x$ ë¿ë§Œ ì•„ë‹ˆë¼ $y$ë¼ëŠ” ë ˆì´ë¸”ì´ ìˆë‹¤ë©´ ì´ê²ƒì— ë”°ë¥¸ ì¡°ê±´ë¶€ í™•ë¥  ë¶„í¬ë¥¼ í•™ìŠµ í•  ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;$\mathcal{L}_ {\text{CVAE}} = \mathbb{E}_ {q_ {\phi}(z \vert x, y)}\left[ \log p_ {\theta}(x \vert z, y) - \text{KL}({q_ {\phi}(z \vert x, y)} \parallel p(z \vert y) )\right]$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ë§ˆì§€ë§‰ìœ¼ë¡œ, VAE with Arbitrary Conditioning (VAEAC)&lt;sup id=&quot;fnref:6&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;ëŠ” ê²°ì¸¡ê°’ ì˜ˆì¸¡ì„ ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì´ë©° í˜‘ì—… í•„í„°ë§ ë¬¸ì œì™€ ìƒë‹¹íˆ ë¹„ìŠ·í•˜ë‹¤. ì£¼ì–´ì§„ ë°ì´í„° $x$ì— ëŒ€í•´ì„œ ê²°ì¸¡ëœ íŠ¹ì„±ì„ $x_ {b}$ ë‚˜ë¨¸ì§€ë¥¼ $x_ {1-b}$ ë¼ê³  í•˜ì. CVAEì—ì„œ $y$ ëŒ€ì‹  $(x_ {1-b}, b)$ë¥¼ ì‚¬ìš©í•˜ë©´ VAEACì˜ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì •ì˜í•  ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;$\mathcal{L}_ {\text{VAEAC}} = \mathbb{E}_ {q_ {\phi}(z \vert x, b)}\left[ \log p_ {\theta}(x_ {b} \vert z, x_ {1-b}, b) - \text{KL}({q_ {\phi}(z \vert x, b)} \parallel p(z \vert x_ {1-b}, b) )\right],$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ì—¬ê¸°ì„œ $b$ëŠ” ì´ì§„ ë§ˆìŠ¤í‚¹ (Binary Masking)ì„ ì˜ë¯¸í•œë‹¤.&lt;/p&gt;

&lt;h3 id=&quot;22-autoencoders-and-regularization-for-collaborative-filtering&quot;&gt;&lt;strong&gt;2.2 Autoencoders and Regularization for Collaborative Filtering&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;$U$, $I$ë¥¼ ìœ ì €ì™€ í•­ëª©ì˜ ì§‘í•©ìœ¼ë¡œ í‘œê¸°í•˜ê³  $X$ë¥¼ ì•”ì‹œì  í”¼ë“œë°± í–‰ë ¬ì´ë¼ê³  í•˜ì. ì¦‰, $x_ {ui} = 1$ ì¸ í•„ìš”ì¶©ë¶„ì¡°ê±´ì€ ìœ ì € $u$ê°€ í•­ëª© $i$ë¥¼ ê¸ì •ì ìœ¼ë¡œ ì‘ìš©í–ˆë‹¤ëŠ” ê²ƒì´ë‹¤. $x_ {u}$ë¥¼ í”¼ë“œë°± ë²¡í„°ë¼ê³  í•˜ì. CDAE&lt;sup id=&quot;fnref:1:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;ëŠ” $x_ {u}$ì— ë§ˆìŠ¤í‚¹ì„ ì ìš©í•´ì„œ ë³µêµ¬í•˜ëŠ” ëª¨ë¸ì´ë¯€ë¡œ 2.1 ì„¹ì…˜ì˜ DVAEë¥¼ í˜‘ì—… í•„í„°ë§ì— ì ìš©í•œ ê²ƒìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;p&gt;Mult-VAE&lt;sup id=&quot;fnref:2:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;ëŠ” í˜‘ì—… í•„í„°ë§ì— ì ìš©í•˜ê¸° ìœ„í•´ì„œ ìš°ë„ë¥¼ ë‹¤í•­ ë¶„í¬ë¡œ ê°€ì •í•œ ë³€ë¶„ ì˜¤í† ì¸ì½”ë” ëª¨ë¸ì´ë‹¤. $n_ {u}:= \sum_{j} (x_ {u})_ {j}$ ë¼ê³  í•˜ë©´ ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;$z_ {u} \sim N(0,I_ {k \times k})$&lt;/li&gt;
    &lt;li&gt;$f_ {\theta}: \mathbb{R}^{k} \rightarrow \mathbb{R}^{\vert I \vert}$ is a neural network.&lt;/li&gt;
    &lt;li&gt;$\pi(z_ {u}) \sim \text{softmax}(f_ {\theta}(z_{u}))$&lt;/li&gt;
    &lt;li&gt;$x_ {u} \sim \text{Multinomial}(n_ {u}, \pi(z_ {u}))$&lt;/li&gt;
    &lt;li&gt;(Objective) $\mathcal{L}_ {\text{Mult-VAE}} = \mathbb{E}_ {q_ {\phi}(z_ {u} \vert x_ {u})} \left[ \log p_ {\theta}(x_ {u} \vert z_ {u}) - \beta \text{KL}({q_ {\phi}(z_{u} \vert x_ {u})} \parallel p(z_ {u}) )\right]$&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;3-method&quot;&gt;&lt;strong&gt;3. Method&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;ê¸°ë³¸ì ìœ¼ë¡œ, ì œì•ˆëœ ëª¨ë¸ RecVAEëŠ” Mult-VAEì˜ í™•ì¥ì´ë‹¤. DAE ë°©ë²•ì„ ì¶”ê°€í•˜ì—¬ ìƒì„±ëª¨ë¸ì„ ì •ì˜í•œë‹¤.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;$p_ {\theta}(x_ {u} \vert z_ {u}) = \text{Multinomial}(x \vert n_ {u}, \pi(z_ {u}))$&lt;/li&gt;
    &lt;li&gt;$\pi(z_ {u}) = \text{softmax}(f_ {\theta}(z_ {u}))$&lt;/li&gt;
    &lt;li&gt;$f_{\theta}(z_ {u})$ is a neural network.&lt;/li&gt;
    &lt;li&gt;$q_ {\phi}(z_ {u} \vert x_ {u}) = N(z_ {u} \vert \psi_ {\phi}(x_ {u}))$&lt;/li&gt;
    &lt;li&gt;(Objective) $\mathcal{L} = \mathbb{E}_ {q_ {\phi}(z_ {u} \vert x_ {u})} \mathbb{E}_ {p(\tilde{x}_ {u} \vert x_ {u})}\left[ \log p_ {\theta}(x_ {u} \vert z_ {u}) - \beta \text{KL}({q_ {\phi}(z_ {u} \vert \tilde{x}_ {u})} \parallel p(z_ {u}) )\right]$&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;31-model-architecture&quot;&gt;&lt;strong&gt;3.1 Model Architecture&lt;/strong&gt;&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://i.ibb.co/xJQrsLz/2023-10-14-164239.png&quot; width=&quot;50%&quot; height=&quot;50%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;ì²«ë²ˆì§¸ ë³€í™”ëŠ” dense CNNs&lt;sup id=&quot;fnref:7&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;, swish activation functions&lt;sup id=&quot;fnref:8&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;, layer normalization&lt;sup id=&quot;fnref:9&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;ê³¼ ê°™ì€ ì•„ì´ë””ì–´ë¥¼ ê²°í•©í•´ í˜‘ì—… í•„í„°ë§ì— ì•Œë§ëŠ” ì¶”ë¡  ë„¤íŠ¸ì›Œí¬ë¥¼ ì œì•ˆí•˜ë©° ìœ„ ê·¸ë¦¼ê³¼ ê°™ì€ êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆë‹¤.&lt;/p&gt;

&lt;h3 id=&quot;32-composite-prior&quot;&gt;&lt;strong&gt;3.2 Composite prior&lt;/strong&gt;&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://i.ibb.co/tJDwC9P/2023-10-14-205240.png&quot; width=&quot;50%&quot; height=&quot;50%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;Mult-VAE êµ¬ì¡°ì—ì„œ ë°ì´í„°ì˜ í¬ì†Œì„± (Sparsity) ë•Œë¬¸ì— ë³€ë¶„ ë¶„í¬ì˜ íŒŒë¼ë¯¸í„° ìµœì í™”ê°€ ì–´ë ¤ì›€ì„ ê²ªì„ ìˆ˜ ìˆë‹¤. ì´ëŠ” ê°•í™”í•™ìŠµì—ì„œ forgetting íš¨ê³¼ë¼ê³  ì•Œë ¤ì ¸ ìˆìœ¼ë©° ì •ì±… ê¸°ë°˜ ê°•í™”í•™ìŠµ ë…¼ë¬¸ì— ë§ì€ ë…¼ì˜ê°€ ìˆì—ˆë‹¤&lt;sup id=&quot;fnref:10&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:10&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt;. ì´ë¥¼ í•´ê²° í•˜ê¸° ìœ„í•œ ë°©ë²•ì¤‘ í•˜ë‚˜ëŠ” í•™ìŠµëœ íŒŒë¼ë¯¸í„°ë¥¼ ê¸°ì–µí•´ë‘ëŠ” ë°©ë²•ì´ë‹¤. ì¦‰, ìƒˆë¡œìš´ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ëŠ” í•™ìŠµì€ ì¢‹ì€ ê²°ê³¼ë¥¼ ì£¼ëŠ” íŒŒë¼ë¯¸í„°ë¡œ ë¶€í„° í¬ê²Œ ë²—ì–´ë‚˜ì§€ ì•Šê²Œ ì •ê·œí™”ë¥¼ ì£¼ì–´ì•¼ í•œë‹¤.&lt;/p&gt;

&lt;p&gt;ì´ëŠ” ì˜¤í† ì¸ì½”ë”ì— êµ¬ì¡°ì—ì„œ $q_ {\phi}(z \vert x)$ë¥¼ ì—…ë°ì´íŠ¸ í•  ë•Œ, ì´ì „ì— ì–»ì—ˆë˜ $q_ {\phi_ {\text{old}}}(z \vert x)$ì„ ì ë‹¹íˆ ìœ ì§€í•˜ê³  ì‹¶ì€ ê²ƒê³¼ ê°™ë‹¤ . ì´ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆëŠ” í•œ ë°©ë²•ì€ ë³¸ë˜ì˜ ì‚¬ì „ë¶„í¬ì™€ $q_ {\phi_ {\text{old}}}(z \vert x)$ì˜ ì»¨ë²¡ìŠ¤ ê²°í•©ì„ ìƒˆë¡œìš´ ì‚¬ì „ë¶„í¬ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤. ì¦‰,&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;$p(z \vert \phi_ {\text{old}},x) = \alpha N(z \vert 0,I) + (1-\alpha)q_ {\phi_ \text{old}}(z \vert x)$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ìµœì¢…ì ì¸ ëª¨ë¸ ì„¤ê³„ëŠ” ìœ„ ì‚¬ì§„ê³¼ ê°™ë‹¤.&lt;/p&gt;

&lt;h3 id=&quot;33-rescaling-kl-divergence&quot;&gt;&lt;strong&gt;3.3 Rescaling KL divergence&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;$\beta$-VAE&lt;sup id=&quot;fnref:3:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;ì€ ì¬í‘œí˜„ì„ í•™ìŠµí•˜ê¸° ìœ„í•œ ì¢‹ì€ ë°©ë²•ì´ì§€ë§Œ íŒŒë¼ë¯¸í„°ë¥¼ ì„ íƒí•˜ëŠ” ë°©ë²•ì´ í•™ìŠµì— í° ì˜í–¥ì„ ë¯¸ì¹œë‹¤. ê¸°ì¡´ì˜ ì—°êµ¬&lt;sup id=&quot;fnref:11&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;ì™€ëŠ” ë‹¤ë¥´ê²Œ í˜‘ì—… í•„í„°ë§ ë³€ë¶„ ì˜¤í† ì¸ì½”ë” ëª¨ë¸ì— ì•Œë§ëŠ” $\beta$ ì„ íƒ ë°©ë²•ì— ëŒ€í•œ ì—°êµ¬ê°€ í•„ìš”í•˜ë‹¤.&lt;/p&gt;

&lt;p&gt;ìœ ì € í”¼ë“œë°±ì´ ë¶€ë¶„ì ìœ¼ë¡œ ì£¼ì–´ì¡Œë‹¤ê³  í•˜ì. ë¶€ë¶„ì ì¸ ë°ì´í„°ì— ëŒ€í•´ì„œ $X_ {u}^{0}$ë¥¼ ìœ ì € $u$ê°€ ê¸ì •ì ìœ¼ë¡œ í‰ê°€í•œ í•­ëª©ì˜ ì§‘í•©ì´ë¼ í•˜ê³  $X_ {u}^{f}$ ê¸ì •ì ìœ¼ë¡œ í‰ê°€í•œ ëª¨ë“  í•­ëª©ì˜ ì§‘í•©ì´ë¼ê³  í•˜ì. í•­ëª©ë“¤ì´ ì› í•« ì¸ì½”ë”©ìœ¼ë¡œ ì£¼ì–´ì¡Œë‹¤ê³  í•˜ê³ , ë‹¤ìŒê³¼ ê°™ì´ ê¸°í˜¸ë¥¼ ì ì.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$x_ {u} = \sum_ {a \in X_ {u}^{0}}1_ {a}$&lt;/li&gt;
  &lt;li&gt;$x_ {u}^{f} = \sum_ {a \in X_ {u}^{f}}1_ {a}$&lt;/li&gt;
  &lt;li&gt;$\text{KL}_ {u} = \text{KL}(q_ {\phi}(z_ {u} \vert x_ {u}) \parallel p(z_ {u}))$&lt;/li&gt;
  &lt;li&gt;$\text{KL}_ {u}^{f} = \text{KL}(q_ {\phi}(z_ {u} \vert x_ {u}^{f}) \parallel p(z_ {u}))$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ì—¬ê¸°ì„œ $1_ {a}$ëŠ” í•­ëª© $a$ì— ëŒ€ì‘ë˜ëŠ” ì› í•« ì¸ì½”ë”©ëœ ë²¡í„°ì´ë‹¤. ê¸°ì¡´ì˜ ELBOë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì •ë¦¬ í•  ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;$\mathcal{L} = \mathbb{E}_ {q_ {\phi}(z_ {u} \vert x_ {u}^{f})}\left[ \log \text{Multinomial}(x_ {u}^{f} \vert \pi(z_ {u})) - \text{KL}_ {u}^{f}\right]$
$= \mathbb{E}_ {q_ {\phi}(z_ {u} \vert x_ {u}^{f})} \left[ \sum_ {a \in X_ {u}^{f}} \log \text{Cat}(1_ {a} \vert \pi(z_ {u})) - \text{KL}_ {u}^{f}\right] + C$
$= \mathbb{E}_ {q_ {\phi}(z_ {u} \vert x_ {u}^{f})}  \sum_ {a \in X_ {u}^{f}} \left[  \log \text{Cat}(1_ {a} \vert \pi(z_ {u})) - \frac{1}{\vert X_ {u}^{f}\vert} \text{KL}_ {u}^{f}\right] + C$,&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ì—¬ê¸°ì„œ $\text{Cat}$ëŠ” ì¹´í…Œê³ ë¦¬ ë¶„í¬ì´ê³  $C$ëŠ” ìµœì í™”ì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ” ìƒìˆ˜ì´ë‹¤. (ì‹¤ì œë¡œ $\text{Multinomial}$ì˜ ì •ê·œí™” ìƒìˆ˜ì´ë‹¤.) ë¶€ë¶„ì  í”¼ë“œë°±ì— ëŒ€í•´ ì£¼ì–´ì§„ ELBOë¥¼ ê·¼ì‚¬ì‹œí‚¤ê¸° ìœ„í•´ì„œ $q_ {\phi}(z_ {u} \vert x_{u}) \approx q_ {\phi}(z_ {u} \vert x_ {u}^{f})$ ê·¸ë¦¬ê³  $\text{KL}_ {u} \approx \text{KL} &lt;em&gt;{u}^{f}$ë¥¼ ê°€ì •í•˜ì. ìœ„ ë§ˆì§€ë§‰ ì‹ì—ì„œ ê¸‰ìˆ˜ì˜ ë²”ìœ„ $X&lt;/em&gt; {u}^{f}$ë¥¼ $X_ {u}^{0}$ë¡œ ëŒ€ì²´í•˜ê³  ì¶”ê°€ì ì¸ ê°€ì •ì„ ì´ìš©í•˜ë©´,&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;$\approx \frac{X_ {u}^{f}}{X_ {u}^{o}} \mathbb{E}_ {q_ {\phi}(z_ {u} \vert x_ {u}^{f})}  \sum_ {a \in X_ {u}^{0}} \left[  \log \text{Cat}(1_ {a} \vert \pi(z_ {u})) - \frac{1}{\vert X_ {u}^{f}\vert} \text{KL}_ {u}^{f}\right] + C$
$\approx \frac{X_ {u}^{f}}{X_ {u}^{o}} \mathbb{E}_ {q_ {\phi}(z_ {u} \vert x_ {u})}  \sum_ {a \in X_ {u}^{0}} \left[  \log \text{Cat}(1_ {a} \vert \pi(z_ {u})) - \frac{1}{\vert X_ {u}^{f}\vert} \text{KL}_ {u}\right] + C$
$= \frac{X_ {u}^{f}}{X_ {u}^{o}} \mathbb{E}_ {q_ {\phi}(z_ {u} \vert x_ {u})} \left[  \sum_ {a \in X_ {u}^{0}} \log \text{Cat}(1_ {a} \vert \pi(z_ {u})) - \frac{\vert X_ {u}^{0}\vert}{\vert X_ {u}^{f}\vert} \text{KL}_ {u}\right] + C$
$= \frac{X_ {u}^{f}}{X_ {u}^{o}} \mathbb{E}_ {q_ {\phi}(z_ {u} \vert x_ {u})} \left[  \log \text{Multinomial}(x_ {u} \vert \pi(z_ {u})) - \frac{\vert X_ {u}^{0}\vert}{\vert X_ {u}^{f}\vert} \text{KL}_ {u}\right] + C$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ë§Œì•½ $u$ ë§ˆë‹¤ $\vert X_ {u}^{f} \vert$ê°€ ì¼ì •í•˜ë‹¤ë©´ ìƒˆë¡œìš´ ìƒìˆ˜ $\gamma = \frac{1}{\vert X_ {u}^{f} \vert}$ë¥¼ ì •ì˜í•˜ì—¬ ìµœì¢…ì ìœ¼ë¡œ ë‹¤ìŒì„ ì–»ëŠ”ë‹¤. (ê¸°ëŒ“ê°’ì˜ ê³„ìˆ˜ëŠ” ì œê±° í•  ìˆ˜ ìˆë‹¤.)&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;$\mathcal{L} \approx \mathbb{E}_ {q_ {\phi}(z_ {u} \vert x_ {u})} \left[  \log \text{Multinomial}(x_ {u} \vert \pi(z_ {u})) - \gamma \vert X_ {u}^{0}\vert \text{KL}_ {u}\right]$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ì´ì™€ ê°™ì€ ë°©ë²•ìœ¼ë¡œ ì•”ì‹œì ì¸ í”¼ë“œë°±ì´ ì£¼ì–´ì¡Œì„ ë•Œ $\beta = \beta(x)$ë¥¼ $\gamma \vert X_ {u}^{0}\vert$ë¡œ ì„ íƒ í•  ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;h3 id=&quot;34-summary&quot;&gt;&lt;strong&gt;3.4 Summary&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;ì„¹ì…˜ 3.1, 3.2, 3.3ì˜ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ê°œì„  ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì œì•ˆí•œë‹¤.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;$\mathcal{L} &lt;em&gt;{\text{RecVAE}} = \mathbb{E}&lt;/em&gt; {q_ {\phi}(z \vert x)} \mathbb{E}_ {p(\tilde{x} \vert x)}\left[ \log p_ {\theta}(x \vert z) - \beta(x) \text{KL}({q_ {\phi}(z \vert \tilde{x})} \parallel p(z \vert \phi_ {\text{old}}, x) )\right]$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ëª¨ë¸ í›ˆë ¨ì„ ë§ˆì¹œ ë’¤, ìƒˆë¡œìš´ ì‚¬ìš©ìì— $x$ì— ëŒ€í•´ì„œ $p_ {\theta}( x \vert q_ {\phi}(z \vert x))$ì€ í•­ëª© ë³„ ê¸ì •ì ìœ¼ë¡œ í‰ê°€í•  í™•ë¥ ì„ ì¤€ë‹¤. ì´ë¥¼ ì´ìš©í•˜ì—¬ ìƒìœ„ í•­ëª©ì„ ì¶”ì²œ í•´ì¤„ ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;4-experiment&quot;&gt;&lt;strong&gt;4. Experiment&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;RecVAEëŠ” Adam&lt;sup id=&quot;fnref:12&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:12&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;ì˜µí‹°ë§ˆì´ì €ë¡œ ìµœì í™” ëìœ¼ë©° $\text{lr} = 5*10^{-4}$ì™€ $500$ì˜ ë°°ì¹˜ í¬ê¸°ê°€ ì‚¬ìš©ë˜ì—ˆë‹¤. ë…¸ì´ì¦ˆëŠ” í‰ê· ì´ $0.5$ì¸ ë² ë¥´ëˆ„ì´ ë¶„í¬ë¡œ ì£¼ì…ëê³  ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ìœ„í•´ $N(0,10I)$ì„ ë³µí•© ì‚¬ì „ë¶„í¬ì— ì¶”ê°€í–ˆë‹¤. ì¦‰, $p(z)$, $q_ {\phi_ {\text{old}}}$, $N(0,10I)$ê°€ ë³µí•© ì‚¬ì „ë¶„í¬ë¡œ ì‚¬ìš©ëê³  ê°ê°ì˜ ë¹„ìœ¨ì€ 3:15:2ê°€ ì í•©í–ˆë‹¤. $\gamma$ëŠ” êµì°¨ê²€ì¦ (Cross-validation)ì„ í†µí•´ ë°ì´í„°ë§ˆë‹¤ ë‹¤ë¥¸ ê°’ì„ ì„ íƒí–ˆë‹¤.&lt;/p&gt;

&lt;h3 id=&quot;41-datasets&quot;&gt;&lt;strong&gt;4.1 Datasets&lt;/strong&gt;&lt;/h3&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Â &lt;/th&gt;
      &lt;th&gt;ë°ì´í„° ì°¨ì›&lt;/th&gt;
      &lt;th&gt;í‰ê°€ëœ í•­ëª© ìˆ˜&lt;/th&gt;
      &lt;th&gt;$\gamma$&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;MovieLens-20M&lt;/td&gt;
      &lt;td&gt;(136677, 20720)&lt;/td&gt;
      &lt;td&gt;9,990,682&lt;/td&gt;
      &lt;td&gt;0.005&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Netflix Prize Dataset&lt;/td&gt;
      &lt;td&gt;(463435, 17769)&lt;/td&gt;
      &lt;td&gt;56,880,037&lt;/td&gt;
      &lt;td&gt;0.0035&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Million Songs Dataset&lt;/td&gt;
      &lt;td&gt;(571355, 41140)&lt;/td&gt;
      &lt;td&gt;33,633,450&lt;/td&gt;
      &lt;td&gt;0.01&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;RecVAEëŠ” MovieLens-20M&lt;sup id=&quot;fnref:13&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:13&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;, Netflix Prize Dataset&lt;sup id=&quot;fnref:14&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:14&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt;, Million Songs Dataset&lt;sup id=&quot;fnref:15&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:15&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt;ì—ì„œ í‰ê°€ë˜ì—ˆìœ¼ë©° ìœ„ í‘œëŠ” ê° ë°ì´í„°ì˜ ì •ë³´ì™€ ì‚¬ìš©ëœ $\gamma$ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ê° ë°ì´í„°ëŠ” 8:2ì˜ ë¹„ìœ¨ë¡œ í›ˆë ¨ë°ì´í„°ì™€ í‰ê°€ë°ì´í„°ë¡œ ë¶„ë¦¬ëë‹¤.&lt;/p&gt;

&lt;h3 id=&quot;42-baselines&quot;&gt;&lt;strong&gt;4.2 Baselines&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;ëª¨ë¸ì„ ë¹„êµí•˜ê¸° ìœ„í•´ì„œ 3ê°€ì§€ ìœ í˜•ì˜ ëª¨ë¸ë“¤ì„ ë¹„êµí•  ê²ƒì´ë‹¤.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Linear models from classical collaborative filtering
    &lt;ul&gt;
      &lt;li&gt;Weighted Matrix Factorization (WMF)&lt;sup id=&quot;fnref:16&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:16&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;16&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
      &lt;li&gt;Sparse LInear Method (SLIM)&lt;sup id=&quot;fnref:17&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:17&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;17&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
      &lt;li&gt;Embarrassingly Shallow Autoencoder (EASE)&lt;sup id=&quot;fnref:18&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:18&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;18&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Rank method
    &lt;ul&gt;
      &lt;li&gt;WARP&lt;sup id=&quot;fnref:19&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:19&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;19&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
      &lt;li&gt;LambdaNet&lt;sup id=&quot;fnref:20&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:20&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;20&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Autoencoder-based method
    &lt;ul&gt;
      &lt;li&gt;CDAE&lt;sup id=&quot;fnref:1:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
      &lt;li&gt;Mult-DAE &amp;amp; Mult-VAE&lt;sup id=&quot;fnref:2:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
      &lt;li&gt;Ranking-Critical Training (RaCT)&lt;sup id=&quot;fnref:21&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:21&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;21&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;43-evaluation-metrics&quot;&gt;&lt;strong&gt;4.3 Evaluation Metrics&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;í…ŒìŠ¤íŠ¸ ìœ ì € $u$ì˜ í•­ëª© $X_ {u}^{t}$ì™€ ëª¨ë¸ì˜ (ë‚´ë¦¼ì°¨ìˆœ) ê²°ê³¼ $R_ {u}^{(n)}$ì— ëŒ€í•´ì„œ $\text{Recall@}k(u)$ì™€ $\text{NDCG@}(k(u)$ê°€ í‰ê°€ ì§€í‘œë¡œì„œ ì‚¬ìš©ë  ê²ƒì´ë‹¤.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;$\text{Recall@}k(u) = \frac{1}{\min(\vert R_ {u}^{(n)} \vert, \vert X_ {u}^{t} \vert)} \sum_{n=1}^{k} 1\left[R_ {u}^{(n)} \in  X_ {u}^{t} \right]$ 
$\text{DGG@}k(u) = \sum_{n=1}^{k}\frac{2^{1\left[R_ {u}^{(n)} \in  X_ {u}^{t} \right]}-1}{\log(n+1)}$
$\text{NDCG@}(k(u) = \text{DCG@}k(u) / \left( \sum_{n=1}^{\vert X_ {u}^{t} \vert } \frac{1}{\log(n+1)} \right)$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;44-results&quot;&gt;&lt;strong&gt;4.4 Results&lt;/strong&gt;&lt;/h3&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://i.ibb.co/ZdpKqMG/2023-10-15-001248.png&quot; width=&quot;50%&quot; height=&quot;50%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;RecVAEì„ ê° ê²½ìŸ ëª¨ë¸ê³¼ ë¹„êµí•œ ê²°ê³¼ì´ë‹¤. ë³¼ë“œì²´ëŠ” ê°€ì¥ ì¢‹ì€ ê²°ê³¼ì´ë©° ë°‘ì¤„ì€ ë‘ë²ˆì§¸ë¡œ ì¢‹ì€ ê²°ê³¼ì´ë‹¤. Million Songs Datasetì—ì„œëŠ” EASEê°€ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ ë‚˜ë¨¸ì§€ ê²°ê³¼ì—ì„  RecVAEê°€ ì¢‹ì€ ëª¨ìŠµì„ ë³´ì—¬ì¤€ë‹¤.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://i.ibb.co/yWsMDnT/2023-10-15-001902.png&quot; width=&quot;50%&quot; height=&quot;50%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;ì¸ì½”ë” ì„¤ê³„, ë³µí•© ì‚¬ì „ë¶„í¬, $\beta$ ì¡°ì •, êµëŒ€í›ˆë ¨, ë…¸ì´ì¦ˆ ì£¼ì…ì— ëŒ€í•œ ì œê±° ì—°êµ¬ (Ablation Study)ì— ëŒ€í•œ ê²°ê³¼ì´ë‹¤. êµëŒ€í›ˆë ¨ì´ë€ ì¸ì½”ë”ì™€ ë””ì½”ë”ë¥¼ ë™ì‹œì— í›ˆë ¨í•˜ì§€ ì•Šê³  ê°ê° í›ˆë ¨í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ìœ„ í‘œì— ë”°ë¥´ë©´ ê°ê°ì˜ ê¸°ëŠ¥ì€ ì„±ëŠ¥ í–¥ìƒì— ë„ì›€ì´ ëœë‹¤. ì¼ë¶€ ê¸°ëŠ¥ì€ ê°œë³„ì ìœ¼ë¡œ ì ìš©ë˜ëŠ” ê²ƒë³´ë‹¤ í•¨ê»˜ ì‚¬ìš©ë˜ëŠ” ê²ƒì´ íš¨ê³¼ì ì´ë‹¤. (ì˜ˆë¥¼ ë“¤ì–´, $\beta$ ì¡°ì •ê³¼ êµëŒ€í›ˆë ¨)&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
&lt;img src=&quot;https://i.ibb.co/VJqBBFt/2023-10-15-002402.png&quot; width=&quot;80%&quot; height=&quot;80%&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;ìœ„ ê·¸ë˜í”„ëŠ” ë³µí•© ì‚¬ì „ë¶„í¬ì˜ ìš©ì´í•¨ì„ ì¦ëª…í•˜ê¸° ìœ„í•´ ì„ì˜ë¡œ ì„ íƒëœ ì‚¬ìš©ìì˜ ì—í­ (epoch)ì— ë”°ë¥¸ NDCG@100ì˜ ë³€í™”ëŸ‰ì„ ê·¸ë¦° ê²ƒì´ë‹¤. ê¸°ì¡´ì˜ ê°€ìš°ì‹œì•ˆ ì‚¬ì „ë¶„í¬ ë³´ë‹¤ ë³µí•© ì‚¬ì „ë¶„í¬ì˜ ë³€ë™ëŸ‰ì´ ë”ìš± ì•ˆì •ì ì¸ ê²ƒì„ í™•ì¸ í•  ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;h2 id=&quot;5-conclusion&quot;&gt;&lt;strong&gt;5. Conclusion&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;ì´ ë…¼ë¬¸ì—ì„œëŠ” Mult-VAEì˜ ê°œì„ ëœ ë²„ì „ì¸ RecVAEë¥¼ ì œì•ˆí•œë‹¤. ì´ëŠ” ìƒˆë¡œìš´ ì¸ì½”ë” êµ¬ì¡°, ë³µí•© ì‚¬ì „ë¶„í¬, í˜‘ì—…í•„í„°ë§ì— ì•Œë§ì€ $\beta$ ì¡°ì • ë°©ì‹ì„ í¬í•¨í•˜ê³  ìˆìœ¼ë©°, ì—¬ëŸ¬ê°€ì§€ ë°ì´í„°ì—ì„œ ë‹¤ë¥¸ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ëŠ¥ê°€í–ˆë‹¤. í–¥í›„ ì—°êµ¬ ë°©í–¥ìœ¼ë¡œì„œ ì£¼ëª©ë˜ëŠ” ì ì€ (1) ì´ ë°©ë²•ì„ ìœ ì €ì™€ í•­ëª©ì„ ë’¤ë°”ê¾¸ì–´ ì‹¤í—˜í•˜ë©´ ì–´ë–»ê²Œ ë ì§€, (2) ì‚¬ì „ë¶„í¬ë¥¼ ë”ìš± ë³µì¡í•˜ê²Œ ë§Œë“¤ë©´ ì–´ë–»ê²Œ ë ì§€, (3) ì»¨ë²¡ìŠ¤ ê²°í•©ì´ ì•„ë‹Œ ë‹¤ë¥¸ ë°©ë²•ì˜ ì •ê·œí™”ë¥¼ ì´ìš©í•˜ì—¬ forgetting problemì„ í•´ê²°í•  ìˆ˜ ìˆëŠ”ì§€ì™€ ê°™ì€ ê²ƒì´ ê³ ë ¤ëœë‹¤.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;additional-materials--references&quot;&gt;&lt;strong&gt;Additional materials &amp;amp; References&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;Code Availability&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;https://github.com/ilya-shenbin/RecVAE&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Author information&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Ilya Shenbin
    &lt;ul&gt;
      &lt;li&gt;Samsung-PDMI Joint AI Center&lt;/li&gt;
      &lt;li&gt;ilya.shenbin@gmail.com&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Anton Alekseev
    &lt;ul&gt;
      &lt;li&gt;Samsung-PDMI Joint AI Center&lt;/li&gt;
      &lt;li&gt;anton.m.alexeyev@gmail.com&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Elena Tutubalina
    &lt;ul&gt;
      &lt;li&gt;Samsung-PDMI Joint AI Center&lt;/li&gt;
      &lt;li&gt;tutubalinaev@gmail.com&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Valentin Malykh
    &lt;ul&gt;
      &lt;li&gt;Neural Systems and Deep Learning Laboratory&lt;/li&gt;
      &lt;li&gt;valentin.malykh@phystech.edu&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Sergey I. Nikolenko
    &lt;ul&gt;
      &lt;li&gt;Samsung-PDMI Joint AI Center&lt;/li&gt;
      &lt;li&gt;sergey@logic.pdmi.ras.ru&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Yao Wu, Christopher DuBois, Alice X Zheng, and Martin Ester. 2016. Collaborative denoising auto-encoders for top-n recommender systems. In Proceedings of the Ninth ACM International Conference on Web Search and Data Mining. ACM, 153â€“162.Â &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;Â &lt;a href=&quot;#fnref:1:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;Â &lt;a href=&quot;#fnref:1:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Dawen Liang, Rahul G Krishnan, Matthew D Hoffman, and Tony Jebara. 2018. Variational autoencoders for collaborative filtering. In Proceedings of the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 689â€“698.Â &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;Â &lt;a href=&quot;#fnref:2:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;Â &lt;a href=&quot;#fnref:2:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. 2017. BetaVAE: Learning basic visual concepts with a constrained variational framework. In International Conference on Learning Representations, Vol. 3.Â &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;Â &lt;a href=&quot;#fnref:3:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Daniel Im Jiwoong Im, Sungjin Ahn, Roland Memisevic, and Yoshua Bengio. 2017. Denoising criterion for variational auto-encoding framework. In Thirty-First AAAI Conference on Artificial Intelligence.Â &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Kihyuk Sohn, Honglak Lee, and Xinchen Yan. 2015. Learning structured output representation using deep conditional generative models. In Advances in neural information processing systems. 3483â€“3491.Â &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Oleg Ivanov, Michael Figurnov, and Dmitry P. Vetrov. 2019. Variational Autoencoder with Arbitrary Conditioning. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. https://openreview.net/forum?id=SyxtJh0qYm.Â &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Gao Huang, Zhuang Liu, and Kilian Q. Weinberger. 2016. Densely Connected Convolutional Networks. CoRR abs/1608.06993 (2016). arXiv:1608.06993 http: //arxiv.org/abs/1608.06993.Â &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:8&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Prajit Ramachandran, Barret Zoph, and Quoc V. Le. 2018. Searching for Activation Functions. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Workshop Track Proceedings. OpenReview.net. https://openreview.net/forum?id=Hkuq2EkPf.Â &lt;a href=&quot;#fnref:8&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:9&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer Normalization. arXiv e-prints, Article arXiv:1607.06450 (Jul 2016), arXiv:1607.06450 pages. arXiv:stat.ML/1607.06450.Â &lt;a href=&quot;#fnref:9&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:10&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Rein Houthooft, Xi Chen, Yan Duan, John Schulman, Filip De Turck, and Pieter Abbeel. 2016. Vime: Variational information maximizing exploration. In Advances in Neural Information Processing Systems. 1109â€“1117.Â &lt;a href=&quot;#fnref:10&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:11&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew Dai, Rafal Jozefowicz, and Samy Bengio. 2016. Generating Sentences from a Continuous Space. In Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning. 10â€“21.Â &lt;a href=&quot;#fnref:11&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:12&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, Yoshua Bengio and Yann LeCun (Eds.). http://arxiv.org/abs/1412.6980Â &lt;a href=&quot;#fnref:12&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:13&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets: History and Context. ACM Trans. Interact. Intell. Syst. 5, 4, Article 19 (Dec. 2015), 19 pages. https://doi.org/10.1145/2827872.Â &lt;a href=&quot;#fnref:13&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:14&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;James Bennett, Stan Lanning, et al. 2007. The netflix prize. In Proceedings of KDD cup and workshop, Vol. 2007. New York, NY, USA., 35.Â &lt;a href=&quot;#fnref:14&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:15&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whitman, and Paul Lamere. 2011. The Million Song Dataset. In Proceedings of the 12th International Conference on Music Information Retrieval (ISMIR 2011).Â &lt;a href=&quot;#fnref:15&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:16&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Yifan Hu, Yehuda Koren, and Chris Volinsky. 2008. Collaborative filtering for implicit feedback datasets. In 2008 Eighth IEEE International Conference on Data Mining. Ieee, 263â€“272.Â &lt;a href=&quot;#fnref:16&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:17&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Xia Ning and George Karypis. 2011. Slim: Sparse linear methods for top-n recommender systems. In 2011 IEEE 11th International Conference on Data Mining. IEEE, 497â€“506.Â &lt;a href=&quot;#fnref:17&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:18&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Harald Steck. 2019. Embarrassingly Shallow Autoencoders for Sparse Data. In The World Wide Web Conference. ACM, 3251â€“3257.Â &lt;a href=&quot;#fnref:18&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:19&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Jason Weston, Samy Bengio, and Nicolas Usunier. 2011. Wsabie: Scaling up to large vocabulary image annotation. In Twenty-Second International Joint Conference on Artificial Intelligence.Â &lt;a href=&quot;#fnref:19&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:20&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Christopher J Burges, Robert Ragno, and Quoc V Le. 2007. Learning to rank with nonsmooth cost functions. In Advances in neural information processing systems. 193â€“200.Â &lt;a href=&quot;#fnref:20&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:21&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Sam Lobel, Chunyuan Li, Jianfeng Gao, and Lawrence Carin. 2019. Towards Amortized Ranking-Critical Training for Collaborative Filtering. arXiv preprint arXiv:1906.04281 (2019).Â &lt;a href=&quot;#fnref:21&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
            <pubDate>Mon, 16 Oct 2023 00:00:00 +0900</pubDate>
            <link>http://dsailatkaist.github.io/2023-10-16-RecVAE_a_New_Variational_Autoencoder_for_Top-N_Recommendations_with_Implicit_Feedback.html</link>
            <guid isPermaLink="true">http://dsailatkaist.github.io/2023-10-16-RecVAE_a_New_Variational_Autoencoder_for_Top-N_Recommendations_with_Implicit_Feedback.html</guid>
            
            <category>reviews</category>
            
            
        </item>
        
        <item>
            <title>[KDD 2022] ROLAND: Graph Learning Framework for Dynamic Graphs</title>
            <description>&lt;!-- ---
title: ROLAND Reivew
sidebar: Introduction_sidebar
keywords: introduction
permalink: template.html
toc: true
folder: introduction
--- --&gt;

&lt;h1 id=&quot;roland-graph-learning-framework-for-dynamic-graphs&quot;&gt;&lt;strong&gt;ROLAND: Graph Learning Framework for Dynamic Graphs&lt;/strong&gt;&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2208.07239&quot;&gt;Paper Link&lt;/a&gt;
&lt;a href=&quot;https://github.com/snap-stanford/roland&quot;&gt;Github Implementation&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ì´ ë…¼ë¬¸ì€ ê¸°ì¡´ ì—°êµ¬ë“¤ì—ì„œ ì œì‹œí•œ dynamic graphì—ì„œì˜ graph representation learningì˜ í•œê³„ì ì„ ì œì‹œí•˜ê³ , static GNNì—ì„œ ì‚¬ìš©í•˜ë˜ í…Œí¬ë‹‰ë“¤ì„ dynamic graphì—ì„œ í™œìš©í•  ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ ë°©ë²•ë¡ ì¸ ROLANDë¥¼ ì œì‹œí•˜ì˜€ë‹¤.&lt;/p&gt;

&lt;h3 id=&quot;before-we-start&quot;&gt;&lt;strong&gt;Before We Start&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Static Graph&lt;/strong&gt;: ì‹œê°„ì— ë”°ë¼ ë³€í•˜ì§€ ì•ŠëŠ” graphë¥¼ ì˜ë¯¸í•œë‹¤.
    &lt;ul&gt;
      &lt;li&gt;$G = {V, E, X}$
        &lt;ul&gt;
          &lt;li&gt;$V$: node set&lt;/li&gt;
          &lt;li&gt;$E$: edge set&lt;/li&gt;
          &lt;li&gt;$X$: attribute matrix&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Dynamic Graph&lt;/strong&gt;: ì‹œê°„ì— ë”°ë¼ node, edge, attribute ë“±ì´ ë³€í™”í•˜ëŠ” graphë¥¼ ì˜ë¯¸í•œë‹¤.
    &lt;ul&gt;
      &lt;li&gt;$G(t) = {V(t), E(t), X_ {v}(t), X_ {e}(t)}$
        &lt;ul&gt;
          &lt;li&gt;$V(t)$: node set in timestep $t$&lt;/li&gt;
          &lt;li&gt;$E(t)$: edge set in timestep $t$&lt;/li&gt;
          &lt;li&gt;$X_v(t)$: node attribute matrix in timestep $t$&lt;/li&gt;
          &lt;li&gt;$X_e(t)$: edge attribute matrix in timestep $t$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Graph Nerual Network (GNN)&lt;/strong&gt;: GNNì˜ ëª©ì ì€ local network neighborhoodë¡œ ë¶€í„° ë°˜ë³µì ì¸ message aggregationì„ í†µí•´ node embeddingì„ learní•˜ëŠ” ê²ƒì´ë‹¤.
    &lt;ul&gt;
      &lt;li&gt;&lt;img src=&quot;https://i.ibb.co/vV6W0th/2023-10-15-03-43-26.png&quot; alt=&quot;img_gnn&quot; /&gt;
        &lt;ul&gt;
          &lt;li&gt;$h^{(l)}$: $l$-th layer GNNì„ apply í•œ ëª¨ë“  nodeë“¤ì˜ embedding&lt;/li&gt;
          &lt;li&gt;$m^{(l)}$: $l$-th layerì—ì„œì˜ message embedding&lt;/li&gt;
          &lt;li&gt;$MSG^{(l)}()$: $l$-th layer message-passing function, ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ functionì´ ìˆìŒ&lt;/li&gt;
          &lt;li&gt;
            &lt;h2 id=&quot;aggl-l-th-layer-aggregation-function-ë‹¤ì–‘í•œ-ì¢…ë¥˜ì˜-functionì´-ìˆìŒ&quot;&gt;$AGG^{(l)}()$: $l$-th layer aggregation function, ë‹¤ì–‘í•œ ì¢…ë¥˜ì˜ functionì´ ìˆìŒ&lt;/h2&gt;
            &lt;h2 id=&quot;introduction&quot;&gt;&lt;strong&gt;Introduction&lt;/strong&gt;&lt;/h2&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Dynamic graphë¥¼ training í•˜ëŠ” ê²ƒì€ fraud detection, anti-money laundering, recommender systems ë“± ë‹¤ì–‘í•œ ë„ë©”ì¸ì—ì„œ í™œìš©ë  ìˆ˜ ìˆë‹¤. ê·¸ë ‡ê¸°ì— ë‹¤ì–‘í•œ ì—°êµ¬ë“¤ì„ í†µí•´ dynamic graphë¥¼ ìœ„í•œ GNNë“¤ì´ ê°œë°œë˜ì—ˆì§€ë§Œ í¬ê²Œ ë‹¤ìŒê³¼ ê°™ì€ í•œê³„ì ë“¤ì´ ì¡´ì¬í–ˆë‹¤.&lt;/p&gt;

&lt;h4 id=&quot;limitations&quot;&gt;Limitations&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;span style=&quot;color:red&quot;&gt; Model Design &lt;/span&gt;
    &lt;ul&gt;
      &lt;li&gt;ê¸°ì¡´ dynamic graph ëª¨ë¸ë“¤ì€ ì„±ëŠ¥ì´ ì¢‹ì€ static GNN ì•„í‚¤í…ì³ë¥¼ ì‘ìš©í•˜ëŠ” ê²ƒì— ì‹¤íŒ¨í–ˆë‹¤.
        &lt;ul&gt;
          &lt;li&gt;GNNì„ feature encoderë¡œ ì‚¬ìš©í•œ í›„ sequence ëª¨ë¸ ì–¹ê¸°&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Skip-connection, batch normalization, edge embeddingê³¼ ê°™ì€ í…Œí¬ë‹‰ë“¤ì€ static graph GNN message passingì— íš¨ê³¼ì ì¸ ì„±ëŠ¥ì„ ë³´ì—¬ì¤¬ì§€ë§Œ, dynamic graphì—ì„œëŠ” ì‘ìš©ë˜ì§€ ëª»í•˜ê³  ìˆë‹¤.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;span style=&quot;color:green&quot;&gt; Evaluation &lt;/span&gt;
    &lt;ul&gt;
      &lt;li&gt;ê¸°ì¡´ ì—°êµ¬ë“¤ì—ì„œëŠ” dynamic graphë¥¼ training ì‹œí‚¤ê¸° ìœ„í•´ datasetì„ training, validation, test datasetìœ¼ë¡œ ë‚˜ëˆŒ ë•Œ ë‹¨ìˆœíˆ ì•ì—ì„œ ë¶€í„° 6:2:2ë¡œ ìë¥´ëŠ” ë“±ì˜ ë°©ë²•ì„ ì‚¬ìš©í•˜ì˜€ë‹¤.&lt;/li&gt;
      &lt;li&gt;ì´ëŠ” dynamic graphê°€ ê°€ì§€ëŠ” ì‹œê°„ì— ë”°ë¥¸ dataset distributionì´ ë³€í™”í•  ìˆ˜ ìˆë‹¤ëŠ” íŠ¹ì„±ì„ ê³ ë ¤í•˜ì§€ ì•Šì€ dataset ë¶„ë¦¬ ë°©ë²•ì´ë‹¤.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;span style=&quot;color:blue&quot;&gt; Training &lt;/span&gt;
    &lt;ul&gt;
      &lt;li&gt;ëŒ€ë¶€ë¶„ì˜ dynamic GNNì—ì„œëŠ” ëª¨ë“  timestep $t$ì— ëŒ€í•œ ê·¸ë˜í”„ ì •ë³´ $G(t)$ë¥¼ GPU ë©”ëª¨ë¦¬ì— ì €ì¥í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— scalability issueê°€ ìˆë‹¤.&lt;/li&gt;
      &lt;li&gt;ê·¸ë ‡ê¸°ì— edge ê°œìˆ˜ 200ë§Œê°œ ì´í•˜ì˜ ì‘ì€ graphë“¤ë¡œ ì‹¤í—˜ì„ ì§„í–‰í•˜ëŠ” ë“±ì˜ ë¬¸ì œê°€ ìˆì—ˆë‹¤.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ROLANDëŠ” dynamic graphì˜ snapshot-based representationì„ ë°”íƒ•ìœ¼ë¡œ ìœ„ì˜ limitationë“¤ì„ íƒ€ê°œí•˜ì—¬ static GNNì˜ state-of-the-art ì•„í‚¤í…ì³ë“¤ì„ í™œìš©í•  ìˆ˜ ìˆë„ë¡ ë§Œë“¤ì–´ì¡Œë‹¤.&lt;/p&gt;

&lt;!-- &lt;p align=&quot;center&quot;&gt;
  &lt;img src=&quot;[http://some_place.com/image.png](https://i.ibb.co/RbLYk5J/2023-10-15-03-18-58.png)&quot; /&gt;
&lt;/p&gt; --&gt;

&lt;p&gt;&lt;img src=&quot;https://i.ibb.co/RbLYk5J/2023-10-15-03-18-58.png&quot; alt=&quot;img_fig1&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;tackling-the-limitations&quot;&gt;Tackling the Limitations&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;span style=&quot;color:red&quot;&gt; Model Design &lt;/span&gt;
    &lt;ul&gt;
      &lt;li&gt;Static GNNì—ì„œ ë‹¤ë¥¸ GNN layerì— ìˆëŠ” node embeddingë“¤ì„ &lt;em&gt;hierarchical node state&lt;/em&gt;ë“¤ë¡œ ë³´ëŠ” ìƒˆë¡œìš´ ê´€ì ì„ ì œì‹œí•˜ì˜€ë‹¤.&lt;/li&gt;
      &lt;li&gt;Static GNNì„ dynamicí•˜ê²Œ generalizeí•˜ê¸° ìœ„í•´ì„œëŠ” &lt;em&gt;hierarchical node state&lt;/em&gt;ë“¤ì„ ìƒˆë¡­ê²Œ ê´€ì°°ë˜ëŠ” nodeì™€ edgeë“¤ì„ ì–´ë–»ê²Œ ì´ìš©í•˜ì—¬ updateí•  ì§€ ì •í•´ì•¼ í•œë‹¤.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;span style=&quot;color:green&quot;&gt; Evaluation &lt;/span&gt;
    &lt;ul&gt;
      &lt;li&gt;Dynamic graphì—ì„œ live-updateë¥¼ í•  ìˆ˜ ìˆëŠ” evaluation settingì„ ì œì‹œí•˜ì˜€ë‹¤.
        &lt;ul&gt;
          &lt;li&gt;ì¼ë³„ì´ë‚˜ ì£¼ë³„ë¡œ batchë¥¼ ë§Œë“¤ì–´ evaluationì„ ì§„í–‰í•˜ê³  updateë¥¼ í•  ìˆ˜ ìˆë„ë¡ í•˜ì˜€ë‹¤.&lt;/li&gt;
          &lt;li&gt;Data distributionì´ timestep $t$ì— ë”°ë¼ ë³€í™”í•˜ëŠ” dynamic graphì˜ íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ model updateë¥¼ í•  ìˆ˜ ìˆë‹¤.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;span style=&quot;color:blue&quot;&gt; Training &lt;/span&gt;
    &lt;ul&gt;
      &lt;li&gt;ì‹œê°„ì— ë”°ë¥¸ ëª¨ë“  graphë¥¼ ì €ì¥í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ training í•  ë•Œ ìƒˆë¡œìš´ graph snapshotê³¼ ê³¼ê±°ì˜ node state ì •ë³´ë“¤ë§Œ GPU ë©”ëª¨ë¦¬ì— ì €ì¥í•˜ì˜€ë‹¤.
        &lt;ul&gt;
          &lt;li&gt;ì´ë¡œ ì¸í•´ 5600ë§Œê°œì˜ edgeë“¤ì„ ê°€ì§€ëŠ” í° graphë¥¼ train ì‹œí‚¬ ìˆ˜ ìˆì—ˆë‹¤.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Dynamic graphì—ì„œì˜ timestep $t$ê°€ ë‹¤ë¥¸ prediction ë¬¸ì œë“¤ì„ ë‹¤ë¥¸ taskë¡œ ìƒê°í•¨ìœ¼ë¡œì¨ ë¬¸ì œë¥¼ meta-learning problemìœ¼ë¡œ formulationí•˜ì˜€ë‹¤.
        &lt;ul&gt;
          &lt;li&gt;ìœ„ì˜ Figure 1 ì°¸ê³ &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;proposed-roland-framework&quot;&gt;&lt;strong&gt;Proposed ROLAND Framework&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;ì´ì œ ìœ„ì˜ Tackling the Limitationsë¥¼ ì¡°ê¸ˆ ë” ìì„¸íˆ ì‚´í´ë³´ê³ ì í•œë‹¤. í•´ë‹¹ ë…¼ë¬¸ì—ì„œ ë‹¤ë£¨ëŠ” ìì„¸í•œ notationë“¤ì€ &lt;a href=&quot;https://arxiv.org/abs/2208.07239&quot;&gt;paper&lt;/a&gt;ì„ ì§ì ‘ ì°¸ê³ í•˜ë©´ ëœë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;model_design&quot;&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;-model-design-&quot;&gt;&lt;span style=&quot;color:red&quot;&gt; Model Design &lt;/span&gt;&lt;/h4&gt;

&lt;p&gt;ë¨¼ì € ì‘ê°€ë“¤ì€ Figure 2ì— ê¸°ì¡´ì˜ static GNNê³¼ ì´ë¥¼ ì–´ë–»ê²Œ dynamic GNNì— ì‘ìš©í• ì§€ë¥¼ ê·¸ë¦¼ìœ¼ë¡œ ë‚˜íƒ€ë‚´ì—ˆë‹¤. ì•„ë˜ Figure 2-(b)ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, embedding update ë¼ëŠ” ë¸”ë¡ì„ í†µí•´ &lt;em&gt;hierarchical node state&lt;/em&gt; $H$ë¥¼ update ì‹œì¼œì¤Œìœ¼ë¡œì¨ static GNNì„ ì‰½ê²Œ dynamic GNNìœ¼ë¡œ í™•ì¥ì‹œì¼°ë‹¤. ì´ ë°©ë²•ë¡ ëŒ€ë¡œë¼ë©´ skip connection, batch normalization ë“± static GNNì—ì„œ ì‚¬ìš©ë˜ë˜ íš¨ê³¼ì ì¸ í…Œí¬ë‹‰ë“¤ì„ dynamic GNNì—ì„œë„ ê·¸ëŒ€ë¡œ í™œìš©í•  ìˆ˜ ìˆë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.ibb.co/C6cjxnN/2023-10-15-13-39-08.png&quot; alt=&quot;img_fig2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ê²°êµ­ ì´ ëª¨ë¸ì˜ í•µì‹¬ì€ &lt;em&gt;hierarchical node state&lt;/em&gt; $H$ë¥¼ ì–´ë–»ê²Œ updateí•  ì§€ì¸ë°, ì´ëŠ” algorithm 1ì— ìì„¸íˆ ì†Œê°œë˜ì–´ ìˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.ibb.co/t4VbsPV/2023-10-15-13-40-33.png&quot; alt=&quot;img_algo1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ì´ ë…¼ë¬¸ì—ì„œ $GNN^ {(l)}$ ë‚´ë¶€ì— message embeddingì˜ ì •ë³´ë¥¼ í•©ì¹˜ëŠ” $AGG$ í•¨ìˆ˜ë¡œ sum, max, meanì„ ì œì•ˆí•˜ì˜€ê³ , node embeddingì„ updateí•˜ëŠ” $UPDATE^ {(l)}$ í•¨ìˆ˜ë¡œ ê¸°ì¡´ì— ì‚¬ìš©ë˜ë˜ ê°„ë‹¨í•˜ì§€ë§Œ íš¨ê³¼ì ì¸ Moving Average, MLP, GRUë¥¼ ì œì•ˆí•˜ì˜€ë‹¤. ì´ëŠ” ê¸°ì¡´ static GNNì—ì„œ ì‚¬ìš©í•˜ë˜ í…Œí¬ë‹‰ì„ ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜¨ ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.
ëª¨ë“  updateê°€ ëë‚˜ë©´ node $u$ì—ì„œ $v$ë¡œ edgeê°€ ìƒê¸¸ í™•ë¥ ì„ $y_ {t}$ë¥¼ MLP prediction head ë¸”ë¡ì„ í†µí•´ ì˜ˆì¸¡í•œë‹¤. ì—¬ê¸°ì„œ ì˜ˆì¸¡í•œ ê°’ì€ ë‹¤ìŒì— ì„¤ëª…ë  live-update evaluationì— ì‚¬ìš©ë˜ì–´ ëª¨ë¸ì„ updateí•˜ëŠ” ê²ƒì— ì‚¬ìš©ëœë‹¤.&lt;/p&gt;

&lt;h4 id=&quot;-evaluation-&quot;&gt;&lt;span style=&quot;color:green&quot;&gt; Evaluation &lt;/span&gt;&lt;/h4&gt;

&lt;p&gt;ê¸°ì¡´ GNN ì—°êµ¬ë“¤ì—ì„œëŠ” í†µìƒì ìœ¼ë¡œ train, test, validation datasetì„ ë‹¨ìˆœíˆ timestep ì•ì—ì„œë¶€í„° 6:2:2ë¡œ ë‚˜ëˆ„ëŠ” ë°©ì‹ì„ ì‚¬ìš©í•´ì™”ë‹¤. í•˜ì§€ë§Œ, ì´ëŠ” ì•ì„œ ì–¸ê¸‰í–ˆë“¯ì´ ë³€í™”í•˜ëŠ” data distributionì„ ê³ ë ¤í•˜ì§€ ëª»í•˜ê²Œ ë˜ëŠ” ì¹˜ëª…ì ì¸ ë‹¨ì ì´ ìˆë‹¤. ì´ paperì—ì„œëŠ” í•´ë‹¹ ë¬¸ì œë¥¼ live-update evaluationì„ ì œì•ˆí•¨ìœ¼ë¡œì¨ í•´ê²°í•œë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.ibb.co/c8B4z0C/2023-10-15-15-03-03.png&quot; alt=&quot;img_fig3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ìœ„ Figure 3ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, ì´ì „ ì‹œì ì˜ node embeddingê³¼ í˜„ì¬ ì‹œì ì˜ ìƒˆë¡œìš´ graph snapshotì„ inputìœ¼ë¡œ GNNì— ë„£ì–´ prediction $\hat{y}_ {t}$ë¥¼ ì–»ì€ í›„, ì‹¤ì œ  $y_ {t}$ì™€ ë¹„êµí•˜ì—¬ mean reciprocal rank (MRR)ì„ í†µí•´ í‰ê°€í•˜ê²Œ ëœë‹¤. ìì„¸í•œ training ê³¼ì •ì€ ì•„ë˜ algorithm 2ì— ì†Œê°œë˜ì–´ ìˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.ibb.co/jrSF50V/2023-10-15-15-02-46.png&quot; alt=&quot;img_algo2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Updateì— ì•ì„œì„œ ë¨¼ì € live-updateë¥¼ ìœ„í•œ link predicion labelë“¤ì„ ê° timestepì— ëª¨ì•„ì¤€ í›„, training setê³¼ validation setìœ¼ë¡œ ë‚˜ëˆ„ì–´ì¤€ë‹¤. Training set $y^ {(train)}$ì€ GNNì„ fine-tuningí•˜ëŠ” ê²ƒì— ì‚¬ìš©ë˜ê³ , validation set $y^ {(val)}$ì€ early stopping criterionìœ¼ë¡œì¨ í™œìš©ëœë‹¤ (Algorithm 2ì˜ 3, 5 ,6ë²ˆì§¸ ì¤„). ì´ëŠ” $y^ {(val)}$ìœ¼ë¡œ ê³„ì‚°ëœ $MRR^ {(val)}$ì´ ì¦ê°€í•˜ëŠ” ê²ƒì´ ë©ˆì¶œ ë•Œê¹Œì§€ ë°˜ë³µëœë‹¤ (Algorithm 2ì˜ 4, 7ë²ˆì§¸ ì¤„).
$MRR$ì€ propose ëœ ê²°ê³¼ë“¤ ì¤‘ ì‹¤ì œ ê°’ì´ ëª‡ ë²ˆì§¸ rankì— ìˆëŠ”ì§€ì— ê´€ë ¨ëœ metricì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë‹¨ìˆ˜í˜• ë‹¨ì–´ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ë³µìˆ˜í˜• ë‹¨ì–´ë¥¼ ë§ì¶°ì•¼í•˜ëŠ” taskê°€ ìˆë‹¤ê³  ìƒê°í•´ë³´ì. ì´ ë•Œ, appleì„ inputìœ¼ë¡œ ì£¼ê³  ëª¨ë¸ì´ appl, apples, applet ë¥¼ ë±‰ì—ˆë‹¤ë©´ RankëŠ” $2$, Reciprocal Rank ($RR$)ëŠ” ê·¸ì˜ ì—­ìˆ˜ì¸ $\frac{1}{2}$ë¡œ ì •ì˜ëœë‹¤. $MRR$ì€ ëª¨ë“  $RR$ì˜ í‰ê· ìœ¼ë¡œ input queryë¥¼ $Q$ë¼ í•  ë•Œ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤.&lt;/p&gt;

&lt;p&gt;$ MRR=\frac{1}{\vert Q \vert} \sum_ {i=1}^ {\vert Q \vert} \frac{1}{rank_ {i}} $&lt;/p&gt;

&lt;p&gt;Trainingì´ ëª¨ë‘ ëë‚˜ë©´ ê° timestep $t$ì— ê³„ì‚°ëœ $MRR_ {t}$ì˜ í‰ê· ì„ performance metricìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í‰ê°€í•œë‹¤.&lt;/p&gt;

&lt;h4 id=&quot;-training-&quot;&gt;&lt;span style=&quot;color:blue&quot;&gt; Training &lt;/span&gt;&lt;/h4&gt;

&lt;p&gt;ìœ„ Figure 3ì— ì œì‹œëœ ROLANDì˜ ì•„í‚¤í…ì²˜ë¥¼ ì‚´í´ë³´ë©´, ê° ì‹œì ì—ì„œ GNNì„ updateí•˜ëŠ” ê²ƒì— í•„ìš”í•œ inputì€ ì´ì „ ì‹œì  node embedding $H_ {t-1}$ê³¼ ìƒˆë¡­ê²Œ ë“¤ì–´ì˜¤ëŠ” graph snapshot $G_ {t}$ì„ì„ ì•Œ ìˆ˜ ìˆë‹¤. ë•Œë¬¸ì—, ROLANDì˜ memory complexityëŠ” graph snapshotì˜ ê°œìˆ˜ì— agnosticí•˜ê³  scalableí•¨ì„ ì•Œ ìˆ˜ ìˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.ibb.co/42gtLdr/2023-10-15-16-19-14.png&quot; alt=&quot;img_algo3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ROLANDì—ì„œ ì œì•ˆëœ ë˜ ë‹¤ë¥¸ ë°©ë²•ë¡  í•œ ê°€ì§€ëŠ” dynamic graphì—ì„œì˜ prediction taskë¥¼ meta-learning ë¬¸ì œë¡œ formulationí•˜ëŠ” ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, daily graph snapshotì´ inputì´ë¼ê³  ìƒê°í–ˆì„ ë•Œ, ê¸ˆìš”ì¼ì˜ ëª¨ë¸ê³¼ í† ìš”ì¼ì˜ ëª¨ë¸ì€ í¬ê²Œ ë‹¤ë¥¼ ê²ƒì´ë‹¤. ë•Œë¬¸ì— ê¸ˆìš”ì¼ ëª¨ë¸ì„ ë‹¨ìˆœíˆ fine-tuning í›„ í† ìš”ì¼ ëª¨ë¸ì´ë¼ê³  í•˜ëŠ” ê²ƒì€ ëª¨ë¸ ì„±ëŠ¥ì— ì¢‹ì§€ ì•Šì€ ì˜í–¥ì„ ë¼ì¹  ìˆ˜ ìˆë‹¤. ì´ì— ëŒ€í•´ ì‘ê°€ë“¤ì€ meta-model $GNN^ {(meta)}$ë¥¼ ì°¾ëŠ” Algorithm 3ì„ ìœ„ì™€ ê°™ì´ ì œì‹œí•˜ì˜€ë‹¤.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;experiments&quot;&gt;&lt;strong&gt;Experiments&lt;/strong&gt;&lt;/h2&gt;

&lt;h4 id=&quot;experiment-setup&quot;&gt;&lt;strong&gt;Experiment setup&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Datasets&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;ì•„ë˜ í‘œëŠ” ì´ paperì—ì„œ ì‚¬ìš©ëœ datasetì˜ ì¢…ë¥˜ì™€ ê° datasetì˜ íŠ¹ì„±ì„ ì •ë¦¬í•´ ë‘” í‘œì´ë‹¤.&lt;/li&gt;
      &lt;li&gt;&lt;img src=&quot;https://i.ibb.co/3hZv9MX/2023-10-15-17-18-24.png&quot; alt=&quot;img_table1&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Baselines&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;EvolveGCN-H&lt;/li&gt;
      &lt;li&gt;EvolveGCN-O&lt;/li&gt;
      &lt;li&gt;T-GCN&lt;/li&gt;
      &lt;li&gt;GCRN-GRU&lt;/li&gt;
      &lt;li&gt;GCRN-LSTM&lt;/li&gt;
      &lt;li&gt;GCRN-Baseline&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Evaluation Metric&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;Mean Reciprocal Rank (MRR)
        &lt;ul&gt;
          &lt;li&gt;$ MRR=\frac{1}{\vert Q \vert} \sum_ {i=1}^ {\vert Q \vert} \frac{1}{rank_ {i}} $&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;results&quot;&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;ì‹¤í—˜ì€ í¬ê²Œ ê¸°ì¡´ dataset splitting ë°©ë²•ê³¼ ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ live-updateë¥¼ ì ìš©í•œ ë°©ë²• ë‘ ê°€ì§€ë¡œ ì§„í–‰ë˜ì—ˆë‹¤. ë¨¼ì € ì•„ë˜ Table 2ëŠ” ê¸°ì¡´ì˜ dataset splitting ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ datasetì„ ë‚˜ëˆ´ì„ ë•Œ ê° baselineë“¤ê³¼ ROLANDì˜ ê²°ê³¼ì´ë‹¤. ROALNDëŠ” ì•ì— &lt;a href=&quot;#model_design&quot;&gt;Model Design&lt;/a&gt;ì—ì„œ ì œì‹œí•œ ê²ƒê³¼ ê°™ì´ $UPDATE$ í•¨ìˆ˜ë¥¼ Moving Average, MLP, GRU 3ê°€ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²°ê³¼ë¥¼ ë¹„êµí–ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.ibb.co/8b3P8q6/2023-10-15-17-19-54.png&quot; alt=&quot;img_table2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ëª¨ë“  datasetì—ì„œ ROLAND GRUê°€ ê°€ì¥ ì¢‹ì€ baseline ëŒ€ë¹„ $MRR$ì´ ì ê²ŒëŠ” 43.33%ì—ì„œ ë§ê²ŒëŠ” 73.74% í–¥ìƒëœ ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì—ˆë‹¤.
Table 3ì—ëŠ” ì´ paperì—ì„œ ì œì‹œí•œ live-update ë°©ë²•ì„ í™œìš©í•˜ì—¬ train ì‹œì¼°ì„ ë•Œì˜ ê²°ê³¼ë¥¼ ê¸°ì¬í•˜ì˜€ë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ, í•œê°€ì§€ datasetì„ ì œì™¸í•œ ëª¨ë“  datasetì—ì„œ baselineë“¤ ëŒ€ë¹„ í° $MRR$ í–¥ìƒì„ ë³´ì—¬ì£¼ì—ˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.ibb.co/nPTtyWx/2023-10-15-17-53-45.png&quot; alt=&quot;img_table3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ROLANDì—ì„œ ì œì‹œí•œ meta-model $GNN^ {(meta)}$ë¥¼ ì‚¬ìš©í•˜ë©´ í‰ê·  performanceê°€ ì ê²ŒëŠ” 2.84%ì—ì„œ ë§ê²ŒëŠ” 13.19% ì¦ê°€í•˜ëŠ” ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì—ˆë‹¤. ë”í•˜ì—¬, $GNN^ {(meta)}$ë¥¼ ì‚¬ìš©í–ˆì„ ë•Œ $MRR$ì˜ standard deviationì´ ì¤„ì–´ë“¦ìœ¼ë¡œì¨ stabilityê°€ ë†’ì•„ì§ì„ ë³´ì˜€ë‹¤.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;conclusion&quot;&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;ì´ ë…¼ë¬¸ì€ ê¸°ì¡´ static GNNì—ì„œ ì‚¬ìš©í•˜ë˜ í…Œí¬ë‹‰ê³¼ ì•„í‚¤í…ì²˜ë“¤ì„ dynamic graphì— ì–´ë–»ê²Œ ì ìš©í•  ìˆ˜ ìˆì„ì§€ ì œì•ˆí•˜ì˜€ë‹¤. Paperì€ í¬ê²Œ model design, evaluation, training 3ê°€ì§€ ë¶€ë¶„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ê¸°ì¡´ ì—°êµ¬ë“¤ì˜ í•œê³„ì ê³¼ ROLANDì˜ contributionì„ ì •ë¦¬í•˜ì˜€ë‹¤.
ìƒˆë¡œìš´ dynamic GNN ëª¨ë¸ì„ scratchë¶€í„° ë§Œë“¤ì§€ ì•Šê³  static GNNê³¼ dynamic GNNì„ ì—°ê²°í•˜ëŠ” ë‹¤ë¦¬ë¥¼ ì œì•ˆí–ˆë‹¤ëŠ” ì , ê·¸ë¦¬ê³  live-updateë¥¼ í†µí•´ scalability issueë¥¼ í•´ê²°í–ˆë‹¤ëŠ” ì ì—ì„œ ì´ ë…¼ë¬¸ì˜ contributionì´ í° ê²ƒ ê°™ë‹¤.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;author-information&quot;&gt;&lt;strong&gt;Author Information&lt;/strong&gt;&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;Author name: Haeun Jeon&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;Contact: haeun39@kaist.ac.kr&lt;/li&gt;
    &lt;li&gt;Affiliation: Financial Engineering Lab., KAIST&lt;/li&gt;
    &lt;li&gt;Research Topic: Stochastic Optimization, End-to-end learning, Portfolio Optimization&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;!-- ## **6. Reference &amp; Additional materials**  

Please write the reference. If paper provides the public code or other materials, refer them.  

&gt; [Github Implementation](https://github.com/snap-stanford/roland)
* Reference   --&gt;
</description>
            <pubDate>Mon, 16 Oct 2023 00:00:00 +0900</pubDate>
            <link>http://dsailatkaist.github.io/2023-10-16-ROLAND_Graph_Learning_Framework_for_Dynamic_Graphs.html</link>
            <guid isPermaLink="true">http://dsailatkaist.github.io/2023-10-16-ROLAND_Graph_Learning_Framework_for_Dynamic_Graphs.html</guid>
            
            <category>reviews</category>
            
            
        </item>
        
        <item>
            <title>[TSRML 2022] Private Data Leakage via Exploiting Access Patterns of Sparse Features in Deep Learning-based Recommendation Systems</title>
            <description>&lt;h1 id=&quot;private-data-leakage-via-exploiting-access-patterns-of-sparse-features-in-deep-learning-recommendation-systems&quot;&gt;Private Data Leakage via Exploiting Access Patterns of Sparse Features in Deep Learning Recommendation Systems&lt;/h1&gt;
&lt;h5 id=&quot;2022-trustworthy-and-socially-responsible-machine-learning-tsrml-2022-co-located-with-neurips-2022&quot;&gt;[2022 Trustworthy and socially responsible Machine learning (TSRML 2022) co-located with NeurIPS 2022]&lt;/h5&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;Sparse and dense features are used in the deep learning-based recommendation models to carry userâ€™s private information and this private data is often protected by the service providers using methods like memory encryption or hashing. In this paper, it is shown that irrespective of the protection used, the attacker would still be able to &lt;em&gt;learn information about which entry of the sparse feature is non-zero through the concept of embedding table access pattern&lt;/em&gt; posing a big threat to the security of customerâ€™s sensitive data.&lt;/p&gt;

&lt;h2 id=&quot;problem-definition&quot;&gt;Problem Definition&lt;/h2&gt;

&lt;p&gt;Deep learning-based recommendation system models exploit different types of information related to the user including user attributes, user preferences, user behavior, social interaction and other contextual information to help the customers with better recommendations and companies with increased revenues. Now, there are two types of features as inputs to a deep neural network to make predictions of items a user may like, namely sparse and dense features.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;The sparse sparce features contain only a few non-zero features whereas the dense features contain a large number of non-zero attributes.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;These features store the information of a user as well the items in different forms. Sparse features are the discrete and categorical attributes associated with users and items whereas the dense features are the continuous and numerical ones. These features contain information which is personal to the user and is protected by the service provider with the help of memory encryption with hardware such as Intelâ€™s SGX. However, we will look into some of the attacks an attacker may proceed with, with the purpose of stealing userâ€™s personal information where methods like encryption or hash functions may not be useful and information like &lt;em&gt;which entries of the sparse features may be non-zero&lt;/em&gt; can be leaked.
This is because sparse features have to be projected into the lower dimensional space through an embedding table where the index of the non-zero entries are used as an index for an embedding table lookup. It is shown in this paper, how this leakage could be enough threat to the sensitive information of the users.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Embedding table is a data structure used to represent and store embeddings of these sparse features, and access patterns means to study how the users interact with the items. So, embedding table access patterns means accessing the patterns of the sparse features embedded into an embedding table.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;It is demonstrated in the paper how it is possible to identify or extract sensitive information of a user with the help of embedding table access patterns under 4 different types of attacks:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Identification Attack&lt;/strong&gt;: identifying a user by with the help of combinations of unidentifiable features&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Sensitive Attribute Attack&lt;/strong&gt;: identifying a user by analyzing the user-item interaction behavior&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Re-Identification Attack&lt;/strong&gt;: identifying a user by tracking the same user by analyzing their interaction history.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Hash Inversion with Frequency-based Attack&lt;/strong&gt;: showing how hashing the sensitive information may not be able to protect it against the attacks, by demonstrating a hash inversion attack based on access frequency.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;working-of-the-dlrm-model-and-threat-model&quot;&gt;Working of the DLRM Model and Threat Model&lt;/h2&gt;

&lt;p&gt;The figure above shows the operation of the representative recommendation model, DLRM. In DLRM, the dense features go through a bottom MLP (multi-layer perceptron) layer whereas the sparse features go through an embedding table layer and get converted into lower-dimensional dense features. Then, these two outputs go through a feature interaction layer and then through a top MLP layer to predict the likelihood of an interaction.
Embedding tables play a pivotal role in transforming the sparse categorical feature into a dense numerical representation. Letâ€™s consider an example to understand this. Consider a scenario where users and movies are represented by categorical features such as user IDs and movie genres, respectively. To effectively utilize these features within a deep learning model, embedding tables are employed. These tables act as large lookup tables, with each row corresponding to a unique category or ID. To convert sparse features into dense representations, a lookup operation is performed using the non-zero entries in the sparse feature as an index. For instance, to convert a specific userâ€™s ID into a dense representation, the corresponding row in the user embedding table is accessed, and similarly, for movie genres, the relevant row in the genre embedding table is retrieved. The outcome of these operations is a dense vector, a numerical representation of the user or genre in a multi-dimensional space. These dense representations are subsequently utilized in the recommendation systemâ€™s deep learning model, enabling accurate and personalized movie recommendations based on user preferences and movie genres. This process highlights the critical role of embedding tables.&lt;/p&gt;
&lt;h4 id=&quot;threat-model&quot;&gt;Threat Model&lt;/h4&gt;
&lt;p&gt;Now, even when the entire dense and sparse features are fully encrypted and are processed under a secure environment, there is a possibility to learn which index holds a non-zero entry by looking at the table access patterns, resulting in compromising with the sensitive user data.
To understand the threat model, letâ€™s assume the scenario. Letâ€™s say a user shared their sensitive information with the service provider to get accurate recommendations from the system. Now, this sensitive information is fully protected with the Intel SGX team, but the access pattern of the embedding table is revealed, more specifically revealing which entries of the table are non-zero. The figure below, demonstrates our threat model.
Like this, even when the information is kept safe with the honest-but-curious service provider, the access pattern of the embedding table can help reveal that sensitive information.&lt;/p&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;As also mentioned earlier, we will test out our theory of being able to extract sensitive information with the help of embedding table access patterns, with the help of four different types of attacks.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Attack&lt;/th&gt;
      &lt;th&gt;Goal&lt;/th&gt;
      &lt;th&gt;Assumptions&lt;/th&gt;
      &lt;th&gt;Evaluation Metric&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Identification&lt;/td&gt;
      &lt;td&gt;Finding the identity of the users&lt;/td&gt;
      &lt;td&gt;Attacker observes accesses, Has prior knowledge about distribution of accesses&lt;/td&gt;
      &lt;td&gt;K-anonymity&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Sensitive Attribute&lt;/td&gt;
      &lt;td&gt;Extracting the sensitive user attributes&lt;/td&gt;
      &lt;td&gt;Attacker observes accesses, Has prior knowledge about distribution of accesses&lt;/td&gt;
      &lt;td&gt;Ambiguity&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Re-Identification&lt;/td&gt;
      &lt;td&gt;Tracking users over time based on interaction history&lt;/td&gt;
      &lt;td&gt;Attacker observes accesses&lt;/td&gt;
      &lt;td&gt;Precision and Recall&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Hash Inversion with Frequency-based Attack&lt;/td&gt;
      &lt;td&gt;Finding users raw feature values&lt;/td&gt;
      &lt;td&gt;Attacker observes accesses, Has prior knowledge about distribution of accesses&lt;/td&gt;
      &lt;td&gt;Inversion Accuracy&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;This table shows a summary of all the attacks, their goals and basic assumptions which have been used to prove the point of the sensitive information not being safe. Each of these attacks are discussed in detail.&lt;/p&gt;

&lt;h2 id=&quot;identification-attack-with-static-user-feature&quot;&gt;Identification Attack with Static User Feature&lt;/h2&gt;

&lt;p&gt;User profile attributes, such as gender, city, etc. are usually static in nature i.e., they donâ€™t change with time. or the frequency of change is very low. We can categorize such features into two parts - identifiable and unidentifiable features.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;ul&gt;
    &lt;li&gt;Identifiable features are the features that are capable of directly or indirectly revealing the identity if the user. For example, name, city of residence, userID, etc.&lt;/li&gt;
    &lt;li&gt;Unidentifiable features are the features that canâ€™t directly expose the identity of the user but can still provide valuable information. For example, gender, education level, search keywords, etc.&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;Now due to strict regulations, most of the recommendation systems donâ€™t usually collect and use the identifiable features. So, the question arises that are the unidentifiable features enough to make an accurate assumption of who the user might be?&lt;/p&gt;

&lt;h4 id=&quot;evaluation-setup&quot;&gt;Evaluation setup&lt;/h4&gt;
&lt;p&gt;To find out whether the unidentifiable features are enough to find out about the user, an open-source dataset by Alibaba has been used, containing static user features, such as userID, groupID, gender, age group, shopping depth, occupation, city level, etc. of around 11.4M users.&lt;/p&gt;

&lt;h4 id=&quot;attack-method&quot;&gt;Attack Method&lt;/h4&gt;
&lt;p&gt;After removing all the identifiable features from the dataset, we will be left with 2.1M possible combinations of the unidentifiable features, which would make any user believe that their identity is anonymous or that revealing any of the remaining unidentifiable features, wonâ€™t reveal their identity. However, in contrast to the userâ€™s belief, it is observed that in the real world only 1120 combinations of these static feature values are possible based on the real open-source data. We refer to these 1120 combinations as user buckets.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;In simple words, user buckets are unique combinations of the feature values. The motto of the attacker can now be said to be able to recognize the users based on their unique combinations of features.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Taking the user bucket number as the x-axis and the percentage of the users per bucket as the y-axis, we plot the histogram to represent how the user distributions follow the long-tail pattern.
I can be seen from the histogram that there are only a few users in the bucket 600-1120 and in fact there are only 989 users on average across all these buckets and the last 56 buckets have only 1 user. So, these seemingly unidentifiable features may give away the userâ€™s identity by allowing the attacker to launch an identification attack to extract the unique userID and identify the user with a high certainty.&lt;/p&gt;

&lt;h4 id=&quot;evaluation-metric&quot;&gt;Evaluation Metric&lt;/h4&gt;
&lt;p&gt;The evaluation metric we have used for this analysis is the &lt;em&gt;K-anonymity&lt;/em&gt;.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;K-anonymity is a privacy property that measure how well the user privacy is preserved.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If a userâ€™s bucket number is revealed and there are K users in the same bucket then the probability of finding the user is 1/K.
The table given below summarizes the number of users with anonymity level below K in the identification attacks.
1-anonymity user means that this is the only user having this particular set of feature values.&lt;/p&gt;

&lt;h4 id=&quot;evaluation-results&quot;&gt;Evaluation Results&lt;/h4&gt;
&lt;p&gt;As shown in the above table, among 56 of the bucket users, there is only 1 user with the specific combination of static features which implies that an attacker can identify these users with 1-anonymity if they can observe this combination of feature values.&lt;/p&gt;

&lt;h2 id=&quot;sensitive-attribute-attack-by-dynamic-user-features&quot;&gt;Sensitive Attribute attack by Dynamic User Features&lt;/h2&gt;

&lt;p&gt;In this type of attack, we would see that even when the userâ€™s hide, how the sensitive attributes such as age, gender, interest, etc. can be inferred by analyzing their user-item interaction behavior and how these sensitive features leak through other non-sensitive features through the concept of cross-correlations.&lt;/p&gt;

&lt;h4 id=&quot;evaluation-setup-1&quot;&gt;Evaluation setup&lt;/h4&gt;
&lt;p&gt;For the purpose of evaluation, we have used the Alibaba Ads Display dataset, which contains user-item interactions. This dataset contains around 723,268,134 tuples and each tuple contains information about the userID, categoryID, brand and btag (browse, cart, favour, buy)&lt;/p&gt;

&lt;h4 id=&quot;attack-method-1&quot;&gt;Attack Method&lt;/h4&gt;
&lt;p&gt;Letâ€™s understand the attack method with the help of an example, say in the data, there are 7 age groups and 5 different brands. The user-item interaction based on the age-group and the brand is given below in the connection graph.
Now, from the above graph a basic idea of the people belonging to a particular age group can be made. The user may not want to reveal their age, but the adversary may deduce their age with a high probability based on the type of brand the user has interacted with.
In general, we can say that the attacker uses their prior knowledge o popularity of the items between different demographic groups. Then, based on this prior information, they link the query to the demographic who formed most of the accesses to that item. The task of knowing the prior information is not that big of a deal.&lt;/p&gt;

&lt;h4 id=&quot;evaluation-metric-1&quot;&gt;Evaluation Metric&lt;/h4&gt;
&lt;p&gt;The evaluation metric we have used for this analysis is called &lt;em&gt;ambiguity&lt;/em&gt;. It helps in determining the likelihood with which an adversary fails to predict a userâ€™s static sparse feature by just viewing their interactions with items. The ambiguity for each item is defined as follows:
&lt;em&gt;ambiguity(i) = 100% - max(frequency(i))&lt;/em&gt;
where, frequency = distribution vector of all accesses to brand i by different user groups.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;An ambiguity(i) = 0 indicates if a user has interacted with item i, the attacker can successfully determine the userâ€™s sparse features.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;evaluation-results-1&quot;&gt;Evaluation Results&lt;/h4&gt;
&lt;p&gt;In the graphs shown below, the x-axis shows the percentage of ambiguity where a value of 0 indicates that there is no ambiguity and this brand is always accessed by only one user bucket, whereas a higher ambiguity value depicts that brands are more popular across multiple user buckets.
In figure 5(A), it ca be seen that the more than 17% of brands are only accessed by 1 user bucket represented by the leftmost tall bar of PDF, meaning that the attacker can determine the user bucket using those brand interactions. On the other hand, in the CDF curve, for 38% of the brands, the attacker can predict the user bucket with a success rate of greater than 50%.
Similarly, the age and gender group versus the ambiguity are shown by graph 5(B) and 5(&lt;em&gt;C&lt;/em&gt;) respectively.&lt;/p&gt;

&lt;h2 id=&quot;re-identification-attack&quot;&gt;Re-Identification Attack&lt;/h2&gt;

&lt;p&gt;In the re-identification attack, the attacker focuses on tracking the same user over time by just analyzing their interaction history.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Re-identification attack is different from the identity resolution attack, as in the identity resolution attack the aim is to link the users across different system, potentially involving cross-referencing userâ€™s information.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Under this attack, we study two important things:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;if the history of the purchases of a user can be used as a tracking identifier for the user.&lt;/li&gt;
  &lt;li&gt;if an attacker can re-identify the same user who sent queries over time by only tracking the history of their purchases, with no access to the static sparse features.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;evaluation-setup-2&quot;&gt;Evaluation setup&lt;/h4&gt;
&lt;p&gt;For the evaluation, we have used the Taobao dataset, and have separated about 9M purchase interactions among more than 723M user-item interactions. Then they have formatted the data into a time-series data structure, as shown below:
&lt;em&gt;user1 = (time1,item1), (time4,item10), (time500,item20)&lt;/em&gt;
&lt;em&gt;user2 = (time3,item100), (time20,item100)&lt;/em&gt; 
.
.
.
.
&lt;em&gt;user_X = (time5,item75), (time20,item50), (time100,item75), (time400,item1), (time420,item10)&lt;/em&gt;
Now for each set of consecutive items purchased by an user, we create a list of users who have the same set of consecutive purchases in exactly that order. We refer to these sets of consecutive recent purchases as &lt;em&gt;keys&lt;/em&gt;.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Multiple users may have the same key.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The goal of this attack is to use &lt;em&gt;m&lt;/em&gt; most recent purchases mad by a user to track them across different interactions sessions. To evaluation setup of this attack can be carried out as follows:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;randomly select a timestamp and a user&lt;/li&gt;
  &lt;li&gt;for that selected user, we check &lt;em&gt;m&lt;/em&gt; most recent purchases at that selected timestamp and form a key&lt;/li&gt;
  &lt;li&gt;we then look up this key in the recent item purchase history dataset
    &lt;ul&gt;
      &lt;li&gt;if the same sequence of m most recent items appear on another user at the same timestamp, this means those recent purchases are not unique or that specific user at that time and hence, doesnâ€™t represent a single user.&lt;/li&gt;
      &lt;li&gt;if the m items purchase history only belongs to that specific user, the duration of the time in which this key forms the most recent purchases of the user is extracted.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;the same is repeated for many random time stamps and users to obtain 200,000 samples.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By plotting the data, we may notice that the 3, 4 and 5 most recent purchases uniquely identify users with 99% probability.&lt;/p&gt;

&lt;h4 id=&quot;attack-method-2&quot;&gt;Attack Method&lt;/h4&gt;
&lt;p&gt;For the period of time the recent purchases remain the same, every query sent by the user has the same list of recent purchases, i.e., most recent items purchased by a user usually do not change with a very high frequency. The attacker uses this knowledge to launch the attack. So, the attacker first selects a time threshold. This time threshold is chosen to help the attacker to decide if the queries come from the same&lt;/p&gt;

</description>
            <pubDate>Mon, 16 Oct 2023 00:00:00 +0900</pubDate>
            <link>http://dsailatkaist.github.io/2023-10-16-Private_Data_Leakage_via_Exploiting_Access_Patterns_of_Sparse_Features_in_Deep_Learning-based_Recommendation_Systems.html</link>
            <guid isPermaLink="true">http://dsailatkaist.github.io/2023-10-16-Private_Data_Leakage_via_Exploiting_Access_Patterns_of_Sparse_Features_in_Deep_Learning-based_Recommendation_Systems.html</guid>
            
            <category>reviews</category>
            
            
        </item>
        
    </channel>
</rss>
