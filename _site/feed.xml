<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>DSAILatKAIST.github.io</title>
        <description>Intended as a documentation theme based on Jekyll for technical writers documenting software and other technical products, this theme has all the elements you would need to handle multiple products with both multi-level sidebar navigation, tags, and other documentation features.</description>
        <link>http://localhost:4000/</link>
        <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
        <pubDate>Thu, 25 Apr 2024 13:46:03 +0900</pubDate>
        <lastBuildDate>Thu, 25 Apr 2024 13:46:03 +0900</lastBuildDate>
        <generator>Jekyll v3.9.2</generator>
        
        <item>
            <title>[NeurIPS 2023] Winner Takes It All: Training Performant RL Populations for Combinatorial Optimization</title>
            <description>&lt;h2 id=&quot;1-problem-definition&quot;&gt;1. Problem Definition&lt;/h2&gt;
&lt;p&gt;최근 수 년간 조합최적화 문제를 강화학습을 활용하여 해결하려는 시도가 늘어나고 있다. 전통적인 조합최적화 문제 해결 방법인 dispatching rule은 빠른 시간 내에 해를 도출할 수 있지만 instance-specific하며 성능이 좋지 않다는 단점이 있다. 강화학습을 활용한 방법론은 dispatching rule의 장점인 짧은 연산시간을 유지하며 handcrafted dispatching rule보다 복잡한 로직을 통해 성능을 높이는, &lt;strong&gt;고도화된 dispatching rule 설계&lt;/strong&gt;를 목적으로 한다. 본 연구는 이러한 아이디어를 기반으로 &lt;strong&gt;조합최적화 문제 해결을 위한 end-to-end 강화학습 알고리즘 개발&lt;/strong&gt;을 목표로 한다.&lt;/p&gt;

&lt;h2 id=&quot;2-motivation&quot;&gt;2. Motivation&lt;/h2&gt;
&lt;p&gt;기존에는 조합최적화 문제 해결에 최적해를 구할 수 있는 수학적 방법론이나 near-optimal solution을 구할 수 있는 메타휴리스틱이 주로 사용되었다. 하지만 다양한 제약으로 문제의 복잡도가 증가하고 문제의 크기가 커짐에 따라, 긴 연산 시간을 필요로 하는 수학적 방법론 및 메타휴리스틱의 한계가 드러나고 있다. 특히 논문에서 주목한 Traveling Salesman Problem(TSP), Capacitated Vehicle Routing Problem(CVRP), Knapsack Problem(KP), Job Shop Scheduling Problem(JSSP)은 모두 NP-hard로, 전통적인 최적화 방법론 혹은 메타휴리스틱 사용 시 연산 시간이 매우 길어진다.&lt;/p&gt;

&lt;p&gt;빠른 시간 내에 좋은 성능의 해를 얻을 수 있는 &lt;strong&gt;constructive 강화학습 방법론&lt;/strong&gt;의 중요성이 높아지고 있다. 최근 연구들은 이러한 강화학습 dispatcher의 긍정적인 가능성을 확인하였지만, 조합최적화 문제는 복잡도가 높아 강화학습만으로는 좋은 해를 얻기 어렵다. 이 때문에 대부분의 연구에서는 이러한 강화학습 dispatcher에 domain knowledge에 기반한 search 방법론을 결합하여 성능을 높인다.&lt;/p&gt;

&lt;p&gt;Search는 긴 연산 시간을 필요로 하며 해의 다양성을 보장하기 어렵다는 문제점이 있다. 따라서 강화학습 dispatcher를 실제 제조 현장의 스케줄링 문제처럼 복잡한 문제에 적용하기 위해서는 짧은 시간 내에 좋은 해를 도출할 수 있는 end-to-end 강화학습 dispatcher의 성능 향상이 필수적이다. 본 논문에서는 이를 위해 population based learning 알고리즘 &lt;strong&gt;Poppy&lt;/strong&gt;를 제안한다. Poppy는 &lt;strong&gt;population based learning을 통해 탐색 공간을 넓힘으로써 다양한 해를 탐색&lt;/strong&gt;하여, search 결합 없이 &lt;strong&gt;end-to-end 강화학습&lt;/strong&gt;의 성능을 높이고자 했다.&lt;/p&gt;

&lt;h2 id=&quot;3-method&quot;&gt;3. Method&lt;/h2&gt;
&lt;h3 id=&quot;31-movitating-example&quot;&gt;3.1. Movitating Example&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Winner_Takes_It_All_Training_Performant_RL_Populations_for_Combinatorial_Optimization/Motivating%20Example.PNG&quot; alt=&quot;Motivating Example&quot; /&gt;&lt;/p&gt;

&lt;p&gt;좌측 그림과 같이 agent가 하나인 경우, agent는 reward의 기댓값이 가장 높은 위쪽 방향으로 움직인다. 하지만 우측 그림과 같이 agent가 여러 개인 경우, 두 agent는 서로 다른 action을 탐색해 보고 가장 좋은 action을 취할 수 있다. Poppy는 여러 agent가 다양한 action을 탐색해 본 후, reward를 최대화할 수 있는 action을 선택하는 전략을 취한다.&lt;/p&gt;

&lt;h3 id=&quot;32-poppy&quot;&gt;3.2. Poppy&lt;/h3&gt;
&lt;p&gt;학습 단계에서 한 번의 rollout만으로도 다양한 policy를 얻어 학습 효율을 높임으로써 성능을 제고하는 것이 본 논문의 주요 목표다. 따라서 objective는 다음과 같이 정의할 수 있다.
\(J_{pop}(\theta_1, \dots , \theta_K) = \mathbb{E}_{\rho \sim D} \mathbb{E}_{\tau_1 \sim \pi_{\theta_1}, \dots, \tau_K \sim \pi_{\theta_K}} \max \left[R(\tau_1), \dots, R(\tau_K)\right]\)
Poppy에서는 동일한 agent를 K번 sampling하여 사용하기 때문에, $\pi_{\theta_1} = \pi_{\theta_2} = \dots = \pi_{\theta_K}$이다. 이를 정리하여 gradient를 구한 결과는 아래와 같다.
\(\nabla J_{pop}(\theta) = \mathbb{E}_{\rho \sim D} \mathbb{E}_{\tau_1 \sim \pi_{\theta_1}, \dots, \tau_K \sim \pi_{\theta_K}} \left(R(\tau_{i^*}) - R(\tau_{i^{**}})\right) \nabla \log p_\theta (\tau_{i^*})\)
where $ i^* = \arg \max_{i \in {1, \dots, K}}\left[R(\tau_i)\right]$ and $ i^{\star\star} = \arg \max_{i \neq i^*}\left[R(\tau_i)\right]$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Winner_Takes_It_All_Training_Performant_RL_Populations_for_Combinatorial_Optimization/Structure.PNG&quot; alt=&quot;Framework&quot; /&gt;&lt;/p&gt;

&lt;p&gt;전체 framework는 위 그림과 같다. 학습은 크게 1) 단일 agent 학습(좌측 그림), 2) K-agent 학습(우측 그림)의 두 단계로 나뉜다. 단일 agent 학습 단계에서는 일반적인 end-to-end 강화학습과 동일하게 reward를 최대화하도록 학습이 진행된다. 이렇게 학습된 모델 $\theta$가 K개로 clone되어 아래 알고리즘과 같은 학습 과정을 거치는 단계가 K-agent 학습이다. 최종적으로는 K개의 agent를 활용하더라도, 처음 학습 단계에서 단일 agent만을 사용해서 학습함으로써 연산 시간을 줄일 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Winner_Takes_It_All_Training_Performant_RL_Populations_for_Combinatorial_Optimization/Training%20Algorithm.PNG&quot; alt=&quot;Training&quot; /&gt;&lt;/p&gt;

&lt;p&gt;K-agent 학습에서는 1단계에서 학습된 모델을 clone하여 K개의 agent를 생성한다. 이 agent들은 encoder parameter는 공유하되, 개별 decoder를 사용한다. 이 학습의 objective는 K개의 reward의 최댓값을 최대화하는 것이기 때문에, backpropagation 과정에서는 가장 좋은 성능을 보인 agent만을 활용한다. 학습 과정에서는 REINFORCE 알고리즘을 사용하였으며, 이때 baseline으로는 symmetry가 있는 환경에서 효과적임이 검증된 POMO(Kwon et al., 2020)를 사용하였다.&lt;/p&gt;

&lt;h2 id=&quot;4-experiment&quot;&gt;4. Experiment&lt;/h2&gt;
&lt;p&gt;논문에서는 조합최적화 문제인 TSP, CVRP, KP, JSSP를 대상으로 실험을 진행하였다. 실험에 사용된 모델은 encoder는 모든 agent가 공유하고, decoder는 agent-specific한 구조이다. 이 리뷰에서는 스케줄링 문제인 JSSP 실험을 집중적으로 다룬다.&lt;/p&gt;

&lt;p&gt;JSSP는 10개의 job과 10개의 설비 환경에서 makespan 최소화를 목적식으로 하였으며, 1000개의 instance를 생성하여 최적해를 도출하는 OR-Tools, GNN을 활용한 L2D(Zhang et al., 2020)와 비교하였다. 실험 환경 세팅은 아래와 같다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Poppy: JAX-based implementation, v3-8 TPU&lt;/li&gt;
  &lt;li&gt;L2D: Intel Core i9-10940X CPU, single Nvidia GeForce 2080Ti GPU&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Winner_Takes_It_All_Training_Performant_RL_Populations_for_Combinatorial_Optimization/Result1.PNG&quot; alt=&quot;Result1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$10\times10$ Job Shop Scheduling Problem 실험에서 동일 환경의 L2D(Zhang et al., 2020)에 비해서는 좋은 성능을 보였지만, OR-Tools가 37초 만에 optimal solution을 도출한 데 반해 Poppy(K=16)는 30분 동안 optimal gap이 6.2%인 스케줄을 도출했다. 이는 20대 이상의 설비에서 다양한 제약을 함께 고려해야 하는 실제 현장에 적용하기 매우 어려울 만큼 긴 연산 시간이다.&lt;/p&gt;

&lt;h2 id=&quot;5-conclusion&quot;&gt;5. Conclusion&lt;/h2&gt;
&lt;h3 id=&quot;51-summary&quot;&gt;5.1. Summary&lt;/h3&gt;
&lt;p&gt;다양한 action을 탐색해 보는 전략은 sequential decision을 요하는 문제에서 특히 유용하다. 조합최적화 문제를 constructive dispatching 형식으로 접근하면 매 decision point에서 취하는 action이 후속 action에 큰 영향을 미치고, 대부분 sparse reward를 가져 해당 action의 영향을 즉각적으로 파악하기 어렵다. 이러한 문제를 해결하기 위해 조합최적화 문제에서 POMO(Kwon et al., 2020)를 시작으로 symmetry를 활용해 search space를 넓히려는 연구가 지속적으로 진행되고 있고, 본 논문에서는 population based learning을 통해 search space를 넓히는 Poppy를 제안하였다.&lt;/p&gt;

&lt;h3 id=&quot;52-insights-for-scheduling-researches&quot;&gt;5.2. Insights for Scheduling Researches&lt;/h3&gt;
&lt;p&gt;Poppy는 기존의 강화학습을 활용한 방법론에 비해 search space가 넓어 스케줄의 성능을 높이는 데는 효과적이지만, 여러 agent가 다양한 action을 탐색하도록 하는 만큼 연산 시간 역시 증가한다. 실제 제조 현장에서는 짧은 시간 내에 합리적인 성능의 스케줄을 도출하는 것이 중요하기 때문에 스케줄링 문제에서 Poppy를 바로 적용하는 데는 한계가 있다. 하지만 population-based learning의 스케줄링 문제 적용 가능성을 보여 주었다는 데 의의가 있다.&lt;/p&gt;

&lt;p&gt;특히 현재까지 제안된 강화학습 기반 스케줄링 방법론은 많은 경우 setup time, precedence constraint와 같은 복잡한 제약을 다루는 데 분명한 한계를 보인다. 본 논문에서 제안하는 population-based learning은 evolutionary algorithm과 결합하여 사용 가능하다. 따라서 population-based learning 단계에서는 제약을 relax한 환경에서 모델을 학습시켜 빠른 시간 내에 좋은 성능의 스케줄을 도출하고, 이를 genetic algorithm과 같은 evolutionary algorithm의 초기해로 활용해 제약을 만족할 수 있도록 해당 스케줄을 modify하는 알고리즘을 고안해볼 수 있다.&lt;/p&gt;

&lt;p&gt;이외에도 제조 환경에서의 domain knowledge를 활용하여 초기 단계부터 search space를 적절히 좁힘으로써 agent가 과도하게 다양한 action을 탐색하는 것을 방지할 수 있다. 본 논문은 해의 다양성을 높여 local optima에 빠지는 것을 방지함으로써 높은 성능의 해를 도출한다는 목적을 가지고 있다. 하지만 기존의 강화학습을 활용한 스케줄링 문제에서 보였던 연산 시간의 한계점이 Poppy에서는 작은 사이즈의 문제에서도 드러났기에, &lt;em&gt;1) 스케줄링 문제에서 강화학습 dispatcher만을 단독으로 사용할 수 있을 만큼 효과적인가, 2) 그렇지 않아 강화학습 dispatcher를 다른 search 방법이나 메타휴리스틱과 결합하여 사용해야 한다면, 강화학습 단계에서 긴 연산 시간을 투자하여 local optima에 빠지는 것을 반드시 막아야 할 필요가 있는가&lt;/em&gt; 에 대한 고민이 충분히 이루어져야 한다.&lt;/p&gt;

&lt;h2 id=&quot;author-information&quot;&gt;Author Information&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Jimin Park
    &lt;ul&gt;
      &lt;li&gt;Affiliation: &lt;a href=&quot;https://msslab.kaist.ac.kr/&quot;&gt;Manufacturing and Service Systems Lab.&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;Research Topic: Scheduling with Reinforcement Learning&lt;/li&gt;
      &lt;li&gt;Contact: jiminpark@kaist.ac.kr&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-reference--additional-materials&quot;&gt;6. Reference &amp;amp; Additional Materials&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;[POMO] Kwon, Y. D., Choo, J., Kim, B., Yoon, I., Gwon, Y., &amp;amp; Min, S. (2020). Pomo: Policy optimization with multiple optima for reinforcement learning. Advances in Neural Information Processing Systems, 33, 21188-21198.&lt;/li&gt;
  &lt;li&gt;[L2D] Zhang, C., Song, W., Cao, Z., Zhang, J., Tan, P. S., &amp;amp; Chi, X. (2020). Learning to dispatch for job shop scheduling via deep reinforcement learning. Advances in neural information processing systems, 33, 1621-1632.&lt;/li&gt;
&lt;/ul&gt;
</description>
            <pubDate>Wed, 17 Apr 2024 00:00:00 +0900</pubDate>
            <link>http://localhost:4000/Winner_Takes_It_All_Training_Performant_RL_Populations_for_Combinatorial_Optimization.html</link>
            <guid isPermaLink="true">http://localhost:4000/Winner_Takes_It_All_Training_Performant_RL_Populations_for_Combinatorial_Optimization.html</guid>
            
            <category>reviews</category>
            
            
        </item>
        
        <item>
            <title>[ICLR 2023] Unsupervised Model Selection for Time-series Anomaly Detection</title>
            <description>&lt;h1 id=&quot;iclr-2023-unsupervised-model-selection-for-time-series-anomaly-detection&quot;&gt;&lt;strong&gt;[ICLR-2023] Unsupervised Model Selection for Time-series Anomaly Detection&lt;/strong&gt;&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;key words&lt;/strong&gt;: Model Selection · Time-series Anomaly Detection · Unsupervised Learning&lt;/p&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;&lt;strong&gt;1. Introduction&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;시계열 데이터에서의 이상 감지는 데이터의 급격한 증가와 일부 자동 시스템의 모니터링 필요성 증대로 인해 중요성이 높아지고 있다. 다양한 이상 감지 기법들이 개발되었으며, 이러한 기법들은 간단한 알고리즘에서부터 복잡한 딥러닝 모델에 이르기까지 다양하다. 이 모델들은 데이터셋에 따라 성능의 차이가 크게 나타나며, 모든 유형의 데이터에서 다른 모델들보다 뛰어난 성능을 보이는 보편적인 방법은 존재하지 않는다. 이러한 상황은 레이블이 없는 상태에서 가장 효과적인 이상 감지 모델을 선택하는 것을 어렵게 만들며, 이는 연구자들이 레이블 없이 효과적인 모델을 선택할 수 있는 방법을 개발하는 데 관심을 갖게 했다. 이는 무엇보다도 데이터의 이상을 식별할 때 사용할 수 있는 비지도(unsupervised) 방식으로 모델을 선택하는 새로운 접근 방식의 필요성을 강조한다.&lt;/p&gt;

&lt;p&gt;저자의 접근 방식은 모델 성능과 상관관계가 있는 “대리(surrogate)” 메트릭을 계산하는 것을 기반으로 하며, 이 메트릭에 의해 유도된 모델 순위를 집계한다(Figure. 1). 의학, 스포츠 등 다양한 분야를 포괄하는 10개의 실제 데이터셋에서의 경험적 평가는 저자의 접근 방식이 데이터의 일부를 레이블링하여(supervised) 선택하는 것만큼 효율적으로 비지도 모델 선택을 수행할 수 있음을 보여준다.&lt;/p&gt;

&lt;p&gt;요약하자면, 본 연구의 기여는 크게 3가지로 볼 수 있다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;본 연구는 시계열 데이터에서 비감독 이상 감지 모델 선택을 위한 최초의 방법 중 하나를 제안한다. 이를 위해 모델 성능에 대한 직관적이고 효과적인 비감독 메트릭을 식별한다. 이전의 작업은 이러한 비지도 방법 중 일부를 시계열 이상 감지 모델 선택 문제가 아닌 다른 문제에 사용했다.&lt;/li&gt;
  &lt;li&gt;여러 multiple surrogate 메트릭을 단일(single) 모델 선택 기준으로 결합하는 새로운 강력한 순위 집계 방법을 제안한다. 본 연구는 접근 방식이 데이터의 부분을 레이블링을 기반으로 선택하는 것과 동등하게 수행함을 보여준다.&lt;/li&gt;
  &lt;li&gt;또한, 275개 이상의 다양한 시계열을 포함하는 대규모 실험을 수행한다. 본 연구에 사용되는 시계열은 의학, 곤충학 등의 다양한 도메인에 걸쳐 있으며, 5개의 인기 있고 널리 사용되는 이상 감지 모델, 각각 1에서 4개의 하이퍼파라미터 조합으로 구성된 5,000개 이상의 훈련된 모델을 사용한다. 
&lt;img src=&quot;../../images/DS503_24S/Unsupervised_Model_Selection_for_Time_series_Anomaly_Detection/Figure1.png&quot; alt=&quot;image_sample&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-preliminaries--the-model-selection-problem&quot;&gt;&lt;strong&gt;2. Preliminaries &amp;amp; The Model Selection Problem&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Problem 1. 비지도 시계열 이상 탐지 모델 선택&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;관측치 $X_ {\text{test}}$와 $X_ {\text{train}}$을 사용하여 훈련된 모델 집합 $M = {A_ i}_ {i=1}^N$,이 주어졌을 때, 레이블을 사용하지 않고 이상 탐지 품질 메트릭 $Q(A_ i(X_ {\text{test}}), Y_ {\text{test}})$를 최대화하는 모델을 선택한다.&lt;/p&gt;

&lt;h4 id=&quot;21-이상-탐지-모델-성능-측정&quot;&gt;2.1 이상 탐지 모델 성능 측정&lt;/h4&gt;

&lt;p&gt;이상 탐지는 이진 분류 문제로 볼 수 있으며, 각 시간 지점을 이상 혹은 정상 관측으로 분류한다. 따라서 모델 $A_ {i}$의 성능은 표준 정밀도와 재현율을 사용하여 측정할 수 있다. 그러나 이런 메트릭들은 시계열의 순차적 특성을 고려하지 않기 때문에, 시계열 이상 감지는 일반적으로 정밀도와 재현율의 조정된 버전을 사용하여 평가된다. 저자는 조정된 정밀도와 재현율의 널리 사용되는 버전을 채택한다. 이 메트릭들은 시간 점들을 독립적인 샘플로 취급하나, 이상이 연속적인 여러 시간 점에 걸쳐 발생하는 경우, 이러한 점 중 하나를 감지하는 것은 모든 이상 구간 내의 점들이 감지된 것으로 간주한다.&lt;/p&gt;

&lt;h2 id=&quot;3-surrogate-metrics-of-model-performance&quot;&gt;&lt;strong&gt;3. Surrogate Metrics of Model Performance&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;비지도 모델 선택은 지도 학습 성능 메트릭과 상관 관계가 있는 비지도 메트릭을 찾는 과정으로 볼 수 있다. 각 비지도 메트릭은 모델 성능의 ‘좋음’을 잡음이 섞인 척도로 제공하여, 이 메트릭에 따라 최고 성능 모델을 선택하는 문제로 단순화된다. time-series 이상 탐지 모델의 성능을 예측하는 데 있어 전문가의 직관과 일치하는 세 가지 유형의 불완전한 메트릭을 식별했다.&lt;/p&gt;

&lt;h4 id=&quot;prediction-error&quot;&gt;Prediction Error&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;모델이 시계열 데이터를 잘 예측하거나 재구성할 수 있다면, 그 모델은 또한 효과적인 이상 탐지 모델이 될 수 있다. 이상 탐지 방법들 중 많은 수가 예측이나 재구성 기반으로, 이상 레이블 없이 예측 오류나 재구성 오류를 계산할 수 있다.&lt;/li&gt;
  &lt;li&gt;예측 오류 메트릭에는 평균 절대 오류(MAE), 평균 제곱 오류(MSE) 등이 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;synthetic-anomaly-injection&quot;&gt;Synthetic Anomaly Injection&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;좋은 이상 감지 모델은 합성적으로 이상을 주입한 데이터에서도 효과적으로 이상을 감지할 수 있어야 합니다.&lt;/li&gt;
  &lt;li&gt;주어진 시계열 데이터에 무작위로 다양한 유형의 이상을 주입하고, 해당 모델이 이를 어떻게 감지하는지를 평가함으로써 모델의 성능을 간접적으로 측정합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;model-centrality-모델-중심성&quot;&gt;Model Centrality (모델 중심성)&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;모든 모델이 실제 상황에 대해 만들어내는 이상 점수들의 ‘중심성’에 기반하여, 가장 진실에 가까운 모델을 식별하는 방법이다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;모델 간의 거리를 측정하는 데 사용되는 켄달 타우 거리(Kendall’s τ distance)는 모델의 순위에 따른 순서의 불일치를 계산하며 수식은 다음과 같다.&lt;/p&gt;

    &lt;p&gt;$d_ {\tau}(\sigma_ k, \sigma_ l) = \sum_ {i &amp;lt; j} \mathbf{I}{(\sigma_ k(i) - \sigma_ k(j))(\sigma_ l(i) - \sigma_ l(j)) &amp;lt; 0}$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-robust-rank-aggregation-강력한-순위-집계&quot;&gt;&lt;strong&gt;4. Robust Rank Aggregation (강력한 순위 집계)&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;많은 연구들이 군중 소싱(crowd-sourcing)과 같은 머신러닝의 다양한 영역에서 여러 소스의 잡음이 있는 정보를 결합하여 오류를 줄이는 이점을 탐구했다. 저자의 각 대리 메트릭(surrogate metrics)은 모델의 순위를 부과하며, 그 결과 노이즈가 있는 순위가 만들어진다. 따라서 이러한 맥락에서 저자는 두 가지 자연스러운 질문을 제기한다: (1) 여러 모델 순위를 어떻게 신뢰성 있게 결합할 수 있는가? (2) 여러 순위를 집계하는 것이 모델 선택에 도움이 되는가?&lt;/p&gt;

&lt;h4 id=&quot;41-모델-선택을-순위-집계-문제로-접근하기&quot;&gt;4.1 모델 선택을 순위 집계 문제로 접근하기&lt;/h4&gt;

&lt;p&gt;이상 탐지 모델의 순위를 매기기 위해 여러 대리 메트릭(surrogate metrics)을 가지고 있다고 가정해보자. 이 메트릭들에 의해 유도된 순위를 집계하는 것이 왜 유익할까? 다음의 정리는 Borda 순위 집계의 이점에 대한 통찰을 제공한다.&lt;/p&gt;

&lt;p&gt;순위 집계 문제는 주어진 $N$개의 항목들에 대한 $M$개의 순위에서 가장 일관성 있는 요약 순위를 찾는 것이다. 주어진 집합에서 각 순위는 항목의 배열이며, 문제는 요약 순위를 나타내는 $\sigma^{*}$를 찾는 것이다.&lt;/p&gt;

&lt;p&gt;순위 집계는 사회 선택, 생물정보학, 머신러닝 문헌에서 광범위하게 연구되었다. 특히, 케메니 순위 집계(Kemeny rank aggregation)는 순위의 집합에서 거리를 선택하고 중앙 순위를 찾는 것과 관련이 있다. 케메니 순위 집계는 다음의 목표를 가지고 있다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Problem 2. 순위 집계&lt;/strong&gt; 
주어진 $N$개의 항목에 대한 $M$개의 순위 $\sigma_ {1}, \ldots, \sigma_ {M} \in S_ {N}$ 집합이 있을 때, 어떤 목표 $C(\sigma^{&lt;em&gt;}, \sigma_ {1}, \ldots, \sigma_ {M})$에 따라 이 순위들을 가장 잘 요약하는 $\sigma^{&lt;/em&gt;} \in S_ {N}$를 찾는다.&lt;/p&gt;

&lt;p&gt;이 문제는 케메니-영(Kemeny-Young) 문제로 알려져 있고, 목표가 다음과 같이 정의될 때, 케메니 순위 집계 문제가 된다.&lt;/p&gt;

&lt;p&gt;$C = \frac{1}{M} \sum_ {i=1}^M d_ {\tau}(\sigma^*, \sigma_ {i})$, 여기서 $d_ {\tau}$​는 켄달의 $\tau$거리를 나타낸다.&lt;/p&gt;

&lt;p&gt;케메니 순위 집계는 많은 바람직한 속성을 가지고 있지만, NP-hard 문제이므로 실제로는 효율적인 근사 해법을 고려해야 한다. 예를 들어, Borda 방법은 각 항목(모델)이 각 순위(대리 메트릭)에서 점수를 받는 방법을 사용한다. Borda 방법은 다음과 같이 점수를 부여한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;예를 들어, 모델이 대리 메트릭에 의해 $r$위치에 있다면, 그 메트릭으로부터 $(N - r)$ 점수를 받는다.&lt;/li&gt;
  &lt;li&gt;모델들은 모든 메트릭으로부터 받은 총 점수에 따라 내림차순으로 순위가 매겨진다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이 접근 방식은 순위를 통한 모델 선택 문제에 대한 보다 구조화된 접근을 가능하게 하며, 다양한 메트릭을 통합하여 보다 견고한 결정을 내릴 수 있도록 한다.&lt;/p&gt;

&lt;h4 id=&quot;42-empirical-influence-and-robust-rank-aggregation&quot;&gt;4.2 Empirical Influence and Robust Rank Aggregation&lt;/h4&gt;
&lt;p&gt;4.2절에서 저자는 Borda 방법을 사용하여 여러 대리 메트릭에서 파생된 순위를 통합하는 강력한 순위 집계 방법에 대해 설명한다. 이 방법은 각 모델이 각 메트릭에서 얻은 순위를 통해 모델의 전체 성능을 평가하는데 사용된다.&lt;/p&gt;
&lt;h4 id=&quot;borda-방법&quot;&gt;Borda 방법&lt;/h4&gt;
&lt;p&gt;Borda 방법은 간단하고 널리 사용되는 순위 집계 방법으로, 각 모델에 대해 각 대리 메트릭의 순위에 따라 점수를 할당하고 이 점수들을 합산하여 모델의 최종 순위를 결정한다.&lt;/p&gt;

&lt;p&gt;$B_ {i} = \sum_ {k=1}^M (N - r_ {ik})$&lt;/p&gt;

&lt;p&gt;여기서 $B_ {i}$​는 모델 $i$의 Borda 점수, $M$은 메트릭의 수, $N$은 모델의 수, r_ {ik}​는 메트릭 $k$에서 모델 $i$의 순위. 모델은 $B_ {i}$​ 점수가 높은 순으로 순위가 매겨진다.&lt;/p&gt;

&lt;p&gt;보르다 방법은 모든 대리 메트릭을 동등하게 취급한다. 그러나 저자의 경험과 이론적 분석은 “좋은” 메트릭만을 사용하여 집계하는 것이 성능을 개선할 수 있다고 제안한다. 그러나 어떻게 이상 레이블이 없이 “좋은” 순위를 식별할 수 있을까? 이에 대한 해답은 보르다 집계 방법과 이를 향상시킬 수 있는 다양한 기법들을 통해 제공된다. 저자는 각 대리 메트릭의 신뢰성을 평가하고, 그에 따라 각 순위에 가중치를 부여하여 향상된 보르다 집계 방법을 제안한다.&lt;/p&gt;

&lt;h4 id=&quot;향상된-borda-방법&quot;&gt;향상된 Borda 방법&lt;/h4&gt;

&lt;p&gt;향상된 Borda 방법은 각 대리 메트릭의 신뢰성을 고려하여 가중치를 조정한다. 더 신뢰할 수 있는 메트릭은 최종 순위 결정에 더 큰 영향을 미친다.&lt;br /&gt;
$B_ {i}^{*} = \sum_ {k=1}^{M} w_ {k}(N - r_ {ik})$,  $w_ {k}$​는 메트릭 $k$의 신뢰도에 따른 가중치&lt;/p&gt;

&lt;p&gt;이 접근 방식을 사용하면, 더 신뢰할 수 있는 메트릭의 영향을 강화하고, 덜 신뢰할 수 있는 메트릭의 영향을 감소시킬 수 있다. 이는 전체적인 모델 선택 과정에서 더 정확하고 신뢰할 수 있는 결정을 내리는 데 도움을 줄 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;5-evaluation-results&quot;&gt;&lt;strong&gt;5. Evaluation Results&lt;/strong&gt;&lt;/h2&gt;
&lt;h4 id=&quot;51-data&quot;&gt;5.1 Data&lt;/h4&gt;
&lt;p&gt;저자는 다양한 시계열과 이상을 포함하는 두 개의 인기 있고 널리 사용되는 실제 데이터 컬렉션에서 실험을 수행한다:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;UCR 이상 아카이브 (UCR) [Wu and Keogh, 2021]&lt;/strong&gt;:  이 컬렉션에는 자연적 이상과 현실적인 합성 이상이 포함되어 있다. 포함된 시계열의 이질성을 고려하여, UCR을 도메인별로 9개의 하위 집합으로 나누었다: (1) 가속도, (2) 공기 온도, (3) 동맥혈압(ABP), (4) 전기 침투 그래프(EPG), (5) 심전도(ECG), (6) 보행, (7) NASA, (8) 전력 수요, (9) 호흡(RESP). 각각의 하위 집합을 별도의 데이터셋으로 참조된다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;서버 머신 데이터셋 (SMD) [Su et al., 2019]&lt;/strong&gt;: SMD는 5주 동안 대규모 인터넷 회사의 28대 서버 머신에서 수집된 38개 특성을 가진 다변량 시계열을 포함한다. 각 엔티티에는 서버 머신의 CPU 부하, 메모리 및 네트워크 사용량과 같은 일반적인 메트릭이 포함되어 있다. 훈련 및 테스트 세트 모두 약 50,000개의 타임스탬프를 포함하며, 그 중 5%가 이상 사례이다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;SMD를 단일 데이터셋으로 참조하며, 따라서 저자는 평가에서 UCR의 9개 데이터셋과 SMD를 합쳐 총 10개의 데이터셋을 고려한다.&lt;/p&gt;

&lt;h4 id=&quot;52-model-set&quot;&gt;5.2 Model set&lt;/h4&gt;
&lt;p&gt;저자는 실험을 위해 5가지 인기 있는 방법의 대표적인 부분 집합을 구현했습니다(Table. 1), 이는 두 가지 다른 학습 유형(unsupervised and semi-supervised)과 세 가지 다른 방법 (forecasting, reconstruction and distance-based)에 걸쳐 있다.&lt;/p&gt;

&lt;p&gt;각 모델의 하이퍼파라미터를 다양하게 하여 3개의 k-NN, 4개의 이동 평균, 4개의 DGHL, 4개의 LSTM-VAE, 4개의 RNN 모델을 포함하는 총 19개 조합의 풀을 생성했으며 모델을 효율적으로 훈련하기 위해, T &amp;gt; 2560인 모든 시계열을 10분의 1로 하위 샘플링하였다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Unsupervised_Model_Selection_for_Time_series_Anomaly_Detection/Table1.png&quot; alt=&quot;image_sample&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;53-experimental-set-up&quot;&gt;5.3 Experimental set up&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Evaluating model selection performance&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;: 모든 모델 선택 전략을 최고의 선택된 모델의 조정된 최고 F1 점수(제 2장 참조)에 기반하여 평가한다.&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Baselines&lt;/strong&gt;: baseline으로는 데이터셋의 일부를 레이블링하고 이 레이블을 기반으로 최고의 방법을 선택하는 접근법을 고려한다. 구체적으로, 각 데이터셋은 선택(20%)과 평가(80%) 세트로 임의 분할된다. 즉, baselines은 선택 세트의 이상 레이블을 기반으로 모델을 선택하지만, 저자의 대리 메트릭은 선택 세트를 사용하지 않고, 대신 레이블을 사용하지 않고 평가 세트에서 계산된다. 전체 실험은 무작위 선택/평가 분할로 5번 반복된다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Model selection strategies&lt;/strong&gt;: 5개의 예측 오류 메트릭, 9개의 합성 이상 주입 메트릭, 3개의 중심성 메트릭), Borda 순위 집계의 4가지 제안된 강력한 변형들을 포함한 17개의 개별 대리 메트릭을 비교한다.(제 3장 참조, 부록 A.7.2 참조).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Pairwise-statistical tests&lt;/strong&gt;:  우리는 유의 수준 $\alpha$ = 0.05에서 one-sided paired Wilcoxon signed rank tests를 수행하여 각 데이터셋에서 모든 모델 선택 전략 및 기준선 간의 유의미한 성능 차이를 식별한다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Experimentation environment&lt;/strong&gt;: 모든 모델과 모델 선택 알고리즘은 scikit-learn 1.1.1, PyTorch 1.11.0를 사용하여 구축되었고 Python 3.9.13을 사용하여 훈련되었다. 모든 실험은 16개의 인텔(R) 제온(R) CPU, 122 GiB RAM 및 8 GiB GPU를 갖춘 AWS g3.4xlarge EC2 인스턴스에서 수행되었다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;54-results-and-discussion&quot;&gt;5.4 Results and Discussion&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Unsupervised_Model_Selection_for_Time_series_Anomaly_Detection/Figure3.png&quot; alt=&quot;image_sample&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이상 탐지 모델 선택을 위한 실험 결과에 따르면(Figure3), 단일 대리 메트릭이 모든 데이터셋에서 최고의 모델을 일관되게 선택하지는 않는다. 특히 예측 오류 메트릭은 일부 데이터셋에서 오라클과 비슷한 성능을 보였지만, 다른 데이터셋에서는 무작위 선택보다 나쁜 결과를 보였다. 최소 영향 메트릭(MIM)과 robust Borda는 각각 supervised 선택과 비교하여 비슷하거나 더 나은 성능을 보였으며, 합성 이상 주입 메트릭이 예측 오류나 중심성 메트릭보다 더 나은 결과를 보여줌으로써 다른 메트릭 클래스를 능가했다.&lt;/p&gt;

&lt;p&gt;실험은 또한 이상 유형에 대한 사전 지식이 모델 선택에 유용하게 활용될 수 있음을 보여준다. 예를 들어, 특정 데이터셋의 특성에 맞는 합성 메트릭은 해당 이상을 잘 감지하여 높은 성능을 보였다. 이는 도메인 전문가들이 데이터에 존재할 수 있는 이상의 유형을 인식하고 이를 기반으로 적절한 모델을 선택할 때, 사전 지식을 효과적으로 활용할 수 있다는 것을 의미한다.&lt;/p&gt;

&lt;h2 id=&quot;6-conclusion&quot;&gt;&lt;strong&gt;6. Conclusion&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;저자는 시계열에서 이상 탐지를 위한 비지도(Unsupervised) 모델 선택 문제를 고려한다. 제안된 모델 선택 접근 방식은 이상 레이블을 요구하지 않으면서도 모델 성능과 다양한 정도로 상관관계가 있는 대리 메트릭(surrogate metric)에 기반을 두고 있다. 본 연구에서는 예측 오류, 합성 이상 주입, 그리고 모델 중심성이라는 세 가지 클래스의 대리 메트릭을 식별한다. 다양한 종류의 합성 이상을 주입하는 효과적인 절차를 고안하고 시계열에 모델 중심성의 개념을 확장한다. 다음으로, 저자는 다양한 메트릭에서 얻은 순위를 강력한 순위 집계 접근 방식을 사용하여 결합할 것을 제안하며 순위 집계에 대한 이론적 근거를 제공한다. 5000개 이상의 훈련된 모델과 17개의 대리 메트릭을 사용한 평가 결과 제안된 접근 방식이 부분 데이터 레이블링을 기반으로 한 선택(supervised)과 동등하게 수행됨을 보여준다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;author-information&quot;&gt;&lt;strong&gt;Author Information&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Jiyu Moon
    &lt;ul&gt;
      &lt;li&gt;Research Topic: Change point detection&lt;/li&gt;
      &lt;li&gt;Contact: jy_moon@kaist.ac.kr&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference--additional-materials&quot;&gt;&lt;strong&gt;Reference &amp;amp; Additional materials&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Please write the reference. If paper provides the public code or other materials, refer them.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Github Implementation
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/mononitogoswami/tsad-model-selection&quot;&gt;Code for the paper&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Reference
    &lt;ul&gt;
      &lt;li&gt;Ane Blázquez-García, Angel Conde, Usue Mori, and Jose A Lozano. A review on outlier/anomaly detection in time series data. ACM Computing Surveys (CSUR), 54(3):1–33, 2021.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
            <pubDate>Wed, 17 Apr 2024 00:00:00 +0900</pubDate>
            <link>http://localhost:4000/Unsupervised_Model_Selection_for_Time_series_Anomaly_Detection.html</link>
            <guid isPermaLink="true">http://localhost:4000/Unsupervised_Model_Selection_for_Time_series_Anomaly_Detection.html</guid>
            
            <category>reviews</category>
            
            
        </item>
        
        <item>
            <title>[ICLR 2023] Universal Few-shot Learning of Dense Prediction Tasks with Visual Token Matching</title>
            <description>&lt;div class=&quot;alert alert-info&quot; role=&quot;alert&quot;&gt;&lt;i class=&quot;fa fa-info-circle&quot;&gt;&lt;/i&gt; &lt;b&gt;Note:&lt;/b&gt; This template is an example, so you don’t have to follow this templates!&lt;/div&gt;

&lt;h1 id=&quot;universal-few-shot-learning-of-dense-prediction-tasks-with-visual-token-matching&quot;&gt;&lt;strong&gt;Universal Few-shot Learning of Dense Prediction Tasks with Visual Token Matching&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;Title of paper that you are going to write&lt;/p&gt;

&lt;h2 id=&quot;1-problem-definition&quot;&gt;&lt;strong&gt;1. Problem Definition&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;computer-vision-in-a-nutshelltasks&quot;&gt;Computer Vision in a nutshell(tasks)&lt;/h3&gt;
&lt;p&gt;CV에서는 아래와 같은 task들을 주로 다룹니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;이게 어떤 장면을 나타내는지&lt;/li&gt;
  &lt;li&gt;어떤 object가 있는지(object의 종류가 무엇인지)&lt;/li&gt;
  &lt;li&gt;object가 어디에 있는지&lt;/li&gt;
  &lt;li&gt;얼마나 많은 object가 있는지&lt;/li&gt;
  &lt;li&gt;object가 어느 영역에 있는지&lt;/li&gt;
  &lt;li&gt;카메라로부터 object까지 얼마나 떨어져있는지(거리)&lt;/li&gt;
  &lt;li&gt;object의 기하학인 표면이 어디에 있는지&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dense-prediction-task-con-situtes-a-fundamental-class-in-cv&quot;&gt;Dense prediction task con situtes a fundamental class in CV&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;CV의 많은 task들이 이러한 함수의 형태로 나타내게 됩니다. H * W 해상도의 이미지를 입력했을 때, 같은 크기의 H * W 크기로 출력하는 CV에서 중요한 task의 집합입니다. 각 task 마다 픽셀별로 정답을 내뱉는다는 의미입니다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$ \tau = \mathbb{R}^{H \times W \times 3} \rightarrow \mathbb{R}^{H \times W \times C_\tau}, C_\tau \in \mathbb{N}$&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;H : 이미지의 높이&lt;/li&gt;
      &lt;li&gt;W : 이미지의 넓이&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Universal_Few-shot_Learning_of_Dense_Prediction_Tasks_with_Visual_Token_Matching/different_channel.png&quot; alt=&quot;different&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;각 taks는 output 채널 수나, label의 차원이나, semantic과 같은 고유한 출력 구조를 가지고 있습니다.
    &lt;ul&gt;
      &lt;li&gt;ex : image &amp;gt; semantic segmatation $C_\tau$ : # of class, object detection $C_\tau$ : # of class + 5, depth estimation $C_\tau$ : 1, surface normal estimation $C_\tau$ : 3&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;이때까지는 각 task를 풀기 위해 다른 네트워크들이 사용되어 왔습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;supervised-learning-of-dense-prediction-tasks&quot;&gt;Supervised learning of dense prediction tasks&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;지도학습은 많은 양의 labeled data(사람이 직접 이미지를 설명하는 정답을 입력한 것)set을 이용하여 모델을 학습합니다. 이러한 지도학습의 문제점이 존재합니다.
    &lt;ol&gt;
      &lt;li&gt;많은 비용과 사람의 노동력이 필요할 수 있습니다.&lt;/li&gt;
      &lt;li&gt;특정한 이유로 데이터를 많이 모을 수 없습니다. (병원의 민감정보, 희귀한 경우 등)&lt;/li&gt;
      &lt;li&gt;task 자체가 지속적으로 변화할 수 있습니다. 따라서 바뀌는 task에 맞춰 데이터셋을 변화하여 모을 수 없습니다.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;few-shot-learning-for-dense-prediction-tasks&quot;&gt;Few-shot learning for dense prediction tasks&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;지도학습의 문제점을 해결하기 위해 즉, 데이터를 많이 모을 수 없는 경우 적은 labeled data를 가지고 모델을 학습하는 few-shot learning 방법을 사용합니다.&lt;/li&gt;
  &lt;li&gt;$\hat{Y}^q = \mathcal{F}(X^q; S_\tau),   S_\tau = {(X^i, Y^i)}_{i\leq	N}$
    &lt;ul&gt;
      &lt;li&gt;$\hat{Y}^q$ : 맞춰야 할 query image의 label&lt;/li&gt;
      &lt;li&gt;$X^q$ : query image&lt;/li&gt;
      &lt;li&gt;$S_\tau$ : support set, 우리가 알고 있는 몇 장 안 되는 label이 달린 image set을 의미합니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;problems-in-existing-methods-for-few-shot-learning&quot;&gt;Problems in existing methods for few-shot learning&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;기존 방법들의 문제점은 특정한 task 종류에 한해서 적용되는 것입니다. 즉 다른 task로 적용이 되지 않아 일반화가 어렵습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-motivation&quot;&gt;&lt;strong&gt;2. Motivation&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Can we design a universal few-shot learner that can flexibly adapt to unseen, arbitrary dense prediction tasks depending on a few labeled data?&lt;/li&gt;
  &lt;li&gt;모든 종류의 dense prediction task에서 few-shot learning을 할 수 있는 universal한 few-shot learner를 만드는 것이 목표입니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Universal_Few-shot_Learning_of_Dense_Prediction_Tasks_with_Visual_Token_Matching/motivation.png&quot; alt=&quot;motivation&quot; /&gt;
    - depth estimation을 support set으로 넣어주면, 풀고싶은 task를 사전에 학습하지 않고도 depth estimation 모델을 학습하여 출력합니다.
    - 다른 방법인 surface normal estimation, semantic segmentation 등도 마찬가지로 적용됩니다.&lt;/p&gt;

&lt;h3 id=&quot;challenges&quot;&gt;Challenges&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Handling arbitrary number of channels in various tasks : 각 task 마다 요구하는 채널의 개수가 달라서 이를 handling 해야합니다. 즉 다양한 채널의 개수로 출력하게 하는 점이 어렵습니다.&lt;/li&gt;
  &lt;li&gt;Designing a task-agnostic architecture : task 자체가 매우 달라서 task에 상관없이 작동하는 하나의 네트워크 아키텍쳐를 디자인해야합니다. task 중 semantic sementation은 물체의 종류를 예측하는 고차원의 task임에 반면에 depth estimation은 픽셀과 카메라 간의 거리만 예측하는 것으로 매우 다른 task입니다.&lt;/li&gt;
  &lt;li&gt;Designing an efficient adaptation mechanism : task 매우 다르므로 하나의 아키텍쳐를 각 task에 맞게끔 adaptation mechanism 적응 메커니즘이 필요합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-method&quot;&gt;&lt;strong&gt;3. Method&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;1-handling-arbitrary-number-of-channels-rightarrow-decomposing-a-task-into-multiple-single-channel-subtasks&quot;&gt;1. Handling arbitrary number of channels $\rightarrow$ Decomposing a task into multiple single-channel subtasks&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;$ \tau = \mathbb{R}^{H \times W \times 3} \rightarrow \mathbb{R}^{H \times W \times C_\tau}, C_\tau \in \mathbb{N}$
여러 개 채널이 있을 때 채널 각각을 하나의 single channel label로 독립적으로 추론합니다.&lt;/li&gt;
  &lt;li&gt;$ \tau_c = \mathbb{R}^{H \times W \times 3} \rightarrow \mathbb{R}^{H \times W \times 1}, \; where \; \tau \in {\tau_1, …, \tau_{c_\tau}}$ multiple 채널의 task를 채널이 1개인 task $C_\tau$개로 바꾸어 예측하고 그 예측값을 원래의 task의 예측으로 사용하였습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-designing-a-task-agnostic-few-shot-learner&quot;&gt;2. Designing a task-agnostic few-shot learner&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Universal_Few-shot_Learning_of_Dense_Prediction_Tasks_with_Visual_Token_Matching/design.png&quot; alt=&quot;design&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Patch-wise matching on embedding space
    &lt;ul&gt;
      &lt;li&gt;patch 단위로 나누는 이유? dense prediction이기 때문에 local 정보가 중요할 수 있습니다. 따라서 이미지를 통째로 사용하는 것보다 패치 단위로 나누어 패치 각각을 매칭하였습니다.  &lt;br /&gt;
  차별점)
  매칭은 few-shot learning에서 많이 쓰이는 기법 중 하나입니다. 일반적인 매칭과의 차별점은 이미지를 그대로 잘라서 매칭하지 않고 dense prediction을 위해 local하게 patch 단위로 나누고 label도 encoding하여 embedding space 안에서 매칭을 한다는 점입니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;$g(\textbf{y}^q_j = \sum_{i&amp;lt;=N} \sum_{k&amp;lt;=M} \sigma(f_\tau(\textbf{x}^q_j),f_\tau(\textbf{x}^i_k) )\;\cdot \; g(\textbf{y}^i_k)$
    &lt;ul&gt;
      &lt;li&gt;$\textbf{x}^q_j$ : query image patch(j-th) 맞추고 싶은 쿼리 이미지를 패치 단위로 자른 것 중 하나를 의미합니다.&lt;/li&gt;
      &lt;li&gt;$\textbf{x}^i_k$ : support image patch(k-th patch from i-th image) 정답을 알고 있는 이미지를 패치 단위로 자른 것 중 하나를 의미합니다.&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;$\textbf{y}^i_k$ : support label patch(k-th patch from i-th label) 정답을 알고 있는 이미지의 라벨을 패치 단위로 자른 것 중 하나를 의미합니다.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;$f_\tau$ : Compute embedding of the query and support images by the shared iamge encoder $f_\tau$&lt;/li&gt;
      &lt;li&gt;$g$ : Embedding of the support labels by the label encoder $g$
        &lt;ul&gt;
          &lt;li&gt;encoder network를 통해서 abstract space로 보냅니다. 각각의 patch에서 embeddnig을 뽑고 그 embedding에 대해서 매칭을 진행합니다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;$\sigma(f_\tau(\textbf{x}^q_j),f_\tau(\textbf{x}^i_k)) $ : Compute the pairwise similarity between the embedded image patches by the similarity measure $\sigma : \mathbb{R}^d \times \mathbb{R}^d \rightarrow [0, 1]$
        &lt;ul&gt;
          &lt;li&gt;query의 embedding vector와 모든 support image의 embedding vector들 간의 similarity를 계산합니다. 따라서 특정 쿼리 패치 하나당 서포트 패치 개수만큼 similarity가 도출됩니다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;$\sum_{i&amp;lt;=N} \sum_{k&amp;lt;=M} \sigma(f_\tau(\textbf{x}^q_j),f_\tau(\textbf{x}^i_k) )\;\cdot \; g(\textbf{y}^i_k)$ : Aggregate the embedding of the label patches based on the image-level similarity. This gives us the embedding of the corresponding query label patch.
        &lt;ul&gt;
          &lt;li&gt;이 similarity를 weight로 사용하여 weighted sum을 계산합니다. 특정한 서포트 패치와 맞추고 싶은 쿼리의 패치랑 비슷하다면 서포트 패치의 label의 가중치를 많이 주고, 반대도 마찬가지로 가중치를 적게 주어서 결과적으로 비슷한 것끼리 매칭하여 비슷한 이미지의 label을 가져오게 합니다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;$h \approx g^{-1}$ : Convert the query label embedding into the original label space by the decoder h
        &lt;ul&gt;
          &lt;li&gt;$\hat{\textbf{y}}^q_j = h(g(\textbf{y}^q_j))$&lt;/li&gt;
          &lt;li&gt;weight는 label의 embedding space에 있으니까 다시 h decoder를 이용하여 label space로 보냅니다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Parameterization of encoders
        &lt;ul&gt;
          &lt;li&gt;Image encoder : $f_\tau(\textbf{x}) =f(\textbf{x};\theta, \theta_\tau) $&lt;/li&gt;
          &lt;li&gt;Label encoder : $g(\textbf{y}) =g(\textbf{y};\phi) $&lt;/li&gt;
          &lt;li&gt;Label decoder : $h(g(\textbf{y})) = h(g(\textbf{y}); \psi)$
  이미지 인코더는 태스크마다 일부의 파라미터를 따로 둬서 학습합니다. similarity가 task의 성질을 결정하는데 중요한 역할을 하게 됩니다. 태스크마다 유사도를 다르게 측정하기 위해서 이미지 인코더에 $\theta_\tau$ 를 넣었습니다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;qa&quot;&gt;Q&amp;amp;A&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;“매칭”이라는 formulation 즉, 하나의 모델로 여러 개의 태스크를 예측 할 수 있나요?
    &lt;ul&gt;
      &lt;li&gt;공통된 임베딩 스페이스에서 보편적으로 적용 가능합니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;새로운 태스크가 주어졌을 때, 예측을 할 수 있나요?
    &lt;ul&gt;
      &lt;li&gt;태스크 특화 파라미터를 두었기 때문에 이것을 통해서 적응이 가능합니다. 특히, 태스크 특화 파라미터를 0.3%만 두고 나머지는 공통 파라미터로 사용하여 과적합을 방지하였습니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;3-visual-token-matching-vtm-network&quot;&gt;&lt;strong&gt;3. Visual Token Matching (VTM) network&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Universal_Few-shot_Learning_of_Dense_Prediction_Tasks_with_Visual_Token_Matching/vtm-network.png&quot; alt=&quot;vtm&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ViT Vision Transformer ViT 기반
    &lt;ul&gt;
      &lt;li&gt;토큰 형태로 특징을 뽑게 됩니다. 이미지가 주어지면 로컬한 패치가 아니라 self-attention이라는 레이어를 통해서 특징을 추출합니다. 이 특징을 추출하면 토큰이 여러 개 나오게 되고, 토큰 하나하나를 매칭에 사용합니다. 토큰 형태로 변환을 하는 것이 아니라 토큰으로 특징을 나타낸다는 의미입니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Image encoder : $f_\tau(\textbf{x}) =f(\textbf{x};\theta, \theta_\tau)$
    &lt;ul&gt;
      &lt;li&gt;ViT 기반.  self-supervised로 사전학습이 된 BEiT로 fine tuning을 진행하였습니다. 어떠한 레이블을 사용하지 않고 이미지만을 이용하여 학습이 되어 있기 때문에 특정한 태스크로 편향되어있지 않습니다.&lt;/li&gt;
      &lt;li&gt;Bias 파라미터를 태스크 특화 파라미터로 튜닝을 하는 “Bias Tuning”을 도입하였습니다. Bias가 큰 비중을 차지 하지는 않지만, 트랜스포머의 behavior를 효율적으로 바꾼다는 것이 알려져 있어서 사용하였습니다. 트랜스포머를 썼을 때 컴퓨팅 자원이 많이 드는 편입니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Label encoder : $g(\textbf{y}) =g(\textbf{y};\phi) $
    &lt;ul&gt;
      &lt;li&gt;ViT 기반. 레이블에 대한 학습은 되어있지 않았기 때문에 처음부터 학습하였습니다. 이미지 인코더와는 다르게 모든 파라미터가 태스크에 상관없이 공유됩니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Matching module&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Universal_Few-shot_Learning_of_Dense_Prediction_Tasks_with_Visual_Token_Matching/token-matching-module.png&quot; alt=&quot;token-matching-module&quot; /&gt;&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Multi-Head Attention MHA의 유사도를 계산한 다음, 취합하게 됩니다. 각 head가 하나의 similarity를 의미하게 됩니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Label decoder : $h(g(\textbf{y})) = h(g(\textbf{y}); \psi)$&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Universal_Few-shot_Learning_of_Dense_Prediction_Tasks_with_Visual_Token_Matching/fusion-block.png&quot; alt=&quot;fusion-block&quot; /&gt;&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;컨볼루션 디코더 기반.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;정리) 이미지 인코더와 레이블 인코더는 비전 트랜스포머로 구성이 되고, 여기서 비전 트랜스포머는 인코딩을 하면서 중간에 피쳐를 계층적으로 뽑습니다. 계층적으로 뽑힌 피쳐는 각 레벨에서  MHA라는 모듈을 통해 매칭이 됩니다. 각각 매칭이 된 피쳐들을 컨볼루션 디코더가 점진적으로 퓨전을 하면서 디코딩을 하게 됩니다.&lt;/p&gt;

&lt;h3 id=&quot;4-training&quot;&gt;&lt;strong&gt;4. Training&lt;/strong&gt;&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;스테이지 1 : episodic meta training&lt;/p&gt;

    &lt;p&gt;\(\underset{f_\tau, g, h, \sigma}{\min} \mathbb{E}_{S_\tau, Q_\tau \sim D_{train}}\left[ \frac{1}{\vert Q_\tau \vert} \underset {(X^q, Y^q)\in Q_\tau}{\sum} L(Y^q, F(X^q;S_{\tau}))\right]\)&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;퓨삿 러닝에서 자주 쓰이는 기법으로 다양한 태스크로 구성된 여러 개의 에피소드를 통해서 학습 모델이 퓨샷 러닝을 진행합니다.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Universal_Few-shot_Learning_of_Dense_Prediction_Tasks_with_Visual_Token_Matching/equ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

        &lt;ul&gt;
          &lt;li&gt;다양한 태스크로 구성된 $D_{train}$ 데이터셋에서 특정한 에피소드에서 특정 태스크에 해당하는 이미지와 레이블 페어를 뽑습니다. 이 페어를 서포트셋와 쿼리셋으로 나눕니다. 여기서 서포트셋은 이 모델이 레이블을 보게 되고 쿼리셋은 레이블을 보지 않고 이미지만 보고 맞추게 됩니다. 이렇게 서포트셋과 쿼리셋을 나누는 이유는 나중에 훈련이 끝나고 퓨샷러닝의 모델을 적용했을 때 레이블이 다르더라도 서포트셋에 있는 정보를 잘 사용해서 쿼리를 맞추도록 하기 위함입니다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;$F(X^q;S_{\tau}))$
        &lt;ul&gt;
          &lt;li&gt;서포트셋을 보고 쿼리셋을 맞추도록 로스함수를 계산합니다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;$\underset{f_\tau, g, h, \sigma}{\min}$
        &lt;ul&gt;
          &lt;li&gt;이러한 에피소드를 여러 가지 태스크에 대해서 많이 반복하여 퓨샷 러닝 모델이 서포트를 보고 쿼리를 맞출 수 있도록 학습하게 됩니다. 이 단계에서는 모든 파라미터를 전부 메타 훈련합니다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Universal_Few-shot_Learning_of_Dense_Prediction_Tasks_with_Visual_Token_Matching/equ7.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;&lt;!-- $\underset{\theta_\tau}{\min} \mathbb{E}_{\tilde{S}, \tilde{Q} \sim S_{\tau_{train}}}\left[ \frac{1}{\vert \tilde{Q} \vert} \underset {(X^q, Y^q)\in \tilde{Q}}{\sum} L(Y^q, F(X^q;\tilde{S}))\right], S_{\tau_{test}} = \tilde{S} \cup \tilde{Q} $ --&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;스테이지 2  : task-specific fine-tuning
굉장히 다르게 생긴 레이블을 가지고 퓨샷러닝을 하면 적응, 일반화가 되지 않는 모습을 확인하였습니다. 더 피팅을 잘 하기 위해 task-specific fine-tuning을 진행합니다.
스테이지 1과 다른 점 두 가지
 1) 다양한 태스크로 구성된 데이터셋이 아니라 실제로 맞추고자 하는 few-shot 데이터를 이용하여 학습하였습니다. 파인튜닝 스테이지는 트레이닝이 다 끝나고 어떤 퓨샷러닝을 풀고 싶을 때 적용하기 때문에 사용할 수 있는 데이터가 한정적입니다. 그 적은 데이터를 서포트셋과 쿼리셋으로 나누어서 파인튜닝을 진행합니다.
 2) 모든 파라미터를 튜닝하는 것이 아니라 task-specific 파라미터만 튜닝합니다. 따라서 나머지 파라미터는 메타 트레이닝 떄 학습이 다 되어있고, task-specific 파라미터만 학습을 함으로써 굉장히 적은 데이터만으로도 효율적으로 학습을 진행할 수 있습니다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;이 단계까지 마무리하고나면 새롭게 보는 태스크도 서포트 셋만 가지고 잘 예측을 하게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Universal_Few-shot_Learning_of_Dense_Prediction_Tasks_with_Visual_Token_Matching/training.png&quot; alt=&quot;training&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;4-experiment&quot;&gt;&lt;strong&gt;4. Experiment&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&quot;experiment-setup&quot;&gt;&lt;strong&gt;Experiment setup&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Dataset : Taskonomy dataset 물체에 대한 사진으로 많은 종류의 태스크 레이블이 존재하는 데이터셋입니다.&lt;/li&gt;
  &lt;li&gt;Dense prediction tasks
    &lt;ol&gt;
      &lt;li&gt;Semantic Segmentation SS&lt;/li&gt;
      &lt;li&gt;Euclidean Distance ED&lt;/li&gt;
      &lt;li&gt;Texture Edge TE&lt;/li&gt;
      &lt;li&gt;2D Keypoints K2&lt;/li&gt;
      &lt;li&gt;Reshading RS&lt;/li&gt;
      &lt;li&gt;Surface Normal SN&lt;/li&gt;
      &lt;li&gt;Zbuffer Depth ZD&lt;/li&gt;
      &lt;li&gt;Occlusion Edge OE&lt;/li&gt;
      &lt;li&gt;3D keypoints K3&lt;/li&gt;
      &lt;li&gt;Principal Curvature PC&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;baseline
    &lt;ul&gt;
      &lt;li&gt;전체 데이터를 사용하는 fully-supervised 베이스라인
        &lt;ul&gt;
          &lt;li&gt;DPT : 각 태스크를 독립적으로 보는 베이스라인&lt;/li&gt;
          &lt;li&gt;InvPT : 모든 태스크를 한 번에 학습하는 멀티태스크 러닝 베이스라인&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;퓨샷러닝 베이스라인: SS를 위한 퓨샷러닝 베이스라인
        &lt;ul&gt;
          &lt;li&gt;HSNet&lt;/li&gt;
          &lt;li&gt;VAT&lt;/li&gt;
          &lt;li&gt;DGPNet&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Evaluation Metric&lt;br /&gt;
5-fold split
5개의 fold로 split를 하였습니다. 첫번쨰 fold에서는 SS와 SN를 퓨샷러닝하는 것을 목적으로 하고 스테이지 1에서는 SS와 SN을 제외한 나머지 태스크는 메타 트레이닝 학습에 사용하고, SS와 SN은 스테이지 2에서 퓨샷러닝 파인튜닝에 사용하였습니다. 두번째 fold에서는 ED, ZD를 타겟으로 두고 나머지를 트레이닝하면서 이 ED, ZD의 성능을 측정하였습니다. 이렇게 다양한 시나리오에 대해서 모델의 성능을 측정하였습니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;result&quot;&gt;&lt;strong&gt;Result&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Universal_Few-shot_Learning_of_Dense_Prediction_Tasks_with_Visual_Token_Matching/experiment.png&quot; alt=&quot;experiment&quot; /&gt;&lt;/p&gt;

&lt;p&gt;퓨샷러닝을 하는 베이스라인 보다 훨씬 성능이 좋았고, 전체 데이터를 학습하는 fully-supervised 베이스라인과 비슷한 성능을 보였습니다.
기존 퓨샷러닝과는 다르게 다양한 태스크가 있음에도 불구하고 퓨샷으로 일반화를 잘 하고 있다는 것을 보여줍니다.&lt;/p&gt;

&lt;h3 id=&quot;qualitative-result&quot;&gt;&lt;strong&gt;Qualitative Result&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;10-shot 결과&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Universal_Few-shot_Learning_of_Dense_Prediction_Tasks_with_Visual_Token_Matching/experiment2.png&quot; alt=&quot;experiment2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;기존에 있는 특정한 태스크를 위해서 파인튜닝된 퓨샷러닝 베이스라인은 다른 태스크에서는 전혀 예측을 못하고 실패하지만 모든 태스크에 대해서 reasonable하게 예측을 하는 모습을 보여줍니다.&lt;/p&gt;

&lt;h3 id=&quot;more-supervision&quot;&gt;&lt;strong&gt;more supervision&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Universal_Few-shot_Learning_of_Dense_Prediction_Tasks_with_Visual_Token_Matching/experiment3.png&quot; alt=&quot;experiment3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;x축이 이미지와 레이블의 개수이고, y축이 성능입니다. 즉, 데이터를 늘려가면서 성능을 쟀을 때를 나타냅니다.&lt;/p&gt;

&lt;h3 id=&quot;attention-map-visualization&quot;&gt;&lt;strong&gt;Attention map visualization&lt;/strong&gt;&lt;/h3&gt;

&lt;dl&gt;
  &lt;dt&gt;&lt;img src=&quot;../../images/DS503_24S/Universal_Few-shot_Learning_of_Dense_Prediction_Tasks_with_Visual_Token_Matching/attention-map-visualization.png&quot; alt=&quot;attention-map-visualization&quot; /&gt;&lt;/dt&gt;
  &lt;dd&gt;
    &lt;p&gt;이 시각화가 task-specific 파라미터가 매칭에서의 similrity가 태스크마다 잘 모듈레이션 하고 있다는 것을 보여줍니다.&lt;/p&gt;
  &lt;/dd&gt;
&lt;/dl&gt;

&lt;h3 id=&quot;generalization-to-unseen-domains&quot;&gt;&lt;strong&gt;Generalization to unseen domains&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Universal_Few-shot_Learning_of_Dense_Prediction_Tasks_with_Visual_Token_Matching/generalization.png&quot; alt=&quot;generalization&quot; /&gt;
video segmentation task 
첫번째 프레임에 대해서 트래킹을 하고 싶은 오브젝트에 GT마스크를 줍니다. 
나머지 프레임에 대해서 이 사람의 바운더리를 따는 태스크입니다.
처음 프레임을 서포트셋으로 두고 나머지 프레임을 쿼리셋으로 두면 퓨샷러닝으로 포뮬레이션을 할 수 있었습니다.&lt;/p&gt;

&lt;p&gt;완전 다른 태스크, 완전 다른 도메인(아웃도어 신)으로 갔을 떄도 일반화가 잘 되는 것을 확인할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;5-conclusion&quot;&gt;&lt;strong&gt;5. Conclusion&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;limitation&quot;&gt;&lt;strong&gt;Limitation&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Universal_Few-shot_Learning_of_Dense_Prediction_Tasks_with_Visual_Token_Matching/limitation.png&quot; alt=&quot;limitation&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;dl&gt;
      &lt;dt&gt;Channel-wise interaction is not allowed&lt;/dt&gt;
      &lt;dd&gt;채널 간 상호작용을 고려하지 못합니다.&lt;/dd&gt;
    &lt;/dl&gt;

    &lt;p&gt;task마다 다른 channel 개수가 다르기 떄문에 multi-channel을 single-channel로 바꾸어서 다양한 task의 다양한 channel 수를 다룰 수 있게 하였습니다. 하지만 채널 간 강한 상호작용이 있는 task에서는 문제가 될 수 있습니다.&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Applying Taskonomy-pretrained VTM to 6D Pose estimation(predicting nine keypoints of 3D bbox)
        &lt;ul&gt;
          &lt;li&gt;6D Pose estimation task : target 물체의 3D bounding box를 예측&lt;/li&gt;
          &lt;li&gt;GT : 각 꼭짓점을 예측을 하게 되는데 바운딩 박스를 예측해야 하기 때문에 직육면체를 띄어야 합니다. 그래서 한 번에 직육면체를 예측하는 경우가 많습니다.&lt;/li&gt;
          &lt;li&gt;Prediction(16-shot) : 하지만 multi-channel을 single-channel로 다루고 있기 때문에 각 꼭짓점을 독립적으로 예측하기 때문에 예측된 포인트가 다른 예측 포인트에 영향을 주지 않는다. 즉, 직육면체를 띄지 않는다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;이 논문에서는 Visual Token Matching VTM이라는 매칭 기반의 모델을 제시합니다.
이 모델은 dense prediction tasks라고 불리는 이미지에서 픽셀별로 레이블을 예측하는 태스크에 대해서 태스크의 종류와는 상관없이 범용적으로 퓨샷러닝을 할 수 있습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Taskonomy 데이터셋에서 모델 검증을 하였고, 10쌍의 데이터만으로도 다양한 태스크에 학습이 가능하다는 것을 확인하였습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;다양한 도메인, 다양한 태스크에도 잘 적용되었습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;채널 간 상호작용이 강한 태스크에서는 잘 적용이 되지 않아 한계점을 가지고 있습니다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;author-information&quot;&gt;&lt;strong&gt;Author Information&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Author name
    &lt;ul&gt;
      &lt;li&gt;Affiliation&lt;/li&gt;
      &lt;li&gt;Research Topic&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-reference--additional-materials&quot;&gt;&lt;strong&gt;6. Reference &amp;amp; Additional materials&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Github Implementation
    &lt;ul&gt;
      &lt;li&gt;https://github.com/GitGyun/visual_token_matching&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Reference
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.eiric.or.kr/community/webinar_detail.php?Seq=123&amp;amp;totalCnt=114&amp;amp;searchBy&amp;amp;searchWord&amp;amp;SnxFlag&amp;amp;pg=1&quot;&gt;EIRIC 세미나&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</description>
            <pubDate>Wed, 17 Apr 2024 00:00:00 +0900</pubDate>
            <link>http://localhost:4000/Universal_Few-shot_Learning_of_Dense_Prediction_Tasks_with_Visual_Token_Matching.html</link>
            <guid isPermaLink="true">http://localhost:4000/Universal_Few-shot_Learning_of_Dense_Prediction_Tasks_with_Visual_Token_Matching.html</guid>
            
            <category>reviews</category>
            
            
        </item>
        
        <item>
            <title>[CVPR 2023] TryOnDiffusion: A Tale of Two UNets</title>
            <description>&lt;h1 id=&quot;title&quot;&gt;&lt;strong&gt;Title&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;TryOnDiffusion: A Tale of Two UNets&lt;/p&gt;

&lt;h2 id=&quot;1-problem-definition&quot;&gt;&lt;strong&gt;1. Problem Definition&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&quot;task-description&quot;&gt;&lt;strong&gt;Task Description&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Virtual Try-On (VTON)&lt;/strong&gt;, which involves putting &lt;strong&gt;a garment&lt;/strong&gt; on a &lt;strong&gt;particular individual&lt;/strong&gt;, holds crucial significance in contemporary e-commerce and the prospective metaverse. The &lt;strong&gt;key challenge&lt;/strong&gt; lies in preserving &lt;strong&gt;intricate clothes texture details&lt;/strong&gt; along with &lt;strong&gt;the target person’s distinctive features&lt;/strong&gt;. Adapting a garment to different body shapes without modifying patterns is particularly challenging, especially when the body appearance varies significantly.&lt;/p&gt;

&lt;h3 id=&quot;general-related-works&quot;&gt;&lt;strong&gt;General related works&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;In general, when approaching this task, there are often two stages of processing: &lt;strong&gt;cloth warping&lt;/strong&gt; and &lt;strong&gt;warped-cloth blending&lt;/strong&gt;. 
At the beginning, depending on the types of clothes, the corresponding changeable regions of the person image will be removed. Specifically, this mechanism is that if the garment is upper types, the upper regions of the body are omitted, which works the same as the lower types and dresses – the full body types.&lt;/p&gt;

&lt;p&gt;In terms of &lt;strong&gt;warping phases&lt;/strong&gt;, the garment information and body appearance are pushed into several algorithms to learn the relative to predict a transformation matrix supporting to generate the most suitable body-fit warped garment. Subsequently, the warped garment is directly added to the person image which has already been removed from the changeable region, then the final output is generated after the calculation - &lt;strong&gt;warped-cloth blending phases&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;This type of process is quite common in recent VTON:&lt;/p&gt;

&lt;p&gt;&lt;a id=&quot;1&quot;&gt;[1]&lt;/a&gt; : Fele, Benjamin, et al. “C-vton: Context-driven image-based virtual try-on network.” Proceedings of the IEEE/CVF winter conference on applications of computer vision. 2022. &lt;a href=&quot;https://arxiv.org/abs/2212.04437&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&quot;2&quot;&gt;[2]&lt;/a&gt; : Lee, Sangyun, et al. “High-resolution virtual try-on with misalignment and occlusion-handled conditions.” European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022. &lt;a href=&quot;https://arxiv.org/abs/2206.14180&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&quot;3&quot;&gt;[3]&lt;/a&gt; : Choi, Seunghwan, et al. “Viton-hd: High-resolution virtual try-on via misalignment-aware normalization.” Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021. &lt;a href=&quot;https://github.com/shadow2496/VITON-HD&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a id=&quot;4&quot;&gt;[4]&lt;/a&gt; : Xie, Zhenyu, et al. “Gp-vton: Towards general purpose virtual try-on via collaborative local-flow global-parsing learning.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023. &lt;a href=&quot;https://arxiv.org/pdf/2303.13756.pdf&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-motivation&quot;&gt;&lt;strong&gt;2. Motivation&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Almost all the recent previous studies use &lt;strong&gt;explicit wapring&lt;/strong&gt; pharse - mentioned aboved.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In my opinion&lt;/strong&gt;, this is a limitation of the whole process because during the training phase of the warping module the targets, which are used for losses calculation and backpropagation, are extracted from another segmenation model. It means that the explicit warping model performance &lt;strong&gt;is always underbound&lt;/strong&gt; the segmentation model used for human parsing.&lt;/p&gt;

&lt;p&gt;In addition, all the mentioned aboved work with GAN-based or flow-based model.&lt;/p&gt;

&lt;p&gt;Meanwhile, &lt;strong&gt;in general task&lt;/strong&gt; recent diffusion-based approaches are proven to be the current state-of-the-art methods in both quantitative and qualitative assessment.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;“whether or not exist an approach using Diffusion model?”&lt;/strong&gt; - just to guide the reader&lt;/p&gt;

&lt;p&gt;Take advantages of generative ability of Diffusion model, this paper focus on the task of garment detail preservation without effective pose and shape variation, or allow try-on with the desired shape and pose but lack garment details in &lt;strong&gt;a single network.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-method&quot;&gt;&lt;strong&gt;3. Method&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;In this paper, the author propose &lt;strong&gt;a diffusion-based architecture&lt;/strong&gt; that unifies two UNets (referred to as Parallel-UNet), which allows us to preserve garment details and warp the garment for significant pose and body change in &lt;strong&gt;a single network.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;key ideas&lt;/strong&gt; behind Parallel-UNet include: 
1) garment is &lt;strong&gt;warped implicitly&lt;/strong&gt; via a cross attention mechanism, 
2) garment warp and person blend happen as part of a &lt;strong&gt;unified process&lt;/strong&gt; as opposed to a sequence of two separate tasks.&lt;/p&gt;

&lt;p&gt;To find out the idea behind the &lt;strong&gt;implicity warping mechanism&lt;/strong&gt;, we need to focus on the warping argorithm which known as the &lt;a href=&quot;https://en.wikipedia.org/wiki/Thin_plate_spline&quot;&gt;Thin Plate Spline&lt;/a&gt;. This algorithm could easily explain as a &lt;strong&gt;scale and transition opertation&lt;/strong&gt;. In previous study, the warping model will try to predict an tranformation matrix to operate this argorithm on picture space or in RGB space. Meanwhile, to unified two model, this process also could also be operated under latent space by adding condition information supporting the implicit warp phase.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Try_On_Diffusion_A_Tale_of_Two_UNets/1.png&quot; alt=&quot;&quot; /&gt;
&lt;!-- ![Image 1](https://ar5iv.labs.arxiv.org/html/2306.08276/assets/x2.png) --&gt;&lt;/p&gt;

&lt;p&gt;Figure 1: Overall pipeline (top): During preprocessing step, the target person is segmented out of the person image creating “clothing agnostic RGB” image, the target garment is segmented out of the garment image, and pose is computed for both person and garment images. These inputs are taken into 128x128 Parallel-UNet (key contribution) to create the 128x128 try-on image which is further sent as input to the 256x256 Parallel-UNet together with the try-on conditional inputs. Output from  256x256 Parallel-UNet is sent to standard super resolution diffusion to create the 1024x1024 image. The architecture of  128x128 Parallel-UNet is visualized at the bottom, see text for details. The 256x256 Parallel-UNet is similar to the 128 one.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Try_On_Diffusion_A_Tale_of_Two_UNets/2.png&quot; alt=&quot;&quot; /&gt;
&lt;!-- ![Image 2](https://ar5iv.labs.arxiv.org/html/2306.08276/assets/x8.png) --&gt;&lt;/p&gt;

&lt;p&gt;Figure 2: Architecture of 256x256 Parallel-UNet&lt;/p&gt;

&lt;h2 id=&quot;preprocessing-of-inputs&quot;&gt;&lt;strong&gt;Preprocessing of inputs&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Note that clothing-agnostic RGB described in VITON-HD [6] leaks information of the original garment for challenging human poses and loose garments. The authors thus adopt a more aggressive way to remove the garment information. Specifically, they first mask out the whole bounding box area of the foreground person, and then copy-paste the head, hands and lower body part on top of it.&lt;/p&gt;

&lt;p&gt;They use Human Parsing Map and 2D keypoints to extract the non-garment body parts. They also normalize pose keypoints to the range of [0, 1] before inputting them to networks.&lt;/p&gt;

&lt;h2 id=&quot;cascaded-diffusion-models-for-try-on&quot;&gt;&lt;strong&gt;Cascaded Diffusion Models for Try-On&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Their cascaded diffusion models consist of &lt;strong&gt;one base diffusion model&lt;/strong&gt; and &lt;strong&gt;two super-resolution (SR) diffusion models&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The base diffusion model is parameterized as a 128×128 Parallel-UNet. It predicts the 128×128 try-on result, taking in the try-on conditional inputs. 
The 128×128→256×256 SR diffusion model is parameterized as a 256×256 Parallel-UNet. It generates the 256×256 try-on result by conditioning on both the 128×128 try-on result and the try-on conditional inputs at 256×256 resolution. The 256×256→1024×1024 SR diffusion model is parameterized as Efficient-UNet introduced by &lt;a href=&quot;https://arxiv.org/abs/2205.11487&quot;&gt;Imagen&lt;/a&gt;. This stage is a pure super-resolution model, with no try-on conditioning.&lt;/p&gt;

&lt;h2 id=&quot;parallel-unet&quot;&gt;&lt;strong&gt;Parallel-UNet&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;The 128×128 Parallel-UNet can be represented as:&lt;/p&gt;

&lt;p&gt;$\epsilon_t = \epsilon_\theta(z_t,t,c_{tryon},t_{na})$&lt;/p&gt;

&lt;p&gt;where $t$ is the diffusion timestep, $𝐳_t$ is the noisy image corrupted from the ground-truth at timestep $t$, $𝐜_{tryon}$ is the try-on conditional inputs, $𝐭_na$ is the set of noise augmentation levels for different conditional images, and $\epsilon_t$ is predicted noise that can be used to recover the ground-truth from $𝐳_t$.&lt;/p&gt;

&lt;h3 id=&quot;implicit-warping&quot;&gt;&lt;strong&gt;Implicit warping&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;“how can we implement implicit warping in the neural network?”&lt;/em&gt; One natural solution is to use a traditional UNet and concatenate the segmented garment $I_c$ and the noisy image $z_t$ along the channel dimension. However, channel-wise concatenation can not handle complex transformations such as garment warping. This is because the computational primitives of the traditional UNet are spatial convolutions and spatial self attention, and these primitives have strong pixel-wise structural bias.&lt;/p&gt;

&lt;p&gt;Implicit warping using cross attention mechanism between streams of information:&lt;/p&gt;

&lt;p&gt;$Attention(Q,K,V) = softmax(\frac{QK^\top}{d})V$&lt;/p&gt;

&lt;p&gt;where $Q \in \mathbb{R}^{M \times d}, K \in \mathbb{R}^{N \times d}, V \in \mathbb{R}^{N \times d}$ are stacked vectors of query, key and value, $M$ is the number of query vectors, $N$ is the number of key and value vectors and $d$ is the dimension of the vector. 
In this case, the query and key-value pairs come from different inputs.
Specifically, $Q$ is the flattened features of $z_t$ and $K$, $V$ are the flattened features of $I_c$.&lt;/p&gt;

&lt;p&gt;The attention map $\frac{QK^\top}{d_k}$ computed through dot-product tells us the similarity between the target person and the source garment, providing a learnable way to represent correspondence for the try-on task.&lt;/p&gt;

&lt;h3 id=&quot;combining-warp-and-blend-in-a-single-pass&quot;&gt;&lt;strong&gt;Combining warp and blend in a single pass&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Instead of warping the garment to the target body and then blending with the target person as done by prior works, we combine the two operations into a single pass. As shown in Figure 1, they achieve it via two UNets that handle the garment and the person respectively.&lt;/p&gt;

&lt;p&gt;The person and garment poses are necessary for guiding the warp and blend process. They are first fed into the linear layers to compute pose embeddings separately. The pose embeddings are then fused to the person-UNet through the attention mechanism, which is implemented by concatenating pose embeddings to the key-value pairs of each self attention layer. Besides, pose embeddings are reduced along the keypoints dimension using CLIP-style 1D attention pooling, and summed with the positional encoding of diffusion timestep $t$ and noise augmentation levels $t_{na}$. The resulting 1D embedding is used to modulate features for both UNets using &lt;a href=&quot;https://staging.distill.pub/2018/feature-wise-transformations/?utm_campaign=The+Batch&amp;amp;%3Butm_source=hs_email&amp;amp;%3Butm_medium=email&amp;amp;%3Butm_content=2&amp;amp;%3B_hsenc=p2ANqtz-_y7LKn2OW8eVKFWN6aYCjxUI-sOF4aNoqsVlfHqHvZqO66RnPZbAPo4wwMyW2fo5iNqSLEHOGgkqNU2QwzSqK0HJUNdw&amp;amp;ref=dl-staging-website.ghost.io&quot;&gt;FiLM&lt;/a&gt; across all scales.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In my opinion&lt;/strong&gt;, the application of &lt;a href=&quot;https://staging.distill.pub/2018/feature-wise-transformations/?utm_campaign=The+Batch&amp;amp;%3Butm_source=hs_email&amp;amp;%3Butm_medium=email&amp;amp;%3Butm_content=2&amp;amp;%3B_hsenc=p2ANqtz-_y7LKn2OW8eVKFWN6aYCjxUI-sOF4aNoqsVlfHqHvZqO66RnPZbAPo4wwMyW2fo5iNqSLEHOGgkqNU2QwzSqK0HJUNdw&amp;amp;ref=dl-staging-website.ghost.io&quot;&gt;FiLM&lt;/a&gt; is the key things that the most similar to implicit warping mechanism. &lt;strong&gt;(Scale and transition operation in the latent space)&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;4-experiment&quot;&gt;&lt;strong&gt;4. Experiment&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;experiment-setup&quot;&gt;&lt;strong&gt;Experiment setup&lt;/strong&gt;&lt;/h3&gt;
&lt;h4 id=&quot;dataset&quot;&gt;&lt;strong&gt;Dataset&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;The authors collect a paired training dataset of 4 Million samples. Each sample consists of two images of the same person wearing the same garment in two different poses. For test, they collect 6K unpaired samples that are never seen during training. Each test sample includes two images of different people wearing different garments under different poses. Both training and test images are cropped and resized to 1024x1024 based on detected 2D human poses. Our dataset includes both men and women captured in different poses, with different body shapes, skin tones, and wearing a wide variety of garments with diverse texture patterns. In addition, we also provide results on the &lt;a href=&quot;https://github.com/shadow2496/VITON-HD&quot;&gt;VITON-HD&lt;/a&gt; dataset.&lt;/p&gt;
&lt;h4 id=&quot;evaluation-metric&quot;&gt;&lt;strong&gt;Evaluation Metric&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;They compute Frechet Inception Distance (FID) and Kernel Inception Distance (KID) as evaluation metrics.&lt;/p&gt;

&lt;h4 id=&quot;comparison-with-other-methods&quot;&gt;&lt;strong&gt;Comparison with other methods.&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;They compare their approach to three methods: &lt;a href=&quot;https://arxiv.org/abs/2101.02285&quot;&gt;TryOnGAN&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2207.09161&quot;&gt;SDAFN&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2206.14180&quot;&gt;HR-VITON&lt;/a&gt;. For fair comparison, they re-train all three methods on our 4 Million samples until convergence. Without re-training, the results of these methods are worse. Released checkpoints of SDAFN and HR-VITON also require layflat garment as input, which is not applicable to our setting. The resolutions of the related methods vary, and they present each method’s results in their native resolution: SDAFN’s at 256x256, TryOnGAN’s at 512x512 and HR-VITON at 1024x1024.&lt;/p&gt;

&lt;h3 id=&quot;result&quot;&gt;&lt;strong&gt;Result&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;The proposed models surpassed all the previous methods quantitatively.&lt;/p&gt;

&lt;h4 id=&quot;quantitative-result&quot;&gt;&lt;strong&gt;Quantitative Result&lt;/strong&gt;&lt;/h4&gt;

&lt;!-- ![Image3](https://ar5iv.labs.arxiv.org/html/2306.08276/assets/x3.png) --&gt;
&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Try_On_Diffusion_A_Tale_of_Two_UNets/3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;!-- ![Image4](https://ar5iv.labs.arxiv.org/html/2306.08276/assets/x4.png) --&gt;
&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Try_On_Diffusion_A_Tale_of_Two_UNets/4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Try_On_Diffusion_A_Tale_of_Two_UNets/5.png&quot; alt=&quot;&quot; /&gt;
&lt;!-- ![Image5](https://ar5iv.labs.arxiv.org/html/2306.08276/assets/x7.png) --&gt;&lt;/p&gt;

&lt;h2 id=&quot;5-conclusion&quot;&gt;&lt;strong&gt;5. Conclusion&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;The proposed method that allows to synthesize try-on given an image of a person and an image of a garment, are overwhelmingly better than state-of-the-art, both in the quality of the warp to new body shapes and poses, and in the preservation of the garment.&lt;/p&gt;

&lt;p&gt;A novel architecture Parallel-UNet, where two UNets are trained in parallel and one UNet sends information to the other via cross attentions&lt;/p&gt;

&lt;p&gt;This project only focus on &lt;strong&gt;upper clothes.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In my opinion,&lt;/strong&gt; the key idea lies on cross-attention and FiLM - Feature-wise transformations. The used of implicited warping removed the underbound of previous methods.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;author-information&quot;&gt;&lt;strong&gt;Author Information&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;(Hoang) Phuong Dam
    &lt;ul&gt;
      &lt;li&gt;AutoID LAB, School of Computing&lt;/li&gt;
      &lt;li&gt;Image Generation, Virtual TryOn&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-reference--additional-materials&quot;&gt;&lt;strong&gt;6. Reference &amp;amp; Additional materials&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Please write the reference. If paper provides the public code or other materials, refer them.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Github Implementation of Tryon Diffusion: &lt;a href=&quot;https://github.com/tryonlabs/tryondiffusion&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Reference:&lt;/p&gt;

    &lt;p&gt;&lt;a id=&quot;1&quot;&gt;[1]&lt;/a&gt; : Fele, Benjamin, et al. “C-vton: Context-driven image-based virtual try-on network.” Proceedings of the IEEE/CVF winter conference on applications of computer vision. 2022. &lt;a href=&quot;https://arxiv.org/abs/2212.04437&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;

    &lt;p&gt;&lt;a id=&quot;2&quot;&gt;[2]&lt;/a&gt; : Lee, Sangyun, et al. “High-resolution virtual try-on with misalignment and occlusion-handled conditions.” European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022. &lt;a href=&quot;https://arxiv.org/abs/2206.14180&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;

    &lt;p&gt;&lt;a id=&quot;3&quot;&gt;[3]&lt;/a&gt; : Choi, Seunghwan, et al. “Viton-hd: High-resolution virtual try-on via misalignment-aware normalization.” Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2021. &lt;a href=&quot;https://github.com/shadow2496/VITON-HD&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;

    &lt;p&gt;&lt;a id=&quot;4&quot;&gt;[4]&lt;/a&gt; : Xie, Zhenyu, et al. “Gp-vton: Towards general purpose virtual try-on via collaborative local-flow global-parsing learning.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023. &lt;a href=&quot;https://arxiv.org/pdf/2303.13756.pdf&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;

    &lt;p&gt;&lt;a id=&quot;5&quot;&gt;[5]&lt;/a&gt; : Bai, Shuai, et al. “Single stage virtual try-on via deformable attention flows.” European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022. &lt;a href=&quot;https://arxiv.org/abs/2207.09161&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;

    &lt;p&gt;&lt;a id=&quot;6&quot;&gt;[6]&lt;/a&gt; : Lewis, Kathleen M., Srivatsan Varadharajan, and Ira Kemelmacher-Shlizerman. “Tryongan: Body-aware try-on via layered interpolation.” ACM Transactions on Graphics (TOG) 40.4 (2021): 1-10. &lt;a href=&quot;https://arxiv.org/abs/2101.02285&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;

    &lt;p&gt;&lt;a id=&quot;7&quot;&gt;[7]&lt;/a&gt; : Dumoulin, Vincent, et al. “Feature-wise transformations.” Distill 3.7 (2018): e11. &lt;a href=&quot;https://staging.distill.pub/2018/feature-wise-transformations/?utm_campaign=The+Batch&amp;amp;%3Butm_source=hs_email&amp;amp;%3Butm_medium=email&amp;amp;%3Butm_content=2&amp;amp;%3B_hsenc=p2ANqtz-_y7LKn2OW8eVKFWN6aYCjxUI-sOF4aNoqsVlfHqHvZqO66RnPZbAPo4wwMyW2fo5iNqSLEHOGgkqNU2QwzSqK0HJUNdw&amp;amp;ref=dl-staging-website.ghost.io&quot;&gt;Link&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</description>
            <pubDate>Wed, 17 Apr 2024 00:00:00 +0900</pubDate>
            <link>http://localhost:4000/Try_On_Diffusion_A_Tale_of_Two_UNets.html</link>
            <guid isPermaLink="true">http://localhost:4000/Try_On_Diffusion_A_Tale_of_Two_UNets.html</guid>
            
            <category>reviews</category>
            
            
        </item>
        
        <item>
            <title>[ICLR 2024] Training Diffusion Models With Reinforcement Learning</title>
            <description>&lt;h1 id=&quot;ddpotraining-diffusion-models-with-reinforcement-learning&quot;&gt;&lt;strong&gt;[DDPO]Training Diffusion Models with Reinforcement Learning&lt;/strong&gt;&lt;/h1&gt;

&lt;h2 id=&quot;1-problem-definition--motivation&quot;&gt;&lt;strong&gt;1. Problem Definition &amp;amp; Motivation&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Diffusion probabilistic models, 일반적으로 Diffusion model라 칭하는 모델이 최근 Image, Video, Audio, Drug/Material Design, Continuous Control 등 다양한 분야에서 두각을 보이고 있다.&lt;/p&gt;

&lt;p&gt;Diffusion model은 노이즈를 점진적으로 추가하는 forward process 와 이를 역으로 되돌리는 reverse process, 즉 sequential denoising process를 적용함으로서, 간단한 prior distribution을 target distribution으로 변환시킨다.
이를 학습하는 데에는, Maximum likelihood estimation에 기반해 Evidence Lower Bound(ELBO*)등의 Trick을 이용해 Variational Lower Bound를 Maximize하는 방식을 사용한다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;ELBO*:&lt;br /&gt;
$\log p_ \theta(\mathbf{x}) \geq  \mathbb{E}_ {q_\phi}  \left[\frac{\log p_ \theta(\mathbf{z},\mathbf{x})}{\log q_ \phi(\mathbf{z}|\mathbf{x})}\right]$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Limitations:&lt;/strong&gt;
하지만 이러한 Likelihood 기반의 학습 방식이 가지고 있는 여러 한계가 존재했다. 
먼저, Likelihood를 직접 최적화 하는 것이 생성된 샘플의 품질 향상과는 직결되지 않는다는 단점이 존재했고(Nichol &amp;amp; Dhariwal, 2021; Kingma et al., 2021), 이를 계산하기 위해 많은 양의 sampling과 복잡한 계산이 필요했다. 이에, 하기 논문 (Ho et al., 2020; Denoising Diffusion Probabilistic Models[DDPM])의 등장으로 부터, 대부분의 Diffusion-based 모델들은 직접적으로 likelihood를 maximize하는 것이 아닌 아래와 같은 Approximation(Denoising Objective)을 minimize하는 방식으로 학습하였다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Modified Objective Function of Diffusion Models[DDPM]&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$L_ {\text {simple }}(\theta):=\mathbb{E}_ {t, \mathbf{x}_ 0, \boldsymbol{\epsilon}}\left[\left\vert\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_ \theta\left(\sqrt{\bar{\alpha}_ t} \mathbf{x}_ 0+\sqrt{1-\bar{\alpha}_ t} \boldsymbol{\epsilon}, t\right)\right\vert^2\right]$&lt;/p&gt;

&lt;p&gt;직접적으로 Diffusion model구조의 data distribution matching에서 Likelihood 연산을 진행하는 것은 계산량이 너무 크기 때문에 (Intractable), 이 논문에서는 denoising process를 &lt;strong&gt;multi-step decision-making task&lt;/strong&gt;로 바라보고, 강화학습의  &lt;strong&gt;Policy gradient&lt;/strong&gt; 방법을 적용하는 방식 DDPO(Denoising Diffusion Policy Optimization)을 제안한다. 이 모델은  &lt;strong&gt;black-box reward function&lt;/strong&gt;과 함께, &lt;strong&gt;Downstream task&lt;/strong&gt;에 대한 Likelihood 최적화가 가능함을 보였다.&lt;/p&gt;

&lt;p&gt;정리하자면, RL algorithm으로 Diffusion Network를 학습시키는 방식을 제안한 것이다.&lt;/p&gt;

&lt;h2 id=&quot;2-preliminaries&quot;&gt;&lt;strong&gt;2. Preliminaries&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;21-diffusion-models&quot;&gt;2.1 Diffusion Models&lt;/h3&gt;
&lt;p&gt;(Ho et al., 2020)등의 이전 논문에서, conditional diffusion probabilistic model 은 아래와 같이 정의되며,&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$p(x_ {0}\vert c)$: sample $x_ 0$의 분포, corresponding context $c$&lt;/li&gt;
  &lt;li&gt;$q(x_ t\vert x_ {t-1})$: Markovian forward process (점진적 노이즈 추가)&lt;/li&gt;
  &lt;li&gt;$\mu_ \theta(x_ t, c, t)$: Reversing forward process (노이즈 제거)에 사용되는 neural net&lt;/li&gt;
  &lt;li&gt;$\tilde {\mu}$: Posterial mean of forward process. $x_ 0, x_ t$의 가중평균.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Training은 Log-likelihood 의 Variational lower bound를 maximize함으로서 유도된, 아래 목적식을 최적화 함으로써 진행된다.&lt;/p&gt;

&lt;p&gt;$\mathcal{L}_ {\mathrm{DDPM}}(\theta)=\mathbb{E}_ {\left(\mathbf{x}_ 0, \mathbf{c}\right) \sim p\left(\mathbf{x}_ 0, \mathbf{c}\right), t \sim  \mathcal{U}{0, T}, \mathbf{x}_ t \sim q\left(\mathbf{x}_ t \mid  \mathbf{x}_ 0\right)}\left[\left\vert\tilde{\boldsymbol{\mu}}\left(\mathbf{x}_ 0, t\right)-\boldsymbol{\mu}_ \theta\left(\mathbf{x}_ t, \mathbf{c}, t\right)\right\vert^2\right]$&lt;/p&gt;

&lt;p&gt;Sampling은 random $x_ T \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$ 로 부터, Reverse process $p_ \theta(x_ {t-1} \vert x_ t, c)$를 거쳐 sample $x_ 0$를 얻어내는 방식으로 진행된다.&lt;/p&gt;

&lt;h3 id=&quot;22-markov-decision-process-and-reinforcement-learning&quot;&gt;2.2 Markov Decision Process and Reinforcement Learning&lt;/h3&gt;

&lt;p&gt;마르코프 결정 과정(Markov Decision Process, MDP)은 순차적 의사 결정 문제(sequential decision-making problem)를 수학적으로 정형화한 모델로,  다음과 같은 요소들로 정의된다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$\mathcal{S}:$ 상태 공간(state space)&lt;/li&gt;
  &lt;li&gt;$\mathcal{A}:$ 행동 공간(action space)&lt;/li&gt;
  &lt;li&gt;$\mathcal{\rho_ 0}:$ 초기 상태 분포(initial state distribution)&lt;/li&gt;
  &lt;li&gt;$\mathcal{P}:$ 전이 커널(transition kernel)&lt;/li&gt;
  &lt;li&gt;$\mathcal{R}:$ 보상 함수(reward function)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;MDP에서 에이전트(agent)는 각 타임스텝(timestep) $t$ 마다 현재 상태(state) $s_ t \in \mathcal{S}$를 관측하고, 정책(policy) $\pi(a\vert s)$에 따라 행동(action) $a_ t \in \mathcal{A}$를 선택한다.&lt;/p&gt;

&lt;p&gt;그 후 환경으로부터 보상(reward) $R(s_ t, a_ t)$을 받고, 전이 커널 $P(s_ {t+1}\vert s_ t, a_ t)$에 따라 다음 상태 $s_{t+1}$로 전이한다. 이런 식으로 에이전트가 환경과 상호작용하며 생성되는 상태-행동의 시퀀스 $\tau = (s_ 0, a_ 0, s_ 1, a_ 1, …, s_ T, a_ T)$를 트래젝토리(trajectory)라고 한다.&lt;/p&gt;

&lt;p&gt;강화학습(Reinforcement Learning, RL)의 목표는 에이전트가 정책 $\pi$를 따라 행동할 때 얻게 되는 기대 누적 보상(expected cumulative reward)을 최대화 하는 것이다. 이는 아래와 같은 수식을 Maximize 함으로서 이루어진다.&lt;/p&gt;

\[J_ {RL}(\pi) = \mathbb{E}_ {\tau \sim p(\tau\vert\pi)}\left[\sum_ {t=0}^T R(s_ t, a_ t)\right]\]

&lt;h2 id=&quot;3-methods&quot;&gt;&lt;strong&gt;3. Methods&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;denoising-diffusion-policy-optimization&quot;&gt;Denoising Diffusion Policy Optimization&lt;/h3&gt;

&lt;p&gt;먼저, pre-trained or random initialize된 Diffusion model이 존재한다고 가정하자. Diffusion model은 sample distribution $p_ \theta (x_ 0 \vert c)$ (context-conditional distribution)을 유도하며, Denoising diffusion RL의 목적식은 아래와 같은 reward signal $r$을 정의된 sample과 context 하에서 maximize하는 것이다.&lt;/p&gt;

&lt;p&gt;$\mathcal{J}_ {\text  {DDRL }}(\theta)=\mathbb{E}_ {\mathbf{c}  \sim p(\mathbf{c}), \mathbf{x}_ 0 \sim p_ \theta\left(\mathbf{x}_ 0 \mid  \mathbf{c}\right)}\left[r\left(\mathbf{x}_ 0, \mathbf{c}\right)\right]$&lt;/p&gt;

&lt;p&gt;기본적인 diffusion training에서 크게 벗어나지 않으면서, $\mathcal{J}_ {\text {DDRL}}$을 maximize 하는 방법론으로는 online RL 방식에서 sampling과 training을 반복하는 One-step MDP인 reward-weighted regression (Peters &amp;amp; Schaal, 2007)이라는 방법론이 존재하나, $\pi$에 대한 KL Divergence term이 존재해서, 명확히 말하자면 이는 Optimality에 도달하지 못한다.&lt;/p&gt;

&lt;p&gt;DDPO에서는 DPOK(Diffusion policy optimiation with KL regularization, 유사한 다른 논문)와 같은 Multi-step MDP formulation을 적용한다.&lt;/p&gt;

&lt;p&gt;$\begin{array}{lrr}\mathbf{s}_ t \triangleq\left(\mathbf{c}, t, \mathbf{x}_ t\right) &amp;amp;  \pi\left(\mathbf{a}_ t \mid  \mathbf{s}_ t\right) \triangleq p_ \theta\left(\mathbf{x}_ {t-1}  \mid  \mathbf{x}_ t, \mathbf{c}\right) &amp;amp; P\left(\mathbf{s}_ {t+1}  \mid  \mathbf{s}_ t, \mathbf{a}_ t\right) \triangleq\left(\delta_ {\mathbf{c}}, \delta_ {t-1}, \delta_ {\mathbf{x}_ {t-1}}\right) \\mathbf{a}_ t \triangleq  \mathbf{x}_ {t-1}  &amp;amp;  \rho_ 0\left(\mathbf{s}_ 0\right) \triangleq\left(p(\mathbf{c}), \delta_ T, \mathcal{N}(\mathbf{0}, \mathbf{I})\right) &amp;amp; R\left(\mathbf{s}_ t, \mathbf{a}_ t\right) \triangleq  \begin{cases}r\left(\mathbf{x}_ 0, \mathbf{c}\right) &amp;amp;  \text  { if } t=0 \0 &amp;amp;  \text  { otherwise }\end{cases}\end{array}$&lt;/p&gt;

&lt;p&gt;식에 대해 차례로 설명하자면,&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;context $c$, diffusion index $t$, $t$ 번째 step의 noised sample $x_ t$의 tuple -&amp;gt; state $s_ t$ ; Denoising Network가 받게되는 현재 상태.&lt;/li&gt;
  &lt;li&gt;denoising 을 진행하는 context conditioned reverse process를 -&amp;gt; policy $\pi(a_ t \vert s_ t)$ ; Denoising을 하는 주체(reverse process)이자, 학습 대상이므로 이를 Policy로 설정한다.&lt;/li&gt;
  &lt;li&gt;Dirac Delta distribution $\delta_ c, \delta_ {t-1}, \delta_ {x_ {t-1}}$의 튜플 -&amp;gt; Transition probability ; 여기서 Dirac Delta는 MDP transition을 Deterministic 하게 (동일한 state, action이 주어질 시 다음 state가 동일하게 정해지게) 유도하기 위한 트릭이라고 이해하면 된다. (이를 통해 최종 state에서 termination 되게 된다.)&lt;/li&gt;
  &lt;li&gt;$x_ {t-1}$ -&amp;gt; action $a_ t$ ; Policy인 reverse process에서 뽑아내는 $x_ {t-1}$이 action과 같은 역할이 된다.&lt;/li&gt;
  &lt;li&gt;context probability $p(c)$, $\delta_T$, Multivariate standard normal distribution 의 tuple -&amp;gt; initial state distribution; 이는 시작지점에 대한 확률분포로 Diffusion Process 시작지점으로서 생각할 수 있다.&lt;/li&gt;
  &lt;li&gt;보상함수 $R(s_ t, a_ t)$는 denoised 된 step $t$에 도달하는 경우에만 주는 것으로 정의한다. 원본 데이터에 얼마나 가까운지로 보상을 준다고 생각하면 된다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;복잡해 보이나, 핵심은 Diffusion model에서 사용되는 component들을 Multi-step MDP의 형태로 formulate 했다는 점이다. 즉 Diffusion Model 의 큰 틀은 변하지 않지만 보는 방식을 다르게 했다는 것이다.&lt;/p&gt;

&lt;p&gt;이런 방식으로 Diffusion component들을 MDP로 변환하면, log-likelihood (여기서는 Policy)를 구할 수 있음과 더불어, diffusion model 파라미터에 대한 gradient(여기서는 Policy Gradient)도 구할 수 있게 된다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Policy gradient estimation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Likelihood, 그리고 likelihood의 gradient에 대해 직접적으로 access가 가능하게 되면서, $\nabla_ \theta \mathcal{J}_ {\text{DDRL}}$에 대한 Monte Carlo estimation을 진행할 수 있다.&lt;/p&gt;

&lt;p&gt;Sampling과 parameter update를 통해 trajectories ${x_ T, x_ {T-1}, … , x_ 0}$ 를 수집하고 Policy gradient 의 대표적인 알고리즘 REINFORCE(Williams, 1992; Mohamed et al., 2020)을 사용함으로서 아래와 같은 estimation을 얻을 수 있다.&lt;/p&gt;

&lt;p&gt;$\nabla_ \theta  \mathcal{J}_ {\mathrm{DDRL}}=\mathbb{E}\left[\sum_ {t=0}^T \nabla_ \theta  \log p_ \theta\left(\mathbf{x}_ {t-1}  \mid  \mathbf{x}_ t, \mathbf{c}\right) r\left(\mathbf{x}_ 0, \mathbf{c}\right)\right]$&lt;/p&gt;

&lt;p&gt;위 식은 REINFORCE algorithm에서 context condition이 추가된 형태다. 논문에서는 이를 DDPO-SF로 명시한다. (Score function Policy gradient estimator)&lt;/p&gt;

&lt;p&gt;하지만 MC approach의 특성상 data collection마다 한 step의 optimization만 가능하기 때문에, 여러번의 optimization을 사용하기 위해서는 importance sampling estimator를 적용할 수 있다.&lt;/p&gt;

&lt;p&gt;$\nabla_ \theta  \mathcal{J}_ {\mathrm{DDRL}}=\mathbb{E}\left[\sum_ {t=0}^T \frac{p_ \theta\left(\mathbf{x}_ {t-1}  \mid  \mathbf{x}_ t, \mathbf{c}\right)}{p_ {\theta_ {\text  {old }}}\left(\mathbf{x}_ {t-1}  \mid  \mathbf{x}_ t, \mathbf{c}\right)}  \nabla_ \theta  \log p_ \theta\left(\mathbf{x}_ {t-1}  \mid  \mathbf{x}_ t, \mathbf{c}\right) r\left(\mathbf{x}_ 0, \mathbf{c}\right)\right]$&lt;/p&gt;

&lt;p&gt;이를 논문에서는 DDPO-IS (Importance sampling Policy gradient estimator) 라고 명시한다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Importance sampling 방식은, offline policy  혹은 offline-RL등에서 주로 사용하는 방법으로, update시에 변화하는 policy에 대해, 다른 policy에서 얻은 정보를 사용하면서도 optimality를 유지하기 위한 방법이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;다만 이 방법을 사용했을 때, policy 가 너무 많이 Update 되어 버리면(old policy와 current policy의 차이가 많이 나면) estimation에 문제가 생길 수 있기 때문에 (Importance sampling weight가 explode 혹은 vanish), Implementation에서는 TRPO에서의 Trust region이나 PPO의 clipping등의 방식을 적용해야 한다. 논문에서는 clipping method를 사용하였다.&lt;/p&gt;

&lt;h3 id=&quot;reward-functions-for-text-to-image-diffusion&quot;&gt;Reward functions for text-to-image diffusion&lt;/h3&gt;
&lt;p&gt;DDPO Equation을 살펴보면, 대부분의 component들은 Diffusion component들에서 명시되어 있으나, 구체적으로 어떤 방식으로 Reward를 줄 지에 대해서는 결정이 필요하다. 논문에서는 세 가지 방식의 reward를 사용했다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Compressibility and Incompressibility&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Text-to-image diffusion model은 학습시, text-image의 co-occurence로 학습을 진행하기 때문에, prompt를 이용해 file size를 명시하는 것이 어렵다. 주로 image에 대한 caption에 file size(kb, mb, gb)를 명시하지 않으므로 (EX: “고양이가 생선을 먹고있는 사진” 으로 파일 사이즈 없이 captioning이 달리므로)모델이 파일크기에 대한 정보를 수급하지 못한다. 이에 논문에서는 diffusion model의 sample들을 512x512로 고정하고, Reward로 file-size를 조정함으로서 얼마나 원본이미지의 중요한 성질을 유지하며 잘 압축시켰는지(compressibility)혹은 그 반대로 얼마나 원본 이미지 보다 더 세밀하게 구현했는지(Incompressibility)등을 하나의 지표로 사용하였다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. Aesthetic Quality&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;두 번째로 사용한 방식은, LAION aesthetics predictor를 reward function으로 사용해, 모델에서 생성해낸 이미지의 예술성 척도를 Reward로 사용해, 더 aesthetic 한 이미지를 생성하는 objective로 적용하였다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;3. Automated Prompt Alignment with Vision-Language Models&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;마지막으로, 인간이 직접 reward를 labeling하는 방식을 적용하면 RLHF(Reinforcement Learning with human feedback)방식이 되나, 논문에서는 이러한 Labeling 과정을 vision-language model(VLM)에서의 feedback으로 대체하였다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Training_Diffusion_Models_With_Reinforcement_Learning/1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 figure가 논문에서 사용한 VLM의 예시인데, 먼저 prompt(“a monkey washing dishes”)를 Diffusion model에 입력해 이미지를 생성한다. 그 다음 LLaVA(Liu et al., 2023)모델에 이미지와  이미지에 대한 짧은 설명을 달라는 prompt를 함께 제공함으로서, short description을 얻는다. 처음에 입력했던 prompt와 요약된 설명의 bert similarity를 계산해 이를 reward로 사용한다.&lt;/p&gt;

&lt;p&gt;직관적으로 생각해 보면, prompt를 통해 생성한 이미지가 과연 그 prompt 에 맞춰 제대로 이미지를 생성해 냈는지를 확인할 수 있는 지표가 된다.&lt;/p&gt;

&lt;h2 id=&quot;4-experiment&quot;&gt;&lt;strong&gt;4. Experiment&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;논문에서 Experiment 의 목적은, User-specified objective들 (reward 세팅으로 정해짐) 에 대해서 RL algorithm으로 Diffusion model을 fine-tuning 하는 방식이 과연 효과적인가를 판단함에 있다. 특히 아래 세 가지 질문들에 답할 수 있는가에 집중하였다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;DDPO와, 기존 방식 RWR(Reward-weighted Regression) 간의 비교&lt;/li&gt;
  &lt;li&gt;VLM 방식이 인간이 수동으로 Labeling하기 힘든 부분들에 대한 대안책이 될 수 있는가?&lt;/li&gt;
  &lt;li&gt;RL fine-tuning이 fine-tuning 과정에서 보지 못했던 prompt 들에 대해서도 generalization이 가능한가? (overfit이 일어나지 않는가?)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Experiment Settings&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;모든 Experiments들에 대한 Base model로는 Stable Diffusion v1.4(Rombach et al., 2022)를 사용하였다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;학습에 사용된 이미지들은 ImageNet-1000 category의 398개의 동물이미지를 Uniform하게 sampling한 이미지로, Compressibility, Incompressibility에서는 이를 모두 이용해 Fine-tuning을 진행했다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Aesthetic quality prompt의 경우에만 조금 더 작은 45 categories의 동물이미지를 사용하였다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;각 실험별 상세한 Setting들은 내용 설명과 함께 진행하겠다.&lt;/p&gt;

&lt;h3 id=&quot;algorithm-comparisons&quot;&gt;Algorithm Comparisons&lt;/h3&gt;
&lt;p&gt;논문에서는 먼저 전반적 성능 확인을 위해 Reward Setting 1, 2번에 대한 결과를 이전 알고리즘(RWR)과 대비해 분석한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Training_Diffusion_Models_With_Reinforcement_Learning/2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림은 서로 다른 reward function에 대한 RL Fine-tuned Result이다. 정성적 관점에서, Aesthetic Quality에서는 기본 pre-trained 이미지에 비해 훨씬 예술적인(빛의 구도 등) 이미지를 연출하며, compressibility에서는 이미지 내에서 가장 중요한 부분들을 살리고 나머지는 간소화 한 것을 확인 할 수 있다. 반대로 Incompressibility에서는 file 크기를 늘리기 위해 이미지 내의 세세한 부분들을 선명하게 구현하였음을 확인할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Training_Diffusion_Models_With_Reinforcement_Learning/3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Reward 지표를 바탕으로한 정량적 분석에서는 DDPO 방식이 기존의 방법론 RWR에 비해 훨씬 지표상으로 많은 Reward를 얻었음을 확인할 수 있다. 또한 Importance sampling 기법을 사용한 것이 Monte Carlo based approach(REINFORCE)에 비해 약간 더 나은 성능을 보였다.&lt;/p&gt;

&lt;h3 id=&quot;automated-prompt-alignment&quot;&gt;Automated Prompt Alignment&lt;/h3&gt;
&lt;p&gt;다음으로, VLM 방식의 효과성을 확인하기 위해, 이전 task들에서 가장 성능이 좋았던 DDPO-IS를 바탕으로, 실험을 진행했다.&lt;/p&gt;

&lt;p&gt;Prompt Setting은 “a(n) [animal] [activity]”를 베이스로, animal의 경우 Aesthetic에서 사용된 45개의 동물 카테고리를 사용였고, activity의 경우 “riding a bike”, “playing chess”, 그리고 “washing dishes” 세 가지만 사용하였다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Training_Diffusion_Models_With_Reinforcement_Learning/4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림은 pretrained 된 stable diffusion으로부터, fine-tuning이 진행될수록 변하는 이미지를 나타내고 있다. 가장 위 “a dolphin riding a bike”를 먼저 살펴보면, pretrained된 상태에서는 이와 유사한 이미지를 생성하지도 못하지만, fine-tuning을 진행하면서 점점 faithful한 이미지를 생성해 냄을 확인할 수 있다.&lt;/p&gt;

&lt;p&gt;그림 우측의 Prompt Alignment Score 에서도 역시 Training Sequence들이 진행되면서 Bert Score 향상의 유의미한 지표를 보여주고 있다.&lt;/p&gt;

&lt;p&gt;논문에서 Fine-tuning이 이루어지며 점점 Cartoon 풍의 이미지로 변한다는 사실을 짚었는데, 이에 대한 해석으로 저자들은 “동물이 사람이 하는 행동을 하는 Image들 자체가 주로 만화에서 등장해서” 라고 이를 분석하였다.&lt;/p&gt;

&lt;h3 id=&quot;generalization&quot;&gt;Generalization&lt;/h3&gt;
&lt;p&gt;RL Finetuning이 Generalization Property를 띈다는 다른 논문 (English instruction이 다른 언어에 대한 capability  역시 상승시켰다는 내용; Ouyang et al., 2022) 의 결과와 마찬가지로, 동물 이미지로 학습한 DDPO에 대해 보지 못했던 새로운 동물, 동물이 아닌 사물, 그리고 새로운 시나리오(앞서 언급한 3가지 행동 prompt가 아닌)에 대해서도 일반적으로 좋은 성능을 냄을 확인할 수 있었다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Training_Diffusion_Models_With_Reinforcement_Learning/5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림에서는 학습 때 사용하지 않았던 홍학, 불가사리, 그리고 사물인 자전거와 냉장고 이미지에서도 Aesthetic fine-tuned 상황에서 이러한 특징을 잘 살리고 있음을 확인할 수 있다. 또한, “a capybara washing dishes” 나 “a duck taking an exam”과 같은 새로운 동물 + 시나리오에 대해서도 그럴듯한 이미지를 생성해 냄을 확인할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;5-conclusion&quot;&gt;&lt;strong&gt;5. Conclusion&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;이 논문에서는 DDPO(Denoising Diffusion Policy Optimization)를 제시해, 기존의 Diffusion model components들을 Multi-step MDP로 재정의하고, 이전 방법론(RWR)의 Suboptimality 한계를 강화학습의 Policy gradient update를 적용함으로서 극복해 Downstream task fine-tuning 성능을 향상시켰다.&lt;/p&gt;

&lt;p&gt;더 나아가 논문은 Stable Diffusion과 같은 text-to-image 모델에 대해, Policy gradient 에 필요한 Reward function을 다양하게 설정하는 방식으로 User-specific objective를 정의하는 방법을 제시하였다. 실제 실험 결과를 통해 이러한 RL-based fine-tuning이 미리 설정된 Reward function 에 따라 효과적으로 목적을 달성함을 확인하였으며, Data-specific 하지 않고 generalization이 가능함을 증명해 보였다. 특히 LLM등에서 사용되어 파장을 일으켰던 RLHF 방법론을  VLM(Vision Language Model)등을 이용해 유저 라벨링 없이도 이용이 가능하게 설계했다는 점에서 상당한 의의가 있다.&lt;/p&gt;

&lt;p&gt;논문에서는 Text-to-image에 한정하여 실험을 진행하였으나, Diffusion 기반 모델들의 Fine-tuning에 RL방법론을 사용할 수 있다는 점은 이 논문이 훨씬 다양한 분야로 확장이 가능함을 의미한다.&lt;/p&gt;

&lt;p&gt;또한, Likelihood Maximization 에 대해, Policy gradient 방법론을 적용함으로서, 조금 더 Optimality에 다다르게 밀어넣을 수 있다는 점은, MDP formulation이 가능한 MLE base의 다양한 모델에 적용해 볼 만하다는 점에서 여러 색다른 접근이 가능하다고 해석할 수 있겠다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;author-information&quot;&gt;&lt;strong&gt;Author Information&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Kiyoung Om
    &lt;ul&gt;
      &lt;li&gt;Affiliation: &lt;a href=&quot;http://silab.kaist.ac.kr/&quot;&gt;SILAB@Kaist&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;Research Topic: Offline-RL, Reinforcement Learning&lt;/li&gt;
      &lt;li&gt;Contact: se99an@kaist.ac.kr&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-reference--additional-materials&quot;&gt;&lt;strong&gt;6. Reference &amp;amp; Additional materials&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Github Implementation
    &lt;ul&gt;
      &lt;li&gt;Official codes : &lt;a href=&quot;https://github.com/jannerm/ddpo&quot;&gt;DDPO&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;Website: &lt;a href=&quot;https://rl-diffusion.github.io/&quot;&gt;DDPO official explanation&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Reference
    &lt;ul&gt;
      &lt;li&gt;Paper: &lt;a href=&quot;https://arxiv.org/abs/2305.13301&quot;&gt;Training Diffusion Models with Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Other References
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2006.11239&quot;&gt;Denoising Diffusion Probabilistic Models&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.16381&quot;&gt;DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://proceedings.neurips.cc/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf&quot;&gt;Policy Gradient Methods for Reinforcement Learning with Function Approximation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://is.mpg.de/fileadmin/user_upload/files/publications/ICML2007-Peters_4493%5B0%5D.pdf&quot;&gt;Reinforcement Learning by Reward-weighted Regression for Operational Space Control&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
            <pubDate>Wed, 17 Apr 2024 00:00:00 +0900</pubDate>
            <link>http://localhost:4000/Training_Diffusion_Models_With_Reinforcement_Learning.html</link>
            <guid isPermaLink="true">http://localhost:4000/Training_Diffusion_Models_With_Reinforcement_Learning.html</guid>
            
            <category>reviews</category>
            
            
        </item>
        
        <item>
            <title>[CIKM 2023] Timestamps as prompts for geography-aware location</title>
            <description>&lt;h1 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;시간 정보를 명시적으로 통합하고 지리 정보 embedding 방법을 변경하여 다음 위치 추천(next location recommendation)을 개선 시키고 더 먼 미래의 위치 추천(interval location recommendation)도 잘 수행하는 모델을 만들었다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;본 연구는 예측할 시간의 시간 정보를 명시적으로 통합하고 새로운 지리 정보 embedding 방법을 제공하여 기존의 위치 추천 시스템의 문제를 개선하고 interval prediction도 더 잘 예측하는 모델(TPG: Temporal Prompt-based and Geography-aware)을 개발했다. 또한 사용자의 real-world check-in data를(Foursquare, Gowalla, Brightkite) 이용해 TPG 모델의 성능을 확인했다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Timestamps_as_prompts_for_geography-aware_location/Untitled.png&quot; alt=&quot;&quot; /&gt;
timestamp 정보를 추가 prompt로 활용해서 next , interval location prediction을 진행하는 과정&lt;/p&gt;

&lt;h1 id=&quot;2-motivation&quot;&gt;2. Motivation&lt;/h1&gt;

&lt;p&gt;위치 추천 문제는 사용자의 여행 경험을 향상 시켜 줄 수 있고 광고 등 상업적 사용에 있어 매우 중요한 요소이다. 정확한 위치 추천을 위해 기존에는 Markov chain, RNN, GNN 등의 모델들이 제안 되어왔고 최근에는 transformer 기반의 모델들이 social relationship, geography information[8] 등을 포착하고 활용하여 우수한 성능을 보여주고 있다. 하지만 기존의 모델들은 위치 추천 문제에 있어 두 가지 문제점이 존재한다.&lt;/p&gt;

&lt;p&gt;첫 번째는 시간 정보를 implictity하게 다른 데이터와 통합해 사용하는 것이다. 기존의 모델들은 check-in data의 시간 정보와 위치 정보 등 다른 정보를 통합하여 embedding vector를 생성한다. 하지만 시간 정보는 다른 정보에 비해 큰 영향을 가진다. 예를 들어 일반적인 사람은 아침에는 직장을 점심에는 식당을 저녁에는 집으로 가는 경향이 있는 것처럼 사람의 특성상 시간 정보는 위치 예측에 있어 중요한 특징이다. 따라서 시간 정보를 다른 특징들과 통합하여 사용하게 되면 모델이 다음 위치 뿐만 아니라 더 먼 미래의 위치를 예측할 때(특히 정확한 시간의 위치를 예측할 때) 유연성과 정확성이 떨어진다. 예를 들어 기존 모델은 이전 100개의 check-in정보를 이용해서 101, 102 번째 위치 예측을 동시에 진행한다면 input이 동일하다는 문제가 발생하고 102번째를 정확히 맞추고 싶다면 다시 101번째까지의 check-in을 학습해야 한다는 문제가 있다. 따라서 TPG 모델은 명시적으로 예측할 시간 정보를 따로 prompt의 형태로 decoder 부분에 제공함으로 시간 정보를 더 잘 반영할 수 있게 해준다. (Temporal Prompt-based Decoder)&lt;/p&gt;

&lt;p&gt;두 번째는 지리 정보를 표현함에 있어 hard-boundary 문제를 해결 하지 못했다는 것이다. Check-in 데이터의 sparcity 문제로(실제 방문한 check-in 정보만 존재) 인해 위치 간의 물리적 근접성과 의존성을 포착하기 어렵다. 최근 GeoSAN[6] 모델에서 제공한 hierarchical gridding(지리적 좌표를 multi gridding으로 mapping하여 처리) 방식으로 지리 정보를 표현하는 경우 인접한 grid에 위치하는 POI(Point Of Interest)들이 인위적으로 분리되는 hard boundary 문제가 발생한다. 따라서 TPG 모델은 shifted window 방식을 적용해 grid의 경계 위치에 있는 POI의 지리적 특성도 잘 확인 할 수 있도록 설계되었다. (Geography-aware Encoder)&lt;/p&gt;

&lt;h1 id=&quot;3-method&quot;&gt;3. Method&lt;/h1&gt;

&lt;p&gt;(TPG 구조)
&lt;img src=&quot;../../images/DS503_24S/Timestamps_as_prompts_for_geography-aware_location/Untitled-12.png&quot; alt=&quot;&quot; /&gt;
E: transformer Encoder
D: transformer Decoder&lt;/p&gt;

&lt;h2 id=&quot;preliminaries&quot;&gt;&lt;strong&gt;Preliminaries&lt;/strong&gt;&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;TPG모델 input&lt;/strong&gt;: , $c^u_i = (u, t_i, p_i, geo_i)$&lt;/p&gt;

    &lt;p&gt;→ 각 유저들의 n개의 historical check-in 정보를 이용해 학습 진행한다. $C^u_ {1→n} = {c^u_i}^n_{i=1}$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;u: user&lt;/li&gt;
  &lt;li&gt;$t_i$: i번째 check-in 시간&lt;/li&gt;
  &lt;li&gt;$p_i$: i번째 check-in POI&lt;/li&gt;
  &lt;li&gt;$geo_i$: POI의 위도, 경도 정보 → $geo_i = (x_i, y_i)$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;💡 최종 목적은 $t_ {n+1}$의 $p_ {n+1}$을 맞추는 것&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;tile map 방식&lt;/strong&gt;(지리 정보를 hierarchical gridding으로 표현한 방식)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Timestamps_as_prompts_for_geography-aware_location/Untitled-2.png&quot; alt=&quot;&quot; /&gt;
최초에 world-map을 512x512 pixel로 표현하고 level이 증가할 수록 가로, 세로 각각 2배씩 확대되는 방식이다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;각 tile은 고유한 quadkey를 가진다. (왼쪽 위를 시작으로 Z 방향으로 0~3값을 부여)
  → cardinality = 4&lt;/li&gt;
  &lt;li&gt;level이 올라갈 수록 각 tile을 하위 4개의 tile들로 나누고 나눠진 tile에는 이전 level의 quadkey에 자신의 위치에 해당하는 고유 숫자를 추가해줘 quadkey를 생성한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;31-geography-aware-encoder&quot;&gt;3.1 Geography-aware Encoder&lt;/h2&gt;

&lt;p&gt;위치 embedding을 생성 $(e^{geo}_i)$한다. 중심 grid와 주변 grid의 공간적 연관성이 고려된 vector를 만들어낸다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Timestamps_as_prompts_for_geography-aware_location/Untitled-3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;지리 정보를 tile map 방식으로 gridding 진행&lt;/li&gt;
  &lt;li&gt;shifted window 적용
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;현재 grid를 기준으로 상, 하, 좌, 우, 대각 8개의 gird를 이용해 window를 생성한다.
 (window size가 0.25일때 window의 모습)
  &lt;img src=&quot;https://i.postimg.cc/j5K43F07/Untitled-4.png&quot; alt=&quot;window의 모습&quot; /&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;quadkey의 cardinality가 4이므로(0~3까지의 값만 가짐) 다양성이 적어 직접 self-attention 적용해 주변 tile과의 correlation을 찾기는 어렵다. 따라서 quadkey를 n-gram방식으로 나눠서 self attention을 적용해 주변 tile과의 correlation을 파악한다.(어느 sequence가 중요한지 파악) self-attention 결과를 feed forward network로 학습 시킨 후 average pooling을 이용해 최종 embedding vector($e^{geo}_i$)를 생성한다.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;💡 이러한 방법을 통해 기존의 hierarchical gridding 방식으로는 해결할 수 없었던 hard-boundary 문제를 해결한다.&lt;/p&gt;

&lt;h2 id=&quot;32-history-encoder&quot;&gt;3.2 History Encoder&lt;/h2&gt;

&lt;p&gt;사용자의 과거 check-in 데이터에 있는 geo, POI, user, time 정보들을 결합해서 하나의 input으로 만들고 transformer encoder를 이용해 학습을 진행한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Timestamps_as_prompts_for_geography-aware_location/Untitled-5.png&quot; alt=&quot;&quot; /&gt;
	$e^{time}$: 1주일을 시간으로 처리 24 x 7 = 168차원
	$e^{user}$ : userID에 대한 embedding
	$e^{POI}$ : 고유 POI에 대한 embedding
	$e^{geo}$ : Geography-aware Encoder 결과&lt;/p&gt;

&lt;p&gt;모든 embedding vector들을 동일한 d차원으로 linear projected된다. 이후 모든 embedding을 결합해서 하나의 input ($e^c$)을 생성한다. 
 $e^c = (e^{poi}, e^{user}, e^{time}, e^{geo})$ 이때 $e^c$는 element product or concatenate 2가지 옵션으로 통합 가능하다. 이후 과정은 transformer encoder 구조를 따른다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$e^c_i = PositionalEmbedding(e^c_i)$&lt;/li&gt;
  &lt;li&gt;$\text{ATTENTION}(e^c_i) = w_z \sum_{j=1}^{N_o} \frac{\exp(w_q e^c_i \cdot w_k e^c_j)}{\sum_{m=1}^{N_o} \exp(w_q e^c_i \cdot w_k e^c_m)} w_v e^c_j + e^c_i$&lt;/li&gt;
  &lt;li&gt;$FFN(e^c_i) = max(0, e^c_iW_1 + b_1)W_2 + b_2$&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;33-temporal-prompt-based-decoder&quot;&gt;3.3 Temporal Prompt-based Decoder&lt;/h2&gt;

&lt;p&gt;Encoder에 과거 check-in 기록에 대한 시간 정보가 통합되어 있긴 하지만 독립적으로 예측을 진행할 시간 정보를 명시적으로 입력해주면 더욱 우수한 성능을 보일 것으로 예상된다. 따라서 직접 시간 정보를 prompt형식으로 decoder에 query로 넘겨 predict에 고려하도록 설정한다. decoder는 input으로 예측 시간 embedding($e^{time}_ {n+1}$)과 encoder의 output($e^C$)을 입력 받고 $e^{time}_ {n+1}$을 query로, $e^C$를 key, value로 이용해서 Self-Attention, Encoder-Decoder Attention, Feed-Foward NN layer를 통과시키면서 최종 POI를 예측한다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$\text{ATTENTION}(e^C, e^{time}_ {n+1}) = w_z \sum_{j=1}^{N_v} \frac{\exp(w_q e^{time}_ {n+1} \cdot w_k e^C)}{\sum_ {m=1}^{N_v} \exp(w_q e^{time}_ {n+1} \cdot w_k e^C} w_v e^C + e^C$&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;평가-함수&quot;&gt;평가 함수&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;$L_{rec}(\tilde{y}) = -\log(\frac{exp(\tilde{y}y^+)}{exp(\tilde{y}y^+)+\sum_{y^-}exp(\tilde{y}y^-)})$
    &lt;ul&gt;
      &lt;li&gt;$\tilde{y}$ : 예측 위치 embedding&lt;/li&gt;
      &lt;li&gt;$y^+$: 실제 방문 위치 embedding&lt;/li&gt;
      &lt;li&gt;$y^-$: 실제 방문하지 않은 위치 embedding&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;4-experiment&quot;&gt;4. Experiment&lt;/h1&gt;

&lt;h2 id=&quot;experiment-setup&quot;&gt;Experiment setup&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Dataset
    &lt;ul&gt;
      &lt;li&gt;5가지 real-world Location-Based Social Network datasets(Gowalla, Brightkite, NYC, TKY, SIN)&lt;/li&gt;
      &lt;li&gt;사용자들 기준 이전 100번의 방문을 학습에 이용한다.(이전 100번 방문이 없는 사용자의 경우 padding 처리)
&lt;img src=&quot;../../images/DS503_24S/Timestamps_as_prompts_for_geography-aware_location/Untitled-11.png&quot; alt=&quot;&quot; /&gt;
  -&amp;gt; NYC, SIN: 모든 데이터 학습, 나머지: 첫 2000명의 데이터 학습&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;baseline
    &lt;ul&gt;
      &lt;li&gt;HSTLSTM[1]: a LSTM based method which introduces spatio-temporal transfer factors and uses an encoder-decoder structure for prediction.&lt;/li&gt;
      &lt;li&gt;DeepMove[2]: an attentional recurrent network which capture the complicated sequential transitions and the multi-level periodicity.&lt;/li&gt;
      &lt;li&gt;LSTPM[3]: a long- and short-term preference modeling framework which consists of a nonlocal network for long-term preference modeling and a geo-dilated RNN for short-term preference learning.&lt;/li&gt;
      &lt;li&gt;CARA[4]: a novel contextual attention recurrent architecture that leverages both sequences of feedback and contextual information associated with the sequences to capture the users’ dynamic preferences.&lt;/li&gt;
      &lt;li&gt;TMCA[5]: a novel temporal and multi-level context attention LSTM-based encoder-decoder framework which is able to adaptively select relevant check-in activities and contextual factors for next POI preference prediction&lt;/li&gt;
      &lt;li&gt;GeoSAN[6]: a geography-aware sequential recommender based on the self-attention network that uses hierarchical gridding of GPS locations for spatial discretization and uses self-attention layers.&lt;/li&gt;
      &lt;li&gt;STAN[7]: a spatial-temporal attention network that explicitly aggregates all relevant check-ins in trajectories, not only just successive ones.&lt;/li&gt;
      &lt;li&gt;MobTCast[8]: a Transformer-based context-aware network combined with a location prediction branch as an auxiliary task. It which captures temporal, semantic, social and geographical contexts.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Evaluation metric
    &lt;ul&gt;
      &lt;li&gt;k = 5, 10&lt;/li&gt;
      &lt;li&gt;Recall@k&lt;/li&gt;
      &lt;li&gt;NDCG@k : 예측 순위가 높은 걸 맞췄을 때 더 높은 점수를 주는 평가 지표&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;result&quot;&gt;Result&lt;/h2&gt;
&lt;p&gt;기본적으로 baseline 모델에서는 user embedding은 추가하지 않고 TPG 모델을 학습했다.&lt;/p&gt;

&lt;h3 id=&quot;next-location-prediction-결과&quot;&gt;&lt;strong&gt;next location prediction 결과&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Timestamps_as_prompts_for_geography-aware_location/Untitled-8.png&quot; alt=&quot;&quot; /&gt;
(101번째 위치 추천), bold: 최고 성능 모델, 밑줄: 두 번째 최고 성능 모델&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;5가지 dataset에 대해서 TPG모델이 모두 baseline 모델보다 우수한 성능을 보인 것을 확인할 수 있다.&lt;/li&gt;
  &lt;li&gt;RNN 기반 모델(HSTLSTM, DeepMove, LSTPM, TMCA, CARA)보다 Attention기반 모델(MobTCast, STAN, GeoSAN, TPG)이 전반적으로 우수한 성능을 보이고 있다. 이는 Attention 기반 모델들이 check-in 데이터에서 시공간 맥락 정보를 더 잘 포착하기 떄문이다.&lt;/li&gt;
  &lt;li&gt;sparsities 지수가 높은 NYC, SIN dataset에서 특히 TPG 모델의 성능이 다른 모델들 보다 우수한 것으로 보아 TPG 모델이 희소한 데이터를 처리하는 능력이 우수하다는 것을 알 수 있다.
  (Sparsities): Gowalla(0.001), Brightkite(0.007), NYC(0.027), TKY(0.009), SIN(0.12)
  → sparsity = #check-in / (#user x #location)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;masking-location-prediction-결과&quot;&gt;&lt;strong&gt;Masking location prediction 결과&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Timestamps_as_prompts_for_geography-aware_location/Untitled-9.png&quot; alt=&quot;&quot; /&gt;
int i : 100번째 기준 이전 i개의 방문을 masking한 데이터셋&lt;/p&gt;

&lt;p&gt;더 먼 미래의 상황을 잘 맞추는지 확인하기 위해 100번째, 99번째, 98번째를 masking하고 모델의 성능을 비교한 결과이다. STAN, GeoSAN 모델은 masking 후의 성능이 전반적으로 떨어지는 것에 비해 TPG 모델은 오히려 성능이 올라가는 경우도 있다. 따라서 TPG 모델이 더 먼 미래의 위치를 맞추는데 효과적임을 알 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;ablation-study&quot;&gt;Ablation Study&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Remove TP (Temporal-based Prompt)&lt;/li&gt;
  &lt;li&gt;Remove TE (Time Embedding)&lt;/li&gt;
  &lt;li&gt;Remove SW (Shifted Window Mechanism)&lt;/li&gt;
  &lt;li&gt;Remove GE (Geography Encoder)&lt;/li&gt;
  &lt;li&gt;Add UE (User Embedding) → baseline에서는 user embeding을 사용하지 않는다. &lt;img src=&quot;../../images/DS503_24S/Timestamps_as_prompts_for_geography-aware_location/Untitled-10.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;모델에서 시간 정보를 제거하면(TP, TE) 전반적으로 성능이 나빠지는 것으로 보아 시간 정보가 위치 예측에서 큰 영향을 미치는 것을 확인할 수 있다. 특히 지역 dataset인 Gowalla, Foursquare dataset에서 TP보다 TE의 성능이 우수한 것으로 보아 예측 시간에 대한 정보가 더 중요하게 작용한다는 사실을 알 수 있다.
위치 정보를 제거한 (SW, GE) 결과가 TPG baseline 보다 낮은 성능을 보이는 것으로 보아 shifted window 방식과 위치 정보 표현 방식이 다음 위치 예측에 있어 효과적으로 작용한다는 것을 알 수 있다.
user embedding을 추가 했을떄 TPG 모델의 성능이 나빠진다. 이는 user embedding이 check-in 시퀀스와 위치 벡터의 불일치를 초래할 수 있기 때문이라고 판단된다. 이는 향후 사용자 정보를 효과적으로 사용해 성능을 향상시키는 방법을 고려해야 할 것이다.&lt;/p&gt;

&lt;h2 id=&quot;parameter-sensitivity-analysis&quot;&gt;Parameter Sensitivity Analysis&lt;/h2&gt;
&lt;p&gt;지리 embedding 차원과 shifted window의 크기가 중요한 파라미터라고 생각되어 두 파라미터에 대해 NYC, TKY dataset을 이용해 Parameter Sensitivity Analysis를 진행했다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;지리 embedding 차원 변화
&lt;img src=&quot;../../images/DS503_24S/Timestamps_as_prompts_for_geography-aware_location/untitle13.jpg&quot; alt=&quot;&quot; /&gt;
embedding 차원을 10부터 60까지 10단위로 변화시키면서 성능을 비교했다. 
embedding 차원을 증가시킬 수록 성능이 좋아지지만 차원이 50을 넘어가면 성능이 떨저지는 것을 확인하였다. 이는 차원이 낮을때는 지리 정보를 다 표현하기에는 부족한 것으로 보이고 차원이 너무 커지면 반대로 너무 많은 차원으로 noise가 발생 할 수 있음을 보여준다. 최종 TPG모델의 차원은 50이다.&lt;/li&gt;
  &lt;li&gt;shifted window step size 변화
&lt;img src=&quot;../../images/DS503_24S/Timestamps_as_prompts_for_geography-aware_location/Untitle14.jpg&quot; alt=&quot;&quot; /&gt;
window step size를 0.25 부터 1까지 0.25씩 증가시키면서 확인했다. 여기서 step size는 window의 길이와 grid 크기의 비율을 의미한다. 
그래프를 확인해 봤을때 step size가 0.25일때 예측 성능이 우수했고 0.75까지 증가할 수록 성능이 떨어지는 것을 확인할 수 있다. 비교적 작은 window의 크기로 인해 중요한 지역의 정보를 집중적으로 분석하면서 주변의 중요한 정보를 충분히 포착 가능하기 때문이다. step size가 0.75로 커질때는 window의 크기 증가로 인해 불필요한 정보도 모델에 포함되면서 중요 정보가 희석된 것으로 볼 수 있다. step size가 1일때 성능이 다시 증가한 이유는 window가 각 레벨에서 주변 grid 전체를 포괄하여 집계하기 때문이다. 이는 각 위치에서 지리적 연속성과 인접 grid간의 상관관계를 직접적으로 고려하는 방식으로 전환되기 때문에 예측의 성능을 올린 것으로 보인다. 최종 TPG모델의 window step size는 0.25이다.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;5-conclusion&quot;&gt;5. Conclusion&lt;/h1&gt;

&lt;p&gt;종합해 보면 TPG모델의 강점은 두 가지가 있다. 첫 번째로 기존 위치 추천 모델들이 간과했던 예측 시간 정보를 직접 prompt를 이용해서 모델에 추가해줘 예측 성능을 높인점이다. 두 번째는 위치 정보를 표현하는 gridding 방식에 shifted window를 적용해서 위치 간 상관 관계를 추가적으로 확인한 점이다.&lt;/p&gt;

&lt;p&gt;하지만 개인적으로는 세 가지의 아쉬운 점 및 추가 연구 필요성을 느꼈다. 첫 번째는 어떻게 시간 정보를 prompting 했는지에 대한 설명이 부족하다는 점이다. 시간 정보를 prompting 방법에 따라서도 성능의 차이가 날 수 있을 것으로 예상되므로 추가 prompting에 대한 연구가 필요해 보인다. 두 번째로는 History Encoder 부분에서 geo embedding, POI embedding, time embedding을 결합하는 부분에서 두 가지 결합 방법(element product, concatenate)을 제시했는데 두 방법에 따른 결과를 비교하는 연구가 추가적으로 진행되어야 할 것으로 보인다. 마지막으로 위치, 시간 정보 말고도 baseline으로 제시된 모델에서 적용했던 것과 같이 social relationship[8] 등의 특성들을 embedding에 추가하면 더 우수한 모델을 만들 수 있을 것으로 예상되어 추가 연구가 필요해 보인다.&lt;/p&gt;

&lt;h1 id=&quot;author-information&quot;&gt;Author Information&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Author: Hongju Lee&lt;/li&gt;
  &lt;li&gt;Affiliation: CDSN Lab in KAIST&lt;/li&gt;
  &lt;li&gt;Research Topic: location recommendation system&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;reference&quot;&gt;Reference&lt;/h1&gt;

&lt;p&gt;[1] Dejiang Kong and Fei Wu. 2018. HST-LSTM: A Hierarchical Spatial-Temporal Long-Short Term Memory Network for Location Prediction. In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI). 2341–2347&lt;/p&gt;

&lt;p&gt;[2] Jie Feng, Yong Li, Chao Zhang, Funing Sun, Fanchao Meng, Ang Guo, and Depeng Jin. 2018. DeepMove: Predicting Human Mobility with Attentional Recurrent Networks. In Proceedings of the World Wide Web Conference (WWW). 1459–1468.&lt;/p&gt;

&lt;p&gt;[3] Ke Sun, Tieyun Qian, Tong Chen, Yile Liang, Quoc Viet Hung Nguyen, and Hongzhi Yin. 2020. Where to Go Next: Modeling Long- and Short-Term User Preferences for Point-of-Interest Recommendation. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI). 214–221&lt;/p&gt;

&lt;p&gt;[4] Jarana Manotumruksa, Craig Macdonald, and Iadh Ounis. 2018. A Contextual Attention Recurrent Architecture for Context-aware Venue Recommendation. In Proceedings of the ACM International Conference on Research and Development in Information Retrieval (SIGIR). 555–564.&lt;/p&gt;

&lt;p&gt;[5] Ranzhen Li, Yanyan Shen, and Yanmin Zhu. 2018. Next Point-of-Interest Recommendation with Temporal and Multi-level Context Attention. In Proceedings of the IEEE International Conference on Data Mining (ICDM). 1110–1115.&lt;/p&gt;

&lt;p&gt;[6] Defu Lian, Yongji Wu, Yong Ge, Xing Xie, and Enhong Chen. 2020. Geographyaware Sequential Location Recommendation. In Proceedings of the ACM International Conference on Knowledge Discovery and Data Mining (KDD). 2009–2019.&lt;/p&gt;

&lt;p&gt;[7] Yingtao Luo, Qiang Liu, and Zhaocheng Liu. 2021. STAN: Spatio-Temporal Attention Network for Next Location Recommendation. In Proceedings of the World Wide Web Conference (WWW). 2177–2185.&lt;/p&gt;

&lt;p&gt;[8] Hao Xue, Flora Salim, Yongli Ren, and Nuria Oliver. 2021. MobTCast: Leveraging Auxiliary Trajectory Forecasting for Human Mobility Prediction. In Advances in Neural Information Processing Systems (NeurIPS). 30380–30391.&lt;/p&gt;
</description>
            <pubDate>Wed, 17 Apr 2024 00:00:00 +0900</pubDate>
            <link>http://localhost:4000/Timestamps_as_prompts_for_geography-aware_location.html</link>
            <guid isPermaLink="true">http://localhost:4000/Timestamps_as_prompts_for_geography-aware_location.html</guid>
            
            <category>reviews</category>
            
            
        </item>
        
        <item>
            <title>[ICLR 2024] Time-LLM: Time Series Forecasting by Reprogramming Large Language Models</title>
            <description>&lt;h2 id=&quot;1-problem-definition&quot;&gt;&lt;strong&gt;1. Problem Definition&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;시계열 데이터 예측은 전통적인  AR, MA, ARIMA부터 현대의 Deep-Learning을 활용한 CNN, LSTM 등 많은 모델에서 연구되어 오고 있다. 하지만, 모델을 학습 시킬 수 있는 시계열 데이터가 부족하여 주로 예측하는 모델이 domain specialized되어 있는 경우가 많다. 최근 연구에 따르면 LLM(Large Language Models)이 복잡한 Token sequence에 대해 강력한 패턴 인식 및 추론 능력을 가지고 있는 것으로 밝혀졌다. 본 논문에서는 이런 LLM의 패턴 인식과 추론 능력을 시계열 예측에 사용하고자 하지만, LLM이 가지고 있는 강점을 시계열 데이터에 사용하기에는 적절한 modality 변환이 필요한 상황이다.&lt;/p&gt;

&lt;p&gt;이런 문제 상황을 해결하기 위해 본 연구는 LLM의 backbone은 그대로 유지하면서 일반적인 시계열 예측을 진행할 수 있게하는 Reprogramming Framework를 제시한다. 크게 두 가지의 방법을 제시하는데, 먼저 시계열 데이터를 LLM의 능력이 강화될 수 있도록 Embedding, Patching을 거친 후 Text vector들과 합쳐주는 Patch Reprogramming이 있고, 그 후에 시계열 데이터의 Context나 해결해야 하는 Task의 정보, 적당한 Statistics를 Input에 합쳐주는 Prompt as Prefix가 존재한다. 이 두 가지 특별한 Process를 통해 시계열 데이터가 LLM을 통해 좋은 성능의 예측이 가능하다는 것을 보여준다.&lt;/p&gt;

&lt;h2 id=&quot;2-background&quot;&gt;&lt;strong&gt;2. Background&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;21-time-series-for-llm&quot;&gt;&lt;strong&gt;2.1. Time Series for LLM&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Time-Series for LLM의 의미는 LLM의 속의 구조를 고정시키고, Downstream task에 대한 Fine-tuning을 진행하기보다 시계열 데이터에 주요한 변화를 주면서 Task의 성능을 높이고자 한다. 본 논문 또한, LLM에 변화를 취하기 보다는 시계열 데이터를 manipulation하는 방법을 사용한다. 이전에 LLM을 활용한 연구(Time Series Forecasting with LLMs: Understanding and Enhancing Model Capabilities, 2024, Jin) 를 살펴보자면,  Human knowledge를 미리 LLM에 추가한다면 예측의 성능이 높아지며 Sequence나 Numerical한 데이터를 LLM이 잘 이해할 수 있도록 Paraphrasing 하는 것 또한 긍정적인 효과를 불러온다는 결과가 있다.&lt;/p&gt;

&lt;p&gt;예를 들어, 전력량 예측에 대한 Task가 존재할 때 LLM에 미리 여름과 겨울에 전기를 많이 사용한다는 사실을 입력한다면 미래의 전력량 예측에 도움을 준다. 또한, 시계열 데이터를 input으로 사용할 때 시점 t에서 시점 t+1은 증가, 시점 t+1에서  시점 t+2은 감소처럼 이런 sequence에 대한 부연 설명을 통해 LLM이 시계열 데이터를 더 잘 이해할 수 있다.&lt;/p&gt;

&lt;p&gt;본 연구에서도 이와 유사한 개념으로 Prompt as Prefix와 Pre-trained word embedding을 사용하기에, 위의 예시를 참고하면 더욱 연구 Process를 이해하기 쉬울 것이다.&lt;/p&gt;

&lt;h3 id=&quot;22-consideration-time-series-for-llm&quot;&gt;&lt;strong&gt;2.2. Consideration Time Series for LLM&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;ㄱ) 시계열 데이터 자체가 많이 존재하지 않는다.&lt;/p&gt;

&lt;p&gt;가장 큰 문제는  2024년 현재까지 시계열 데이터 셋 중 가장 크다고 여겨지는 것의 용량이 10GB 미만으로 Vision, NLP 등 다른 분야에 비해 Foundation Model을 학습시킬 데이터가 현저하게 부족하다.그렇기에 이를 해결하기 위해 GAN 같은 방법을 사용하거나 LLM 자체를 Domain에 따라 미리 Prompt를 넣어주기도 한다.&lt;/p&gt;

&lt;p&gt;ㄴ) 각 시계열 데이터셋의 특징이나 모양이 상이하다.&lt;/p&gt;

&lt;p&gt;먼저, Domain마다 데이터셋마다 통계적인 특성이나 Scale에서 차이가 난다. 예를 들어 제조 과정에서 얻어지는 변동성의 정도와 금융 시장의 변동성은 차원이 다른 수준이기에 이를 한 번에 통합하여 학습시키기 힘들다.&lt;/p&gt;

&lt;p&gt;두 번째로 Granularity의 문제가 있다. 풀어서 얘기하자면 데이터의 time-step이 각 데이터 셋마다 다르다는 의미이다.&lt;/p&gt;

&lt;h2 id=&quot;3-method&quot;&gt;&lt;strong&gt;3. Method&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;31-model-setting&quot;&gt;&lt;strong&gt;3.1. Model Setting&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Time-LLM_Time_Series_Forecasting_by_Reprogramming_Large_Language_Models/image1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;!-- &lt;div align=&quot;center&quot;&gt;
 &lt;img src=&quot;https://lh3.googleusercontent.com/fife/ALs6j_Gh2QNRWGAzt4yBO3JbB4MWxAlM3GrTb6uRJbHe3TOoU8gCBkNopwg2qrJf4h5LNdFao8_7yBaZXbjvrWG4xaMfWmorLQdXzTCvKRmFJL82QkIFrV3ix57pVc7JGmWZBt4540A-mmmNdNHufQp2rowXztCCS4kabAoB7ZafnYwDLM-NJ_Mqbw0bxQhWCqwTc_d5stfhflvqXYsdmlbHVNwbk2EGRR75nwtNLg6Q3PWpZDuLS_Gh2ILI3dSMRgHXTzmVQbX5o1vH2T8Xh42cfBzf4VZQ2-Cx8_0LZhEMqNOgxCo_dcLx8IMurYFnwWw1mv5TmQOXFAGcT2nwe6UDZTRatL_Bt2zQPY0rq-95O9SXYbLjJ9IhPjS7g_NCUj5sBDpIYBrI5CMnwKiTkcpphCuKEadWSZQtkXUbV8eVwaCDi0rtWXp4VEDnnST8jbi8sCPfTTySrBazrtR09fvxRFw1b0QP2IGLtmHLw2sN_Z3rKV1wA6rtinWtqGbdI5H_8AKTaXPwI7D-HlY5f019k013Kdsp9aRVXlaaOT_h2n_jDPKpWp0rzZoy7jDaSfAQtLDhx85YGDgHWvQ9B-gq-5vrmWz6qVg6HvQ5DZJQTueZfs6w7FBdrbdvqyrd5NdAsNOHOo7qnSXjDl5EDIbXv513RfjnC9E6h6DN2o8RiU1CDg22VBAl9Nm5Y_Li--n31PUIeR0mcgcEtwyYVOVr_Rfstm8FYHPAfmjp2CP9O7y22iv2MqU6qZJod4WT9Solx3ecNw8F42xnakKwdtuprpq5zLNoXj2u3SiRBVjLRJw7t_jAEAcW45tvTo0wCWKOSruuhPdofF8hzyyhwcnLYYamxj84yRw79JQgC6l-YpJe23VEJgTONwRHpYX5hZzh3r05VPtap3BrNTRkwjaY68sCU5kv6G0fwobM8hBvAOlTzet26B_SG_vZ_Ix5FmbByS_qh5vr4-MJ3HO5ZXXmWZA6YsbYbJpxXJG4hjay3_VpfRcPSLI0OSdCOh8MDoSNbo5l4jBrN40h0TAqr1bGVP-nlpf4rlxEg752i7px7cOjBB5qJkT32_RjilzXyKTcI88S5DVJA9ZkxYeaIrR1X6v6OhQtWxU04u0az5iB51E7xKyjThpxjrkoJxTBl_D8gUPak836jH-YQk6m93Y-eeLH-RMxRSw3paQDOXnljIn2y1AP8FOj-wmlgKTVAB5xAwj6vqH47pPjjXMkaXLQ2vX9s4xVKu1nJIf64bgJmFUVCd4icSOzUyWA8GNVKynUUUAzXsvkF2t_Cdy35AfJzDJHw4lwhTnSztssPE4N8_C5uNIdm7veynj77yjfuvLQYtQtyZ2Zg1FuCIFVBTqeKSlCIcmsFUEVbz8Pr0NX_IiwH2LvJMB8VIgD1ou1zCUEO7Fqe02Hsm_s-N3txBwfbdTq7zYcrA_ucMX5d4NXtgEF2ac6Euq3B_2ypDOCExGswKrESBn9Xry1u98-gyOq6QjjD-MpnLLoZJRNbAc4Kz0C85l-fl0cHTWVYPOvVEKKdsFV14zKXOO2Aiugv12LzJt0_eb4zhBiMD19GBQ4LhiZvDqwKKOL62M99sUkobomv-4flRzSyN6QMw=w1920-h919&quot;&gt;
&lt;/div&gt; --&gt;

&lt;blockquote&gt;
  &lt;p&gt;Framework ot Time LLM&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;위의 그림이 전체적인 모델의 Framework를 보여준다. 크게 Model Setting, Patch Reprogramming, Prompt as Prefix, Output Generation파트로 나눠지게 된다.
먼저 Model Setting 파트를 보게 된다면, Multivariae Time Series Data를 변수별로 나누고 Window Size만큼 input으로 사용한다.&lt;/p&gt;

&lt;p&gt;$\huge \mathbf{X} \in \mathbb{R}^{N \times T} \rightarrow \mathbf{X}^{(i)} \in \mathbb{R}^{1 \times T}$&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Basic Time-series Data&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이후 각 단변수 시계열 데이터마다 Normalization을 진행한다. 이는 시계열 데이터가 주로 시간 변화에 따라 Distribution이 바뀌는 문제때문에 진행하는데, 이런 Distribution shift는 Forecating model이  generalization되지 않게 만드는 원인이다. 본 논문에서는 Reversible Instance Normalization(RevIN)을 사용하여 이 Distribution shift 현상을 해결하는데, 이는 따로 논문이 존재하니 더 자세히 알고 싶다면 아래의 논문을 참고하면 좋다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://openreview.net/forum?id=cGDAkQo1C0p&quot; title=&quot;Reversible Instance Normalization&quot;&gt;Reversible Instance Normalization for Accurate Time-Series Forecasting against Distribution Shift&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Nomalization 이후 시계열 데이터 셋에 대해 Patching을 해주게 된다. 시계열 데이터의 특성상 시간에 연속적이기에 각 Time-step끼리는 연관되어 있는 Semantic Information이 있기에 이를 Patching을 통해서 단일 시점의 데이터들을 통합하여 Local semantic information을 보존한다.&lt;/p&gt;

&lt;p&gt;$\huge \mathbf{X}^{(i)} \in \mathbb{R}^{1 \times T} \rightarrow \mathbf{X}_ P^{(i)} \in \mathbb{R}^{P \times L_ p}$&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Patching&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;32-patch-reprogramming&quot;&gt;&lt;strong&gt;3.2. Patch Reprogramming&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;시계열 데이터를 Natural Language처럼 처리될 수 있게 Modality를 Align해주는 과정이 필요하다. 이를 통해 시계열 데이터를 Backbone model이 이해하기 쉬워지고, 시계열 데이터 속에서 Temporal Interrelationship을 잘 capture하게 된다. 그래서 이 과정에서 시계열 데이터의 특성이나 변화를 설명할 수 있는 Text 등을 이전의 Patching이 된 데이터에 Cross-Attention을 수행하여 Patch Representation을 진행한다. (시계열 데이터를 설명하는 Text 예시: “Short”, “Up”, “Late, “Steady” 등)&lt;/p&gt;

&lt;p&gt;먼저 이전의 Patching을 끝낸 시계열 데이터에 대해 Linear layer를 통해 embedding을 진행시킨다.&lt;/p&gt;

&lt;p&gt;$\huge \mathbf{X}_ P^{(i)} \in \mathbb{R}^{P \times L_ p} \rightarrow \hat{\mathbf{X}}_ P^{(i)} \in \mathbb{R}^{P \times d_ m}$&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Time-series data embedding&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;그 후 위에서 얘기한 Modality align과정을 진행하게 되는데, 기존의 존재하는 vocabulary에는 너무 많은 단어가 존재하기에 전체 Word embedding을 사용하기에는 큰 cost가 존재하고, Time-series forecasting에 필요하지 않은 Prior knowledge가 많다. 이를 해결하기 위해 Voca를 미리 Linear layer를 통과시켜 핵심 단어로만 이루어진 Text Prototypes을 만든다.&lt;/p&gt;

&lt;p&gt;(e.g. voca 속에 있는 apple, banana 등 관련 없는 단어는 없어짐)&lt;/p&gt;

&lt;p&gt;$\huge E \in \mathbb{R}^{V \times D} \rightarrow E’ \in \mathbb{R}^{V’ \times D}$&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Text Prototypes Generation&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이후 Embedding한 시계열 데이터와 위의 Text Prototype을 Cross-attention을 활용해 Align한다. 이때 Embedding TS는 Query로 Text-Prototype은 key와 value로 이용한다.&lt;/p&gt;

&lt;p&gt;$\large Q_ k^{(i)} = \hat{X}_ P^{(i)} W_ k^Q, W_ k^Q \in \mathbb{R}^{d_ m \times d}$&lt;/p&gt;

&lt;p&gt;$\large K_ k^{(i)} = E’W_ k^K, W_ k^K \in \mathbb{R}^{D \times d}$&lt;/p&gt;

&lt;p&gt;$\large V_ k^{(i)} = E’W_ k^V, W_ k^V \in \mathbb{R}^{D \times d}$&lt;/p&gt;

&lt;p&gt;$D: Backbone-model-Hidden-dimension$&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Query, Key, Value&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;위의 Query, Key, Value를 활용해 Multi-head Cross Attention 과정을 거친다.&lt;/p&gt;

&lt;p&gt;$\large Z_ k^{(i)} = \text{ATTENTION}\left(Q_ k^{(i)}, K_ k^{(i)}, V_ k^{(i)}\right) = \text{SOFTMAX}\left(\frac{Q_ k^{(i)} {K_ k^{(i)}}^T}{\sqrt{d_ k}}\right) V_ k^{(i)}, \quad Z_ k^{(i)} \in \mathbb{R}^{P \times d}$&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Attention process&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이렇게 만들어진 각 head를 Concat하여 Attention Output을 만들고&lt;/p&gt;

&lt;p&gt;$\huge Z^{(i)} \in \mathbb{R}^{P \times d_ m}$&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Attention Output&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이 Output을 이후 Backbonemodel에 Align하기 위해 Projection을 진행하여 Patch Reprogemming을 마무리한다.&lt;/p&gt;

&lt;p&gt;$\huge O^{(i)} \in \mathbb{R}^{P \times D}$&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Patch Reprogramming Output&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;33-prompt-as-prefix&quot;&gt;&lt;strong&gt;3.3. Prompt as Prefix&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;시계열 데이터셋의 사전 정보를 자연어 형태로 제공함으로써, LLM의 패턴 인식과 추론 능력을 향상시킨다. 이때 사전 정보는 Dataset Context, Task instruction, Input Statistics이다.&lt;/p&gt;

&lt;p&gt;(e.g. 전력량 예측의 경우: 전력은 여름에 많이 쓰여, 너는 내년 여름의 전력량을 예측해야 해, 올해 전력량 평균은 —이야)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Time-LLM_Time_Series_Forecasting_by_Reprogramming_Large_Language_Models/image2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;!-- &lt;div align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/fife/ALs6j_F0HUI9VSS9Rt_VeZOQCbVXk9l01UtSQl7o3uo-nfVUlvmm94fJ3MWWNZjkblpb6tTk2Nw6t7Nd0PLUjX2-XAbFU0r-6daJRiOS-JO2EaRDB0wSKe-4_3Dlqh1tvzSB81C5-5KgfcP41EaSA11Cxg71EJ6Sar8lJEafa7YJrtPIq7HjMK84wOV-uaEBWZ-RDGJxIjORHk_5E_FHXpGlDpVzsi_SripjPmm9eBzg1VBfqDk_nqgpPPNJjxCeIYsHwPUN1A8cLqg1Mrs-FBD01Hio-1_4aKqm2NqkItNatRwLMLJVUlOVwpr_K4247bte2A8JyvcwmqGodjoWKjrUYFOgNmYkJc1TgtZOLuQ8oG5mBGj8F5dssKymoABR8ag1NKf-ZpJr3EwKSrmM4XhQFaj1D2gpEWVE-XbudvKtc1VbusrH_rJx52v2kqwUm9YDpbEvL3_7oEVIjLj0DJIcp4I9vlGA1b9URv5ansmBC2W_5EqI_kyXa3sVqL_hXk9tXf6AGz2Y3fZiGPF59PaBMIxRDfYJZznrBoDKFKE3VA0bmx2-FwPv3zHSH1JhAyH_IwE2sZR9IHS5aIa2IrvOQsQe5Nc1-va6rtG1z--_MrOF52EAxi0GrmarTkwRgYiper7FUq_QQx897msZKjtseSk_CltkaSB7pRQf_b_nO022YGqC69XKN5Vj5C4fYzyl--9nTz2yxVLeksfhQjuuxTYEDr4V34JzJxYAAQbmh2vesgY-ZbVFw7YGBwHo4mBDDqWywVUN2iRvkvKmnH9pntFaOl3Y7qogvEpvw40SBx2y5jCxTpjpULMokWX7kyQh02_VftB9JCbBkHQpE5ImSO7MsP-y-HD82T_GfEDrgfBaDiYw0YOemIAKArOF4VZxOaKIsoUcggAXDJCHo6ea_l6BeskX0DjnhxvtPVH9Cu3elPSqW-CfHPc7YrLsfgaG1ZFTULQqpBrIGCOfpm3Y-TGaFqTCo5Vcaadv20sJyjQSDuk8H6bGWIvyVQO0IU55gRdDmIK0XFd6qs8NyeF2ff70nPVt-K6fE4BGDnGxmhEYvyAxJzT8x5Y4NG7yfurNPFfIu27xlqUDiVkLl5xWYab9KcESNhyDOgAtKDVuJdnJN0RzxkjgilPUFR0ZMQkbFEXs8R80schsC03mkio25u656j5hyW69ajJDGatBx0QpK8hp7NMdry-K97uKEgZwm7gh_PWVU_x-IpexP5S5YrjRCCA9uNwlmjeOdI3ifR9VbXeKhA7Awh3tNTXUENq9G4dY02-1jyLPpzRzulMhhsmhdIsxYM3lg2uxMLOW4bXwgBfy0mV45fRY3vaAUpZEO1fzOZvE3pwU0RgYe5cEbE11zlLNYP3lgl-D4YpAobTl3OFsJ_csIxk1YbfuMoeuzhv8ebpkXdQ0R7QGUVu5OvV6D_wTA_6v1d7caWlW8rzbsn5EVSz_cLVlKGVKmSKVfGEVb5VjaszcAy_GbJnovG0H2ps3B6oC6iH20p8Os3190gPYS6Y4oviSLEeQTsV5-xB3IzaxUJ5IWy4_l11zCjwMgNxKYHwCRTmaae5stYX12GQezI_GBJ4ZHmQg7B11b4vK6Fj7CCLVdw=w1920-h919&quot;&gt;
&lt;/div&gt;   --&gt;

&lt;blockquote&gt;
  &lt;p&gt;Prompt-as-Prefix&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;위의 framework를 통해 진행되며, Pre-trained LLM을 통해 시계열 데이터셋의 사전정보를 Embedding 시키고, 이전에 Patch Reprogramming의 결과물과 Concat한다.&lt;/p&gt;

&lt;h3 id=&quot;34-output-generation&quot;&gt;&lt;strong&gt;3.4. Output Generation&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;이제 위의 과정을 통해 만들어진 Final Input을 Pre-trained LLM에 넣어주고, 이후 나온 Output의 Prefix part를 제거한 후 원래 시계열 데이터 부분만 남기고 Output Representation을 진행한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Time-LLM_Time_Series_Forecasting_by_Reprogramming_Large_Language_Models/image3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;!-- &lt;div align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/fife/ALs6j_H8fmxe9ZCVIcYp5eaNXZAPs8Z12BEyg2ezINMaSrhiT49y0-F_XVPdzGFO6ydeh5Du5BJi9W4-s0iwGkzm_IuWedUtiNkLM7MCFKKYQd74JVMu6vzIwIzjcY-MPnCWhTp3-VlXp1DyvQjJoVq00dxVRSvtCCIFQCaMX_eoAP_4b_Bx7dK5MlVHfmNoTmx3JCjKVlQY3AwRkszlGX6-18bouoe9tSvra_FIAImwO2oXfvAxkt34rw6RwdKxMtHSbLKcu3qeX6xTvWWZEpKKTLRGX_0Nm6jdvzXf4GgHGdKsjLP0y-1LIaHtjHyWekh9JxY75iYYNL_Rrudeuf-9jT37VEnrUONtu3Q_Z7CyUXclGn87aKklNoe59nI5TidYbTHT65dx4pyuTmyzBV4uB2FovhbXEkE00xsct5rBY840bBDcfywBSZDxWw6V3Saqye3e8zd9D4CInK3aXJb_BSTbzIG_3zHX92dAUX758N5hFtIbeHaAnoRaRAIECxKvALBCFmZthFBXHPnFLazXWpGDjEIyvafn3GeXGDVsYLKS25kn_J38YPueqVUiqq_KE5Or4-GMoD-sQZ3h_t9Y_qfjU-zI7Q0GzbNFKaFt2KAuo4HoCZv6TyQbvqFw1doP8mwH9WTsdlR8bKIxCKTNsY4AYaX99MGOPaSPCoT1aT75nfjXop969u4Psuvs_tEYYiwdJnloc5inCuNYymhmL51mrkbl4cpjeFSvOkXOUQ5hN5q14P1yqJ7HYV-VLrbD-Db_OUW3uqB8EYGXEtZCdbHoD6QQrzVUmdEITVDpjruZjceWv0zMAq6Kl_R-USZWKWh0g9NLvUbGkMN5ChiXjn0kHZbpvuVGu_4UU7T74RctMnZHz4BGAQQ--K9EPHMmu6ja-7NUUh16gXxfdUTBtO3fLjL0LcW0iENLwAVdIAGtCcjFFPXIhhRhM6sKOVCoIpUl6ZxQhnCIDV_jcjzC0XLwCcDuAyHKhkGpG9aJUybeIdd6qfotUD7d0DxZcfFt-uafxSSuHq2fUv6sXR9E8t5bDJ6e9TJkpKNeM456O-ckzIJiJC__5Kzh-7x669Tb6clPCkU0MbMDTOEXcqyczLQPDin3-bdQ2g3AhAU2w-q1Q45gGDzlkFPy5DIjpIoXIZzUVRDoGBlJ3kkcZD6IPePJxlTDQUEpzzZ9V9Mm5LNlxFGeskXmp0Askx21xmXz1XwzwW5Bx_3Or9aIumo631hi_MXsqsinU2_ilLMg4c8go1Yd6YGbOd_Q0KPQJEKfXXuUDONvwXDWQYe-R2UHhrcEddfaQKORqAAp0UjtqTF1X0xVZ6bI5yG1ZoxKdrobvkehlR-Nvv0V_c9Fmi7ni0xlLK5K2fJ16w043dDnyGWmpFhLMkPPUZYe4lnR7dVvWmEKzFR3fIMRUP16q3U6RcKSNDDreRLdiXXV05wnjG2_p1_cpvTnyIprX1-xLPFl9kJTuxRnr9qepTxoylhbuwxyVuYmZGFdIfWP8c9iKe5_WXCOEQ3uSsu_4IR5LAx2KSRFfODge8MH5nT13lZqrRDNzm77ASMvm6E1LEF9V9gpGNSBI_o5KOOuOcoVg4T5DWlgdQlkTuT-=w1920-h919&quot;&gt;
&lt;/div&gt;   --&gt;

&lt;blockquote&gt;
  &lt;p&gt;Output Generation&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;마지막으로 그림과 같은 과정을 지나 나온 Output 시계열 embedding을 다시 시계열 형태로 바꿔주게 된다. 이때 데이터가 Patch 형태로 이루어져 있기 때문에 Flat하게 바꿔준 후 Projection을 진행한다.&lt;/p&gt;

&lt;p&gt;$\huge \tilde{\mathbf{O}}^{(i)} \in \mathbb{R}^{P \times D} \longrightarrow \hat{\mathbf{Y}}^{(i)} \in \mathbb{R}^{1 \times H}$&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Final Output&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;4-experiment&quot;&gt;&lt;strong&gt;4. Experiment&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&quot;41-dataset&quot;&gt;&lt;strong&gt;4.1. Dataset&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;아래와 같이 Long-term Forecasting과 Short-term Forecasting에 대한 데이터 셋을 나눠 놓았다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Time-LLM_Time_Series_Forecasting_by_Reprogramming_Large_Language_Models/image4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;!-- &lt;div align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/fife/ALs6j_EDFODBAxlmhc-1tWPNDbBy8sLgcSJ0mSTaKIR6QdUNhsUZtVooVGneHmY5QJgRESxv1p8ECIUOHaTWHsNWDRQaKlwwfMn2JQguJQofr0unmEsP0iWbt9XAo4_I16vn0gBg0xBZBXb4EQUj34ugsPm6nOlZmTfn4Fq9sZJy_e_jS_ZYSgDvtEJ05V-9wmHW_VdSkqRjXJrWNtBTuEXYBbtE7qPhjlgls6wC5YPiafFLVlfqozGYI1Ni6Yv6_yg6eFzsCOS9_3KTh9er4kC6D-H-ggAj4zglw3nbN8JeJCoYYwIQvjAfvLYpl4mgZtCtvNpnpwFsjifrKaMGpjXGxSUhoY0-QpUp3gBU4xelu8wdn6LKAgkZZi7xERfE0pSkrxlLAijCVyYFboU-Jq6t6jTig1pvjnhXY9VuNSFDlRk2mlZ9_wEwpNDnKg0hOjAjKI8Krc8kfX5p8zfURfsT48zzcnDA6QJfzGpKX9-rZgp5eTGaMNW77qNxxhdA5_6m_TTKNDb_-VeoSE2ZMHlKTOmLG3jTyg-U0aIXlDM_QtHIwJRmrbtaxvQ9CJOPT0KS5uA0mTHsfbxuH1-SJGePZ2j5osyWsfumYdx8LVTac2mBVg18rPbMfxrURswnBGFdfXwoSgTlDcWocmsKalVPtDX-Dxj8w1R1ys8XvD3FimW522Y-DM0aAJQM9chqi0k-NlLUpPZbntj5JoRFo5JIqIEOR9GAqLJhXWCQ9Vg0ySXNvjdPnpNBsdlwj8VozIgEcz0bNiX15e--Zb7GPSXIHD6lSW2gzGgdbURjVMPokxTeArBv2OQ2SD9S5srcxsrvUmd2eqDNDDJU4HjnUd4nO7J-_MYBrwteqSe1T_vMiJQnk773Q67-m1CmNZtKUoZ_H7UMgKrw9kdDp0D_Php1Sw1o2yNBktHxRcFXqmH_GmlaVYrgOhkthG3SoXnKW-viCl3EZwJIWYMFq71J5NYCOOW_2AroT8OkmNwMJTFr8g83heeG_tQwHWeIJ_XPslHEviNBUQgIO41-8qp3Z3vZun8LkplMil1-xW_2t6w6szWLuFWIsr4tVo8pO1m4hJVy6sHyYw_h2YCx-E7seNuHWv8n8TBCmNQ7nxIMj9Y0kWZbbV9uusWi_3Ov12Mg6nmh--gwADx4YXbtQnN4jf6AuzL2TdjiF2Jju9wJ-0ESn_Gkh6r2LfzjJEn9Dc9laIto1Hm0YZfToUhw-_NRy0024lZKb3vqninyarMp9nYXvfIMz5HX5D310gKJmEHPaql2lD--vv8sC-J10-8sSW8tId9JKm7LbeH05eqvSUwjpXVAt5n6e5ABxZ8bn8xEGvlSHNvaphFrRcniLE9rqAC1_V6jcIPObQ8u7ZwY7cUGPYE20D1g_HTLDaBU_mkjRVIVEfY0gsWYHezPB9jY0zXqYzkzJbhj7PujaqZ4_tMGJl6ZNEbsvqDnnY8sJZhP47e_gPcG9pqqJw8RpWQDVvkLduj1Tl4og2lK26wZle8qrz-WU5bchhSfVO2VpkSIwg4sh2SdSd3hVO9eogPtGkEOh1-kEo1Xpx5AnBFLCE9wWGklieMtDdLQNXjHrBQXwQHmwOY1cgGz03nW=w1920-h919&quot;&gt;
&lt;/div&gt;   --&gt;

&lt;blockquote&gt;
  &lt;p&gt;Dataset for Experiments&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;이 데이터 셋에 대해 Input window size는 512로 모두 고정했고, 다만 데이터셋의 규모가 매우 작은 ILI만 예외로 window size를 96으로 진행했다. 이는 보통 Input window size를 96으로 고정하고 ILI 셋의 경우 36으로 지정하는 것과 다르게 본 논문에서 볼 수 있는 특이한 양상이다.&lt;/p&gt;

&lt;h3 id=&quot;42-evaluation-metric&quot;&gt;&lt;strong&gt;4.2. Evaluation Metric&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;본 논문은 Long-term Forecasting의 경우 MSE, MAE를 Evaluation Metric으로 사용했고, Short-term Forecastiong의 경우는 SMAPE, MSAE, OWA를 사용했다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Time-LLM_Time_Series_Forecasting_by_Reprogramming_Large_Language_Models/image5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;!-- &lt;div align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/fife/ALs6j_G6HzrC9U6bExOGxx8MGOORhA1ZAeQ8WhfaRkTI39E_oJzhGSnSIEu-YsJDf1v-kmb4HM3PBu8Pcl8l2d9amwi7Lu28J-trGaMCY2QSzB1zvjE_d5L_Uj7kprZSaRH6F5gHi-vZ8myBqRfYEWB6LIJ9KYUitpPt8aY3XjJq8x65b8Y9oe9HTvlnCYajJyTcWuuQ3jVQ9UiIuyT3O1NlRgrX_5XW39UeENrbXuz1R7lFLAcFqn4tbyrKYS_5hCBndDFRTSe9a99K8rZPVmakvCTDc-XgvVHpjJNudeWr0_PzRnJq_g1_3IhwpU96mj0X6ljwxLmY1DApEOB4jEIecEiB5GlYsgkgd98WWXK3DGVea5yB1cxGNuqai0JuIgjkM1myAiIaSugePdDXCbbRAroEMgck09uJjgQr1vpfH_b-nfcrBar9w15gmegnc0XG1sgP3VCO1ZjCnVIGTaqiT2qfduucvt154FUfEV3sZ43z2jYjJHLKFFTg701CmaiIGN82xKOwhvdyYdhBcB2yx2L1PtC3-NJAa2Nk-PFZPE8n9HOG0KF8HhSj21SS5HXLVmp5RpNCoeySIAfIAOf45tt6MGfcVngcL9qJ7s6ahBkb65-Kwm5WLsrN8Resqt39E1T9-L5-DhJInifNHIzz3IbtvUBRio-vW3BRoVb-VB4VHVXtspEpiWvtm3G75WE1dvVEJRtDJyAG5YOV9IhuaKPuHRVRH4iBhIiPcO0XE37gW5-RB54U_neQBifxl4d0g1gaBntAdvYKVDCTFjHpXlmDI5kEWD0OoarxFnL456dmBSDUP5liw4kUMXDpWpRDo8q5IpWPq9ItxCTgXJLLCjimAv0d5DfW-0wZTweQCLu6ucZjWTvhkYgL9QDFgraOuORT6UgPuzwC-OrJyjch8yHmWK6x7o0ak4LNIZAyeAVEwA1d2kphioFbXTrzQvykXQQAUPPwWO_jBdZTDTsX4XIQziXF8SGYktzDM2MRtTFyElqwlDhsDq-1zGHu91UdFotuBH05kcCXzfEejY8SNJ8VhikkGu8iSJHxOJ5LAoMg7mF7dpdC0r0Fo4k-5_oJRqJPXxN99vtCIICz8C1lcpoI-h9Tycet7vuiLTUO_C98js82ku1uEF-xO6D6fq8Uw9JGiKxKYuCbNgEeR7nIbWOdlN2PkFmnRqA5mdVnSbVODNOoPZ-fqK64XvLvw5KtGiS9FrrRuKI0gw07ZcGZ79fd0p9O-aUgT8xs08wDx0YdE0Rfk0e7OuqnecQAi-QhBzsTts-EtJecBvUqgytFRKP6aeNRApna-WQhZdML7YiGsT-iyDPIoEJLXcMe5Z9cv8aB80xvSPlWDRtsrMZJ-BkARQtNlj_q5uWzKZ4-5XWTQV6iQe2oXuoIKTSl6u-WvfEW7vXjHlWXCyiLRxxAylEnlLIkxd2FeCx4HgergbSyOPDGuYj0ngW20qTmQFDwEwKudMgvYO0YM8MoWgGlMaGRSA5-w3lfoEwK6esrKquAN_uq1t7lW_j90hALfpOl4qvDWcKFQ767PIQzbHeBzrGqtYtIw08kT2rC-gXwJMOSLljOC3lQn1Lg-cELVBFzbfBzQ615NFpMig=w1920-h919&quot;&gt;
&lt;/div&gt;   --&gt;

&lt;blockquote&gt;
  &lt;p&gt;Evaluation Metric&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;43-baseline&quot;&gt;&lt;strong&gt;4.3. Baseline&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;LLM Backbone은 Llama-7B을 사용했다. 아래는 Baseline을 보여주는 표이다.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Transformer based Model&lt;/th&gt;
      &lt;th&gt;MLP based Model&lt;/th&gt;
      &lt;th&gt;CNN based Model&lt;/th&gt;
      &lt;th&gt;LLM based Model&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;PatchTST(2023),&lt;/td&gt;
      &lt;td&gt;Dliner(2023),&lt;/td&gt;
      &lt;td&gt;TimesNet(2023)&lt;/td&gt;
      &lt;td&gt;GPT4TS(OFA, 2023),&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;FEDformer(2022),&lt;/td&gt;
      &lt;td&gt;LightTS(2022)&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;LLMTime(2023)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Autoformer(2021),&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Non-stationary transformer(2022),&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ETSformer(2022),&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Informer(2021), Reformer(2020)&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;44-result&quot;&gt;&lt;strong&gt;4.4. Result&lt;/strong&gt;&lt;/h3&gt;
&lt;h4 id=&quot;441-long-term-forecasting&quot;&gt;&lt;strong&gt;4.4.1. Long-term Forecasting&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Time-LLM_Time_Series_Forecasting_by_Reprogramming_Large_Language_Models/image6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;!-- &lt;div align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/fife/ALs6j_GpWQxQHoNEOijHLapEbdg8YYTZFtE6OA29eiDY1zpzGYm8j8yLoTIrPHGJiN3LdBav9v2Fzoe5KYWy_HL1QlBT7pgSfi7fC81M2509Duc5sytheA4Tgi9Oi_yphVe9Vv4XO2MKzO-YIaytdLntaF72w-nB-MkLsFHUcwT5jIfl_LV7CJtIUaMlrvTQl51fFWBwfCr_vF2Rox94WYtnPw_KN_oAyGCkxeJTN1xcBUMpAWMqas3j3uf4MAaA7kfC3uKNmWhjrMy1yTrsrg0tlg28AvNfGarFjrVu9kwPs51EWPEo_ofCPmgjNNA5u4wHORX6q1lM7QoDxz_38vi92eCqwdjo9efePts2t_chadCJ49jRU-WvjJvxo3tZPP4Zp5lPptmqY7UYQQeIEeKDSrXsZ2HLvkcNe1V7-4fZKXClw8ulocXQCW0oR96bDqKLYInO6MpmDINvpsqIU7N1QqmcUj9xEaN8vRw1ib6n0ZI6CPDqYUNXVaVgBZzlsuQcmoQ1v29AusiERK8pNCU3D_EWyd89UcWRl4yiWXyc2cp3UNMhpox8Yor4dEGDKHFYdD7m8pC9T8z349n9Su8bcxcZptlS2R2vEyfJ44cgJa5uYnVoiUUsXGEG7c6kz981gjvlbDT1eTTAc6VzCn-ve8l_jyNoPwQTEyqKTSb92VAfhwSq7Lv68tryu5KJAV8Ud8hkA0SXoQPUDA3Kl1bONMfI5jvpAGGIItl0tTFaSEmdCAUTm6kEDMXWLynPIBF7okBUou1JQtU-JjJaq-WmZ3--w0rYE8MoTot-XtAaxq8krg2i02qMpWBpX4fwiuI3ySBbQP2x3r4zBLdy08CirRiSE-CUpCe1iXqNtZVVl0MtdqTPzR5X_pVzYRsOV4vHIUaWJ7t3lEk9HCLJF4lwgAOo5xNidAUfW-qpjFZCp4-cBxmkq1bhodbHmlbC8nPEaT2Zj4lIDoQ-Ft95X2R-dYtOVgKBDr59nQMoA80mvKOaDnOpFJgZ_5Rj0PtepmwtkIJM4xxrSFzU1QtCLleaSQH1XuJb2XlVjXiojJiztfuVMCw8FAp_2mHFC65u0psqJz54UaNZVUrj7l8YTa7f0hez3KeAC3WaSKDJEygQZpi96z8WVD483t7JNBhaqXgNV2ZJohs8w14yML6REBFH4lNE3yKhDtqrOBy3t_Wgrzn8g33INmG1Ff8MRiDv408CVj6-ah-LRAs5Q2WpjWeR-9C8ftMKjh9iVrl-v8tUbKC6kOWtzlpW7N5Sa8cpwWtWBeTv67EbQquo08sG15WMJiNP74JpyX_iSZY72XQMbPV_LF3eyNy9wqiztNfzL_4g4aCfMmQATe89T4aNRxb3Fx4nsLdA_64C9M_Ks2d_UG9KsUffI2moNDdCmHpgWWjhGuvqLSaifBoA0Ptm4qtq3Z7oRLCon_d0xu0EyAMDVg15ph2rieh-Hem85A91FqOUtOn9rFyHA4FlCJ4yt-sqvyck_vaUK7ZGQW1PyEP2FzvWDNKdyAepXCfriI7pvL3qATihM3ec2UpvmTc_w1pkD0vwsmoDYHwxfGkApA6CMp1odG-yyXSChf2iMVTjXuth9aJXsXm-TZjQ=w1920-h919&quot;&gt;
&lt;/div&gt;   --&gt;

&lt;blockquote&gt;
  &lt;p&gt;Long-term Prediction Result&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;여러 종류의 Prediction Horizon(예측하는 y의 길이)을 사용해 Metric 평균 값으로 정리한 결과 대부분의 경우 모든 Baseline보다 뛰어난 성능을 보였다. 기존 SOTA model인 PatchTST에 비해 MSE가 감소한 것과 본 논문의 모델과 유사하게 LLM을 사용하는 GPT4TS에 비해 큰 성능 향상을 보이는 것이 특별한 점이다. 하지만, GPT4TS는 Backbone 모델을 GPT2를 사용했기에 이에 유의해야 한다.&lt;/p&gt;

&lt;h4 id=&quot;442-short-term-forecasting&quot;&gt;&lt;strong&gt;4.4.2. Short-term Forecasting&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Time-LLM_Time_Series_Forecasting_by_Reprogramming_Large_Language_Models/image7.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;!-- &lt;div align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/fife/ALs6j_HPHyo12MW2GMk2BVmDunSEB2fjtP3gqWx_i6CzkDji4EqNKbfHaRY_ruUjT7PHfe9C6pFVuw3pLid-Bedh7-gf7TueeWFjzzmeISbg2c_V_d4nfJ1VYadnnUi3Ro3PHQILEHFaxV5gdYXRZcw87JJVl3CVhU1lmiTKX7OvMY7Vrm9bIP8JjZFCgxqZgjvstk5vpYB4UPWuoxJObYKUkoNImtJb6ha8ES64WfEDCKbuTHfVxHJ8ZzGLiHDDdQJxIC0Uc0LFC-3gED4hPe-lHFxhxYYLXPYNsaGYn6vV34zCaS1nPVMUJtSyGQ5vFUpyWebU-Qa-I0zR7uO2sRZz0Qr2JwK_Uhhcatuvzdds7avGJdZNDJBqDuc_FvOWH0Mq0Rwm1yFFbXsBOFhN8JEyqOow-IFoAD01qC-Lnw1pp7vdvH09KPqvf6Kr7OoIG_unEv1vhAnnBxXERGbV4WJvA8fBZNer9JzjtlLLyKmQOzUKPqMtTOTWgL535e9eKWETUlDmdmH0lrY09eMRb1DiMbTgkXgy_IGiDDXXJWwoMim1YuLhhg2JifLLA2Joi6YNOMsA7b7euKofcdnnjjdIZHHUBCFitXwjxuYh46vQJOMxUlLLvH1YC9ZgjqWLzhQWRoJJNG44S2ZM7YSIAPjlyj6DAgOc28Mc_xZHokMMjZYuEbp7-UAPqFWKj9CsAWksR2XC-HOVnPrez-lCJYbr54P8oVjcQlBFKff5SGttrUeY6u3KJxscQrVYAdGssTH2BJ57wA9A-sIUWMCEZrop-ajQmQDpejGgoT5KxlsimZjZ_4HDch9hUULlZC7EtlMrxtDOezCb3_rJ5wTRsO4JOiTsXjAf8fKjxu5mcH1DgbBLPH9jE6Sx_H7cJuf1hJ7vdzXVWpOAELLQ6EqeYRzPd0fFrxTAJC2f2d73A5cy8S-zV1DTlQRGruXQTm4hI8qiZCQOOWzd16gVofk5Fq39_c-fWVNOujZ41buA6Cju3UO612GegsSZTFhPLxgBH6XezCwASeeHWx0V092nTGd-yaFMoYkT_gzIvNBh3M7mCiYk-QgTj-LNgwlYXxtXztV3Fu-3z1Y6EVMhNaZm4zAAORv9AwwTTKnjfw4Lk-4iYnaQQ0ZUwoQkMUCOJNU7NTvXD7fVJjyDvS6wI0oMr3-Rk7vdAWS1Q-2Un60pRFZR6W7MsQ7zUPchu5cFElkuNXB9tOW695FrkmXoEeWDBougx3O71CoS9z84FwPswrLDGQRa0zSpH91fqvjx61BA5KM2T8ni7qMlM8vwpenKLdyPQ0FBzYLNTqehzSKFwcpcR3WElPJYQ39AyUeCWNBMsyK_nipFg8ABI3OwlX9fZzbpSIlMQ38zJXlOjxKojVhm8UjCIB9yHYL3sEAUFVPovbN7dzzOrng-_bhp5w7QKS1Oeyu_WZ80GSGoYaIZcuCPxKBhWgZBROyn5OgaUiWcHIPBEYebCMt8K2v1-5xzRJsB9qSFWIyUEMwoZZnB2t1hM__4q88SeqcPGILMvZybvzmArTQh1xYXwdb9eV67cToKQd8HMEeZLsSB_Ys8T7cI94GQ9pxOuyMHXf1b8MiA2lH2gcdrklZqJLrUcA=w1920-h919&quot;&gt;
&lt;/div&gt;   --&gt;

&lt;blockquote&gt;
  &lt;p&gt;Short-term Prediction Result&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Short-term Prediction에 사용되는 데이터셋은 Prediction horizon을 6과 48 사이의 값을 사용하고, Input length는 Prediction horizon의 2배의 길이를 채택했다. 이 경우 또한, Time-LLm이 기존 SOTA인 N-HiTS와 비교했을 때보다 좋은 성능을 갖고, 모든 Baseline보다 뛰어난 성능을 보인다. 특히나, 같은 LLM을 사용하는 GPT4TS보다 성능이 우수하여 장단기 시계열 예측 모두 더 좋은 성능을 갖는다고 말할 수 있다.&lt;/p&gt;

&lt;h4 id=&quot;443-few-shot-learning&quot;&gt;&lt;strong&gt;4.4.3. Few-shot Learning&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Time-LLM_Time_Series_Forecasting_by_Reprogramming_Large_Language_Models/image8.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;!-- &lt;div align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/fife/ALs6j_G5auJGqnm-o0xWvURe8VtU_EreqZ4vs501dy3Keak54-wefOGdelWmRaayXISUTaaRt-WC1niLR6nInaFklU4gUlDQGH2qeUKooSg015fpLQjtQ36SWGvodqTrftsHaiKrE4PmXJ3rAb9HFO6zWRZC-3cDsr6GkO-XLSXrWy4NbHXj9FcWR_1CjMCylVdlhQFH9KVvqbMsBXFlqqguNDMyZKrRiM28gG_sGaLA0NNrMGTFcMVGPOkTHrCidwAm2THZomHapM3fksTHepYIoxf4g-ZeIn-U1ynskWlug5jA2kU9B6f5A-1KnXPzvuqITu0v32TT15qJhY8NYHrmktOZaPKWGPHtQY3ia8bf9VCly4nYBUZV6R8FWRfvBcRaZBuXCp53JmG6YwYQeldG20QAs-9ZIGETaRS9bO_hSrGZ5SBOCMNdwX7thgV_95cTZLuYaUBou1tTmjsX_ArrYbULyrFCtBEtTUcEmGOlxP2GC2vKMB6Rt7g5hBcjuLb0EmbT4K24iY530QvjXdWmK9nWq9wSbDUO5Ro5RABhU230_ZZ1iI5zEnYds5itonVsBXnEynl1pFJE32SYch7xcmaDaAAZ5wmTe30XsBiyBJLJnnVb7bhDoqrjRoW3n4iVv5e1gQGRNueIolN7Xfg-QNVyDKhQZhIAk6A6bqc3ThvR0-uJWp303ynpbvS4LaGo2pK2TD1hNcSWSSetzhzRYYYXyu3zDHCSnZS9Z0__L5FMSaofbwi-w9VUqkkiBCiwxuDyOiNmULfXe0xAdB5iE6PUCgoKzuKOFdKDI53FBzUJmyLvO-KXmeHnn_P1paY_yOuRDNd-Rmd46rY8MtjIk2jyWHxYgAjKezcznVBOWp1ZcLiFXLTYkc-tGe7DSlZCdQBvbTSRPCYtNfXY8COL9brX-vFHaLWvyHviyh7SonmBCwy2iyK-Pm_YzeZGTmWEmEuHad1NjlRjr8vdQz1PnVqsHMDkfaYvwo57cu3Kg7NFNHwiRAm_Auk7btmminDgkGnsLfUTwAeehSMV0WgMSSpH5yKm5vSmsB-r3ychTbcV-DAPN-zELkExJp3IKD52M9WHpX31_8Sj5DrakeSoJOj-PVArEsEA6MqZvlTZyncPrtFZNe5BdtP1IwxVDZNfI_ldMnVLQnk5O0RtX_Y5dxwUapn8hmliBupdWs5rlvkoJXQqJDViw-t_HVzN_S5DwnIKdDw3BBksFQmQh3qiOC1A46V0qGHQvtAlZWUxq3nxdcog1Xefs83x3dks5HY7zEOoGAlJ3wIre1LXDCYzniJYbATYWbKN0nvOOCDZwyfSeuWIx5K_CMXE7HgY64OFoWvJ5JXu8OFrc6HGjTZAEB7tX4tOAAEpy5_pEoeTTli8ItYBhf42qVQD_CqlmW357-Ae7RSv9K4XHCxtm8DP2TzaxjARemfrY5Cg-rCX-exHLcRe-9-9_0WuLkW9C5GCfBQ1l0cFgu1mORrZa8t0tn3l-ncvliugkFS0liYZRPHsUsEovlOqnPhblq1zx5QXYVANiNlVCUGFbJ49TP1RH_99uew9v3oXoBjGH1DA4kznUGSvUEPkCBXvx4qbFzRH7XYx1ViT_nX92Q=w1920-h919&quot;&gt;
&lt;/div&gt;   --&gt;

&lt;blockquote&gt;
  &lt;p&gt;Few-shot Learning Result with 10% Training Data&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Few-shot learning을 사용해서 예측을 진행한 결과 대부분의 Baseline보다 좋은 성능을 갖는다. 또한, LLM 기반의 모델들이 좋은 성능을 보이는 것을 알 수 있는데 이는 Pre-trained LLM이 자체적으로 뛰어난 패턴 인식 능력과 추론 능력을 가지고 있기 때문이라고 말할 수 있다. 특히, 위 결과를 보면 10%의 Training Data만 사용한 Few-shot learning의 경우 거의 모든 데이터셋에서 가장 좋은 성능을 갖는다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Time-LLM_Time_Series_Forecasting_by_Reprogramming_Large_Language_Models/image9.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;!-- &lt;div align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/fife/ALs6j_HiuZYla4SM86JSHdZwuUbYxP8DhK8c45Oqhlv52WnIt9sVlEdDD2Cd86CMmgKEk4wbsPMllo_aiV6PMznoikfCJRXC47GaoYUBjYF5EyFd1PfE7S8m1UBHQguyKLVNKvaC2BV-ARyzBLkOcwGAkj53AdQU6PnsV9_NsBPY1CMnNY_jefdL9eLiMBUkzd1n16oYB2wSArHZIV9A4A4UgiKol9GUY3yc1qyYMF5EzJRsiwOkMkdG51tP7qlYgPoZ8gme_VdMSR6-9m_tGm5lvob6bVx96kYxzCMvb6jY6-bbHV_9h2CEMUXAZx0v7UxL9JXvXDNd-ewB_0gW72U_Tq2jeQG2es9RRavLk8JjlUQlFnnrIK8FImsvFNIuqDaf_md1aqRYB7XYbipTjZwydUmaiv5F9kCnj4zvdRJnPnTzVaIdDsjUdo2vAPx7TksDBMZCNmb52KUMOES4Zxa10onsCEPzOmymXJfWndNSzICoprOzMTG7R1fc0YHYYF38qGEhdfI8iyw5DIT7nIktnK7cyaB_WOx3skJcK6Q7Z7ZTM1wfSAERRfe9VPoeH8t30TMjSjKsdC6a8ZW8kqk59aMU3vaE1mggSs8lLXfEe-u6b7Svv7VPXzcM5RBpqv_EdhE6-sU0HwsOhP8pf0RzzY_X--K4JA380KVZwzyxDVgf-Fl2qQOOzhTu--61cbrNQlTnjzRsgL5owBFUedl6Pv-NGWuQ88pDchZc8X8YxmBBZp9poq-Ho6UJrMsBCURMqU6q_nmv9JV-BVb_qxpwGpRiA_LifSfekccCpHuQ4jNMtcHe-ue5-5qbZhFO4xUVFrv9maXCo_86roAqBILxNI0YD24lVA5t0JdRQLWgahq8_ZMIX2cLnuOCsDMC2UeIIl4Obum40fA5PXtenwAfs-8djqp8mS2pOoQaYg9n3b9bGFmMLqcZeODiKMbCUVDEFOwJohU4Rba8UXI034ECEyPqarUsE4WSmQ28gGskUj3lPSJmeo21vZhJntQWhRVm13-2_DyeHJt6bNK7A123ZXZWQQ4VX2FSDe5PfXaYKTmLi59DvF2WCGloT6UBS5tEfbCbi9YNJYd0s4rydC2dBMCYeZs-0qpPu-h6BykeVFta8TYzkvKwBuzm3f23Sm4W1p7LOxL1c8M8KE6-ch-7qyf0KV7pAdUfkIOUeQiXg-YIt2HGxbQtH87kesJvCG50WygsUfHNNxPYCxQBf117gTy7DBDbvhQwVYIuc4TSQ4jnbbD8LlmcAMcB8RhOvpOCJ8FnfbpENbVMa-1vZE89vk0P_8yXl6hdNd9c0ookCT3zcBStYaN9tobKP9noW9hFpg8_ndTSUqCtLFYR39F8W6G4FBp9lb2Jm-YcyZ7UzXah4xcqrmtqGvq6hQlrWNc-5ufgdN69vTREwk9rUCRbhwX9PWcX44Z5U5BP5pkIedGr2EQBFlwKDscxKIWF97W85oRx_a32vY5bA1Fi38GIed0mo8y1nmIMv-y4CHj3z-C9nDdcC5VzEPyCsR_keJGNg2KxlrObPMsi8dZzUwYSMi9v_Bn4vjRNYgdNgUbQj54Jy-pTBsZaLiZldF3MQpxbPsKnLNzrWQgbdQ=w1920-h919&quot;&gt;
&lt;/div&gt;   --&gt;

&lt;blockquote&gt;
  &lt;p&gt;Few-shot Learning Result with 5% Training Data&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;5%의 Training Data만 사용한 Few-shot learning의 경우에서는 종종 Transformer의 변형 Model들도 좋은 결과를 갖는 것을 보이는데, 이는 LLM의 패턴 인식을 활용하기 위한 데이터의 양이 비교적 부족함을 의미한다.&lt;/p&gt;

&lt;h4 id=&quot;444-zero-shot-learning&quot;&gt;&lt;strong&gt;4.4.4. Zero-shot Learning&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Time-LLM_Time_Series_Forecasting_by_Reprogramming_Large_Language_Models/image10.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;!-- &lt;div align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/fife/ALs6j_G1XibOUXSRy7dzfWZHt1gLhE3qCr1PDLTNThuoGLMm1e3_K6Dqh3Um5_LPSl_UKpmnbEncHfc0V3fIl3WuHejx6XedVc6LInoye_7fS_uTXaNau5VcAS_D7-C3pOrK4tXQViIDzNr1yP513nHLrIHYlmEHbThMFTpstCHFC8YH2cTs0fLl_DUPrHaXgmqMgXWlvPi7VejTTI2i2P1AcNMS8SR9tguimWbh0EaTo3bmsz1usOEeJCd_oPiCeAkJwlMFFWVGPqtPxF08lWMs1isDhUysE2zBXATi_NPp5x9XolN_oEy55idIUX27O1SqWbvKF9vwjQgzeIlYUFqnpLJeGGm1evFrG2xTsTFF4Wl_izENHlppe7ynNGmh3fYFtZtD6BLgEtJ3e_i8opIARUWsg1cZ_z-x9JM5x6BafC1RLxNxc_pnVAVBd7g-Ju5DJC40vxC-MvI5srTBNWTXSGXG9hyChnFJUE1jBVr1tGZeOpdOL0jpViNIRBdq_P3_GaLjhBj6MyDXQahto7ipbHhTXWEctqd4Liz0eb6y5racWAcaeIHX_Dwsfkf_2PgOvcMU-6rsDPyeuRIRv1F6qY2l9vCVxDI2cv7sbu1MOnqA01gZhRLsHUHyHnc8iat1DkPZR2CMpf1fmgxJlbld45-DFkMCo6bUy5oy-72TZXI8jHPrg7CEZUrcgLM1sbYlqZCbSV7rMKouiDihjwl5lZVFxXLohLbNO5XuL65JUPHZ5XvzLXphqMCYs0wtFEq-SuAHhcCFWPwu6XsFI27ij-jm1VvoOSbnlbjZioBtQkpBwi0D-J7ge_yKU_0ZQLx2O13EMEdIzEdglM1-Tp0-jPWYg5InIbyH168Cn-P-TozWmNl7-2T7EzsQZZPmo3Hi8-4RBa3BfcJfDcw-KShIwegoaCcm5Qp0vXOMQacWNC9dZQ2USg44OzSdwL1JzaRUuMuu4_I9cZAiPDtEgiQxqGNmOjx_KS8LDWGYSV-EsPl7DxJosN7YvdhRw6N-HKNmiF8Ufh2TuuAUSa1qWLx_r-DBBBriMDdxFIWosaEk0HdztFdM0f-bWKX5L7cZy7E7Dy9e_jP6xJXIjP73WISZBPecPA-G__UxOBHOYa49Ggp-v_YvHPiGsid7QjCUT65giCkYVdJNYcT02fXh7wEI23IcZ5e3R7RY1NmaRp4ZdcUeTMc-v-BDNglpCz3zR0Ah3J_AL79XiZhPi3UEC7B5LnizJi0sXLBNzfUCglD0bEkVd_R6lOpLJE5ZPfiyS8xfiBwqmy4wAXTaVCvKV83ovnSQq8I7BOs0Qlk-rR4F3Yjq5FoprDzKAQKbyOUDN2DN6sZ5az5hNllw-mcd5-xKhnUQtPARkGaXPUUDuNHzwsYkiSPbieU6HW2BnFynzsOdtsCfaDiMFkowxBwleEYM9Qj5Y8sgi-9q12UlvdG5Dc7Z-y3ITe-93DQLYkE76XOCGNaSeHfk9Bw4keV-t-9cf12BSZKSbzkqisqfRzk2AP-WCSdQWhr3gWZ-36_IrgjMb_iQUWDYeNA1hWtdy9r4RGvFjJBzR5Pl88F_aWDsa3xs2iZP4F3-wvqvs4E-f9Kq8YM0hYD4ywSizA=w1365-h919&quot;&gt;
&lt;/div&gt;   --&gt;

&lt;blockquote&gt;
  &lt;p&gt;Zero-shot Learning Result&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;위의 그림에서 A –&amp;gt; B라고 적혀져 있는 것은 A 데이터셋으로 학습한 후, B 데이터셋으로 테스트한 결과를 보여주는 것이다.&lt;/p&gt;

&lt;p&gt;전체적으로 모든 Baseline보다 좋은 성능을 갖으며, 특히나 다른 LLM 기반 모델들보다 훨씬 좋은 성능을 보이기에 현실적으로 시계열 데이터가 부족한 상황에서 더 유용하다고 생각한다.&lt;/p&gt;

&lt;h4 id=&quot;445-model-analysis&quot;&gt;&lt;strong&gt;4.4.5. Model Analysis&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;이 파트에서는 Abalation analysis를 진행하여 Language Model별, Multi Modality Data를 처리하는 방법별, External Info별로 성능이 어떻게 변하는지 결과를 보여준다. 특히, 본 논문의 Framework는 잘 따르면서 LLM만 Llama(32)를 사용한 것이 가장 좋은 성능을 내었다. 이를 통해 결국 Backbone Model의 성능이 시계열 분석의 성능을 좌지우지함을 예상할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Time-LLM_Time_Series_Forecasting_by_Reprogramming_Large_Language_Models/image11.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;!-- &lt;div align=&quot;center&quot;&gt;
    &lt;img src=&quot;https://lh3.googleusercontent.com/fife/ALs6j_Hfz13PEddITymSpkbnpX4M9kCS7x_WEwslHm6vKR8CPXdxsUlnr7D4HsHAwIQUPuB9d37bD86KQsgEbHI7YxhlZEeO6_9ulpUi2UONhEcddM_dFsO974jGmabyjkrjzoV4B33YbVDJ6lDcdZ22foqDAxNwGYP7K6LQVhbL__mEafY_DRvUCNBPeRomcz_Woam8RHEB04jjsfJVJYhtyGFskmIlP2JYJWesNoAN3srsfLo0icXOFMZsOc_-kesuh6YMaCbnDHexpSyiydv2YtmC0hL4YxxBqSJCtB42qlwG-Zk4jQXPKrU-VoxHGQvCxCwGLR70hPjvsbwJolaiuOyxg1qG5eHDxqhI9yXWc6cnBVB4GZtjFrtue5wAAnpb9PHgIg0qqxaN89yKNemKTJqjTJzlxTkSaKjUgWy3mP8bHdAnFilH53VWJvGOtiTqSZf9ncwLNYUqsDSxLqx5cZ_pGWnJevpVPEaHs09Gjc87GNTtgcZHm-IiJvtFeSEVqEbsAfMC_NOWAfnkARgoLvx3bhNqul1tnV07gWMQOHok9OD9QcJbO0UvA9q1iyY8YVi52Au0gkt0dAyQMiAqhnBGTQIZseHdsvxc9HJV-99FqYFIi2ihqNNdiyjlpyog5utMHdo9j-iMlSrqRThr5OCXdKPntjwAuRLjfSP5jtbevIVuqI_Zu2jmdyomfj07f-4mDwtsYsX7NA7Cs6eKbkV_66vE5-x1TpjC55PrW4Gzxv5X30ZsEUS8GpPtlXkijeV8a5IT4oxgtHDqB_kabzEUDI0h6whHgYfN23lME0UXfVf5EgmHrc5pqRq8Dc0uwvI6JT6nN6Aje6jKlKHkfbCKviQdT6IfE7eJvmk69ic-tYc__InZ75hwYugQ-X4m2ZMu4U5T9WoScQvcfCUonNYFKRMhZGX4d3k-ItTThRfMISST9sOAb-Q9JarJJ7ZLuUMjI-ujauGPGyxEBjj4oJ16h51JtIZ8VPBTQTiBGbKJ7SLcZpsBXKjl1F7Ve8MQhD-uO8fTYQz2-SxJmtfGBIVd9ITOirs6P-PRG9Vhk_Zg2NPlzjDUNu8QK2Ro-P-bL3hNrUpS4ILv4D-fniO0DbE8NkNowhjshy1ER4IZEzJ6v-ttOEo0dixa_9Ynhyjjknhx0YsdvdizANY_tZrKlZ7R78GK7Y2rtRQzNyJ3y8alFjnXWBoXfE8PuawLgFhOEVRcSg2_oGVH3t6D7rASFO_E_-zW3f4pvnR-rqIk5KZEgdr1JgcyyOqdXsY3pAhWJaj2x5gr-T6h-LNYaLlVJMBxGtne1avJt6HVVhGNQgixEZa-UuhVKrVx1IiQ2avW95scGdzHRqrTSLOLV2frH6asPmcG7N2T41JDK_XomflcKwHCsDGwKxWsGUEyHK7IZOLg7cNH--c0DNZqNndJdCY4LuNV20j5-K_4lYsJC42zb3iKgCUlD3wHEWGwMdCdVfCJNrlbR8T0OcqJKUWCV4_b-4uSbZN7qKOs7GFDJEdeQBAWhgGX2UFdRf7NgJ3LbTjY3gOQI1ZbWvNZl3D-7XHR16F7S8O0fHfz1MNc3rPS2zzHZSTS-eHzn4pyt4Oa9z03L9Ida2IL=w1365-h919&quot;&gt;
&lt;/div&gt;   --&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Ablation Result(MSE reported)&lt;/p&gt;
  &lt;h2 id=&quot;5-conclusion&quot;&gt;&lt;strong&gt;5. Conclusion&lt;/strong&gt;&lt;/h2&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;ㄱ-llm-기반-시계열-데이터-예측-방법인-time-llm-제안&quot;&gt;&lt;strong&gt;ㄱ) LLM 기반 시계열 데이터 예측 방법인 Time-LLM 제안&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;시계열 데이터와 Natural Language를 align시키는 Patch reprogramming을 선보였고, 나아가 LLM의 성능을 높이기 위한 Prompt-as-Prefix를 제안하였다. 특히, 이전의 연구에서는 Natural language와 시계열 데이터 간의 관계를 크게 고려하지 않았지만 이 연구를 통해 직접적으로 연결고리를 고려하여 성능을 높이는데 성공했다.&lt;/p&gt;

&lt;h4 id=&quot;ㄴ-baseline보다-더-나은-성능&quot;&gt;&lt;strong&gt;ㄴ) Baseline보다 더 나은 성능&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Long-term, Short-term, Few-show, Zero-show에 대한 예측 성능이 거의 모든 Baseline보다 좋은 결과를 보여줬고, 특히나 Zero-shot의 경우 다른 LLM 기반 모델에 비해서 크게 뛰어난 성능을 가지고 있어, 현실 세계에 더 유용하다고 생각된다.&lt;/p&gt;

&lt;p&gt;하지만 아직 LLM의 무엇이 시계열 데이터 예측이나 분석을 잘 수행시킬 수 있는지는 알려지지 않았으며, 패턴 인식 혹은 추론 능력 정도로만 가정할 뿐이기 때문에 앞으로 더 깊은 탐구가 필요하다. 개인적으로 Backbone LLM의 발전은 결국 성능의 증가를 불러일으킬 것이라고 생각하며, 앞으로 발전된 모델이 나올 때마다 시계열 분석을 진행하면 좋을 것 같다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;author-information&quot;&gt;&lt;strong&gt;Author Information&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Sanha Chang
    &lt;ul&gt;
      &lt;li&gt;Affiliation: &lt;a href=&quot;https://istat.kaist.ac.kr/&quot;&gt;iStat Lab&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;Research Topic: Spatial-Temporal Data Analysis, Deep Learning&lt;/li&gt;
      &lt;li&gt;Contact: jsh0319@kaist.ac.kr&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-reference--additional-materials&quot;&gt;&lt;strong&gt;6. Reference &amp;amp; Additional materials&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Please write the reference. If paper provides the public code or other materials, refer them.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Github Implementation
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/KimMeen/Time-LLM&quot;&gt;Code for Paper&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Reference
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://openreview.net/forum?id=Unb5CVPtae&quot;&gt;Time-LLM: Time Series Forecasting by Reprogramming Large Language Models&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://openreview.net/forum?id=cGDAkQo1C0p&quot; title=&quot;Reversible Instance Normalization&quot;&gt;Reversible Instance Normalization for Accurate Time-Series Forecasting against Distribution Shift&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</description>
            <pubDate>Wed, 17 Apr 2024 00:00:00 +0900</pubDate>
            <link>http://localhost:4000/Time-LLM_Time_Series_Forecasting_by_Reprogramming_Large_Language_Models.html</link>
            <guid isPermaLink="true">http://localhost:4000/Time-LLM_Time_Series_Forecasting_by_Reprogramming_Large_Language_Models.html</guid>
            
            <category>reviews</category>
            
            
        </item>
        
        <item>
            <title>[CVPR 2022] Text to Image generation with Semantic-Spatial Aware GAN</title>
            <description>&lt;h2 id=&quot;information&quot;&gt;&lt;strong&gt;Information&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Authors&lt;/strong&gt;: Wentong Liao , Kai Hu , Michael Ying Yang , Bodo Rosenhahn
    &lt;ul&gt;
      &lt;li&gt;Affiliation: TNT, Leibniz University Hannover, Germany, SUG, University of Twente, The Netherlands&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Topics&lt;/strong&gt;: GAN, Text-to-image synthesis(T2I)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://openaccess.thecvf.com/content/CVPR2022/papers/Liao_Text_to_Image_Generation_With_Semantic-Spatial_Aware_GAN_CVPR_2022_paper.pdf&quot;&gt;Paper links&lt;/a&gt;
    &lt;h2 id=&quot;1-introduction&quot;&gt;&lt;strong&gt;1. Introduction&lt;/strong&gt;&lt;/h2&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;problem-definition&quot;&gt;Problem definition&lt;/h3&gt;
    &lt;p&gt;Text-to-image synthesis (T2I)는 text 설명과 의미적으로 일치하는 실제 이미지를 생성하는 것을 목표로 하는 task이다. 해당 task는 text가 사람에게 시각적 장면을 묘사하는데 가장 간편하고 자연스러운 매체라는 점에서 각광 받고 있다. 그러나 해당 방법론은 text에서 image 생성이라는 cross-modal problem이며 주어진 text에 대해서 전반적으로 일치하면서도 지역적으로도 일치하는 이미지를 생성해야 한다는 점에서 도전적인 task이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;limitations&quot;&gt;Limitations&lt;/h3&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1711.10485&quot;&gt;AttnGAN&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;AttnGAN은 전체 문장 벡터를 이용하여 초기 단계에서 저해상도 이미지를 생성하고, 이후 단계에서는 Attention mechanism을 통해  각 sub-region별로 가장 관련 있는 단어에 초점을 맞추어 이미지를 세밀하게 정제하는 모델이다. 더불어, Deep Attentional Multimodal Similarity Model (DAMSM)을 통해 생성된 이미지와 텍스트 사이의 유사성을 기반으로 loss를 제공하여 생성자의 학습을 돕는다. AttnGAN은 기존 GAN 모델들을 뛰어넘는 성능을 보여, 이후 대다수의 T2I에서 기본이 되는 모델이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Multi-stage refinement frameworks:&lt;/strong&gt; 생성 과정이 여러 단계로 이루어지는 multi-stage refinement framework를 사용하기 때문에, 복수의 생성자와 판별자가 필요하다. 이로 인해 계산 복잡도가 증가한다. 더불어  초기 생성자가 생성한 이미지의 품질에 크게 의존하게 된다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Sub-region을 잘 생성하지 못함:&lt;/strong&gt; text는 객체나 장면의 일부만 묘사 (ex. white crown; 어느 위치에 왕관을 어떻게 묘사할지 알 수 없음) 하기 때문에 이미지의 sub-region을 잘 생성하지 못하고 공간적 정보를 반영하지 못한다. AttGAN의 word level attention 방법론은 높은 계산 복잡도를 가져올 뿐만 아니라 sub-region을 세부적으로  표현하기에는 한계가 있다. 그래서 &lt;a href=&quot;https://cocodataset.org/#home&quot;&gt;COCO 데이터&lt;/a&gt; 같은 다수의 객체가 있는 복잡한 데이터 셋에서 좋은 성능을 보이지 못한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h3 id=&quot;contribution&quot;&gt;Contribution&lt;/h3&gt;

    &lt;p&gt;위와 같은 문제를 해결하기 위해서 본 논문에서는 새로운 frame work인 Semantic-Spatial Generative Adversarial Network (SSA-GAN)을 제안하고자 한다.&lt;/p&gt;
    &lt;ol&gt;
      &lt;li&gt;
        &lt;p&gt;하나의 생성자와 판별자만 있는 one-stage framework SSA-GAN으로 낮은 계산 복잡도와 안정된 학습 과정을 보인다.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;생성 과정에서 word 단위가 아닌 sentence embedding만 활용하기 때문에 구조가 단순하여 계산 비용을 낮춘다.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;새로운 SSA block을 제안; 해당 block은 text 정보를 기반으로 각 픽셀이 text 설명에 어떻게  부합해야 하는 지 (아핀 변환 등)를 정의하는 Semantic mask를 생성해낸다. 더불어 이 mask predictor는 weakly-supervised way로 학습이 진행되어 추가적인 annotation이 필요하지 않는다.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-method&quot;&gt;&lt;strong&gt;2. Method&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Text_to_Image_Generation_with_Semantic_Spatial_Aware_GAN/figure1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;!-- ![image 1](https://ifh.cc/g/WMs8PX.png) --&gt;

&lt;p&gt;SSA-GAN 구조는 위 그림 1에서 볼 수 있듯이  text encoder, text-image fusion을 위해 7개의 SSA block 으로 이뤄진 생성자,  생성된 이미지가 주어진 text와 일치하는 지 구별하는 구별자로 이뤄져 있다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;1. Text encoder&lt;/em&gt; 
본 연구에서는 text와 image 사이의 유사도를  기반으로 계산되는 DAMSM loss로 pretrain된 Bidirectional LSTM text encoder를 사용했다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;텍스트 벡터 인코딩&lt;/strong&gt;: 주어진 text는 텍스트 벡터 $ē ∈ \mathbb{R}^{256}$로 인코딩된다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;단어 벡터 인코딩&lt;/strong&gt;:  문장의 길이가 18이라면, 단어 벡터들은 $e ∈\mathbb{R}^{256×18}$로 인코딩된다. 이때 $e$의 $i$번째 열인 $e_ i$는 $i$번째 단어의 특성 벡터를 나타낸다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;2. Semantic-Spatial Aware block&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Text_to_Image_Generation_with_Semantic_Spatial_Aware_GAN/figure2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;!-- ![image2](https://ifh.cc/g/AVyFGA.png) --&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;notation&lt;/strong&gt; 
  $f_ i$ :  feature map generated by $i$th SSA block
  $w_ i$ : width of $f_ i$
  $h_ i$:  height of $f_ i$
  $ch_ i$: number of channels of $f_i$
  $N$ : batch size&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;각 SSA block은 이중 선형 보간법을 기반으로 기존 이미지 feature map 해상도를 2배로 높이는 Upsample block
,Semantic mask predictor 그리고 Semantic-spatial condition batch normalization block으로 구성되어 있다. 또한,  text 정보에 압도되어 기존 이미지 feature 가 손상되는 것을 막기 위해 residual connection이 사용되었다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Upsample&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;1번째 SSA block에서는 노이즈 벡터 $z$를 Fully Connected (FC) layer를 기반으로 visual domain으로 사영(projection)시킨 후에 $4 \times 4 \times 512$ 로 reshape 시킨 것을 input으로 별도의 upsampling 없이 feature map $f_1$을 생성해낸다.&lt;/p&gt;

&lt;p&gt;$i$th SSA block $(i&amp;gt;1)$은 text feature vector $ē$와 이전 image feature map $f_ {i-1} \in \mathbb{R}^{ch_ {i-1} \times \frac{h_i}{2} \times \frac{w_i}{2}}$ 을 input으로 $f_ i \in \mathbb{R}^{ch_ {i} \times h_ i \times w_ i}$ 를 생성해낸다. 즉 7번의 SSA block 중 6번의 block에서 upsampling이 이뤄지고 최종 feature maps들은 $256 \times 256$ resolution을 가진다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Weakly supervised Semantic Mask Predictor&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;그림2에서 회색 점선 박스로  묘사된 구조로 upsample 된 결과물을 input으로하여  semantic mask map $m_ i \in \mathbb{R}^{h_ i \times w_ i}$를 예측한다. 각 요소들의 값 $m_ {i,(h,w)}$은 [0,1] 사이의 값을 가진다. 각 값은 $(h,w)$에서 아핀 변환이 어떻게 이뤄져야하는지 결정한다. 직관적으로 말하자면 현재의 feature map 이미지가 주어진 text에 대해서 의미론적으로 더 일치하기 위해서 어떤 부분이 더 강화 되어야하는지 설명해준다. 이 예측기는 특별한 loss나 mask annotation 없이 구별자가 주는 adversarial loss로 전체 네트워크와 함께 학습한다. 그렇기에 이는 약한 지도학습 과정이다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Semantic Condition Batch Normalization&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Batch Normalization (BN) 
Batch $x \in \mathbb{R}^{N \times C \times H \times W}$에 대해서 BN은 아래의 식을 통해 각 feature channel마다 평균을 0 , 표준 편차를 1로 정규화한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$\hat{x}_ {nchw} = \frac{x_ {nchw} - \mu_ c(x)}{\sigma_ c(x)}$&lt;/p&gt;

&lt;p&gt;$\mu_ c(x) = \frac{1}{NHW} \sum_ {{n,h,w}} x_ {nchw}$&lt;/p&gt;

&lt;p&gt;$\sigma_ c(x) = \sqrt{\frac{1}{NHW} \sum_ {{n,h,w}} (x_ {nchw} - \mu_ c)^2 + \varepsilon}$&lt;/p&gt;

&lt;p&gt;여기서 $\varepsilon$는 안정적인 학습을 위한 작은 양수 상수이다. 이후, 각 채널에 대해서 독립적으로 아핀 변환을 진행한다.&lt;/p&gt;

&lt;p&gt;$\tilde{x}_ {nchw} = \gamma_ c \hat{x}_ {nchw} + \beta_ c$&lt;/p&gt;

&lt;p&gt;여기서 $\gamma_ c$ 와 $\beta_ c$ 는 배치 내의 모든 샘플들의 모든 공간 위치에 동일하게 적용되는 학습 가능한 매개변수들이다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Conditional Batch Normalization 
기존의 BN에서 나아가 &lt;a href=&quot;https://arxiv.org/abs/1610.07629&quot;&gt;Dumoulin et al.&lt;/a&gt;은 CBN(Conditional Batch Normalization)을 제안했다. CBN은 특정 조건에 맞게 아핀변환을 할 수 있도록 modulation paramter  $\gamma$ 와 $\beta$를 학습하는 것을 말한다. CBN 공식은 아래와 같다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$\tilde{x}_ {nchw} = \gamma(\text{con})\tilde{x}_ {nchw} + \beta(\text{con})$&lt;/p&gt;

&lt;p&gt;Text 정보를 image feature와 혼합하기 위해서 modulation parameter $\gamma$ 와 $\beta$는 text vector $ē$로 부터 학습을 진행한다.&lt;/p&gt;

&lt;p&gt;$\gamma_ c = P_ {\gamma}(\hat{e})$$\quad \beta_ c = P_ {\beta}(\hat{e})$&lt;/p&gt;

&lt;p&gt;$P_ {\gamma}(\cdot)$와 $P_ {\beta}(\cdot)$ 는 $\gamma_ c$와 $\beta_ c$를 학습시키기 위한 MLP를 나타낸다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Semantic-Spatial Aware Batch Normalization&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Semantic aware BN는 각 image feature map에 대해서 공간적으로 동일하게 적용된다. Text와 일치하는 이미지를 생성하기 위해서는 우리는 변환이 text와 관련 있는 부분에서만 이뤄져야 한다. 이를 위해 앞서 mask predictor로 예측한 mask 값을 활용한다.&lt;/p&gt;

&lt;p&gt;$\tilde{x}_ {nchw} = m_ {i,(h,w)}\left(\gamma_ c(\hat{e})\hat{x}_ {nchw} + \beta_ c(\bar{e})\right)$&lt;/p&gt;

&lt;p&gt;여기서 $m_ {i,(h,w)}$는 text information을 어디에다 더 해줘야할 지 알려줄 뿐만 아니라 얼만큼 더해줘야 할지도 픽셀 수준으로 알려준다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Summary&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;요약하자면, modulation paramter  $\gamma$ 와 $\beta$는 text information을 조건으로 학습하고 예측된 mask는 아핀 변환을 통제한다. 그래서 text-image fusion이 의미론적과 공간적 정보를 모두 반영하며 이뤄진다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;3. Discriminator&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Text_to_Image_Generation_with_Semantic_Spatial_Aware_GAN/figure4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;!-- ![image4](https://ifh.cc/g/yKHox1.png) --&gt;

&lt;p&gt;본 연구에서는  구별자의 구조에서 별도의 contribution을  가져오지 않으며 &lt;a href=&quot;https://arxiv.org/abs/2008.05865&quot;&gt;DF-GAN&lt;/a&gt;에서 제시한 구별자를 채택해서 사용하였다.&lt;/p&gt;

&lt;p&gt;이해를 위해 간략하게 설명하자면 DF-GAN의 구별자는 Target-Aware Discriminator로 Matching-Aware Gradient Penalty (MA-GP) and One-Way Output 구조로 구성되어 있다.&lt;/p&gt;

&lt;p&gt;MA-GP는 생성된 이미지가 관련 텍스트와 더 잘 일치하도록 유도하는 데 사용되는 gradient penalty 방법론이다. 실제 데이터 일 때,  gradient penalty를 적용하면 실제 데이터 주변의 기울기가 줄어든다. 이는 실제 데이터 주변의 손실 함수를 평활화시켜 생성자가 실제 데이터를 더 잘 모방하도록 유도한다.  다시 말해, target 데이터에 구별자가 gradient penalty를 적용하면 생성자가 보다 빠르고 안정적으로 수렴하도록 유도한다는 것이다.  본 논문에서는 주어진 text와 일치하는 실제 데이터에  gradient penalty를 주어서 생성자가 text와 일치하는 실제 image 데이터로 더 잘 수렴할 수 있도록 하였다. 이는 후에 판별자의 목적식에서 살펴 볼 수 있다.&lt;/p&gt;

&lt;p&gt;One-Way Output은 image의 특징과 text 벡터를 결합한 뒤, 두 개의 합성곱 층을 통해 하나의 적대적 손실을 계산하는 방식이다. DF-GAN 이전 모델들에서 사용된 Two-Way Output 방식은 실제 이미지인지 생성된 이미지 인지를 판별하는 것과 동시에 text와 image 사이의 의미적 일관성을 평가한다. 이후 이 두 가지 gradient를 단순히 합산하는 방식이기 때문에 최적화가 제대로 잘 이뤄지지 않을 수 있으며 상대적으로 학습 과정이 비효율적이다. 그에 비해 one-way output은 text와 image간에 일관성 및 실제 데이터 여부에 대해서 하나의 gradient 만을 구하기 때문에 생성자의 수렴과정을 최적화하고 가속화 시킬 수 있다.&lt;/p&gt;

&lt;p&gt;MA-GP와 One-Way Output의 결합을 통해 구별자는 생성자가 실제 데이터 같으면서도 text 내용에 부합하는 이미지를 더 안정적으로 생성하도록 유도한다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;4. Objective Function&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;먼저, 판별자의 목적함수부터 살펴 보겠다.&lt;/p&gt;

&lt;!-- ![image5](https://ifh.cc/g/xlGvzw.png) --&gt;
&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Text_to_Image_Generation_with_Semantic_Spatial_Aware_GAN/figure5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;notation&lt;/strong&gt; 
  $s$ :  주어진 text description
  $\hat{s}$ :  잘못된(mismatched) text description
  $\hat{x}$: 생성된 image 
  $P_ {data}$: 실제 데이터 분포
  $P_ {G}$: 생성된 데이터 분포&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;첫 번째 항은 주어진 text와 일치하는 실제 데이터에 대한 손실, 두 번째 항은 주어진 text에 대해서 생성된 데이터에 대한 손실, 세 번째항은 잘못 매칭된 text가 주어졌을 때 실제 데이터에 대한 손실이다. 마지막으로 네 번째항은 주어진 text와 일치하는 실제 데이터 (target data)가 주어졌을 때, 앞서 언급한 MA-GP 기반의 gradient penalty를 주는 과정이다.  이때, $\lambda_ {MA}$와 $p$는 penalty 정도를 조절하는 하이퍼파라미터이다.&lt;/p&gt;

&lt;p&gt;다음은 생성자의 목적함수이다. 생성자의 총 loss는 adversarial loss와 DAMSM loss로 이뤄져있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Text_to_Image_Generation_with_Semantic_Spatial_Aware_GAN/figure6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;!-- ![image6](https://ifh.cc/g/bCzYdl.png) --&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;notation&lt;/strong&gt; 
  $\mathcal{L}_ {adv}^G$:  생성자의 적대적 손실
  $\mathcal{L}_ {DAMSM}$:  DAMSM 손실 
  $D_ i$ : 문장 (text)
  $Q_i$ :  image
  $M$ : 배치 크기&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$\mathcal{L}_ {adv}^G$는 생성자의 적대적 손실을 나타낸다.$E_ {x \sim P_ G}[\cdot]$는 생성자의 데이터 분포인$P_ G$에서 샘플링된 이미지에 대한 기대값을 의미하고, $D(\hat{x}, s)$ 는 구별자가 해당 이미지 $\hat{x}$ 와 text $s$를 받았을 때 출력하는 점수이다. 구별자가 생성된 데이터를 -1 (생성된 데이터) 에 가깝게 학습할 수 록 loss가 커진다.&lt;/p&gt;

&lt;p&gt;이러한 적대적 손실과  Deep Attentional Multimodal Similarity Model(DAMSM) 손실의 합이 생성자의 최종 손실이 된다. DAMSM이란 AttnGAN에서 반지도 학습 방식으로 학습할 수 있도록 설계된 손실함수이다.  AttGAN에서 정의한 이미지 $D_ i$가 문장 $Q_ i$와 일치하는 후방확률 (Posterior probability)은 다음과 같다.&lt;/p&gt;

&lt;p&gt;$P(D_ i \vert Q_ i) = \frac{\exp(\gamma_3 R(Q_ i, D_ i))}{\sum_{j=1}^{M} \exp(\gamma_3 R(Q_ i, D_ j))}$&lt;/p&gt;

&lt;p&gt;여기서 $R(Q_ i, D_ i)$는 코사인 유사도를 기반으로 두 모달리티의 matching score를 계산하는 식이고  $\gamma_3$는 smoothing factor로 하이퍼파라미터 변수이다. 이 배치에서, 오직 문장 $D_i$​만이 이미지 $Q_i$​와 일치하고, 나머지 M−1개의 문장들은 이미지 $Q_ i​$와 일치하지 않는 설명으로 다룬다. 후에, $P(D_ i \vert Q_ i)$를 log negative posterior probability로 변환하여 이미지가 상응하는 text와 얼마나 match되는 지를 기반으로 loss값을 구할 수 있다.&lt;/p&gt;

&lt;p&gt;DAMSM loss에서는 아래와 같이 4가지 확률을 기반으로 loss를 구해서 합을 구한다.&lt;/p&gt;

&lt;p&gt;(1) 단어가 주어졌을 때 상응하는 image segment와 match 될 확률
(2) image segment가 주어졌을 때 상응하는 단어와 match 될 확률
(3) 문장이 주어졌을 때 상응하는 image와 match 될 확률
(4) image가 주어졌을 때 상응하는 문장과 match 될 확률&lt;/p&gt;

&lt;p&gt;다시 말해, 	$\mathcal{L}_ {DAMSM}$는 단어 수준의 fine-grained image-text matching loss 이며 $\mathcal{L}_ {DA}$는 DAMSM loss의 가중치이다.&lt;/p&gt;

&lt;h2 id=&quot;3-experiment&quot;&gt;&lt;strong&gt;3. Experiment&lt;/strong&gt;&lt;/h2&gt;

&lt;h3 id=&quot;experiment-setup&quot;&gt;&lt;strong&gt;Experiment setup&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Dataset
    &lt;ul&gt;
      &lt;li&gt;
        &lt;dl&gt;
          &lt;dt&gt;COCO (image 당 5개의 text description)&lt;/dt&gt;
          &lt;dd&gt;많은 객체를 가지고 있는 이미지 데이터 셋&lt;/dd&gt;
        &lt;/dl&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;dl&gt;
          &lt;dt&gt;CUB bird (image 당 10개의 text description)&lt;/dt&gt;
          &lt;dd&gt;많은 상세한 특성을 담고 있는 이미지 데이터 셋&lt;/dd&gt;
        &lt;/dl&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Evaluation Metric
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;Inception score (IS) ($\uparrow$)
  생성된 이미지의 클래스 조건부 분포와 전체 클레스 분포 사이의 KL-divergence 점수&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;Frechet Inception Distance (FID) ($\downarrow$)
  생성된 이미지의 feature 분포와 실제 이미지의 feature 분포 사이의 distance&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;R-precision  ($\uparrow$)
  Cosine distance를 기반으로 구하는 image와 text 사이의 의미적 일관성&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Baseline 
StackGAN++, AttnGAN,Control-GAN, SD-GAN, DM-GAN,DF-GAN,DAE-GAN&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;result&quot;&gt;&lt;strong&gt;Result&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Quantitative Results&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Text_to_Image_Generation_with_Semantic_Spatial_Aware_GAN/figure7.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;!-- ![image 6](https://ifh.cc/g/B3a1nh.png) --&gt;&lt;/p&gt;

&lt;p&gt;IS score가 높을 수록 생성된 이미지의 품질이 높고 text -image 사이의 일관성이 높다는 것을 의미한다. FID score의 경우 낮을 수록 실제 이미지와 유사하다는 것을 의미한다. R precision은 높을 수록 text와 의미적 유관성이 높음을 알 수 있다.&lt;/p&gt;

&lt;p&gt;IS score의 경우, 기존의 SOTA 였던 DF-GAN에 비해 더 좋은 성능을 보이며 SOTA를 달성했다. FID score는 CUB bird set에서 최고 성능을 달성하지는 못했지만 기존의 다른 모델에 비해 낮은 값을 보이고 있다. R score 역시 DAE-GAN을 제외한 다른 모델들보다 좋은 성능을 보이고 있다. 요약하자면, 세부 사항이 많은 이미지 뿐만 아니라 다양한 객체를 가지고 있는 이미지 모두에서 SSA-GAN은 우수한 성능을 보인다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Qualitative Results&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;T2I task에서 최근 SOTA 모델이었던 DM-GAN, DF-GAN 그리고 DAE과 SSA-GAN의 생성 이미지를 질적으로 비교해보았다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Text_to_Image_Generation_with_Semantic_Spatial_Aware_GAN/figure8.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;!-- ![image8](https://ifh.cc/g/Cz7RQX.jpg) --&gt;

&lt;p&gt;전반적으로 SSA-GAN이 깔끔한 배경과 함께 text description과 일치하는 이미지를 생성해냄을 볼 수 있다. 예를 들어 CUB bird dataset을 다룬 첫 번째 열의 ‘회색 crown과 가슴 그리고 오렌지 색 부리를 가진 작은 새’ 라는 text 에 대해서 DM-GAN은 작은 새라는 특성을 반영하지 못 했고, DF-GAN은 회색 crown과 가슴이라는 특성을 반영하지 못했다. 그에 비해 SSA-GAN은 text와 높은 일관성을 보인다. 다양한 객체를 가진 COCO dataset을 다룬 6번째 열에서 SSA-GAN을 통해 생성된 이미지 속 소들이 상대적으로 뭉쳐있는 DF-GAN 속 소들보다 더 잘 인식되어진다. 요약하자면 다양한 text에서  SSA-GAN이 가장 의미적으로 일관된 이미지를 생성해냄을 볼 수 있다.&lt;/p&gt;

&lt;h3 id=&quot;ablation-study&quot;&gt;&lt;strong&gt;Ablation study&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;(1) SSA Block and DAMSM&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Text_to_Image_Generation_with_Semantic_Spatial_Aware_GAN/figure9.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;!-- ![Image9](https://ifh.cc/g/x3741m.png) --&gt;
&lt;p&gt;SSA block과 DAMSM의 성능을 확인하기 위해서 ablation study를 진행했다. 기존의 SSA block 대신 DF-GAN의 UPB block (Upsampling 기반의 block)을 사용하여 결과를 비교하였다. 그 결과, IS score는 fine-tuning 한 DAMSM loss와 SSA block을 활용하였을 때, 가장 높은 성능을 보였고 FID score는 fine-tuning 하지 않은 DAMSM loss와 SSA block을 사용하였을 때 가장 높은 성능을 보였다. 이는 fine-tuning을 통해서 생성된 이미지의 다양성이 커지면서 KL divergence가 커지면서 오히려 지표상 더 낮은 성능을 달성한 것으로 추측할 수 있다. 주목할 점은 본 연구에서 제안한 SSA block을 사용하지 않은 ID0보다 사용한 ID1에서 두 지표 모두에서 우수한 성능을 보였다는 점이다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(2) Semantic Mask&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Text_to_Image_Generation_with_Semantic_Spatial_Aware_GAN/figure9_.png&quot; alt=&quot;&quot; /&gt;
&lt;!-- ![image9](https://ifh.cc/g/ld40G1.png) --&gt;&lt;/p&gt;

&lt;p&gt;한 SSA block마다 하나의 mask를 추가하고 성능을 비교하는 방식으로 mask의 성능을 증명했다. mask를 7개 추가 하였을 때, (7개의 SSA-block)이 있을 때, IS score에서 가장 높은 성능을 보였고 FID score에서 두 번째로 높은 성능을 보였다. IS score 점수를 통해 mask가 더 의미론적으로 일치할 수 있도록 image를 생성하게 유도함을 알 수 있고,  FID score를 통해 더 많은 text 정보를 담을 수록 생성된 이미지의 다양성이 높아져 오히려 점수가 높아짐을 확인할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;4-conclusion&quot;&gt;&lt;strong&gt;4. Conclusion&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;본 논문에서는 T2I를 위한새로운 framework인 SSA-GAN을 제안했다. 해당 모델은 하나의 생성자와 판별자를 사용하며 end-to-end 방식으로 학습을 진행한다. 모델의 핵심인 SSA block은 현재 생성된 이미지를 기반으로 mask를 예측하고 text vector로부터 affine parameter를 학습하면서  semantic spatial condition batch normalization을 진행한다. 이는 text- image fusion 과정을 더 깊게 가능하게 하며 두 modality 사이의 일관성을 높인다. 실험을 통해서 T2I task에서 해당 모델이 SOTA임을 증명했다.&lt;/p&gt;

&lt;h2 id=&quot;5-reference--additional-materials&quot;&gt;&lt;strong&gt;5. Reference &amp;amp; Additional materials&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/wtliao/text2image&quot;&gt;Github&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Reference
    &lt;ul&gt;
      &lt;li&gt;AttGAN&lt;/li&gt;
      &lt;li&gt;StackGAN&lt;/li&gt;
      &lt;li&gt;DF-GAN&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

</description>
            <pubDate>Wed, 17 Apr 2024 00:00:00 +0900</pubDate>
            <link>http://localhost:4000/Text_to_Image_Generation_with_Semantic-Spatial_Aware_GAN.html</link>
            <guid isPermaLink="true">http://localhost:4000/Text_to_Image_Generation_with_Semantic-Spatial_Aware_GAN.html</guid>
            
            <category>reviews</category>
            
            
        </item>
        
        <item>
            <title>[NeurIPS 2022] Simulation-guided Beam search for Neural Combinatorial Optimization</title>
            <description>&lt;h1 id=&quot;simulation-guided-beam-search-for-neural-combinatorial-optimization&quot;&gt;Simulation-guided Beam Search for Neural Combinatorial Optimization&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;h2 id=&quot;0-abstract&quot;&gt;&lt;strong&gt;0. Abstract&lt;/strong&gt;&lt;/h2&gt;
&lt;/blockquote&gt;

&lt;p&gt;최근, 강화학습을 활용하여 순차적 의사결정을 통해 solution을 구성해가는 방법(policy)을 학습하여 Combinatorial Optimization 문제를 푸는 deep learning 기반 방법론들이 많이 제안되고 있다.  neural net을 통한 방식은 한번의 실행으로(greedy select) 빠른 시간 안에 high-quality solution을 얻을 수 있지만, 이러한 방식은 솔루션 도출을 위해 사용 가능한 시간(초단위의 빠른 솔루션 도출이 아닌 더 긴 시간을 활용할 수 있는 경우)을 충분히 활용하지 못하는 아쉬움이 있다. &lt;strong&gt;본 논문은 사용 가능한 시간을 충분히 활용하기 위해 강력한 solution search 절차를 제공하고자 하는 목표로 simulation-guided beam search(SGBS)를 제안한다.&lt;/strong&gt; SGBS는 solution을 형성해가는 decoding 단계에서 solution 후보에 대해 학습된 neural net policy를 활용하여 평가하고, 이를 활용하여 더 좋은 solution을 찾아가고자 한다. 또한 SGBS와 Efficient active search(EAS)를 결합한 SGBS+EAS를 제안함으로써 solution의 quality를 더욱 향상시키고자 한다. TSP, CVRP, FFSP task 대한 실험을 통해, 본 논문의 방법이 합리적인 시간안에 솔루션을 크게 향상시키고 optimal에 근사한 값을 도출하는 것을 확인할 수 있다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;h2 id=&quot;1-problem-definition--motivation&quot;&gt;&lt;strong&gt;1. Problem definition &amp;amp; Motivation&lt;/strong&gt;&lt;/h2&gt;
&lt;/blockquote&gt;

&lt;p&gt;Neural Network을 통해 combinatorial optimization(CO) 문제를 푸는 연구는 최근 많은 관심을 받고 있다. 강화학습을 통해, 전문가의 지식 혹은 정답에 대한 label 없이도 high-quality solution을 제공할 수 있도록 Deep neural net을 학습시킬 수 있기 때문이다.&lt;/p&gt;

&lt;p&gt;CO문제의 solution은 neural network에 의해 계산된 policy(action에 대한 probability distribution)를 따라 순차적 의사결정에 의해 step-by-step manner [Construction method]로 이뤄진다. 이전의 대부분 연구들은 효과적인 policy를 도출하기 위한 neural network 설계, 즉 construction method 연구에 집중해온 반면, construction method에 의해 decoding 단계에서 도출되는 solution을 더욱 향상시키고자, test 단계에서 solution의 quality를 개선하는 방법론인 inference method에 대한 연구는 거의 이뤄지지 않았다. 기존에 MCTS(Monte-Carlo tree search), Beam search와 같은 전통적인 search 전략들이 활용되어 왔지만, 너무 많은 시간이 요구되거나, 효과적이지 못한 문제점들이 존재했다. 따라서 본 논문에서는 기존 방법론들의 단점들은 보완하면서 장점은 살리고자 MCTS와 beam search를 결합한 simulation-guided beam search(SGBS) 전략을 제안한다. 또한 SGBS의 performance를 더욱 향상시키고자 efficient active search(EAS)와 결합된 SGBS+EAS도 함께 제안한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;논문을 이해하기 위해선 EAS와 관련된 사전지식 필요하여, EAS 관련 내용은 이후  method 부분에서 기술함&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;h2 id=&quot;2-method&quot;&gt;&lt;strong&gt;2. Method&lt;/strong&gt;&lt;/h2&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;2.1 Preliminaries&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;N개의 decision variable $a_ {0},a_ {1},…,a_ {n-1}$을 가진 일반적인 CO 문제를 가정한다. CO문제를 풀 때, 우리의 목표는 objective function을 최대화/최소화하는 솔루션(= decision variable의 sequence)을 찾는 것이다. 이를 위해 θ에 의해 매겨변수화된 $π_ {θ}(a_ {d} \vert s_ {d})$를 만들고, 이를 이용해 한 번에 하나씩 solution을 생성해 간다. (매 step마다 decision variable 중 하나를 선택해가며 solution을 구성해감) 예를 들어, Partially completed(d&amp;lt;N)된 solution $s_ {d}$에 대해, 신경망에 의해 도출된 확률분포 $π_ {θ}(a_ {d} \vert s_ {d})$ 에 따라 $a_ {d}$를 선택하여 솔루션을 생성해가는 방식이다. 빈 solution tuple($s_ {0}=()$)로부터 시작하여, 위와 같은 절차를 complete solution($s_ {N}=(a_ {0}, a_ {1},…, a_ {N-1})$)이 생성될 때까지 반복한다.&lt;/p&gt;

&lt;p&gt;Policy-based reinforcement learning 에서 $π, s_ {d}, a_ {d}, R(s_ {N})$은 각각 policy, state, action, reward에 해당한다. 이때 reward(objective value)는 complete solution이 생성되어야 지만 알 수 있다. Policy neural net( $π_ {θ}$)의 파라미터 θ는 expected reward를 증가/감소시키고자 강화학습을 통해 점진적으로 업데이트 되어진다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Simulation-guided_Beam_Search_for_Neural_Combinatorial_Optimization/image.png&quot; alt=&quot;&quot; /&gt;
&lt;!-- ![image.png](https://i.postimg.cc/rpwsWXJ2/image.png) --&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;P&lt;/em&gt;는 target problem instances의 distiribution을 의미하며, &lt;em&gt;R’&lt;/em&gt;은 &lt;em&gt;P&lt;/em&gt;에서 sampling된 instance를 의미한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;2.2 SGBS Algorithm and its three phases&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;CO 문제는 depth가 d인 노드가 partially completed solution인 $s_ {d}$를 나타내고 이 노드에서의 branch는 $s_ {d}$상태에서 d+1번째 decision variable을 표현하는 decision tree에서의 search problem으로 볼 수 있다. 즉 문제의 목적은 objective function을 최대화/최소화하는 root node부터 leaf node까지의 path(decision variable의 sequence를 표현)를 찾는 문제로 볼 수 있다.&lt;/p&gt;

&lt;p&gt;SGBS 알고리즘은 solution을 만들어가는 decoding 과정에서 root node로부터 시작해서(빈 solution tuple) 매 step마다 solution 후보를 탐색하고, 탐색 결과를 바탕으로 tree의 depth를 확장해 간다(=순차적 의사결정을 통해 솔루션을 구성해 감).&lt;/p&gt;

&lt;p&gt;각 depth level에서 SGBS는 3가지 동작을 수행한다: &lt;strong&gt;&lt;em&gt;1) expansion, 2) simulation, 3) pruning&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Simulation-guided_Beam_Search_for_Neural_Combinatorial_Optimization/image (1).png&quot; alt=&quot;&quot; /&gt;
&lt;!-- ![image1.png](https://i.postimg.cc/dVHTtjMB/image.png) --&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2.2.1 Expansion(Pre-Pruning)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Simulation-guided_Beam_Search_for_Neural_Combinatorial_Optimization/image (2).png&quot; alt=&quot;&quot; /&gt;
&lt;!-- ![image2.png](https://i.postimg.cc/sDwgzf7d/image.png) --&gt;&lt;/p&gt;

&lt;p&gt;Expansion factor Γ가 주어지면, &lt;strong&gt;각 beam에 포함된 노드(파랑색 노드) $s_ {d}$로부터 가능한 모든 자식 노드 중 가장 큰  $π_ {θ}(. \vert s_ {d})$를 가진(다음 의사결정으로 선택될 확률이 높은) 상위 Γ개의 자식노드가 선택된다.&lt;/strong&gt; 따라서 각 depth마다 expansion 단계에서 Β * Γ 개의 자식노드가 선택되고 나머지는 node는 search tree로부터 가지치기 된다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2.2.2 Simulation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Simulation-guided_Beam_Search_for_Neural_Combinatorial_Optimization/image (3).png&quot; alt=&quot;&quot; /&gt;
&lt;!-- ![image3.png](https://i.postimg.cc/qMs6fdM2/image.png) --&gt;&lt;/p&gt;

&lt;p&gt;Expansion 단계에서 선택된  Β * Γ 개의 child node에 &lt;strong&gt;greedy rollout&lt;/strong&gt; (policy에 따라 greedy action selection을 통해 solution을 끝까지 만들어 보는 것) 을 진행한다. Greedy rollout을 통해 각 child node의 complete solution을 구하고, 이때의 reward를 계산한다. 이를 통해 child node의 objective value를 알 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2.2.3 Pruning&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Simulation-guided_Beam_Search_for_Neural_Combinatorial_Optimization/image (4).png&quot; alt=&quot;&quot; /&gt;
&lt;!-- ![image4.png](https://i.postimg.cc/PxbbSvc8/image.png) --&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Β * Γ 개의 child node중 reward 기준 상위 Β개의 reward를 가진 node가 다음 step의 Beam node가 된다.&lt;/strong&gt; 선택되지 못한 나머지 child node는 가지치기 돼서 앞으로 고려되지 않는다.&lt;/p&gt;

&lt;p&gt;위와 같은 3가지 과정을 모든 path가 completed solution을 구성할 때까지 반복하고, 최종적으로 생성된 여러 path중, best solution이 최종 solution으로 선택된다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;2.3 SGBS + EAS&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;SGBS의 search 과정동안 neural net의 파라미터는 바뀌지 않으므로(같은 state에 대해 policy distribution이 변하지 않으므로) 고정된 Β, Γ가정하에 고정된 결과만을 보여준다. (greedy action을 한다는 가정하에, 여러 번 SGBS를 실행할 필요X) 이에 주어진 시간을 충분히 활용하면서 SGBS의 performance를 더욱 향상시키고자 EAS와 결합된 &lt;strong&gt;SGBS+EAS&lt;/strong&gt;를 제안한다.&lt;/p&gt;

&lt;hr /&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Efficient Active Search(EAS)&lt;/strong&gt; : &lt;a href=&quot;https://arxiv.org/abs/2106.05126&quot;&gt;Hottung et al., 2022&lt;/a&gt;에서 제시된 Efficient Active Search(EAS)는 기존에 제시되었던 inference method인 active search를 효과적으로 개선한 버전이다.  &lt;a href=&quot;https://arxiv.org/abs/1611.09940&quot;&gt;Bello et al., 2017&lt;/a&gt;에서 제시한 Active search는 강화학습을 통해 test시 single instance에 대해 추론 성능을 높이고자 학습된 모델의 파라미터를 조정하는 fine-tuning 과정을 거치는데, 각 test instance마다 모델의 모든 가중치를 조정하므로 시간과 메무리가 너무 많이 요구되고, 효과적이지 못한 문제가 있었다. EAS는 모델의 모든 parameter를 조정하는 대신, search 과정동안 모델 파라미터 일부 subset만 update하는 전략을 제시했다.&lt;/p&gt;

  &lt;p&gt;본 논문에서는 EAS 전략 중, EAS-Lay 전략을 사용했다. EAS-Lay란 훈련된 모델에 추가적인 layer(논문에선 residual layer라고 지칭)를 삽입하는 방식으로, inference 과정 중 새로 삽입된 layer의 파라미터만 업데이트 되고, 기존 모델의 가중치는 고정된다. 실험을 통해 EAS -Lay전략이 inference method로 아주 효과적임이 밝혀져있다.&lt;/p&gt;

  &lt;p&gt;다음과 같은 두 식의 결합을 loss function의 gradient로써 사용해서 삽입된 layer의 파라미터를 update한다.
&lt;img src=&quot;../../images/DS503_24S/Simulation-guided_Beam_Search_for_Neural_Combinatorial_Optimization/image (5).png&quot; alt=&quot;&quot; /&gt;
&lt;!-- &gt; ![image5.png](https://i.postimg.cc/HnnGwMsT/image.png) --&gt;&lt;/p&gt;

  &lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Simulation-guided_Beam_Search_for_Neural_Combinatorial_Optimization/image (6).png&quot; alt=&quot;&quot; /&gt;
&lt;!-- &gt; ![image6.png](https://i.postimg.cc/sxJgH75q/image.png) --&gt;&lt;/p&gt;

  &lt;p&gt;(1) 수식은, 일반적인 REINFORCE with baseline algorithm에 따른 gradient로 생성된 solution들의 expected cost를 줄이는 것을 목적으로 한다. baseline으로는  &lt;a href=&quot;https://arxiv.org/abs/2010.16011&quot;&gt;Kwon et al., 2020&lt;/a&gt; 에서 제시한 POMO baseline을 사용하였다. (2) 수식은 tearcher forcing을 활용한 imitation loss gradient이다. 이를 통해 search 과정 중 찾은 best solution의 의사결정 과정을 모방하도록 학습된다. 전체적인 loss의 gradient는 다음과 같이 두 gradient의 가중합으로 정의된다.&lt;/p&gt;

  &lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Simulation-guided_Beam_Search_for_Neural_Combinatorial_Optimization/image (7).png&quot; alt=&quot;&quot; /&gt;
&lt;!-- &gt; &lt;img src=&quot;https://i.postimg.cc/q7dnFXKM/image.png&quot; alt=&quot;image7.png&quot; style=&quot;zoom:80%;&quot; /&gt; --&gt;&lt;/p&gt;

  &lt;p&gt;λ는 사용자 파라미터로, 높은 λ value를 설정한다면, 현재까지 찾은 솔루션 중, best solution을 모방하는데 focus하여 모델의 파라미터가 튜닝될 것이다.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;SGBS+EAS의 알고리즘은 다음과 같다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Simulation-guided_Beam_Search_for_Neural_Combinatorial_Optimization/image (8).png&quot; alt=&quot;&quot; /&gt;
&lt;!-- ![image8.png](https://i.postimg.cc/J7c133r5/image.png) --&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;SGBS는 광범위하고 효과적인 search procedure를 통해 현재 솔루션을 개선함으로써 EAS가 local optimal로부터 벗어날 수 있도록 돕고&lt;/strong&gt;(search를 통해 더 좋은 solution을 찾아갈 수 있으므로), 동시에 &lt;strong&gt;EAS가 모델 파라미터를 지속적으로 업데이트 함으로써 SGBS가 새로운, 이전과 겹치지 않는 searh tree를 생성할 수 있게 한다.&lt;/strong&gt; 즉 EAS를 통해 search tree의 탐색 영역을 더 pormising한 search space로 보내고, promising한 space에서의 SGBS를 통한 search procedure를 통해 solution quality를 더욱 향상시킬 수 있다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;h2 id=&quot;3-experiments&quot;&gt;&lt;strong&gt;3. Experiments&lt;/strong&gt;&lt;/h2&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;3.1 Performance of different search methods for the CVRP in three (a)-(c) scenarios&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Experiments setting&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;SGBS의 효과를 검증하고자 3가지 다른 시나리오 환경을 구상하고 실험 진행&lt;/li&gt;
          &lt;li&gt;node가 100개 존재하는 CVRP 문제를 대상으로 진행&lt;/li&gt;
          &lt;li&gt;solution을 만드는 trained model은 같은 model 사용&lt;/li&gt;
          &lt;li&gt;Greedy를 제외한 각각의 method는 1200번의 search를 수행하고 Best solution 도출&lt;/li&gt;
          &lt;li&gt;SGBS parameter: (Β=4, Γ=4)&lt;/li&gt;
          &lt;li&gt;Y축: HGS algorithm에 의해 얻어진 solution과의 gap&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Simulation-guided_Beam_Search_for_Neural_Combinatorial_Optimization/image (9).png&quot; alt=&quot;&quot; /&gt;
&lt;!-- ![image9.png](https://i.postimg.cc/RF1zR9BZ/image.png) --&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(a) 시나리오&lt;/strong&gt;: 학습한 instance와 같은 distribution에서 sampling된 instance에 대한 실험 진행. EAS를 제외하고는 SGBS가 가장 좋은 효과를 보여주었다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(b) 시나리오&lt;/strong&gt;: 학습한 instance와 다른 distribution에서 sampling된 instance에 대한 실험 진행. 즉 기존 trained model로는 좋은 성능을 내기 어려운 상황(domain shift)을 가정하였다.&lt;/p&gt;

&lt;p&gt;실험 결과, SGBS 방법이 가장 우수한 성능을 보여주었다. MCTS 방법도 유사한 성능을 보여주었지만, 계산 시간이 너무 오래 걸린다는 단점이 있다. &lt;strong&gt;이처럼 instance의 distribution이 바뀌는 경우, 기존에는 model의 re-train이 요구되었지만 SGBS를 활용한다면 짧은 시간에 매우 효과적인 성능을 보여줄 수 있다. (본 논문의 주요 contribution!)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;(C) 시나리오&lt;/strong&gt;: search 진행하기 이전에 test instance에 대해 fine-tuning 진행한 시나리오. 저자들이 말하길, 다른 method의 경우 local optimal에 빠지는 것을 확인할 수 있었지만, &lt;strong&gt;SGBS의 경우 광범위한 탐색을 통해 solution을 찾아가므로 local optima 문제로부터 비교적 자유로운 것을 확인할 수 있다.&lt;/strong&gt; 또한 이는 SGBS+EAS(fine-tuning + search procedure)가 어떻게 큰 성능 향상을 가져오는지에 대한 설명을 제공한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;3.2  TSP  &amp;amp; CVRP&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;NP-hard routing문제인 TSP, CVRP instance에 대해 다양한 instace size에 대한 실험을 진행하였다.&lt;/p&gt;

    &lt;p&gt;TSP는 n 개의 node(cities)가 주어졌을 때, 모든 노드를 정확히 한 번만 방문하는 최단거리를 구하는 문제이다. CVRP는 TSP와 유사하지만 다른 점은 각 node마다 demand가 할당되고,  vehicle에 한 번(route)에 처리 가능한 deamd 합 제한인 capacity 제약이 추가된 문제이다.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Experiments setting&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1803.08475&quot;&gt;Kool et al., 2019&lt;/a&gt;에서 제안한 instances generation 방식 사용&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;노드가 100개인 instance 10000개로 Test 진행&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;일반화 성능을 검증하고자 , Train instance와 다른 distribution에서 sampling된 노드가 100, 150, 200개인 instance 1000개씩 생성하여 Test 진행&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;Baseline algorithm:
            &lt;ul&gt;
              &lt;li&gt;TSP: Concorde, LKH3&lt;/li&gt;
              &lt;li&gt;CVRP: HGS, LKH3&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;비교 neural net기반 알고리즘: DACT, NLNS, DPDP, POMO, EAS 단독&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Our method: policy network를 학습시키기 위해  &lt;a href=&quot;https://arxiv.org/abs/2010.16011&quot;&gt;Kwon et al., 2020&lt;/a&gt; 에서 제시된 &lt;strong&gt;POMO&lt;/strong&gt; &lt;strong&gt;training algorithm&lt;/strong&gt;[construction method] 사용. POMO는 REINFORCE algorithm을 기반으로 여러 CO 문제에서 좋은 성능을 보임이 검증되었다. (본 논문의 방법론은 inference method로,  construction method를 이용한 학습된 모델 필요. POMO이외에도 어느 construction method에도 적용 가능)&lt;/p&gt;

            &lt;p&gt;한 epoch당 &lt;a href=&quot;https://arxiv.org/abs/1803.08475&quot;&gt;Kool et al., 2019&lt;/a&gt;따라 random하게 생성된 instances 100000개를 사용하여 학습. TSP task는 1900epoch의 학습 진행하였고, CVRP instance에 대해서 10000epoch 학습 진행.&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;두 가지 버전의 SGBS+EAS, EAS 실험 결과 제공: &lt;strong&gt;1)&lt;/strong&gt; 특정 시간동안만 search 진행한 결과, &lt;strong&gt;2)&lt;/strong&gt; 수렴할때까지 진행한 결과&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;SGBS parameter
            &lt;ul&gt;
              &lt;li&gt;TSP: Β=10, Γ=10&lt;/li&gt;
              &lt;li&gt;CVRP: Β=4, Γ=4&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Simulation-guided_Beam_Search_for_Neural_Combinatorial_Optimization/image (10).png&quot; alt=&quot;&quot; /&gt;
&lt;!-- ![image10.png](https://i.postimg.cc/3R8tsHGR/image.png) --&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Table1.&lt;/strong&gt;은 Concorde알고리즘 대비 알고리즘들의 성능 Gap(%)을 보여준다. TSP, CVRP 실험에서 모두 SGBS+EAS가 모든 neural net기반 method를 능가하는 성능을 보여는 것을 확인할 수 있다. 또한  SGBS, EAS를 단독으로 사용하는 것보다 결합했을 때 더욱 효과적임을 볼 수 있다. 특히 CVRP 실험에선 SGBS+EAS가 routing에 특화된 매우 효과적인 알고리즘인 LKH3를 능가하는 성능을 보이며 합리적인 시간 안에 near-optimal 한 성능을 보여줌을 확인할 수 있다.&lt;/p&gt;

&lt;p&gt;비록 SGBS+EA가 Concorde, HGS와 같은 baseline의 성능을 넘는 결과를 보여준 것은 아니지만, 주목할 점은 Concode와 HGS는 routing에 특화된 handcrafted-heurisitc 알고리즘이지만, SGBS+EAS는 여러 CO문제에 적용될 수 있는 general한 data-driven 방식이라는 점이다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;3.3 FFSP&lt;/strong&gt;&lt;/p&gt;

    &lt;p&gt;SGBS는 routing문제에 특화된게 아니라 여러 CO 문제의 어느 construction method에도 적용할 수 있는 general inference method임을 보이고자, scheduling 문제 중 하나인 Flexible Flow shp problem(FFSP)에 대한 실험을 진행하였다.&lt;/p&gt;

    &lt;p&gt;FFS란, 작업들이 여러 stage를 순차적으로 거치며 공정이 이뤄지는데, 각 stage마다 여러 machine이 존재하고 같은 stage에서는 같은 작업을 처리하더라도 machine마다 다른 처리속도를 가진 시스템을 말한다. 이때 한 machine에서는 한 번에 하나의 작업만 처리할 수 있다는 제약이 존재한다.&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Simulation-guided_Beam_Search_for_Neural_Combinatorial_Optimization/image (11).png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

    &lt;!-- ![image11.png](https://i.postimg.cc/x1CXfHqn/image.png) --&gt;

    &lt;p&gt;따라서 scheduling method는 작업을 각 stage에서 어느 머신에 할당할 것인지(machine selection), 각 머신에서 어떤 순서로 작업을 처리할 것인지(Jobs sequencing) 관한 의사결정을 수행해야 한다.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Experiments setting&lt;/strong&gt;
        &lt;ul&gt;
          &lt;li&gt;
            &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2106.11113&quot;&gt;Kwon et al., 2021&lt;/a&gt;에서 제안한 instances generation 방식 사용&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;3개의 stage, stage별 4개의 machine이 존재하는 상황&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;작업이 20, 50, 100개 존재하는 instance를 각각 1000개씩 생성해서 Test 진행&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Baseline algorithm: MIP(CPLEX solver 이용), meta-heuristic solver, Neural Net base solver(Matnet)&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;Our method: policy networ의 파라미터를 학습시키기 위해 train 단계에서 &lt;a href=&quot;https://arxiv.org/abs/2106.11113&quot;&gt;Kwon et al., 2021&lt;/a&gt;에서 제시한 &lt;strong&gt;Matnet&lt;/strong&gt; &lt;strong&gt;training algorithm&lt;/strong&gt; [construction method] 사용. Matnet은 REINFORCE알고리즘에 기반하여 관계 데이터를 효과적으로 처리하고 좋은 성능을 보임이 검증되었다.&lt;/p&gt;

            &lt;p&gt;한 epoch당 &lt;a href=&quot;https://arxiv.org/abs/2106.11113&quot;&gt;Kwon et al., 2021&lt;/a&gt;따라 random하게 생성된 instances 1000개를 사용하여 학습. 20, 50, 100개의 작업을 처리하는 모델에 대해 각각 54, 95, 120 epochs 학습 진행&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;두 가지 버전의 SGBS+EAS, EAS 실험 결과 제공: &lt;strong&gt;1)&lt;/strong&gt; 특정 시간동안만 search procedure 진행한 결과, &lt;strong&gt;2)&lt;/strong&gt; 수렴할때까지 진행한 결과&lt;/p&gt;
          &lt;/li&gt;
          &lt;li&gt;
            &lt;p&gt;SGBS parameter:  Β=5, Γ=6&lt;/p&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Simulation-guided_Beam_Search_for_Neural_Combinatorial_Optimization/image (12).png&quot; alt=&quot;&quot; /&gt;
&lt;!-- ![image12.png](https://i.postimg.cc/4Nf5Yn98/image.png) --&gt;
&lt;strong&gt;Table2.&lt;/strong&gt; 는 SGBS+EAS 대비 알고리즘들의 성능 Gap(%)을 보여준다. 실험 결과, SGBS+EAS method가 기존 방법론들을 크게 앞서는 것을 확인할 수 있다. CPLEX의 경우 합리적인 시간 내에 solution을 도출하지 못하였다.  meta-heuristic 알고리즘들 또한 매우 큰 연산시간이 걸린거에 비해 만족스러운 solution을 도출하지 못하였다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;h2 id=&quot;4-conclusion&quot;&gt;&lt;strong&gt;4. Conclusion&lt;/strong&gt;&lt;/h2&gt;
&lt;/blockquote&gt;

&lt;p&gt;CO문제를 효과적으로 풀기 위해 simulation-guided bean search(SGBS)와 EAS가 결합된 inference method 제안하였다. 기존의 construction method와 손쉽게 결합될 수 있는 장점이 있다. 실험을 통해, SGBS+EAS가 solution을 quality를 크게 높이는 것을 확인할 수 있었으며, 전문가, 도메인 지식 없이도 state-of-the-art handcrafted heuristic method와 매우 근사한 near-optimal solution의 결과를 보여주었다.&lt;/p&gt;

&lt;p&gt;SGBS를 단독으로 사용하는 것보다 EAS와 결합하여 사용함으로써 EAS에 의해 더 promising한 search space로 tree가 보내지고, promising한 space에서의 광범위한 tree search를 통해 solution의 quality를 높이는 상호유기적인 모습이 인상깊었다.&lt;/p&gt;

&lt;p&gt;실제 환경은 학습한 instances distribution과 다른 distribution에서 sampling된 문제를 풀어야 하는 경우가 대다수일텐데, 이런 환경 속에서 모델을 매번 re-traning하는 것이 아니라 SGBS+EAS와 같은 inference method를 통해 빠른 시간 안에 fine-tuning하는 것이 실제 환경에서 매우 효과적일 것이라고 생각한다.&lt;/p&gt;

&lt;p&gt;하지만, SGBS+EAS는 inference method로 여전히 효과적인 construction method에 의해 학습된 model이 요구된다. 어느 contsruction method 알고리즘에 붙어서 작용하냐에 따라 성능의 차이가 클 것이라고 예상된다. 따라서 어느 하나가 중요한게 아니라 서로 상호보완하며 같이 연구되고 성장하는 것이 중요할 것이라고 보인다.&lt;/p&gt;

&lt;h3 id=&quot;author-information&quot;&gt;Author Information&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Author&lt;/strong&gt;: Inguk Choi
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Affiliation&lt;/strong&gt;: KAIST MSS Lab&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Research Topic&lt;/strong&gt;: Neural Combinatorial Optimization, scheduling with Reinforcement Learning&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;h2 id=&quot;5-reference&quot;&gt;5. Reference&lt;/h2&gt;
&lt;/blockquote&gt;

&lt;p&gt;[1] Choo, Jinho, et al. “Simulation-guided beam search for neural combinatorial optimization.” &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 35 (2022): 8760-8772. &lt;a href=&quot;https://arxiv.org/abs/2207.06190&quot;&gt;[LINK]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] Kwon, Yeong-Dae, et al. “Pomo: Policy optimization with multiple optima for reinforcement learning.” &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 33 (2020): 21188-21198. &lt;a href=&quot;https://proceedings.neurips.cc/paper/2020/hash/f231f2107df69eab0a3862d50018a9b2-Abstract.html&quot;&gt;[LINK]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] Kool, Wouter, Herke Van Hoof, and Max Welling. “Attention, learn to solve routing problems!.” &lt;em&gt;arXiv preprint arXiv:1803.08475&lt;/em&gt; (2018). &lt;a href=&quot;https://arxiv.org/abs/1803.08475&quot;&gt;[LINK]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[4] Kwon, Yeong-Dae, et al. “Matrix encoding networks for neural combinatorial optimization.” &lt;em&gt;Advances in Neural Information Processing Systems&lt;/em&gt; 34 (2021): 5138-5149.&lt;a href=&quot;https://arxiv.org/abs/2106.11113&quot;&gt;[LINK]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[5] Bello, Irwan, et al. “Neural combinatorial optimization with reinforcement learning.” &lt;em&gt;arXiv preprint arXiv:1611.09940&lt;/em&gt; (2016).&lt;a href=&quot;https://arxiv.org/abs/1611.09940&quot;&gt;[LINK]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[6] Hottung, André, Yeong-Dae Kwon, and Kevin Tierney. “Efficient active search for combinatorial optimization problems.” &lt;em&gt;arXiv preprint arXiv:2106.05126&lt;/em&gt; (2021).&lt;a href=&quot;https://arxiv.org/abs/2106.05126&quot;&gt;[LINK]&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;h2 id=&quot;6additional-materials&quot;&gt;&lt;strong&gt;6.Additional materials&lt;/strong&gt;&lt;/h2&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/yd-kwon/SGBS&quot;&gt;[Link for public code]&lt;/a&gt;&lt;/p&gt;

</description>
            <pubDate>Wed, 17 Apr 2024 00:00:00 +0900</pubDate>
            <link>http://localhost:4000/Simulation-guided_Beam_Search_for_Neural_Combinatorial_Optimization.html</link>
            <guid isPermaLink="true">http://localhost:4000/Simulation-guided_Beam_Search_for_Neural_Combinatorial_Optimization.html</guid>
            
            <category>reviews</category>
            
            
        </item>
        
        <item>
            <title>[ICDM 2023] Reserve Price optimization in First-Price Auctions via Multi-Task Learning</title>
            <description>&lt;h1 id=&quot;icdm-2023-reserve-price-optimization-in-first-price-auctions-via-multi-task-learning&quot;&gt;&lt;strong&gt;[ICDM 2023] Reserve Price optimization in First-Price Auctions via Multi-Task Learning&lt;/strong&gt;&lt;/h1&gt;

&lt;p&gt;Reserve Price optimization in First-Price Auctions via Multi-Task Learning&lt;/p&gt;
&lt;h2 id=&quot;glossary&quot;&gt;&lt;strong&gt;Glossary&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Publisher&lt;/strong&gt; : Advertiser가 맏긴 광고를 게재해주는 업체,출판사 (e.g., Forbes)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Advertiser&lt;/strong&gt; : Publisher에게 광고게재를 맡기는 기업,회사 (e.g., Volkswagen)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reserve price&lt;/strong&gt; : Publisher가 광고게재를 받아들일 최소 가격&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;AD impression&lt;/strong&gt; : 온라인 페이지에서의 광고 표시(One display of an AD in a page view)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;RTB&lt;/strong&gt; : Real-time-bidding&lt;/p&gt;

&lt;h2 id=&quot;1-problem-definition&quot;&gt;&lt;strong&gt;1. Problem Definition&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;     온라인 디스플레이 광고는 advertiser로부터 받은 돈을 대가로 advertiser에 대한 정보와 서비스를 제공하는 대부분의 publisher의 가장 중요한 수익원이다. 현재는 광고게재를 위해 실시간으로 advertiser가 입찰하는 RTB방식으로 광고가 판매된다. Reserve price보다 높은 가격으로 입찰한 advertiser중, 가장 높은 금액을 제시한 advertiser는 최종적으로 publisher의 웹페이지에 본인들의 광고를 게재할 수 있게된다. 기존에는 winning advertiser가 두번째로 높았던 입찰가를 지불하는 second-price auction방식이 주로 사용되어왔으나, 2019년부터는 winning advertiser가 제안한 가장 높은 입찰가를 지불하는 first-price auction방식이 사용되고 있다.&lt;/p&gt;

&lt;p&gt;     first-price auction에서는 advertiser측에서 publisher가 사전에 정해놓은 reserve price보다 훨씬 높은 금액을 제시하여 winning advertiser가 되었다면, 본인들이 제시한 금액을 전부 지불하여 광고를 게재하는 방식이다. 또한 first-price auction에서는 모든 입찰가가 reserve price보다 낮다면 모든 광고를 게재하지 않는다. 따라서 publisher는 reserve price보다 큰 입찰가중에서 highest bid (가장 높은 금액)이 본인들의 수익이 되므로 최적의 reserve price를 설정하는것이 매우 중요하다. reserve price에 따라 분류될 수 있는 입찰의 종류는 다음과 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Reserve_Price_optimization_in_First-Price_Auctions_via_Multi-Task_Learning/img1.png&quot; alt=&quot;&quot; /&gt;
&lt;!-- ![img1](https://ifh.cc/g/0zkOnf.png) --&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Underbid Impressions : &lt;strong&gt;Reserve price&lt;/strong&gt; &amp;gt; &lt;strong&gt;the higest bid&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;여러 advertiser가 제시한 모든 금액이 publisher가 설정한 reserve price보다 낮으므로 publisher는 어떤 광고도 게재하지 않으며, 수익을 창출하지 못함&lt;/li&gt;
      &lt;li&gt;publisher는 수익 창출을 위해 다음 입찰의 reserve price를 advertiser가 제시한 highest bid보다 조금낮게 하향조정할 필요가 있음&lt;/li&gt;
      &lt;li&gt;그러나 Underbid가 발생하면 publisher측은 이전 입찰에서 advertiser가 reserve price보다 낮은 금액을 제시한 사실만 알 수 있고, 정확한 제시 금액을 알 수 없음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Outbid Impressions : &lt;strong&gt;Reserve price&lt;/strong&gt; &amp;lt;= &lt;strong&gt;the higest bid&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;이 경우 가장 높게 제시된 금액이 publisher의 수익이 됨&lt;/li&gt;
      &lt;li&gt;reserve price가 너무 낮게 설정되면, advertiser는 높은 금액을 제시하지 않아 수익을 극대화하기 어려움&lt;/li&gt;
      &lt;li&gt;Outbid 상황에서 일반적으로, 높은 확률로 advertiser는 reserve price보다 조금 높은 금액을 제시하는 경향이 있음&lt;/li&gt;
      &lt;li&gt;따라서 publisher는 highest bid보다 조금 낮은 금액을 reserve price로 설정한다면 advertiser들의 경쟁을 부추겨 수익을 극대화할 수 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;     이처럼 first-price auction에서 최적의 reserve price를 설정하는것은 고려해야할 요소가 매우 많은 복잡한 문제이며 publisher의 수익에 직접적인 영향을 미치는 요소이다.&lt;/p&gt;

&lt;h2 id=&quot;2-motivation&quot;&gt;&lt;strong&gt;2. Motivation&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;first-price auction에서 reserve price를 예측하는 것을 어렵게 만드는 대표적인 이유들은 다음과 같다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;advertiser들의 highest bid를 &lt;strong&gt;직접 예측&lt;/strong&gt;하는것은 매우 risky함
    &lt;ul&gt;
      &lt;li&gt;과거 outbid의 거래 데이터를 바탕으로 advertiser의 highest bid를 바로 예측하는 ML모델을 구축하여 reserve price설정에 활용할 수 있음&lt;/li&gt;
      &lt;li&gt;그러나 광고시장 자체의 불확실성, data noise, 구축한 ML 모델의 오작동과 같은 이유들이 publisher의 수익창출에 불확실성을 제공함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Underbid가 발생하면 publisher가 reserve price를 예측하는데 highest bid를 &lt;strong&gt;직접적&lt;/strong&gt;으로 사용할 수 없음
    &lt;ul&gt;
      &lt;li&gt;Underbid에서 publisher는 설정한 reserve price를 바탕으로 underbid가 발생한 사실만 알 수 있음&lt;/li&gt;
      &lt;li&gt;advertiser가 제시한 정확한 highest bid를 알 수 없으므로, underbid의 highest bid를 직접적으로 사용하지못함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;publisher는 user에 대해 매우 제한적인 정보만을 가짐
    &lt;ul&gt;
      &lt;li&gt;user와 platform을 통해 직접적으로 소통하는 advertiser대비 publisher는 user에 대한 정보를 많이 알기 힘듦&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;3가지 어려움을 다루기 위해 해당 논문에서 제시한 solution은 다음과 같다.&lt;/p&gt;

&lt;p&gt;Solution to 1&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;highest bid $b$를 점추정을 통해 직접 구하지 않고 &lt;strong&gt;구간추정&lt;/strong&gt;을 사용해 $[b_ {L},+\infty]$을 구함&lt;/li&gt;
  &lt;li&gt;publisher의 전략을 반영할 수 있는 confidence level (1 −  $\alpha$)에 따라 highest bid의 하한인 $b_L$을 예측함&lt;/li&gt;
  &lt;li&gt;예를들어, publisher가 high risk-high return 전략을 택한다면 $\alpha$(risk level)를 늘리고, low risk-low return 전략을 택하면 $\alpha$를 줄여 $b_ {L}$을 계산&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Solution to 2&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;main task가 highest bid의 하한인 $b_ {L}$을 예측하고 auxiliary task가 설정된 reserve price가 underbid될 확률을 계산하는 &lt;strong&gt;multi-task learning&lt;/strong&gt;을 도입함&lt;/li&gt;
  &lt;li&gt;outbid 데이터만 활용해 highest bid $b$만 추정하는 선행 연구와 달리, underbid 데이터 또한 사용하여 outbid,underbid 데이터를 모두 활용함&lt;/li&gt;
  &lt;li&gt;multi-task learning을 통해 공유된 learning parameters를 바탕으로 main task의 성능또한 향상시킬 수 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Solution to 3&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;DNN을 사용하여 feature간 복잡한 관계를 파악&lt;/li&gt;
  &lt;li&gt;publisher가 가진 data를 바탕으로 user, page등 각 feature에 embedding을 적용하여  &lt;strong&gt;latent feature&lt;/strong&gt;를 학습 및 활용&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-method&quot;&gt;&lt;strong&gt;3. Method&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Reserve_Price_optimization_in_First-Price_Auctions_via_Multi-Task_Learning/img2.png&quot; alt=&quot;&quot; /&gt;
&lt;!-- ![img2](https://ifh.cc/g/ZOaQCn.png) --&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Main task를 통해, advertiser의 highest bid가 최소 (1 −  $\alpha$)%확률로 outbid될 수 있도록 하는 reserve price의 하한 $b_ {L}$를 예측함&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Reserve_Price_optimization_in_First-Price_Auctions_via_Multi-Task_Learning/img3.png&quot; alt=&quot;&quot; /&gt;
&lt;!-- ![img3](https://ifh.cc/g/AzGvmg.png) --&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;예측된 $b_ {L}$을 바탕으로 underbid될 확률(failure rate) $h$를 예측할 수 있으므로 main task와 auxiliary task는 밀접하게 관련이 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;31-a-loss-of-highest-bid-lower-bound-prediction&quot;&gt;3.1-A Loss of Highest Bid Lower Bound Prediction&lt;/h3&gt;
&lt;p&gt;     main task의 loss function을 정의하게 위해 해당 논문에서는 [1]에서 제시한 QD loss function을 활용한다. QD는 데이터의 특정 비율을 캡쳐하여 가능한 좁은 고품질의 예측 구간을 생성하는 방식으로, 전통적인 방법에 비해 직접적으로 예측 구간의 품질을 향상시킬 수 있다. 
QD loss function의 overall loss는 두가지 요소의 합으로 구성된다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. Captured Mean Prediction Interval Width($MPIW_ {capt}$)&lt;/strong&gt;
&lt;img src=&quot;../../images/DS503_24S/Reserve_Price_optimization_in_First-Price_Auctions_via_Multi-Task_Learning/img4.png&quot; alt=&quot;&quot; /&gt;
&lt;!-- ![img4](https://ifh.cc/g/oM4q4z.png) --&gt;
$MPIW_ {capt-(1)}$&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;가능한 좋은 예측 구간을 파악하기위해, ground truth를 포함하는 예측구간의 평균 너비를 측정&lt;/li&gt;
  &lt;li&gt;$k_ {i}$는 $i$번째 sample의 ground truth가 추정된 예측구간에 포함된 여부를 나타내는 Boolean&lt;/li&gt;
  &lt;li&gt;$MPIW_ {capt-(1)}$값이 높을수록 추정된 예측구간의 퀄리티가 좋음&lt;/li&gt;
  &lt;li&gt;그러나 highest bid의 upper bound($\hat{b}_ {U_ {i}}$)는 $+\infty$이므로 $MPIW_ {capt-(1)}$수식을 적절히 수정할 필요가 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Reserve_Price_optimization_in_First-Price_Auctions_via_Multi-Task_Learning/img5.png&quot; alt=&quot;&quot; /&gt;
&lt;!-- ![img5](https://ifh.cc/g/BsqtZm.png) --&gt;
$MPIW_ {capt-(2)}$&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;다음처럼 수정하여 highest bid의 하한($\hat{b}_ {L_ {i}}$)만 고려할 수 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;2. Prediction Interval Coverage Probability(PICP)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Reserve_Price_optimization_in_First-Price_Auctions_via_Multi-Task_Learning/img6.png&quot; alt=&quot;&quot; /&gt;
&lt;!-- ![img6](https://ifh.cc/g/WaDAbZ.png) --&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;전체 중 몇개의 ground truth가 정확하게 예측구간에서 capture되었는지를 나타내는 예측구간의 coverage 확률&lt;/li&gt;
  &lt;li&gt;$PICP$는 예측구간의 quality를 나타내는 매우 중요한 척도임&lt;/li&gt;
&lt;/ul&gt;

&lt;!-- - 모델에서는 $L_ {\theta} = L({\theta | k,\alpha})$ 에 대한 neagative log likelihood를 최소화 하는 ${\theta}$ 를 학습함( $\alpha$ 는 risk level) --&gt;

&lt;p&gt;&lt;strong&gt;3. QD Loss function&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;모델에서는 $L_ {\theta} = L( \theta \vert k, \alpha)$ 에 대한 negative log likelihood 를 최소화 하는 $\theta$ 를 학습함 ($\alpha$ 는 risk level)&lt;/li&gt;
  &lt;li&gt;$L_ {\theta} = \frac{n!}{c!(n-c)!}(1-\alpha)^ c\alpha^ {n-c}(c = \sum_{i=1}^{n} k_i)$&lt;/li&gt;
  &lt;li&gt;위의 이항분포 식은 Moivre-Laplace theorem에 의해 정규분포로 근사될 수 있고 negative log likelihood는 다음과 같음&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Reserve_Price_optimization_in_First-Price_Auctions_via_Multi-Task_Learning/img7.png&quot; alt=&quot;&quot; /&gt;
    &lt;!-- ![img7](https://ifh.cc/g/93oGyA.png) --&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;위의 변형들을 통해 &lt;strong&gt;예측구간의 coverage 확률과 너비를 동시에 고려한&lt;/strong&gt; highest bid의 하한을 예측을 진행할 수 있으며 최종 QD loss function은 다음과 같음&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Reserve_Price_optimization_in_First-Price_Auctions_via_Multi-Task_Learning/img8.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;$\lambda 는 PICP$의 중요도를 조절하는 parameter
    &lt;!-- ![img8](https://ifh.cc/g/g9kMtL.png) --&gt;&lt;/p&gt;

&lt;h3 id=&quot;31-b-loss-of-failure-rate-prediction&quot;&gt;3.1-B Loss of Failure Rate Prediction&lt;/h3&gt;
&lt;p&gt;     Underbid가 발생하면 advertiser가 제시한 highest bid를 알 수 없으므로 3.1-A에서 제시한 QD estimation을 적용할 수 없다. 따라서 하나의 impression을 일련의 feature를 가진 instance로 정의하고 survival analysis을 적용하여 outbid impression과 underbid impression를 모두 활용한 reserve price의 failure rate(underbid될 확률)을 계산한다. 
본 논문에서는 [2]에서 제시한 the Cox PH model을 활용한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Reserve_Price_optimization_in_First-Price_Auctions_via_Multi-Task_Learning/img9.png&quot; alt=&quot;&quot; /&gt;
&lt;!-- ![img9](https://ifh.cc/g/aDAPlT.png)  --&gt;&lt;/p&gt;

&lt;p&gt;the Cox PH model은 두가지 부분으로 구성되어있고 설명은 다음과 같다.&lt;/p&gt;

&lt;p&gt;$h_ {0}(t)$ : underlying baseline harzard function, describes how the risk of an event per time unit changes over t at baseline levels of explanatory variable&lt;/p&gt;

&lt;p&gt;$\hat{y}_ {i}$ : describes how the hazard varies in response to explanatory variables $X_i$&lt;/p&gt;

&lt;p&gt;     또한 the Cox PH model에서는 baseline harzard function $h_ {0}(t)$의 분포에대한 가정이 필요하지 않은데, 이런 특징이 복잡하고 동적인 광고 시장에서의 문제에 적용하기 용이하게 만든다. 따라서 the Cox PH model을 통해 특정 reserve price $r_i$에서 impression $X_i$가 underbid될 확률 $h(r_ {i},X_ {i})$을 계산할 수 있다.&lt;/p&gt;

&lt;p&gt;     the COX partial liklihood function을 사용해 $\theta$를 추정한다면 outbid impression과 underbid impression를 모두 활용할 수 있다. reserve price가 $r_ {i}$일때 하나의 underbid impression($A_ {i}$)에 대해 $b_ {j}(highest bid)&amp;gt;r_ {i}$를 만족하는 모든 outbid impression($A_ {j}$)을 사용하여 $h(r_ {i}, X_ {i}) - h(r_ {i}, X_ {j})$를 최대화 하는 $\theta$를 찾고자 한다. reserve price = $r_ {i}$의 partial likelihood를 통해 outbid impression과 비교하여 underbid impression의 상대적인 가치를 학습할 수 있으며 수식은 다음과 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Reserve_Price_optimization_in_First-Price_Auctions_via_Multi-Task_Learning/img10.png&quot; alt=&quot;&quot; /&gt;
&lt;!-- ![img10](https://ifh.cc/g/8c6RAs.png) --&gt;&lt;/p&gt;

&lt;p&gt;각 impression이 독립일때 underbid impression의 joint probability은 $L_ {\theta} = \Pi_ {A_ {i}\in U}L_ {i}$($U$는 underbid impression의 집합)이다.&lt;/p&gt;

&lt;p&gt;Cox모델의 loss(negative log partial likelihood)는 다음과 같다&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Reserve_Price_optimization_in_First-Price_Auctions_via_Multi-Task_Learning/img11.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;!-- ![img11](https://ifh.cc/g/j7QXM7.png) --&gt;&lt;/p&gt;

&lt;p&gt;따라서, &lt;strong&gt;multi-task learning의 최종 loss function&lt;/strong&gt;은 다음처럼 정의된다.($\mu$는 failure rate의 중요성을 조절하는 parameter)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Reserve_Price_optimization_in_First-Price_Auctions_via_Multi-Task_Learning/img12.png&quot; alt=&quot;&quot; /&gt;
&lt;!-- ![img12](https://ifh.cc/g/ox0vMg.png) --&gt;&lt;/p&gt;

&lt;h3 id=&quot;31-c-predicting-highest-bid-lower-bounds-and-failure-rates&quot;&gt;3.1-C Predicting Highest Bid Lower Bounds and Failure Rates&lt;/h3&gt;
&lt;p&gt;     ad impression의 가치가 다음 4가지 feature에 영향을 받음에 착안하여 multi-task learning의 loss function을 최소화하는 $\hat{b}_ {L_ {i}}$, $\hat{y}_ {i}$를 찾는 모델을 구축한다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1. User interest&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Main idea : 사용자가 product에 관심이 많으면, advertiser 또한 많은 투자를 할것임&lt;/li&gt;
  &lt;li&gt;user ID, state-level locations, operating systems, Internet browser types,network bandwidths, devices와 같은 publisher선에서 접근가능한 사용자의 feature들을 모델에서 사용&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;2. AD placement&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Main idea : [3]에따라, 광고가 페이지의 상단에 존재할때 더 가시성이 좋으며 높은 bid를 유도함&lt;/li&gt;
  &lt;li&gt;ad position, ad unit size 두가지 placement feature들을 모델에서 사용&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;3. Page information&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Main idea : 예를들어, 정치 기사가 실린 페이지를 읽는 user보다 전자제품 기사가 실린 페이지를 읽는 user가 쇼핑 의도가 있을 가능성이 더 높음&lt;/li&gt;
  &lt;li&gt;page URLs, channels(e.g., business,
lifestyle), sub-channels/sections, the trending status of the
page labeled by the publishers’ editors와 같은 feature 사용&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;4. Context&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Main idea : 광고의 가치는 광고를 보는 상황에서의 context에 영향을 받음&lt;/li&gt;
  &lt;li&gt;include hour of the day, referrer URLs(i.e., in which page the request for the current page originated) feautre 사용&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;     RTB 알고리즘과 광고시장의 불확실성 등을 고려했을때 latent feature와 feature간 상호작용을 파악하는것이 중요하므로 DNN architecture을 사용한다. parameter를 공유하는 multi-task learning을 적용하기위한 learning framework는 아래와 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Reserve_Price_optimization_in_First-Price_Auctions_via_Multi-Task_Learning/img13.png&quot; alt=&quot;&quot; /&gt;
&lt;!-- ![img13](https://ifh.cc/g/b6NHQV.png) --&gt;&lt;/p&gt;

&lt;p&gt;각 layer에 대한 설명은 다음과 같다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Input layer&lt;/strong&gt; : 모든 input feature들은 범주형이거나 쉽게 범주형으로 변환될 수 있으므로, one-hot encoding을 적용한다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Embedding layer &amp;amp; Concatenation layer&lt;/strong&gt; : User, page, ad placement feature들을 각 latent embedding vector로 표현하여 모델 성능 향상을 도모한다. 앞에 언급한 3개의 feature들이 ReLU함수에 매핑되어 embedding vector로 표현되고, 각 embedding vector들과 context feature를 concat한 unified vector를 생성한다. 이 과정을 통해 데이터간 비선형 관계와 상호작용을 효과적으로 학습할 수 있다. (widths of embedding = 128)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Abstraction layer&lt;/strong&gt; : Concatenation layer에서 생성된 unified vector가 3겹의 abstraction layer를 거쳐 더 dense한 표현으로 변환되도록 하여 모델의 성능을 향상시킬 수 있도록 한다. (widths of abstraction = 256,128,64)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Output layer&lt;/strong&gt; : [0,+$\infty$]의 범위를 가지는 highest bid의 하한 $\hat{b}_ {L_ {i}}$, the Cox PH model의 exponential인 $\hat{y}_ {U_ {i}}$를 예측하여 최종 loss function을 계산한다.&lt;/p&gt;

&lt;h2 id=&quot;4-experiment&quot;&gt;&lt;strong&gt;4. Experiment&lt;/strong&gt;&lt;/h2&gt;
&lt;h3 id=&quot;41-a-experiment-setup&quot;&gt;4.1-A &lt;strong&gt;Experiment setup&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Dataset&lt;/strong&gt; : first-price auctions collected on the Forbes Media’s website in early 2021&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Computer Spec&lt;/strong&gt; : Desktop with an i7 3.60Hz CPU, 32GB RAM, and an NVIDIA GeForce GTX
1060 6G GPU&lt;/li&gt;
  &lt;li&gt;Using Stochastic Gradient Descent (SGD) optimizer with a learning rate of $10^ {-3}$&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;traning batch size is 256&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$\alpha$(risk level) = 30%, $\lambda$(PICP의 중요도를 조절하는 parameter) = 10, $\mu$(failure rate의 중요도를 조절하는 parameter) = 0.1&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Evaluation Metrics&lt;/strong&gt;
    &lt;ol&gt;
      &lt;li&gt;$PICP$(Prediction Interval Coverage Probability)
        &lt;ul&gt;
          &lt;li&gt;몇개의 ground truth가 정확하게 예측구간에서 capture되었는지를 나타내는 예측구간의 coverage 확률&lt;/li&gt;
          &lt;li&gt;$PICP$는 (1-$\alpha$)% 이상이길 예상됨&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;$MORP$(Median Outbid Reserve Price)
        &lt;ul&gt;
          &lt;li&gt;모든 testing outbid impressions에 대한 예측 reserve price의 median&lt;/li&gt;
          &lt;li&gt;$MORP$는 predicted reserve price가 얼마나 높은지를 반영함&lt;/li&gt;
          &lt;li&gt;section 3에서 제시한 $MPIW$는 음수 값을 가지고 덜 직관적이므로, $MORP$를 사용&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;$CORP$(Covered Outbid Reserve Price)
        &lt;ul&gt;
          &lt;li&gt;$CORP$ ~ $PICP$ * $MORP$&lt;/li&gt;
          &lt;li&gt;$PICP$와 $MORP$를 balancing한 unifed metric&lt;/li&gt;
          &lt;li&gt;balancing을 통해 predicted reserve price와 higehst bid의 관계를 반영한 예측 reserve price의 average&lt;/li&gt;
          &lt;li&gt;$CORP$를 통해 모델이 얼마나 정확하게 reserve price를 예측하는지 판단가능&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;41-b-comparison-system&quot;&gt;4.1-B &lt;strong&gt;Comparison System&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;     일반적으로 예측구간 추정에 많이 사용되는 3가지 방법과 해당 논문의 방법론의 성능을 비교한다. 비교대상이 될 방법론은 다음과 같다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;1.MVE(Minimum Variance Estimation)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Error가 target의 실제 평균을 중심으로 정규분포를 따르며, input의 집합에 대해 target의 분산이 dependence하다고 가정&lt;/li&gt;
  &lt;li&gt;NN을 사용해 $\hat{\mu}$, $\hat{\sigma^ {2}}$를 예측하며, 최종 reserve price 하한 $\hat{b}_ {L_ {i}}$에 대한 $\alpha$(risk level)를 구함
&lt;img src=&quot;../../images/DS503_24S/Reserve_Price_optimization_in_First-Price_Auctions_via_Multi-Task_Learning/img14.png&quot; alt=&quot;&quot; /&gt;
&lt;!-- ![img14](https://ifh.cc/g/4PZ38M.png) --&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;2.Bootstrap&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;다른 parameter의 subset을 사용해 $B$개의 NN을 만든후, ensemble을 통해 collective한 decision을 내림&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Reserve_Price_optimization_in_First-Price_Auctions_via_Multi-Task_Learning/img15.png&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;../../images/DS503_24S/Reserve_Price_optimization_in_First-Price_Auctions_via_Multi-Task_Learning/img16.png&quot; alt=&quot;&quot; /&gt; &lt;img src=&quot;../../images/DS503_24S/Reserve_Price_optimization_in_First-Price_Auctions_via_Multi-Task_Learning/img17.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;!-- ![img15](https://ifh.cc/g/z24axt.png) --&gt;
&lt;!-- ![img16](https://ifh.cc/g/RyqYFF.png) --&gt;
&lt;!-- ![img17](https://ifh.cc/g/ALXhl8.png) --&gt;
&lt;ul&gt;
  &lt;li&gt;위의 예측구간을 통해 $\alpha$(risk level)를 반영한 $\hat{b}_ {L_ {i}}$를 구함&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;3.LUBE&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;$PICP$ 와 normalized $MPIW$를 고려한 method&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;예측 구간의 폭을 줄이면서 $PICP$를 늘릴 수 있는 $\hat{b}_ {L_ {i}}$를 구함&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;41-c-performance-by-method&quot;&gt;4.1-C &lt;strong&gt;Performance by method&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;QD : reserve price의 하한이 highest bid를 포함할지만을 예측&lt;/p&gt;

&lt;p&gt;QD + Cox : QD에 더불어 reserve price의 하한이 highest bid를 초과할 가능성 또한 고려함&lt;/p&gt;

&lt;!-- ![img18](https://ifh.cc/g/SoDoap.png) --&gt;
&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Reserve_Price_optimization_in_First-Price_Auctions_via_Multi-Task_Learning/img18.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;     모든 metric에 대해 해당 논문에서 제시한 QD + Cox 가 가장 성능이 좋음을 알 수 있다.&lt;/p&gt;

&lt;p&gt;     예측구간이 ground truth를 포함할 확률인 $PICP$측면에서 다른 방법론들은 risk를 고려한 최소 커버확률인 70% (1-$\alpha$)를 달성하지 못하는 경우도 있으나, QD + Cox모델은 항상 70%를 만족함과 동시에 가장 높은 $PICP$를 달성하였다.&lt;/p&gt;

&lt;p&gt;      $MORP$측면에서도 QD + Cox모델의 성능이 가장 우수함을 알 수 있다. 일반적으로, $MORP$가 높으면 reserve price가 높으므로 highest bid를 포함하지 못할 확률이 낮아질 수 있어 $PICP$가 낮다. 그러나 MVE와 bootstrap에서는 타 방법론보다 상대적으로 $MORP$가 낮음에도 불구하고 $PICP$가 낮은데, 이는 두 방법론이 RTB 광고 시장에서 사용하기 다소 적합하지 않음을 시사한다. 또한 $PICP$와 $MORP$를 동시에 사용하는 $CORP$에서도 QD + Cox모델의 성능이 가장 우수하다. 이를 통해 해당 논문에서 제시한 모델에서 예측한 reserve price의 하한이 가장 높은 확률로 advertiser의 highest bid를 포함할 수 있으며, reserve price의 하한이 가장 큰 값을 예측함으로써 publisher가 효과적으로 수익을 최대화 할 수 있게된다.&lt;/p&gt;

&lt;h3 id=&quot;41-d-performance-with-different-risk-levels&quot;&gt;4.1-D &lt;strong&gt;Performance with Different Risk levels&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;     가장 좋은 성능을 보였던 hyperparmeter의 조합인 $\lambda$ = 10, $\mu$ = 0.1을 대상으로 $\alpha$(risk level)을 바꿔가며 QD + Cox모델의 performance를 확인하였다.&lt;/p&gt;

&lt;!-- ![img19](https://ifh.cc/g/pg1DP9.png)  --&gt;
&lt;p&gt;&lt;img src=&quot;../../images/DS503_24S/Reserve_Price_optimization_in_First-Price_Auctions_via_Multi-Task_Learning/img19.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;     $\alpha$가 높아짐에 따라 $PICP$는 점점 낮아지지만 $MORP$는 높아지는 결과를 확인하여 high-risk전략을 취하면 수익을 전혀 얻지 못할 확률이 높아지나, 수익이 발생하면 많은 금액을 publisher가 벌 수 있음을 알 수 있다. 반대로 low-risk전략을 취하면 수익을 전혀 얻지 못할 확률은 낮으나, publisher는 비교적 적은 금액을 벌게된다.&lt;/p&gt;

&lt;p&gt;     (a)의 결과를 통해 $\alpha$=10%,20%로 상대적으로 낮을때 높은 수준의 coverage를 기대하나, $PICP$가 (1-$\alpha$)%이상 만족되지 않음을 알 수 있다. 이 결과는 광고시장이 가지는 불확실성, data noise때문이며 마냥 낮은 수준의 $\alpha$가 무조건적인 coverage를 보장하지 않는다는 것을 알 수 있다.따라서 publisher는 해당 문제를 인식하고 적합한 수준의 $\alpha$를 설정할 필요가 있다.&lt;/p&gt;

&lt;p&gt;     또한 $MORP$는 $\alpha$=40%일때가 가장 높지만, $PICP$까지 함께 고려한 reserve price인 $CORP$는 $\alpha$=30%일때 가장 높음을 알 수 있다. 이는 $\alpha$=40%일때, risk가 증가함에따라 coverage될 확률이 급격히 낮아져 $PICP$가 $CORP$에 큰 영향을 미쳤기 때문이다.&lt;/p&gt;

&lt;p&gt;     따라서 높은 $\alpha$에서는 underbid를 발생시킬 확률이 높으나, outbid가 발생만 하게된다면 publisher가 높은 수익을 얻을 수 있다. 반대로 낮은 $\alpha$에서 publisher가 많은 impression을 판매할 수는 있지만, advertiser의 RTB 알고리즘이 높은 bid를 제시할 필요가 없음을 빠르게 알 수 있어 지속적으로 높은 bid를 제시하지 않아 publisher가 수익을 최대화 하긴 어렵다. 따라서 적절한 risk level을 설정하는것 또한 publisher의 수익에 지대한 영향을 미침을 알 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;5-conclusion&quot;&gt;&lt;strong&gt;5. Conclusion&lt;/strong&gt;&lt;/h2&gt;
&lt;p&gt;     First-price auction에서 publisher가 ad 판매로 수익을 최대화하고 advertiser로 하여금 미래에 더 높은 가격으로 bid하게 유도하는데에 최적의 reserve price를 설정하는것이 매우 중요한 task이다. 해당 논문에서는 publisher가 설정한 risk level($\alpha$)에 따라 예측되는 advertiser의 highest bid와 underbid될 확률을 multi-task framework를 사용하여 효율적으로 예측하는 DNN 모델을 제시하였다. DNN에서 publisher의 광고판매를 통한 수익에 영향을 줄 수 있는 feature들을 고려하였으며, 해당 QD + Cox 모델이 타 방법론 대비 우수한 성능을 제시함과 동시에 publisher에게 다양한 insight를 제시하였다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;author-information&quot;&gt;&lt;strong&gt;Author Information&lt;/strong&gt;&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Achir Kalra
    &lt;ul&gt;
      &lt;li&gt;Forbes Media LLC&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Chong Wang
    &lt;ul&gt;
      &lt;li&gt;Amazon Ads&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Cristian Borcea
    &lt;ul&gt;
      &lt;li&gt;New Jersey Institute of Technology&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Yi Chen
    &lt;ul&gt;
      &lt;li&gt;New Jersey Institute of Technology&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-reference--additional-materials&quot;&gt;&lt;strong&gt;6. Reference &amp;amp; Additional materials&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;[1] T. Pearce, A. Brintrup, M. Zaki, and A. Neely, “High-quality prediction
intervals for deep learning: A distribution-free, ensembled approach,” in
ICML’18. PMLR, 2018, pp. 4075–4084.&lt;/p&gt;

&lt;p&gt;[2] D. G. Kleinbaum and M. Klein, Survival Analysis. Springer, 2010.&lt;/p&gt;

&lt;p&gt;[3] C. Wang, A. Kalra, L. Zhou, C. Borcea, and Y. Chen, “Probabilistic
models for ad viewability prediction on the web,” TKDE, vol. 29, no. 9,
pp. 2012–2025, 2017.&lt;/p&gt;

</description>
            <pubDate>Wed, 17 Apr 2024 00:00:00 +0900</pubDate>
            <link>http://localhost:4000/Reserve_Price_optimization_in_First-Price_Auctions_via_Multi-Task_Learning.html</link>
            <guid isPermaLink="true">http://localhost:4000/Reserve_Price_optimization_in_First-Price_Auctions_via_Multi-Task_Learning.html</guid>
            
            <category>reviews</category>
            
            
        </item>
        
    </channel>
</rss>
