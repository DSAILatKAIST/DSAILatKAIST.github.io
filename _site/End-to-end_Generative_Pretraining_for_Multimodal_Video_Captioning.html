<!DOCTYPE html>
<html>
<head>
    <meta name="google-site-verification" content="1tAPTdgw0-t8G2Bya463OpBtYUbj9Um93gfnsowYKLw" />
    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-CV4PTXQSTW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-CV4PTXQSTW');
</script>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="">
<meta name="keywords" content="reviews,  ">
<title>[CVPR 2022] End-to-end Generative Pretraining for Multimodal Video Captioning | Awesome Reviews</title>
<link rel="stylesheet" href="css/syntax.css">

<link rel="stylesheet" type="text/css" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
<!--<link rel="stylesheet" type="text/css" href="css/bootstrap.min.css">-->
<link rel="stylesheet" href="css/modern-business.css">
<!-- Latest compiled and minified CSS -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<link rel="stylesheet" href="css/customstyles.css">
<link rel="stylesheet" href="css/boxshadowproperties.css">
<!-- most color styles are extracted out to here -->
<link rel="stylesheet" href="css/theme-blue.css">

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="js/jquery.navgoco.min.js"></script>


<!-- Latest compiled and minified JavaScript -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
<!-- Anchor.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.2.0/anchor.min.js"></script>
<script src="js/toc.js"></script>
<script src="js/customscripts.js"></script>

<link rel="shortcut icon" href="images/favicon.ico">

<!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
<![endif]-->

<link rel="alternate" type="application/rss+xml" title="DSAILatKAIST.github.io" href="http://localhost:4000/feed.xml">


	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	TeX: {
		equationNumbers: {
		autoNumber: "AMS"
		}
	},
	tex2jax: {
		inlineMath: [ ['$', '$'] ],
		displayMath: [ ['$$', '$$'] ],
		processEscapes: true,
		}
	});
	MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
		alert("Math Processing Error: "+message[1]);
	});
	MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
		alert("Math Processing Error: "+message[1]);
	});
</script>

<script type="text/javascript" async
	src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



<script type="text/javascript" async
 src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>




    <script>
        $(document).ready(function() {
            // Initialize navgoco with default options
            $("#mysidebar").navgoco({
                caretHtml: '',
                accordion: true,
                openClass: 'active', // open
                save: false, // leave false or nav highlighting doesn't work right
                cookie: {
                    name: 'navgoco',
                    expires: false,
                    path: '/'
                },
                slide: {
                    duration: 400,
                    easing: 'swing'
                }
            });

            $("#collapseAll").click(function(e) {
                e.preventDefault();
                $("#mysidebar").navgoco('toggle', false);
            });

            $("#expandAll").click(function(e) {
                e.preventDefault();
                $("#mysidebar").navgoco('toggle', true);
            });

        });

    </script>
    <script>
        $(function () {
            $('[data-toggle="tooltip"]').tooltip()
        })
    </script>
    <script>
        $(document).ready(function() {
            $("#tg-sb-link").click(function() {
                $("#tg-sb-sidebar").toggle();
                $("#tg-sb-content").toggleClass('col-md-9');
                $("#tg-sb-content").toggleClass('col-md-12');
                $("#tg-sb-icon").toggleClass('fa-toggle-on');
                $("#tg-sb-icon").toggleClass('fa-toggle-off');
            });
        });
    </script>
    


	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({
	TeX: {
		equationNumbers: {
		autoNumber: "AMS"
		}
	},
	tex2jax: {
		inlineMath: [ ['$', '$'] ],
		displayMath: [ ['$$', '$$'] ],
		processEscapes: true,
		}
	});
	MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
		alert("Math Processing Error: "+message[1]);
	});
	MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
		alert("Math Processing Error: "+message[1]);
	});
</script>

<script type="text/javascript" async
	src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>



<script type="text/javascript" async
 src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>


</head>
<body>
<!-- Navigation -->
<nav class="navbar navbar-inverse navbar-static-top">
    <div class="container topnavlinks">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="fa fa-home fa-lg navbar-brand" href="index.html">&nbsp;<span class="projectTitle"> Awesome Reviews</span></a>
        </div>
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">
                <!-- toggle sidebar button -->
                <li><a id="tg-sb-link" href="#"><i id="tg-sb-icon" class="fa fa-toggle-on"></i> Nav</a></li>
                <!-- entries without drop-downs appear here -->




                
                
                
                <li><a href="introduction">Introduction</a></li>
                
                
                
                <li><a href="https://statistics.kaist.ac.kr/" target="_blank" rel="noopener">KAIST ISysE</a></li>
                
                
                
                <!-- entries with drop-downs appear here -->
                <!-- conditional logic to control which topnav appears for the audience defined in the configuration file.-->
                
                
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">Reviews<b class="caret"></b></a>
                    <ul class="dropdown-menu">
                        
                        
                        <li><a href="reviews_kse801_2022.html">KSE801 (2022)</a></li>
                        
                        
                        
                        <li><a href="reviews_DS503_2023.html">DS503 (2023)</a></li>
                        
                        
                        
                        <li><a href="reviews_DS535_2023.html">DS535 (2023)</a></li>
                        
                        
                        
                        <li><a href="reviews_DS503_2024.html">DS503 (2024)</a></li>
                        
                        
                    </ul>
                </li>
                
                
                
			<li>



  <a class="email" title="Submit feedback" href="#" onclick="javascript:window.location='mailto:swkim@kaist.ac.kr?subject=Question about reviews feedback&body=I have some feedback about the [CVPR 2022] End-to-end Generative Pretraining for Multimodal Video Captioning page: ' + window.location.href;"><i class="fa fa-envelope-o"></i> Feedback</a>

</li>



		
                <!--comment out this block if you want to hide search-->
                <li>
                    <!--start search-->
                    <div id="search-demo-container">
                        <input type="text" id="search-input" placeholder="search...">
                        <ul id="results-container"></ul>
                    </div>
                    <script src="js/jekyll-search.js" type="text/javascript"></script>
                    <script type="text/javascript">
                            SimpleJekyllSearch.init({
                                searchInput: document.getElementById('search-input'),
                                resultsContainer: document.getElementById('results-container'),
                                dataSource: 'search.json',
                                searchResultTemplate: '<li><a href="{url}" title="[CVPR 2022] End-to-end Generative Pretraining for Multimodal Video Captioning">{title}</a></li>',
                    noResultsText: 'No results found.',
                            limit: 10,
                            fuzzy: true,
                    })
                    </script>
                    <!--end search-->
                </li>
            </ul>
        </div>
        </div>
        <!-- /.container -->
</nav>



<!-- Page Content -->
<div class="container">
  <div id="main">
    <!-- Content Row -->
    <div class="row">
        
        
            <!-- Sidebar Column -->
            <div class="col-md-3" id="tg-sb-sidebar">
                

<ul id="mysidebar" class="nav">
  <li class="sidebarTitle"> </li>
  
  
  
  <li>
      <a title="How to contribute" href="#">How to contribute</a>
      <ul>
          
          
          
          <li><a title="How to contribute?" href="how_to_contribute.html">How to contribute?</a></li>
          
          
          
          
          
          
          <li><a title="Review template (Example)" href="template.html">Review template (Example)</a></li>
          
          
          
          
      </ul>
   </li>
     
      
  
  <li>
      <a title="Reviews" href="#">Reviews</a>
      <ul>
          
          
          
          <li><a title="KSE801 (2022F)" href="reviews_kse801_2022.html">KSE801 (2022F)</a></li>
          
          
          
          
          
          
          <li><a title="DS503 (2023S)" href="reviews_DS503_2023.html">DS503 (2023S)</a></li>
          
          
          
          
          
          
          <li><a title="DS535 (2023F)" href="reviews_DS535_2023.html">DS535 (2023F)</a></li>
          
          
          
          
      </ul>
   </li>
     
      
      
      <!-- if you aren't using the accordion, uncomment this block:
         <p class="external">
             <a href="#" id="collapseAll">Collapse All</a> | <a href="#" id="expandAll">Expand All</a>
         </p>
         -->
</ul>

<!-- this highlights the active parent class in the navgoco sidebar. this is critical so that the parent expands when you're viewing a page. This must appear below the sidebar code above. Otherwise, if placed inside customscripts.js, the script runs before the sidebar code runs and the class never gets inserted.-->
<script>$("li.active").parents('li').toggleClass("active");</script>



            </div>
            
        

        <!-- Content Column -->
        <div class="col-md-9" id="tg-sb-content">
            <script type="text/x-mathjax-config">
MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/math...">
</script>
<article class="post" itemscope itemtype="https://schema.org/BlogPosting">

    <header class="post-header">
        <h1 class="post-title" itemprop="name headline">[CVPR 2022] End-to-end Generative Pretraining for Multimodal Video Captioning</h1>
        <p class="post-meta"><time datetime="2024-04-17T00:00:00+09:00" itemprop="datePublished">Apr 17, 2024</time> /
            
            
            
            
            

        </p>


    </header>

    <div class="post-content" itemprop="articleBody">

        

        <h2 id="1-problem-definition">1. Problem Definition</h2>
<p>The paper tackles the challenge of multimodal video captioning, learning from unlabelled videos and aiming to generate accurate and coherent captions for videos.</p>

<h2 id="2-motivation">2. Motivation</h2>
<ul>
  <li>
    <p><strong>Emerging Benchmark in AI:</strong></p>

    <ul>
      <li>Multimodal video captioning is an emerging benchmark of progress in AI fields.</li>
    </ul>
  </li>
  <li>
    <p><strong>Requirements for Success:</strong></p>

    <ul>
      <li>A successful model must:
        <ul>
          <li>Understand multimodal streams of input video.</li>
          <li>Generate coherent descriptions of the content.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Major Limitations/Challenges:</strong></p>

    <ol>
      <li><strong>Lack of Decoders:</strong>
        <ul>
          <li>Recent video and language pretraining frameworks often lack the ability to generate sentences due to the absence of decoders.</li>
        </ul>
      </li>
      <li><strong>Annotated Captions:</strong>
        <ul>
          <li>Datasets often lack annotated captions in unlabelled videos.</li>
        </ul>
      </li>
    </ol>
  </li>
  <li>
    <p><strong>Proposed Solutions:</strong></p>

    <ul>
      <li><strong>Integrating Decoders:</strong>
        <ul>
          <li>Propose integrating a decoder into the framework to enable models to generate previously unseen sentences.</li>
        </ul>
      </li>
      <li><strong>Joint End-to-End Training:</strong>
        <ul>
          <li>Advocate for joint end-to-end training of the entire encoder-decoder model, unlike prior approaches that only pretrain the encoder.</li>
        </ul>
      </li>
      <li><strong>Novel Pretraining Objective:</strong>
        <ul>
          <li>Introduce a pretraining objective that requires no annotated captions, instead using utterances sampled at various times within the same video.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>Additional Improvement:</strong></p>

    <ul>
      <li><strong>Trainable Encoder from Raw Inputs:</strong>
        <ul>
          <li>Make the encoder trainable from raw pixels and words directly, contrasting with existing methods that rely on pre-extracted visual features, which limits transfer to new domains.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="3-method">3. Method</h2>
<p>The proposed framework is called Multimodal Video Generative Pretraining (MV-GPT) as illustrated in the figure below.
<img src="../../images/DS503_24S/End-to-end_Generative_Pretraining_for_Multimodal_Video_Captioning/MV-GPT_framework.png" alt="" />
<!-- ![MV-GPT Framework](images/MV-GPT%20framework.png) --></p>

<p>Recall that the objective is to pretrain a model that can effectively encode multimodal videos (visual frames and transcribed speech) as well as decode natural language sentences.</p>

<h3 id="pretraining-objectives-and-losses">Pretraining Objectives and Losses</h3>
<ul>
  <li><strong>Framework Overview:</strong>
    <ul>
      <li>Leverages unlabelled instructional video data containing video frames and associated utterances.</li>
    </ul>
  </li>
  <li><strong>Training Challenges:</strong>
    <ul>
      <li>Unlabelled videos lack captioning targets.</li>
    </ul>
  </li>
  <li><strong>Training Approach:</strong>
    <ul>
      <li><strong>Forward Generation:</strong>
        <ul>
          <li>Train the model to generate a future utterance (caption target) based on the current video context and utterances.</li>
        </ul>
      </li>
      <li><strong>Backward Generation:</strong>
        <ul>
          <li>Add an extra backward generation loss, where the model generates the current utterance based on the current video frames and a future utterance.</li>
        </ul>
      </li>
      <li><strong>Bidirectional Approach:</strong>
        <ul>
          <li>Encourages generated sentences to be temporally aligned with visual inputs.</li>
        </ul>
      </li>
      <li><strong>Loss Functions:</strong>
        <ul>
          <li>Minimize the negative log-likelihood of the true future and present utterances.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="model">Model</h3>
<ul>
  <li><strong>Model Architecture:</strong>
    <ul>
      <li><strong>Transformers:</strong>
        <ul>
          <li>Uses transformer blocks throughout the model.</li>
          <li>Trained directly from pixels and word tokens.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Encoders:</strong>
    <ul>
      <li><strong>Text Encoder:</strong>
        <ul>
          <li>Uses BERT-base architecture.</li>
        </ul>
      </li>
      <li><strong>Visual Encoder:</strong>
        <ul>
          <li>Uses ViViT architecture.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Multimodal Fusion:</strong>
    <ul>
      <li><strong>Co-Attentional Transformer:</strong>
        <ul>
          <li>Fuses text and visual streams using a co-attentional transformer.</li>
          <li>Each layer is a stack of transformer blocks specific to each stream (textual and visual).</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Output Generation:</strong>
    <ul>
      <li><strong>GPT-2 Decoder:</strong>
        <ul>
          <li>Generates output autoregressively.</li>
          <li>Conditions on multimodal video features.</li>
          <li>Uses masked language modeling and special tokens (BOS and EOS).</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Pretraining Details:</strong>
    <ul>
      <li><strong>Optimizer:</strong>
        <ul>
          <li>Pretrained end-to-end using the Adam optimizer.</li>
        </ul>
      </li>
      <li><strong>Batch Size:</strong>
        <ul>
          <li>Uses a batch size of 2048.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="4-experiment">4. Experiment</h2>
<h3 id="experiment-setup">Experiment Setup</h3>
<ul>
  <li>The authors utilize HowTo100M as the pretraining dataset and then evaluate on these four downstream captioning benchmarks through finetuning: YouCook2, Video Timeline Tags (ViTT), MSR-VTT, and ActivityNet-Captions</li>
  <li>The results are reported using the following metrics: BLEU-4 (B-4), CIDEr (C ), METEOR (M), and ROUGE-L (R-L). For ViTT, BLEU-1 (B-1) is measured instead of BLEU-4.
    <h3 id="result">Result</h3>
    <p>The comparisons of the proposed MV-GPT to existing methods on all four datasets are shown in the original paper. For simplicity, only the performance comparison on one dataset is displayed here while the rest can be found in the appendix section. It can be seen that the MV-GPT outperforms all prior works with a margin in all datasets and all metrics.</p>
  </li>
</ul>

<p><img src="../../images/DS503_24S/End-to-end_Generative_Pretraining_for_Multimodal_Video_Captioning/compare_youcook2.png" alt="" />
<!-- ![compare youcook2](images/compare%20youcook2.png) --></p>

<p>Next, the author examines the influence of several critical design choices in MV-GPT, focusing on the backbone and objective functions. This includes analyzing pretrained losses, the effects of each loss term, the impact of random weight initialization, and the benefits of end-to-end training. The tables below illustrate the contributions and superiority of the proposed methods.</p>

<p><img src="../../images/DS503_24S/End-to-end_Generative_Pretraining_for_Multimodal_Video_Captioning/pretraining loss.png" alt="" /></p>

<!-- ![pretraining loss](images/pretraining%20loss.png) -->
<ul>
  <li>Pretraining only the encoder provides small improvements compared to training from scratch for all tested losses.</li>
  <li>Pretraining both the encoder and decoder together significantly boosts performance.</li>
  <li>MV-GPT outperforms other joint pretraining techniques.</li>
</ul>

<p><img src="../../images/DS503_24S/End-to-end_Generative_Pretraining_for_Multimodal_Video_Captioning/loss components.png" alt="" />
<!-- ![loss components](images/loss%20components.png) --></p>

<ul>
  <li>The forward generation (FG) loss offers strong supervision.</li>
  <li>Using the masked language modeling loss on decoder outputs (MLM-D) instead of encoder outputs (MLM-E) slightly improves performance due to better input contextualization.</li>
  <li>Adding the backward generation (BG) loss further boosts all metrics.</li>
  <li>Using weight decay (WD) provides extra gains.</li>
</ul>

<p><img src="../../images/DS503_24S/End-to-end_Generative_Pretraining_for_Multimodal_Video_Captioning/visual encoder config.png" alt="" />
<!-- ![visual encoder](images/visual%20encoder%20config.png) --></p>

<ul>
  <li>Training a visual encoder with HowTo100M shows significant gains for both architectures due to the similarity with the YouCook2 dataset.</li>
  <li>However, ViViT sees larger improvements because its encoder is optimized for generative losses and can be jointly trained with other components due to its lower complexity.</li>
  <li>These results highlight the benefits of end-to-end pretraining.</li>
  <li>Examining the effects of end-to-end training for finetuning, for YouCook2, naive end-to-end finetuning from the start slightly degrades performance (row 4 to 5).</li>
  <li>This is overcome by initially freezing the visual encoder and starting end-to-end training after convergence, resulting in a minor gain (row 6).</li>
  <li>This suggests our pretrained visual encoder already captures strong representations for similar domains, making end-to-end finetuning less critical.</li>
  <li>However, for MSR-VTT, end-to-end finetuning is crucial due to the larger domain gap, leading to significant gains (row 7 to 8).</li>
</ul>

<p><img src="../../images/DS503_24S/End-to-end_Generative_Pretraining_for_Multimodal_Video_Captioning/weight init.png" alt="" /></p>

<!-- ![weight init](images/weight%20init.png) -->
<ul>
  <li>Testing the model’s ability to learn from scratch involved initializing it randomly or with pretrained BERT, ViViT, and GPT-2 weights.</li>
  <li>Table 4 illustrates that with random initialization, the method performs very well (row 2).</li>
  <li>Interestingly, it even outperforms the model initialized with public BERT, GPT-2, and ViViT weights (row 3).</li>
</ul>

<p>Besides, the authors also show results on non-generative video understanding tasks (including VideoQA, video retrieval and action classification) to emphasize the capability other than just being a generative model. The results can be found in the appendix section.</p>

<p>By the end, they illustrate qualitative examples on YouCook2 (first row) and MSR-VTT (last two rows) as follows.
<img src="../../images/DS503_24S/End-to-end_Generative_Pretraining_for_Multimodal_Video_Captioning/qualitative_example.png" alt="" />
<!-- ![qualitative example](images/qualitative%20example.png) --></p>

<h2 id="5-conclusion">5. Conclusion</h2>
<p>The authors introduce a new framework for multimodal video captioning called MV-GPT, where both the encoder for multimodal inputs and the decoder for generating captions are trained together using a bidirectional generative objective. This involves sampling utterances from unlabeled videos at various time points instead of caption target. The model undergoes end-to-end training during both pretraining and finetuning stages. It achieves top performance on various video captioning benchmarks and other tasks such as video understanding.
In my opinion, this seems like a cutting-edge framework that will allow us to have an automated video captioning system (still being challenging nowadays) for any platforms in the near future.</p>

<h3 id="author-information">Author Information</h3>
<ul>
  <li>Author name: Paul Hongsuck Seo
    <ul>
      <li>Affiliation: Korea University</li>
      <li>Research Topic: Multimodal Interactive Intelligence, Vision, Speech and Language Understanding</li>
    </ul>
  </li>
  <li>Author name: Arsha Nagrani
    <ul>
      <li>Affiliation: Google Inc.</li>
      <li>Research Topic: Machine learning, Computer Vision, Speech Technology, Deep Learning</li>
    </ul>
  </li>
  <li>Author name: Anurag Arnab
    <ul>
      <li>Affiliation: University of Oxford</li>
      <li>Research Topic: Computer Vision, Machine Learning, Deep Learning</li>
    </ul>
  </li>
  <li>Author name: Cordelia Schmid
    <ul>
      <li>Affiliation: Institut national de recherche en informatique et en automatique</li>
      <li>Research Topic: Computer vision, Object Recognition, Video Recognition, Learning</li>
    </ul>
  </li>
</ul>

<h2 id="6-reference--additional-materials">6. Reference &amp; Additional Materials</h2>
<ul>
  <li><a href="https://arxiv.org/abs/2201.08264">[CVPR 2022] End-to-end Generative Pretraining for Multimodal Video Captioning</a></li>
</ul>

<h2 id="7-appendix">7. Appendix</h2>
<p>The performance comparison between SOTA on other datasets.</p>

<p><img src="../../images/DS503_24S/End-to-end_Generative_Pretraining_for_Multimodal_Video_Captioning/compare_vitt.png" alt="" />
<!-- ![compare ViTT](images/compare%20vitt.png) --></p>

<p><img src="../../images/DS503_24S/End-to-end_Generative_Pretraining_for_Multimodal_Video_Captioning/compare_msr-vtt.png" alt="" />
<!-- ![compare MSR-VTT](images/compare%20msr-vtt.png) --></p>

<p><img src="../../images/DS503_24S/End-to-end_Generative_Pretraining_for_Multimodal_Video_Captioning/compare_act.png" alt="" />
<!-- ![compare Act](images/compare%20act.png) --></p>

<p>The performance on video retrieval tasks.</p>

<p><img src="../../images/DS503_24S/End-to-end_Generative_Pretraining_for_Multimodal_Video_Captioning/video retrieval.png" alt="" />
<!-- ![compare other tasks](images/video%20retrieval.png) --></p>

    </div>



</article>
<script src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.5.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>







<hr class="shaded"/>

<footer>
            <div class="row">
                <div class="col-lg-12 footer">
               &copy;2024 Copyright 2021 Google LLC. All rights reserved. <br />
 Site last generated: Jun 4, 2024 <br />
<p><img src="images/company_logo.png" alt="Company logo"/></p>
                </div>
            </div>
</footer>






        </div>
    <!-- /.row -->
</div>
<!-- /.container -->
</div>
<!-- /#main -->
    </div>

</body>

<!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-66296557-1','auto');ga('require','displayfeatures');ga('send','pageview');</script>





</html>


